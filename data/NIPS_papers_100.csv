year,title,event_type,pdf_name,abstract,paper_text
1997,Independent Component Analysis for Identification of Artifacts in Magnetoencephalographic Recordings,,1466-independent-component-analysis-for-identification-of-artifacts-in-magnetoencephalographic-recordings.pdf,Abstract Missing,"Independent Component Analysis for
identification of artifacts in
Magnetoencephalographic recordings

Ricardo Vigario 1 ; Veikko J ousmiiki2 ,
Matti Hiimiiliiinen2, Riitta Hari2, and Erkki Oja 1
1 Lab.

of Computer & Info. Science
Helsinki University of Technology
P.O. Box 2200, FIN-02015 HUT, Finland
{Ricardo.Vigario, Erkki.Oja}@hut.fi
2 Brain

Research Unit, Low Temperature Lab.
Helsinki University of Technology
P.O. Box 2200, FIN-02015 HUT, Finland
{veikko, msh, hari}@neuro.hut.fi

Abstract
We have studied the application of an independent component analysis
(ICA) approach to the identification and possible removal of artifacts
from a magnetoencephalographic (MEG) recording. This statistical technique separates components according to the kurtosis of their amplitude
distributions over time, thus distinguishing between strictly periodical
signals, and regularly and irregularly occurring signals. Many artifacts
belong to the last category. In order to assess the effectiveness of the
method, controlled artifacts were produced, which included saccadic eye
movements and blinks, increased muscular tension due to biting and the
presence of a digital watch inside the magnetically shielded room. The
results demonstrate the capability of the method to identify and clearly
isolate the produced artifacts.

1 Introduction
When using a magnetoencephalographic (MEG) record, as a research or clinical tool, the
investigator may face a problem of extracting the essential features of the neuromagnetic
? Corresponding author

R. Vigario,

230

v. Jousmiiki, M. Hiimiiliiinen, R. Hari and E. Oja

signals in the presence of artifacts. The amplitude of the disturbance may be higher than
that of the brain signals, and the artifacts may resemble pathological signals in shape. For
example, the heart's electrical activity, captured by the lowest sensors of a whole-scalp
magnetometer array, may resemble epileptic spikes and slow waves (Jousmili and Hari
1996).
The identification and eventual removal of artifacts is a common problem in electroencephalography (EEG), but has been very infrequently discussed in context to MEG (Hari
1993; Berg and Scherg 1994).
The simplest and eventually most commonly used artifact correction method is rejection,
based on discarding portions of MEG that coincide with those artifacts. Other methods
tend to restrict the subject from producing the artifacts (e.g. by asking the subject to fix the
eyes on a target to avoid eye-related artifacts, or to relax to avoid muscular artifacts). The
effectiveness of those methods can be questionable in studies of neurological patients, or
other non-co-operative subjects. In eye artifact canceling, other methods are available and
have recently been reviewed by Vigario (I 997b) whose method is close to the one presented
here, and in Jung et aI. (1998).
This paper introduces a new method to separate brain activity from artifacts, based on the
assumption that the brain activity and the artifacts are anatomically and physiologically
separate processes, and that their independence is reflected in the statistical relation between the magnetic signals generated by those processes.
The remaining of the paper will include an introduction to the independent component
analysis, with a presentation of the algorithm employed and some justification of this approach. Experimental data are used to illustrate the feasibility of the technique, followed
by a discussion on the results.

2

Independent Component Analysis

Independent component analysis is a useful extension of the principal component analysis
(PC A). It has been developed some years ago in context with blind source separation applications (Jutten and Herault 1991; Comon 1994). In PCA. the eigenvectors of the signal
covariance matrix C = E{xx T } give the directions oflargest variance on the input data
x. The principal components found by projecting x onto those perpendicular basis vectors
are uncorrelated, and their directions orthogonal.
However, standard PCA is not suited for dealing with non-Gaussian data. Several authors, from the signal processing to the artificial neural network communities, have shown
that information obtained from a second-order method such as PCA is not enough and
higher-order statistics are needed when dealing with the more demanding restriction of
independence (Jutten and Herault 1991; Comon 1994). A good tutorial on neural ICA implementations is available by Karhunen et al. (1997). The particular algorithm used in this
study was presented and derived by Hyvarinen and Oja (1997a. 1997b).

2.1

The model

In blind source separation, the original independent sources are assumed to be unknown,
and we only have access to their weighted sum. In this model, the signals recorded in an
MEG study are noted as xk(i) (i ranging from 1 to L, the number of sensors used, and
k denoting discrete time); see Fig. 1. Each xk(i) is expressed as the weighted sum of M

ICAfor Identification of Artifacts in MEG Recordings

231

independent signals Sk(j), following the vector expression:
M

Xk = La(j)sdj) = ASk,

(1)

j=l

where Xk = [xk(1), ... , xk(L)]T is an L-dimensional data vector, made up of the L mixtures at discrete time k. The sk(1), ... , sk(M) are the M zero mean independent source
signals, and A = [a(1), . .. , a(M)] is a mixing matrix independent of time whose elements
ail are th.e unknown coefficients of the mixtures. In order to perform ICA, it is necessary
to have at least as many mixtures as there are independent sources (L ~ M). When this
relation is not fully guaranteed, and the dimensionality of the problem is high enough,
we should expect the first independent components to present clearly the most strongly
independent signals, while the last components still consist of mixtures of the remaining
signals. In our study, we did expect that the artifacts, being clearly independent from the
brain activity, should come out in the first independent components. The remaining of the
brain activity (e.g. a and J-L rhythms) may need some further processing.
The mixing matrix A is a function of the geometry of the sources and the electrical conductivities of the brain, cerebrospinal fluid, skull and scalp. Although this matrix is unknown.
we assume it to be constant, or slowly changing (to preserve some local constancy).
The problem is now to estimate the independent signals Sk (j) from their mixtures, or the
equivalent problem of finding the separating matrix B that satisfies (see Eq. 1)
(2)
In our algorithm, the solution uses the statistical definition of fourth-order cumulant or
kurtosis that, for the ith source signal, is defined as

kurt(s(i)) = E{s(i)4} - 3[E{s(i)2}]2,
where E( s) denotes the mathematical expectation of s.

2.2 The algorithm
The initial step in source separation, using the method described in this article, is whitening, or sphering. This projection of the data is used to achieve the uncorrelation between
the solutions found, which is a prerequisite of statistical independence (Hyvarinen and Oja
1997a). The whitening can as well be seen to ease the separation of the independent signals (Karhunen et al. 1997). It may be accomplished by PCA projection: v = V x, with
E{ vv T } = I. The whitening matrix V is given by
-=T ,
V -- A- 1 / 2 .....

where A = diag[-\(1), ... , -\(M)] is a diagonal matrix with the eigenvalues of the data
covariance matrix E{xxT}, and 8 a matrix with the corresponding eigenvectors as its
columns.
Consider a linear combination y = w T v of a sphered data vector v, with Ilwll = 1. Then
E{y2} = .1 andkurt(y) = E{y4}-3, whose gradientwithrespecttow is 4E{v(wTv)3} .
Based on this, Hyvarinen and Oja (1997a) introduced a simple and efficient fixed-point
algorithm for computing ICA, calculated over sphered zero-mean vectors v, that is able to
find one of the rows of the separating matrix B (noted w) and so identify one independent
source at a time - the corresponding independent source can then be found using Eq. 2.
This algorithm, a gradient descent over the kurtosis, is defined for a particular k as
1. Take a random initial vector Wo of unit norm. Let l = 1.

232

R. Vigario,

v. Jousmiiki, M. Hiimiiliiinen, R. Hari and E. Oja

2. Let Wi = E{V(W[.1 v)3} - 3Wl-I. The expectation can be estimated using a
large sample OfVk vectors (say, 1,000 vectors).
3. Divide Wi by its norm (e.g. the Euclidean norm
4.

Ilwll = JLi wI J.

lflwT wi-II is not close enough to 1, let I = 1+1 andgo back to step 2.

Otherwise,

output the vector Wi.

In order to estimate more than one solution, and up to a maximum of lvI, the algorithm
may be run as many times as required. It is, nevertheless, necessary to remove the infonnation contained in the solutions already found, to estimate each time a different independent
component. This can be achieved, after the fourth step of the algorithm, by simply subtracting the estimated solution s = w T v from the unsphered data Xk . As the solution is
defined up to a multiplying constant, the subtracted vector must be multiplied by a vector
containing the regression coefficients over each vector component of Xk.

3

Methods

The MEG signals were recorded in a magnetically shielded room with a 122-channel
whole-scalp Neuromag-122 neuromagnetometer. This device collects data at 61 locations
over the scalp, using orthogonal double-loop pick-up coils that couple strongly to a local
source just underneath, thus making the measurement ""near-sighted"" (HamaHi.inen et al.
1993).
One of the authors served as the subject and was seated under the magnetometer. He kept
his head immobile during the measurement. He was asked to blink and make horizontal
saccades, in order to produce typical ocular artifacts. Moreover, to produce myographic
artifacts, the subject was asked to bite his teeth for as long as 20 seconds. Yet another
artifact was created by placing a digital watch one meter away from the helmet into the
shieded room. Finally, to produce breathing artifacts, a piece of metal was placed next
to the navel. Vertical and horizontal electro-oculograms (VEOG and HEOG) and electrocardiogram (ECG) between both wrists were recorded simultaneously with the MEG, in
order to guide and ease the identification of the independent components. The bandpassfiltered MEG (0.03-90 Hz), VEOG, HEOG, and ECG (0.1-100 Hz) signals were digitized
at 297 Hz, and further digitally low-pass filtered, with a cutoff frequency of 45 Hz and
downsampled by a factor of 2. The total length of the recording was 2 minutes. A second
set of recordings was perfonned, to assess the reproducibility of the results.
Figure 1 presents a subset of 12 spontaneous MEG signals from the frontal, temporal and
occipital areas. Due to the dimension of the data (122 magnetic signals were recorded), it
is impractical to plot all MEG signals (the complete set is available on the internet - see
reference list for the adress (Vigario 1997a?. Also both EOG channels and the electrocardiogram are presented.

4

Results

Figure 2 shows sections of9 independent components (IC's) found from the recorded data,
corresponding to a I min period, starting 1 min after the beginning of the measurements.
The first two IC's, with a broad band spectrum, are clearly due to the musclular activity
originated from the biting. Their separation into two components seems to correspond, on
the basis of the field patterns, to two different sets of muscles that were activated during
the process. IC3 and IC5 are, respectively showing the horizontal eye movements and the
eye blinks, respectively. IC4 represents cardiac artifact that is very clearly extracted. In
agreement with Jousmaki and Hari (1996), the magnetic field pattern of IC4 shows some
predominance on the left.

ICA/or Identification 0/ Artifacts in MEG Recordings

233
MEG [ 1000 fTlcm

I--

---l

saccades

I--

---l

blinking

EOG [

500 IlV

ECG [

500 IlV

I--

biting

---l MEG

~=::::::::::::::=::
~?'104~

rJ. .........

M

,J.\.......1iIIiM~..

::
t...

:;::::::;:::~=

~::::::::;::=
~ ?? "",~Jrt

..,.

t

....

~,.~ . ? .J..

.

.../\""""$""""~I

2 t

:;

:;
4

~

5 t

., ... ...., ,'fIJ'\,
..........-.

,..,d

,LIlt ... .,

I?

.,............. ................. ""

....,..,.""........ .

.... Dei ..... ""

.'''IIb'''*. rt

-1I\JY. ? ---

I p"", . . . . , . . . . . . . . . . . . at ...'....

I; rp ..

,P....

,

.,...............' tMn':M.U

... ,

, ..... '

U\..,.--II..------'-__

ooII..Jl,,-

"".'tIItS

5 ~

6 t

VEOG

It ... 11.1. HEOG

~UijuJJJ.LU Wl Uij.lJU.LllU.UUUllUUij,UU~ijJJJ

ECG

10 s

Figure 1: Samples of MEG signals, showing artifacts produced by blinking, saccades,
biting and cardiac cycle. For each of the 6 positions shown, the two orthogonal directions
of the sensors are plotted.
The breathing artifact was visible in several independent components, e.g. IC6 and IC7. It
is possible that, in each breathing the relative position and orientation of the metallic piece
with respect to the magnetometer has changed. Therefore, the breathing artifact would be
associated with more than one column of the mixing matrix A, or to a time varying mixing
vector.
To make the analysis less sensible to the breathing artifact, and to find the remaining artifacts, the data were high-pass filtered, with cutoff frequency at 1 Hz. Next, the independent
component IC8 was found. It shows clearly the artifact originated at the digital watch,
located to the right side of the magnetometer.
The last independent component shown, relating to the first minute of the measurement,
shows an independent component that is related to a sensor presenting higher RMS (root
mean squared) noise than the others.

5

Discussion

The present paper introduces a new approach to artifact identification from MEG recordings, based on the statistical technique of Independent Component Analysis. Using this
method, we were able to isolate both eye movement and eye blinking artifacts, as well as

R. Vigario,

234

v. Jousmiiki, M HtJmlJliiinen, R. Hari and E. Oja

cardiac, myographic, and respiratory artifacts.
The basic asswnption made upon the data used in the study is that of independence between brain and artifact waveforms. In most cases this independence can be verified by the
known differences in physiological origins of those signals. Nevertheless, in some eventrelated potential (ERP) studies (e.g. when using infrequent or painful stimuli), both the
cerebral and ocular signals can be similarly time-locked to the stimulus. This local time
dependence could in principle affect these particular ICA studies. However, as the independence between two signals is a measure of the similarity between their joint amplitude
distribution and the product of each signal's distribution (calculated throughout the entire
signal, and not only close to the stimulus applied), it can be expected that the very local
relation between those two signals, during stimulation, will not affect their global statistical
relation.

6

Acknowledgment

Supported by a grant from Junta Nacional de Investiga~ao Cientifica e Tecnologica, under
its 'Programa PRAXIS XXI' (R.Y.) and the Academy of Finland (R.H.).

References
Berg, P. and M. Scherg (1994). A multiple source approach to the correction of eye
artifacts. Electroenceph. clin. Neurophysiol. 90, 229-241.
Comon, P. (1994). Independent component analysis - a new concept? Signal Processing 36,287-314.
Hamalainen, M., R. Hari, R. Ilmoniemi, 1. Knuutila, and O. Y. Lounasmaa (1993, April).
Magnetoencephalography-theory, instrumentation, and applications to noninvasive
studies of the working human brain. Reviews o/Modern Physics 65(2), 413-497.
Hari, R. (1993). Magnetoencephalography as a tool of clinical neurophysiology. In
E. Niedermeyer and F. L. da Silva (Eds.), Electroencephalography. Basic principles, clinical applications, and relatedjields, pp. 1035-1061 . Baltimore: Williams
& Wilkins.
Hyvarinen, A. and E. Oja (l997a). A fast fixed-point algorithm for independent component analysis. Neural Computation (9), 1483-1492.
Hyvarinen, A. and E. Oja (1997b). One-unit learning rules for independent component
analysis. In Neural Information Processing Systems 9 (Proc. NIPS '96). MIT Press.
Jousmiiki, Y. and R. Hari (1996). Cardiac artifacts in magnetoencephalogram. Journal
o/Clinical Neurophysiology 13(2), 172-176.
Jung, T.-P., C. Hwnphries, T.-W. Lee, S. Makeig, M. J. McKeown, Y. lragui, and
T. Sejnowski (1998). Extended ica removes artifacts from electroencephalographic
recordings. In Neural Information Processing Systems 10 (Proc. NIPS '97). MIT
Press.
Jutten, C. and 1. Herault (1991). Blind separation of sources, part i: an adaptive algorithm based on neuromimetic architecture. Signal Processing 24, 1-10.
Karhunen, J., E. Oja, L. Wang, R. Vigmo, and J. Joutsensalo (1997). A class of neural
networks for independent component analysis. IEEE Trans. Neural Networks 8(3),
1-19.
Vigmo, R. (1997a). WWW adress for the MEG data:
http://nuc1eus.hut.firrvigarioINIPS97_data.html.
Vigmo, R. (1997b). Extraction of ocular artifacts from eeg using independent component analysis. To appear in Electroenceph. c/in. Neurophysiol.

ICAfor Identification ofArtifacts in MEG Recordings

235

~~~

IC1

------,--y~-------------------------.-.------~~.. ,.. ~
U

...

IC2

IC3
."",

''' ... '' .. '

<> .
).\~
.\ C:> ?
\

~~~}a

~~-""

____I4-_. _
. . . ._.---_._. . . .-.__
. """"""""""""?t;_-""'' '....
~

. . . . .-......,.....

~_1

IC4

IC5

IC6
~
...W""
....
""1011
...~""_f....
.."".,.""""'_

/tJ'IfII/'h

I' ......

d1b ..

~*W,.'tJ ......

r' .. ns...

IC7

ICB

ICg
~._-~.,.

. . . . .t . .

Wt:n:ePWt.~..,.~I'NJ'~~
I

10 s

I

Figure 2: Nine independent components found from the MEG data. For each component the
left, back and right views of the field patterns generated by these components are shown full line stands for magnetic flux coming out from the head, and dotted line the flux inwards.

"
2007,Near-Maximum Entropy Models for Binary Neural Representations of Natural Images,,3336-near-maximum-entropy-models-for-binary-neural-representations-of-natural-images.pdf,Abstract Missing,"Near-Maximum Entropy Models for Binary
Neural Representations of Natural Images

Matthias Bethge and Philipp Berens
Max Planck Institute for Biological Cybernetics
Spemannstrasse 41, 72076, T?ubingen, Germany
mbethge,berens@tuebingen.mpg.de

Abstract
Maximum entropy analysis of binary variables provides an elegant way for studying the role of pairwise correlations in neural populations. Unfortunately, these
approaches suffer from their poor scalability to high dimensions. In sensory coding, however, high-dimensional data is ubiquitous. Here, we introduce a new
approach using a near-maximum entropy model, that makes this type of analysis feasible for very high-dimensional data?the model parameters can be derived
in closed form and sampling is easy. Therefore, our NearMaxEnt approach can
serve as a tool for testing predictions from a pairwise maximum entropy model not
only for low-dimensional marginals, but also for high dimensional measurements
of more than thousand units. We demonstrate its usefulness by studying natural
images with dichotomized pixel intensities. Our results indicate that the statistics
of such higher-dimensional measurements exhibit additional structure that are not
predicted by pairwise correlations, despite the fact that pairwise correlations explain the lower-dimensional marginal statistics surprisingly well up to the limit of
dimensionality where estimation of the full joint distribution is feasible.

1

Introduction

A core issue in sensory coding is to seek out and model statistical regularities in high-dimensional
data. In particular, motivated by developments in information theory, it has been hypothesized
that modeling these regularities by means of redundancy reduction constitutes an important goal of
early visual processing [2]. Recent studies conjectured that the binary spike responses of retinal
ganglion cells may be characterized completely in terms of second-order correlations when using
a maximum entropy approach [13, 12]. In light of what we know about the statistics of the visual
input, however, this would be very surprising: Natural images are known to exhibit complex higherorder correlations which are extremely difficult to model yet being perceptually relevant. Thus, if
we assume that retinal ganglion cells do not discard the information underlying these higher-order
correlations altogether, it would be a very difficult signal processing task to remove all of those
already within the retinal network.
Oftentimes, neurons involved in early visual processing are modeled as rather simple computational
units akin to generalized linear models, where a linear filter is followed by a point-wise nonlinearity.
For such simple neuron models, the possibility of removing higher-order correlations present in the
input is very limited [3].
Here, we study the role of second-order correlations in the multivariate binary output statistics of
such linear-nonlinear model neurons with a threshold nonlinearity responding to natural images.
That is, each unit can be described by an affine transformation zk = wkT x + ? followed by a
point-wise signum function sk = sgn(zk ). Our interest in this model is twofold: (A) It can be
regarded a parsimonious model for the analysis of population codes of natural images for which the
1

A

?3

B

3

0

2

C

6

JS?Divergence (bits)

?H (%)

x 10

4

6

8

Dimension

1

log? H (%)

?H (%)

4
3
2
1

2

4

6

8

Dimension

10

D
10

0.5

0

?5

5

0

10

x 10

12

14

16

18

0

10

?1

10

?2

20

10

Dimension

12

14

16

18

20

log 2 (Number of Samples)

Figure 1: Similarity between the Ising and the DG model. A+C: Entropy difference ?H between the Ising
model and the Dichotomized Gaussian distribution as a function of dimensionality. A: Up to 10 dimensions
we can compute HDG directly by evaluating Eq. 6. Gray dots correspond to different sets of parameters. For
m ? 4, the relatively large scatter and the existence of negative values is due to the limited numerical precision
of the Monte-Carlo integration. Errorbars show standard error of the mean. B. JS-divergence DJS between PI
and PDG . C. ?H as above, for higher dimensions. Up to 20 dimensions ?H remains very small. The increase
for m ? 20 is most likely due to undersampling of the distributions. D. ?H as function of sample size used
to estimate HDG , at seven (black) and ten (grey) dimensions (note log scale on both axes). ?H decreases with
a power law with increasing sample sizes.

computational power and the bandwidth of each unit is limited. (B) The same model can also be
used more generally to fit multivariate binary data with given pairwise correlations, if x is drawn
from a Gaussian distribution. In particular, we will show that the resulting distribution closely
resembles the binary maximum entropy models known as Ising models or Boltzmann machines
which have recently become popular for the analysis of spike train recordings from retinal ganglion
cell responses [13, 12].
Motivated by the analysis in [12, 13] and the discussion in [10] we are interested at a more general level in the following questions: are pairwise interactions enough for understanding the statistical regularities in high-dimensional natural data (given that they provide a good fit in the lowdimensional case)? If we suppose that pairwise interactions are enough, what can we say about the
amount of redundancies in high-dimensional data? In comparison with neural spike data, natural
images provide two advantages for studying these questions: 1) It is much easier to obtain large
amounts of data with millions of samples which are less prone to nonstationarities. 2) Often differences in the higher-order statistics such as between pink noise and natural images can be recognized
by eye.

2

Second order models for binary variables

In order to study whether pairwise interactions are enough to determine the statistical regularities
in high-dimensional data, it is necessary to be able to compute the maximum entropy distribution
for large number of dimensions N . Given a set of measured statistics, maximum entropy models
yield a full probability distribution that is consistent with these constraints but does not impose any
2

0.05
0
0
?4

1
?

2

1

2

x 10

2
1
0

0

?

Figure 2: Examples of covariance matrices (A+B.) and their learned approximations (C+D) at m = 10 for
clarity. ? is the parameter controlling the steepness of correlation decrease. E+F. Eigenvalue spectra of both
matrices. G. Entropy difference ?H and H. JS-divergence between the distribution of samples obtained from
the two models at m = 7.

additional structure on the distribution [7]. For binary data with given mean activations ?i = hsi i
and correlations between neurons ?ij = hsi sj i ? hsi ihsj i, one obtains a quadratic exponential
probability mass function known as the Ising model in physics or as the Boltzmann machine in
machine learning.
Currently all methods used to determine the parameters of such binary maximum entropy models
suffer from the same drawback: since the parameters do not correspond directly to any of the measured statistics, they have to be inferred (or ?learned?) from data. In high dimensions though, this
poses a difficult computational problem. Therefore the characterization of complete neural circuits
with possibly hundreds of neurons is still out of reach, even though analysis was recently extended
to up to forty neurons [14].
To make the maximum entropy approach feasible in high dimensions, we propose a new strategy:
Sampling from a ?near-maximum? entropy model that does not require any complicated learning
of parameters. In order to justify this approach, we verify empirically that the entropy of the full
probability distributions obtained with the near-maximum entropy model are indistinguishable from
those obtained with classical methods such as Gibbs sampling for up to 20 dimensions.
2.1

Boltzmann machine learning

For a binary vector of neural activities s ? {?1, 1}m and specified ?i and ?ij the Ising model takes
the form
?
?
m
X
X
1
1
PI (s) = exp ?
hi si +
Jij si sj ? ,
(1)
Z
2
i=1
i6=j

where the local fields hi and the couplings Jij have to be chosen such that hsi i = ?i and hsi sj i ?
hsi ihsj i = ?ij . Unfortunately, finding the correct parameters turns out to be a difficult problem
which cannot be solved in closed form.
Therefore, one has to resort to an optimization approach to learn the model parameters hi and Jij
from data. This problem is called Boltzmann machine learning and is based on maximization of the
log-likelihood L = ln PI ({si }N
i=1 |h, J) [1] where N is the number of samples. The gradient of the
likelihood can be computed in terms of the empirical covariance and the covariance of si and sj as
produced by the current model:
?L
= hsi sj iData ? hsi sj iModel
?Jij

(2)

The second term on the right hand side is difficult to compute, as it requires sampling from the model.
Since the partition function Z in Eq. (1) is not available in closed form, Monte-Carlo methods such
3

Figure 3: Random samples of dichotomized 4x4 patches from the van Hateren image data base (left) and from
the corresponding dichotomized Gaussian distribution with equal covariance matrix (middle). It is not possible
to see any systematic difference between the samples from the two distributions. For comparison, this is not so
for the sample from the independent model (right).

as Gibbs sampling are employed [9] in order to approximate the required model average. This is
computationally demanding as sampling is necessary for each individual update. While efficient
sampling algorithms exist for special cases [6], it still remains a hard and time consuming problem
in the general case. Additionally, most sampling algorithms do not come with guarantees for the
quality of the approximation of the required average. In conclusion, parameter fitting of the Ising
model is slow and oftentimes painstaking, especially in high dimensions.
2.2

Modeling with the dichotomized Gaussian

Here we explore an intriguing alternative to the Monte-Carlo approach: We replace the Ising model
by a ?near-maximum? entropy model, for which both parameter computation and sampling is easy. A
very convenient, but in this context rarely recognized, candidate model is the dichotomized Gaussian
distribution (DG) [11, 5, 4]. It is obtained by supposing that the observed binary vector s is generated
from a hidden Gaussian variable
z ? N (?, ?) ,

si = sgn(zi ).

(3)

Without loss of generality, we can assume unit variances for the Gaussian, i.e. ?ii = 1, the mean ?
and the covariance matrix ? of s are given by
?i = 2?(?i ) ? 1 ,

?ii = 4?(?i )?(??i ) ,

?ij = 4?(?i , ?j , ?ij ) for i 6= j

(4)

where ?(x, y, ?) = ?2 (x, y, ?) ? ?(x)?(y) . Here ? is the univariate standardized cumulative
Gaussian distribution and ?2 its bivariate counterpart. While the computation of the model parameters was hard for the Ising model, these equations can be easily inverted to find the parameters of
the hidden Gaussian distribution:


?i + 1
?i = ??1
(5)
2
Determining ?ij generally requires to find a suitable value such that ?ij ? 4?(?i , ?j , ?ij ) = 0.
This can be efficently solved by numerical computations, since the function is monotonic in ?ij
and has a unique zero crossing. We obtain an especially easy case, when ?i = ?j = 0, as then
?ij = sin ?2 ?ij .
It is also possible to evaluate the probability mass function of the DG model by numerical integration,
Z b1
Z bm

1
T ?1
PDG (s) =
.
.
.
exp
?(s
?
?)
?
(s
?
?)
,
(6)
(2?)N/2 |?|1/2 a1
am
where the integration limits are chosen as ai = 0 and bi = ?, if si = 1, and ai = ?? and bi = 0,
otherwise.
In summary, the proposed model has two advantages over the traditional Ising model: (1) Sampling
is easy, and (2) finding the model parameters is easy too.
4

3

Near-maximum entropy behavior of the dichotomized Gaussian
distribution

In the previous section we introduced the dichotomized Gaussian distribution. Our conjecture is that
in many cases it can serve as a convenient approximation to the Ising model. Now, we investigate
how good this approximation is. For a wide range of interaction terms and mean activations we
verify that the DG model closely resembles the Ising model. In particular we show that the entropy of
the DG distribution is not smaller than the entropy of the Ising model even at rather high dimensions.
3.1

Random Connectivity

We created randomly connected networks of varying size m, where mean activations hi and
interactions
terms Jij were drawn from N (0, 0.4). First, we compared the entropy HI =
P
? s PI (s) log2 PI (s) of the thus specified Ising model obtained by evaluating Eq. 1 with the entropy of the DG distribution HDG computed by numerical integration1 from Eq. 6 (twenty parameter
sets). The entropy difference ?H = HI ? HDG was smaller than 0.002 percent of HI (Fig. 1 A,
note scale) and probably within the range of the numerical integration accuracy. In addition, we
computed the Jensen-Shannon divergence DJS [PI kPDG ] = 12 (DKL [PI kM ] + DKL [PDG kM ]),
where M = 21 (PI + PDG ) [8]. We find that DJS [PI kPDG ] is extremly small up to 10 dimensions
(Fig. 1 B). Therefore, the distributions seem to be not only close in their respective entropy, but also
to have a very similar structure.
Next, we extended this analysis to networks of larger size and repeated the same analysis for up to
twenty dimensions. Since the integration in Eq. 6 becomes too time-consuming for m ? 20 due
to the large number of states, we used a histogram based estimate of PDG (using 3 ? 106 samples
for m < 15 and 15 ? 106 samples for m ? 15). The estimate of ?H is still very small at high
dimensions (Fig. 1 C, below 0.5%). We also computed DJS , which scaled similarly to ?H (data
not shown).
In Fig. 1 C, ?H seems to increase with dimensionality. Therefore, we investigated how the estimate
of ?H is influenced by the number of samples used. We computed both quantities for varying numbers of samples from the DG distribution (for m = 7, 10). As ?H decreases according to a power
law with increasing m, the rise of ?H observed in Fig. 1 C is most likely due to undersampling of
the distribution.
3.2

Specified covariance structure

To explore the relationship between the two techniques more systematically, we generated covariance matrices with varying eigenvalue spectra. We used a parametric Toeplitz form, where the nth
diagonal is set to a constant value exp(?? ? n) (Fig. 2A and B, m = 7, 10). We varied the decay
parameter ?, which led to a widely varying covariance structure (For eigenvalue spectra, see Fig. 2E
and F). We fit the Ising models using the Boltzmann machine gradient descent procedure. The covariance matrix of the samples drawn from the Ising model resembles the original very closely (Fig.
2C and D). We also computed the entropy of the DG model using the desired covariance structure.
We estimated ?H and DJS [PG kPDG ] averaged over 10 trials with 105 samples obtained by Gibbs
sampling from the Ising model. ?H is very close to zero (Fig. 2G, m = 7) except for small ?s
and never exceeded 0.05%. Moreover, the structure of both distributions seems to be very similar as
well (Fig. 2H, m = 7). At m = 10, both quantities scaled qualitatively similair (data not shown).
We also repeated this analysis using equations 1 and 6 as before, which lead to similar results (data
not shown).
Our experiments demonstrate clearly that the dichotomized Gaussian distribution constitutes a good
approximation to the quadratic exponential distribution for a large parameter range. In the following
section, we will exploit the similarity between the two models to study how the role of second-order
correlations may change between low-dimensional and high-dimensional statistics in case of natural
images.
1
For integration, we used the mvncdf function of Matlab. For m ? 4 this function employs Monte-Carlo
integration.

5

Figure 4: A: Negative log probabilities of the DG model are plotted against ground truth (red dots). Identical
distributions fall on the diagonal. Data points outside the area enclosed by the dashed lines indicate significant
differences between the model and ground truth. The DG model matches the true distribution very well. For
comparison the independent model is shown as well (blue crosses). B: The multi-information of the true
distribution (blue dots) accurately agrees with the multi-information of the DG model (red line). Similar to
the analysis in [12], we observe a power law behavior of the entropy of the independent model (black solid
line) and the mutli-information. Linear extrapolation (in the log-log plot) to higher dimensions is indicated by
dashed lines. C: Different way of presentation of the same data as in B: the joint entropy H = Hindep ? I
(blue dots) is plotted instead of I and the axis are in linear scale. The dashed red line represents the same
extrapolation as in B.

4

Natural images: Second order and beyond

We now investigate to which extent the statistics of natural images with dichotomized pixel intensities can be characterized by pairwise correlations only. In particular, we would like to know how
the role of pairwise correlations opposed to higher-order correlations changes depending on the dimensionality. Thanks to the DG model introduced above, we are in the position to study the effect
of pairwise correlations for high-dimensional binary random variables (N ? 1000 or even larger).
We use the van Hateren image database in log-intensity scale, from which we sample small image
patches at random positions. The threshold for the dichotomization is set to the median of pixel
intensities. That is, each binary variable encodes whether the corresponding pixel intensity is above
or below the median over the ensemble. Up to patch sizes of 4 ? 4 pixel, the true joint statistics can
be assessed using nonparametric histogram methods. Before we present quantitative comparisons, it
is instructive to look at random samples from the true distribution (Fig. 3, left), from the DG model
with same mean and covariance (Fig. 3, middle), and from the corresponding independent model
(Fig. 3, right). By visual inspection, it seems that the DG model fits the true distribution well.
In order to quantify how well the DG model matches the true distribution, we draw two independent
sets of samples from each (N = 2 ? 106 for each set) and generate a scatter plot as shown in
Fig. 4 A for 4 ? 4 image patches. Each dot corresponds to one of the 216 = 65536 possible different
binary patterns. The relative frequencies of these patterns according to the DG model (red dots) and
according to the independent model (blue dots) are plotted against the relative frequencies obtained
from the natural image patches. The solid diagonal line corresponds to a perfect match between
model and ground truth. The dashed lines enclose the regions within which deviations are to be
expected due to the finite sampling size. Since most of the red dots fall within this region, the DG
model fits the data distribution very well.
P
We also systematically evaluated the JS-divergence and the multi-information I[S] = k H[Sk ] ?
H[S] as a function of dimensionality. That is, we started with the bivariate marginal distribution
of two randomly selected pixels. Then we incrementally added more pixels of random location
until the random vector contains all the 16 pixels of the 4 ? 4 image patches. Independent of the
dimension, the JS-divergence between the DG model and the true distribution is smaller than 0.015
bits. For comparison, the JS-divergence between the independent model and the true distribution
increases with dimensionality from roughly 0.2 bits in the case of two pixels up to 0.839 bits in
the case of 16 pixels. For two independent sets of samples both drawn from natural image data the
JS-divergence ranges between 0.006 and 0.007 bits for 4 ? 4 patches setting the gold standard for
the minimal possible JS-divergence one could achieve with any model due to finite sampling size.
Carrying out the same type of analysis as in [12], we make qualitatively the same observations as it
was reported there: as shown above, we find a quite accurate match between the two distributions.
6

Figure 5: Random samples of dichotomized 32x32 patches from the van Hateren image data base (left) and
from the corresponding dichotomized Gaussian distribution with equal covariance matrix (right). For the latter, the percept of typical objects is missing due to the ignorance of higher-order correlations. This striking
difference is not obvious, however, at the level of 4x4 patches, for which we found an excellent match of the
dichotomized Gaussian to the ensemble of natural images.

Furthermore, the multi-information of the DG model (red solid line) and of the true distribution (blue
dots) increases linearly on a loglog-scale with the number of dimensions (Fig. 4 B). Both findings
can be verified only up to a rather limited number of dimensions (less than 20). Nevertheless, in [12],
two claims about the higher-dimensional statistics have been based on these two observations: First,
that pairwise correlations may be sufficient to determine the full statistics of binary responses, and
secondly, that the convergent scaling behavior in the log-log plot may indicate a transition towards
strong order.
Using natural images instead of retinal ganglion cell data, we would like to verify to what extent
the low-dimensional observations can be used to support these claims about the high-dimensional
statistics [10]. To this end we study the same kind of extrapolation (Fig. 4 B) to higher dimensions
(dashed lines) as in [12]. The difference between the entropy of the independent model and the
multi-information yields the joint entropy of the respective distribution. If the extrapolation is taken
seriously, this difference seems to vanish at the order of 50 dimensions suggesting that the joint
entropy of the neural responses approaches zero at this size?say for 7 ? 7 image patches (Fig. 4 C).
Though it was not taken literally, this point of ?freezing? has been pointed out in [12] as a critical
network size at which a transition to strong order is to be expected. The meaning of this assertion,
however, is not clear. First of all, the joint entropy of a distribution can never be smaller than the
joint entropy of any of its marginals. Therefore, the joint entropy cannot decrease with increasing
number of dimensions as the extrapolation would suggest (Fig. 4 C). Instead it would be necessary to
ask more precisely how the growth rate of the joint entropy can be characterized and whether there
is a critical number of dimensions at which the growth rate suddenly drops. In our study with natural
images, visual inspection does not indicate anything special to happen at the ?critical patch size? of
7 ? 7 pixels. Rather, for all patch sizes, the DG model yields dichotomized pink noise. In Fig. 5
(right) we show a sample from the DG model for 32?32 image patches (i.e. 1024 dimensions) which
provides no indication for a particularly interesting change in the statistics towards strong order. The
exact law according to which the multi-information grows with the number of dimensions for large
m, however, is not easily assessed and remains to be explored.
Finally, we point out that the sufficiency of pairwise correlations at the level of m = 16 dimensions
does not hold any more in the case of large m: the samples from the true distribution at the left
hand side of Fig. 5 clearly show much more structure than the samples from the DG model (Fig. 5,
right), indicating that pairwise correlations do not suffice to determine the full statistics of large
image patches. Even if the match between the DG model and the Ising model may turn out to be
less accurate in high dimensions, this would not affect our conclusion. Any mismatch would only
introduce more order in the DG model than justified by pairwise correlations only.

5

Conclusion and Outlook

We proposed a new approach to maximum entropy modeling of binary variables, extending maximum entropy analysis to previously infeasible high dimensions: As both sampling and finding pa7

rameters is easy for the dichotomized Gaussian model, it overcomes the computational drawbacks of
Monte-Carlo methods. We verified numerically that the empirical entropy of the DG model is comparable to that obtained with Gibbs sampling at least up to 20 dimensions. For practical purposes,
the DG distribution can even be superior to the Gibbs sampler in terms of entropy maximization due
to the lack of independence between consecutive samples in the Gibbs sampler.
Although the Ising model and the DG model are in principle different, the match between the two
turns out to be surprisingly good for a large region of the parameter space. Currently, we are trying
to determine where the close similarity between the Ising model and the DG model breaks down.
In addition, we explore the possibility to use the dichotomized Gaussian distribution as a proposal
density for Monte-Carlo methods such as importance sampling. As it is a very close approximation
to the Ising model, we expect this combination to yield highly efficient sampling behaviour. In
summary, by linking the DG model to the Ising model, we believe that maximum entropy modeling
of multivariate binary random variables will become much more practical in the future.
We used the DG model to investigate the role of second-order correlations in the context of sensory coding of natural images. While for small image patches the DG model provided an excellent
fit to the true distribution, we were able to show that this agreement breakes down in the case
of larger image patches. Thus caution is required when extrapolating from low-dimensional measurements to higher-dimensional distributions because higher-order correlations may be invisible in
low-dimensional marginal distributions. Nevertheless, the maximum entropy approach seems to be
a promising tool for the analysis of correlated neural activities, and the DG model can facilitate its
use significantly in practice.
Acknowledgments
We thank Jakob Macke, Pierre Garrigues, and Greg Stephens for helpful comments and stimulating discussions, as well as Alexander Ecker and Andreas Hoenselaar for last minute advice. An implementation of the DG model in Matlab and R will be avaible at our website
http://www.kyb.tuebingen.mpg.de/bethgegroup/code/DGsampling.

References
[1] D.H. Ackley, G.E. Hinton, and T.J. Sejnowski. A learning algorithm for boltzmann machines. Cognitive
Science, 9:147?169, 1985.
[2] H.B. Barlow. Sensory mechanisms, the reduction of redundancy, and intelligence. In The Mechanisation
of Thought Processes, pages 535?539, London: Her Majesty?s Stationery Office, 1959.
[3] M. Bethge. Factorial coding of natural images: How effective are linear model in removing higher-order
dependencies? J. Opt. Soc. Am. A, 23(6):1253?1268, June 2006.
[4] D.R. Cox and N. Wermuth. On some models for multivariate binary variables parallel in complexity with
the multivariate gaussian distribution. Biometrika, 89:462?469, 2002.
[5] L.J. Emrich and M.R. Piedmonte. A method for generating high-dimensional multivariate binary variates.
The American Statistician, 45(4):302?304, 1991.
[6] M. Huber. A bounding chain for swendsen-wang. Random Structures & Algorithms, 22:53?59, 2002.
[7] E.T. Jaynes. Where do we stand on maximum entropy inference. In R.D. Levine and M. Tribus, editors,
The Maximum Entropy Formalism. MIT Press, Cambridge, MA, 1978.
[8] J. Linn. Divergence measures based on the shannon entropy. IEEE Trans Inf Theory, 37:145?151, 1991.
[9] D. J. C. MacKay. Information Theory, Inference and Learning Algorithms. Cambridge University Press,
2003.
[10] Sheila H Nirenberg and Jonathan D Victor. Analyzing the activity of large populations of neurons: how
tractable is the problem? Current Opinion in Neurobiology, 17:397?400, August 2007.
[11] Karl Pearson. On a new method of determining correlation between a measured character a, and a character b, of which only the percentage of cases wherein b exceeds (or falls short of) a given intensity is
recorded for each grade of a. Biometrika, 7:96?105, 1909.
[12] Elad Schneidman, Michael J Berry, Ronen Segev, and William Bialek. Weak pairwise correlations imply
strongly correlated network states in a neural population. Nature, 440(7087):1007?1012, Apr 2006.
[13] J Shlens, JD Field, JL Gauthier, MI Grivich, D Petrusca, A Sher, AM Litke, and EJ Chichilnisky. The
structure of multi-neuron firing patterns in primate retina. J Neurosci, 26(32):8254?8266, Aug 2006.
[14] G. Tkacik, E. Schneidman, M.J. Berry, and W. Bialek. Ising models for networks of real neurons. arXiv:qbio.NC/0611072, 1:1?4, 2006.

8

"
2017,"Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions",Poster,6755-nearest-neighbor-sample-compression-efficiency-consistency-infinite-dimensions.pdf,"We examine the Bayes-consistency of a recently proposed 1-nearest-neighbor-based multiclass learning algorithm. This algorithm is derived from sample compression bounds and enjoys the statistical advantages of tight, fully empirical generalization bounds, as well as the algorithmic advantages of a faster runtime and memory savings. We prove that this algorithm is strongly Bayes-consistent in metric spaces with finite doubling dimension --- the first consistency result for an efficient nearest-neighbor sample compression scheme. Rather surprisingly, we discover that this algorithm continues to be Bayes-consistent even in a certain infinite-dimensional setting, in which the basic measure-theoretic conditions on which classic consistency proofs hinge are violated. This is all the more surprising, since it is known that k-NN is not Bayes-consistent in this setting. We pose several challenging open problems for future research.","Nearest-Neighbor Sample Compression:
Efficiency, Consistency, Infinite Dimensions

Aryeh Kontorovich
Department of Computer Science
Ben-Gurion University of the Negev
karyeh@cs.bgu.ac.il

Sivan Sabato
Department of Computer Science
Ben-Gurion University of the Negev
sabatos@bgu.ac.il

Roi Weiss
Department of Computer Science and Applied Mathematics
Weizmann Institute of Science
roiw@weizmann.ac.il

Abstract
We examine the Bayes-consistency of a recently proposed 1-nearest-neighbor-based
multiclass learning algorithm. This algorithm is derived from sample compression
bounds and enjoys the statistical advantages of tight, fully empirical generalization
bounds, as well as the algorithmic advantages of a faster runtime and memory
savings. We prove that this algorithm is strongly Bayes-consistent in metric
spaces with finite doubling dimension ? the first consistency result for an efficient
nearest-neighbor sample compression scheme. Rather surprisingly, we discover
that this algorithm continues to be Bayes-consistent even in a certain infinitedimensional setting, in which the basic measure-theoretic conditions on which
classic consistency proofs hinge are violated. This is all the more surprising, since
it is known that k-NN is not Bayes-consistent in this setting. We pose several
challenging open problems for future research.

1

Introduction

This paper deals with Nearest-Neighbor (NN) learning algorithms in metric spaces. Initiated by
Fix and Hodges in 1951 [16], this seemingly naive learning paradigm remains competitive against
more sophisticated methods [8, 46] and, in its celebrated k-NN version, has been placed on a solid
theoretical foundation [11, 44, 13, 47].
Although the classic 1-NN is well known to be inconsistent in general, in recent years a series of
papers has presented variations on the theme of a regularized 1-NN classifier, as an alternative to the
Bayes-consistent k-NN. Gottlieb et al. [18] showed that approximate nearest neighbor search can
act as a regularizer, actually improving generalization performance rather than just injecting noise.
In a follow-up work, [27] showed that applying Structural Risk Minimization to (essentially) the
margin-regularized data-dependent bound in [18] yields a strongly Bayes-consistent 1-NN classifier.
A further development has seen margin-based regularization analyzed through the lens of sample
compression: a near-optimal nearest neighbor condensing algorithm was presented [20] and later
extended to cover semimetric spaces [21]; an activized version also appeared [25]. As detailed in
[27], margin-regularized 1-NN methods enjoy a number of statistical and computational advantages
over the traditional k-NN classifier. Salient among these are explicit data-dependent generalization
bounds, and considerable runtime and memory savings. Sample compression affords additional
advantages, in the form of tighter generalization bounds and increased efficiency in time and space.
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

In this work we study the Bayes-consistency of a compression-based 1-NN multiclass learning
algorithm, in both finite-dimensional and infinite-dimensional metric spaces. The algorithm is
essentially the passive component of the active learner proposed by Kontorovich, Sabato, and Urner
in [25], and we refer to it in the sequel as KSU; for completeness, we present it here in full (Alg. 1).
We show that in finite-dimensional metric spaces, KSU is both computationally efficient and Bayesconsistent. This is the first compression-based multiclass 1-NN algorithm proven to possess both of
these properties. We further exhibit a surprising phenomenon in infinite-dimensional spaces, where
we construct a distribution for which KSU is Bayes-consistent while k-NN is not.
Main results. Our main contributions consist of analyzing the performance of KSU in finite and
infinite dimensional settings, and comparing it to the classical k-NN learner. Our key findings are
summarized below.
? In Theorem 2, we show that KSU is computationally efficient and strongly Bayes-consistent
in metric spaces with a finite doubling dimension. This is the first (strong or otherwise)
Bayes-consistency result for an efficient sample compression scheme for a multiclass (or
even binary)1 1-NN algorithm. This result should be contrasted with the one in [27], where
margin-based regularization was employed, but not compression; the proof techniques
from [27] do not carry over to the compression-based scheme. Instead, novel arguments
are required, as we discuss below. The new sample compression technique provides a
Bayes-consistency proof for multiple (even countably many) labels; this is contrasted with
the multiclass 1-NN algorithm in [28], which is not compression-based, and requires solving
a minimum vertex cover problem, thereby imposing a 2-approximation factor whenever
there are more than two labels.
? In Theorem 4, we make the surprising discovery that KSU continues to be Bayes-consistent
in a certain infinite-dimensional setting, even though this setting violates the basic measuretheoretic conditions on which classic consistency proofs hinge, including Theorem 2. This
is all the more surprising, since it is known that k-NN is not Bayes-consistent for this
construction [9]. We are currently unaware of any separable2 metric probability space on
which KSU fails to be Bayes-consistent; this is posed as an intriguing open problem.
Our results indicate that in finite dimensions, an efficient, compression-based, Bayes-consistent
multiclass 1-NN algorithm exists, and hence can be offered as an alternative to k-NN, which is well
known to be Bayes-consistent in finite dimensions [12, 41]. In contrast, in infinite dimensions, our
results show that the condition characterizing the Bayes-consistency of k-NN does not extend to all
NN algorithms. It is an open problem to characterize the necessary and sufficient conditions for the
existence of a Bayes-consistent NN-based algorithm in infinite dimensions.
Related work. Following the pioneering work of [11] on nearest-neighbor classification, it was
shown by [13, 47, 14] that the k-NN classifier is strongly Bayes consistent in Rd . These results
made extensive use of the Euclidean structure of Rd , but in [41] a weak Bayes-consistency result was
shown for metric spaces with a bounded diameter and a bounded doubling dimension, and additional
distributional smoothness assumptions. More recently, some of the classic results on k-NN risk
decay rates were refined by [10] in an analysis that captures the interplay between the metric and the
sampling distribution. The worst-case rates have an exponential dependence on the dimension (i.e.,
the so-called curse of dimensionality), and Pestov [33, 34] examines this phenomenon closely under
various distributional and structural assumptions.
Consistency of NN-type algorithms in more general (and in particular infinite-dimensional) metric
spaces was discussed in [1, 5, 6, 9, 30]. In [1, 9], characterizations of Bayes-consistency were
given in terms of Besicovitch-type conditions (see Eq. (3)). In [1], a generalized ?moving window?
classification rule is used and additional regularity conditions on the regression function are imposed.
The filtering technique (i.e., taking the first d coordinates in some basis representation) was shown to
be universally consistent in [5]. However, that algorithm suffers from the cost of cross-validating
over both the dimension d and number of neighbors k. Also, the technique is only applicable in
1
An efficient sample compression algorithm was given in [20] for the binary case, but no Bayes-consistency
guarantee is known for it.
2
C?rou and Guyader [9] gave a simple example of a nonseparable metric on which all known nearest-neighbor
methods, including k-NN and KSU, obviously fail.

2

Hilbert spaces (as opposed to more general metric spaces) and provides only asymptotic consistency,
without finite-sample bounds such as those provided by KSU. The insight of [5] is extended to the
more general Banach spaces in [6] under various regularity assumptions.
None of the aforementioned generalization results for NN-based techniques are in the form of
fully empirical, explicitly computable sample-dependent error bounds. Rather, they are stated in
terms of the unknown Bayes-optimal rate, and some involve additional parameters quantifying the
well-behavedness of the unknown distribution (see [27] for a detailed discussion). As such, these
guarantees do not enable a practitioner to compute a numerical generalization error estimate for a
given training sample, much less allow for a data-dependent selection of k, which must be tuned via
cross-validation. The asymptotic expansions in [43, 37, 23, 40] likewise do not provide a computable
finite-sample bound. The quest for such bounds was a key motivation behind the series of works
[18, 28, 20], of which KSU [25] is the latest development.
The work of Devroye et al. [14, Theorem 21.2] has implications for 1-NN classifiers in Rd that
are defined based on data-dependent majority-vote partitions of the space. It is shown that under
some conditions, a fixed mapping from each sample size to a data-dependent partition rule induces a
strongly Bayes-consistent algorithm. This result requires the partition rule to have a bounded VC
dimension, and since this rule must be fixed in advance, the algorithm is not fully adaptive. Theorem
19.3 ibid. proves weak consistency for an inefficient compression-based algorithm, which selects
among all the possible compression sets of a certain size, and maintains a certain rate of compression
relative to the sample size. The generalizing power of sample compression was independently
discovered by [31], and later elaborated upon by [22]. In the context of NN classification, [14] lists
various condensing heuristics (which have no known performance guarantees) and leaves open the
algorithmic question of how to minimize the empirical risk over all subsets of a given size.
The first compression-based 1-NN algorithm with provable optimality guarantees was given in [20];
it was based on constructing ?-nets in spaces with a finite doubling dimension. The compression
size of this construction was shown to be nearly unimprovable by an efficient algorithm unless P=NP.
With ?-nets as its algorithmic engine, KSU inherits this near-optimality. The compression-based
1-NN paradigm was later extended to semimetrics in [21], where it was shown to survive violations
of the triangle inequality, while the hierarchy-based search methods that have become standard for
metric spaces (such as [4, 18] and related approaches) all break down.
It was shown in [27] that a margin-regularized 1-NN learner (essentially, the one proposed in [18],
which, unlike [20], did not involve sample compression) becomes strongly Bayes-consistent when the
margin is chosen optimally in an explicitly prescribed sample-dependent fashion. The margin-based
technique developed in [18] for the binary case was extended to multiclass in [28]. Since the algorithm
relied on computing a minimum vertex cover, it was not possible to make it both computationally
efficient and Bayes-consistent when the number of lables exceeds two. An additional improvement
over [28] is that the generalization bounds presented there had an explicit (logarithmic) dependence
on the number of labels, while our compression scheme extends seamlessly to countable label spaces.
Paper outline. After fixing the notation and setup in Sec. 2, in Sec. 3 we present KSU, the
compression-based 1-NN algorithm we analyze in this work. Sec. 4 discusses our main contributions
regarding KSU, together with some open problems. High-level proof sketches are given in Sec. 5 for
the finite-dimensional case, and Sec. 6 for the infinite-dimensional case. Full detailed proofs can be
found in [26].

2

Setting and Notation

Our instance space is the metric space (X , ?), where X is the instance domain and ? is the metric.
(See Appendix A in [26] for relevant background on metric measure spaces.) We consider a countable
label space Y. The unknown sampling distribution is a probability measure ?
? over X ? Y, with
marginal ? over X . Denote by (X, Y ) ? ?
? a pair drawn according to ?
?. The generalization error of a
classifier f : X ? Y is given by err?? (f ) := P?? (Y 6= f (X)),
P and its empirical error with respect to
a labeled set S 0 ? X ? Y is given by err(f,
c S 0 ) := |S10 | (x,y)?S 0 1[y 6= f (x)]. The optimal Bayes
risk of ?
? is R??? := inf err?? (f ), where the infimum is taken over all measurable classifiers f : X ? Y.
We say that ?
? is realizable when R??? = 0. We omit the overline in ?
? in the sequel when there is no
ambiguity.
3

For a finite labeled set S ? X ? Y and any x ? X , let Xnn (x, S) be the nearest neighbor of x with
respect to S and let Ynn (x, S) be the nearest neighbor label of x with respect to S:
(Xnn (x, S), Ynn (x, S)) := argmin ?(x, x0 ),
(x0 ,y 0 )?S

where ties are broken arbitrarily. The 1-NN classifier induced by S is denoted by hS (x) :=
Ynn (x, S). The set of points in S, denoted by X = {X1 , . . . , X|S| } ? X , induces
a Voronoi partition of X , V(X) := {V1 (X), . . . , V|S| (X)}, where each Voronoi cell is
Vi (X) := {x ? X : argminj?{1,...,|S|} ?(x, Xj ) = i}. By definition, ?x ? Vi (X), hS (x) = Yi .
A 1-NN algorithm is a mapping from an i.i.d. labeled sample Sn ? ?
?n to a labeled set Sn0 ? X ? Y,
yielding the 1-NN classifier hSn0 . While the classic 1-NN algorithm sets Sn0 := Sn , in this work we
study a compression-based algorithm which sets Sn0 adaptively, as discussed further below.
A 1-NN algorithm is strongly Bayes-consistent on ?
? if err(hSn0 ) converges to R? almost surely,
that is P[limn?? err(hSn0 ) = R? ] = 1. An algorithm is weakly Bayes-consistent on ?
? if err(hSn0 )
converges to R? in expectation, limn?? E[err(hSn0 )] = R? . Obviously, the former implies the
latter. We say that an algorithm is Bayes-consistent on a metric space if it is Bayes-consistent on all
distributions in the metric space.
A convenient property that is used when studying the Bayes-consistency of algorithms in metric
spaces is the doubling dimension. Denote the open ball of radius r around x by Br (x) := {x0 ?
?r (x) denote the corresponding closed ball. The doubling dimension of a
X : ?(x, x0 ) < r} and let B
metric space (X , ?) is defined as follows. Let n be the smallest number such that every ball in X can
be covered by n balls of half its radius, where all balls are centered at points of X . Formally,
n := min{n ? N : ?x ? X , r > 0, ?x1 , . . . , xn ? X s.t. Br (x) ? ?ni=1 Br/2 (xi )}.
Then the doubling dimension of (X , ?) is defined by ddim(X , ?) := log2 n.
For an integer n, let [n] := {1, . . . , n}. Denote the set of all index vectors of length d by In,d :=
[n]d . Given a labeled set Sn = (Xi , Yi )i?[n] and any i = {i1 , . . . , id } ? In,d , denote the subsample of Sn indexed by i by Sn (i) := {(Xi1 , Yi1 ), . . . , (Xid , Yid )}. Similarly, for a vector Y 0 =
{Y10 , . . . , Yd0 } ? Y d , denote by Sn (i, Y 0 ) := {(Xi1 , Y10 ), . . . , (Xid , Yd0 )}, namely the sub-sample
of Sn as determined by i where the labels are replaced with Y 0 . Lastly, for i, j ? In,d , we denote
Sn (i; j) := {(Xi1 , Yj1 ), . . . , (Xid , Yjd )}.

3

1-NN majority-based compression

In this work we consider the 1-NN majority-based compression algorithm proposed in [25], which
we refer to as KSU. This algorithm is based on constructing ?-nets at different scales; for ? > 0
and A ? X , a set X ? A is said to be a ?-net of A if ?a ? A, ?x ? X : ?(a, x) ? ? and for all
x 6= x0 ? X, ?(x, x0 ) > ?.3
The algorithm (see Alg. 1) operates as follows. Given an input sample Sn , whose set of points is
denoted Xn = {X1 , . . . , Xn }, KSU considers all possible scales ? > 0. For each such scale it
constructs a ?-net of Xn . Denote this ?-net by X(?) := {Xi1 , . . . , Xim }, where m ? m(?) denotes
its size and i ? i(?) := {i1 , . . . , im } ? In,m denotes the indices selected from Sn for this ?-net.
For every such ?-net, the algorithm attaches the labels Y 0 ? Y 0 (?) ? Y m , which are the empirical
majority-vote labels in the respective Voronoi cells in the partition V(X(?)) = {V1 , . . . , Vm }.
Formally, for i ? [m],
Yi0 ? argmax |{j ? [n] | Xj ? Vi , Yj = y}|,
(1)
y?Y

where ties are broken arbitrarily. This procedure creates a labeled set Sn0 (?) := Sn (i(?), Y 0 (?)) for
every relevant ? ? {?(Xi , Xj ) | i, j ? [n]} \ {0}. The algorithm then selects a single ?, denoted
? ? ? ?n? , and outputs hSn0 (? ? ) . The scale ? ? is selected so as to minimize a generalization error
bound, which upper bounds err(Sn0 (?)) with high probability. This error bound, denoted Q in the
algorithm, can be derived using a compression-based analysis, as described below.
3
For technical reasons, having to do with the construction in Sec. 6, we depart slightly from the standard
definition of a ?-net X ? A. The classic definition requires that (i) ?a ? A, ?x ? X : ?(a, x) < ? and (ii)
?x 6= x0 ? X : ?(x, x0 ) ? ?. In our definition, the relations < and ? in (i) and (ii) are replaced by ? and >.

4

Algorithm 1 KSU: 1-NN compression-based algorithm
Require: Sample Sn = (Xi , Yi )i?[n] , confidence ?
Ensure: A 1-NN classifier
1: Let ? := {?(Xi , Xj ) | i, j ? [n]} \ {0}
2: for ? ? ? do
3:
Let X(?) be a ?-net of {X1 , . . . , Xn }
4:
Let m(?) := |X(?)|
5:
For each i ? [m(?)], let Yi0 be the majority label in Vi (X(?)) as defined in Eq. (1)
6:
Set Sn0 (?) := (X(?), Y 0 (?))
7: end for
8: Set ?(?) := err(h
c Sn0 (?) , Sn )
9: Find ?n? ? argmin??? Q(n, ?(?), 2m(?), ?), where Q is, e.g., as in Eq. (2)
10: Set Sn0 := Sn0 (?n? )
11: return hSn0

m
We say that a mapping Sn 7? Sn0 is a compression scheme if there is a function C : ??
m=0 (X ?Y) ?
2X ?Y , from sub-samples to subsets of X ? Y, such that for every Sn there exists an m and a sequence
i ? In,m such that Sn0 = C(Sn (i)). Given a compression scheme Sn 7? Sn0 and a matching function
C, we say that a specific Sn0 is an (?, m)-compression of a given Sn if Sn0 = C(Sn (i)) for some
i ? In,m and err(h
c Sn0 , Sn ) ? ?. The generalization power of compression was recognized by [17]
and [22]. Specifically, it was shown in [21, Theorem 8] that if the mapping Sn 7? Sn0 is a compression
scheme, then with probability at least 1 ? ?, for any Sn0 which is an (?, m)-compression of Sn ? ?
?n ,
we have (omitting the constants, explicitly provided therein, which do not affect our analysis)
s
nm
? log(n) + log(1/?)
n
m log(n) + log(1/?)
err(hSn0 ) ?
? + O(
) + O( n?m
). (2)
n?m
n?m
n?m

Defining Q(n, ?, m, ?) as the RHS of Eq. (2) provides KSU with a compression bound. The following
proposition shows that KSU is a compression scheme, which enables us to use Eq. (2) with the
appropriate substitution.4
Proposition 1. The mapping Sn 7? Sn0 defined by Alg. 1 is a compression scheme whose output Sn0
is a (err(h
c Sn0 ), 2|Sn0 |)-compression of Sn .
? i , Y?i )i?[2m] ) = (X
? i , Y?i+m )i?[m] , and observe that for all
Proof. Define the function C by C((X
0
Sn , we have Sn = C(Sn (i(?); j(?))), where i(?) is the ?-net index set as defined above, and
j(?) = {j1 , . . . , jm(?) } ? In,m(?) is some index vector such that Yi0 = Yji for every i ? [m(?)].
Since Yi0 is an empirical majority vote, clearly such a j exists. Under this scheme, the output Sn0 of
this algorithm is a (err(h
c Sn0 ), 2|Sn0 |)-compression.
KSU is efficient, for any countable Y. Indeed, Alg. 1 has a naive runtime complexity of O(n4 ), since
O(n2 ) values of ? are considered and a ?-net is constructed for each one in time O(n2 ) (see [20,
Algorithm 1]). Improved runtimes can be obtained, e.g., using the methods in [29, 18]. In this work
we focus on the Bayes-consistency of KSU, rather than optimize its computational complexity. Our
Bayes-consistency results below hold for KSU, whenever the generalization bound Q(n, ?, m, ?n )
satisfies the following properties:
Property 1 For any integer n and ? ? (0, 1), with probability 1 ? ? over the i.i.d. random sample
Sn ? ?
?n , for all ? ? [0, 1] and m ? [n]: If Sn0 is an (?, m)-compression of Sn , then
err(hSn0 ) ? Q(n, ?, m, ?).
Property 2 Q is monotonically increasing in ? and in m.
Property 3 There is a sequence {?n }?
n=1 , ?n ? (0, 1) such that
lim

P?

n=1 ?n

< ? and for all m,

sup (Q(n, ?, m, ?n ) ? ?) = 0.

n?? ??[0,1]
4

In [25] the analysis was based on compression with side information, and does not extend to infinite Y.

5

The compression bound in Eq. (2) clearly
P?satisfies these properties. Note that Property 3 is satisfied
by Eq. (2) using any convergent series n=1 ?n < ? such that ?n = e?o(n) ; in particular, the decay
of ?n cannot be too rapid.

4

Main results

In this section we describe our main results. The proofs appear in subsequent sections. First, we show
that KSU is Bayes-consistent if the instance space has a finite doubling dimension. This contrasts
with classical 1-NN, which is only Bayes-consistent if the distribution is realizable.
Theorem 2. Let (X , ?) be a metric space with a finite doubling-dimension. Let Q be a generalization
bound that satisfies Properties 1-3, and let ?n be as stipulated by Property 3 for Q. If the input
confidence ? for input size n is set to ?n , then the 1-NN classifier hSn0 (?n? ) calculated by KSU is
strongly Bayes consistent on (X , ?): P(limn?? err(hSn0 ) = R? ) = 1.
The proof, provided in Sec. 5, closely follows the line of reasoning in [27], where the strong Bayesconsistency of an adaptive margin-regularized 1-NN algorithm was proved, but with several crucial
differences. In particular, the generalization bounds used by KSU are purely compression-based, as
opposed to the Rademacher-based generalization bounds used in [27]. The former can be much tighter
in practice and guarantee Bayes-consistency of KSU even for countably many labels. This however
requires novel technical arguments, which are discussed in detail in Appendix B.1 in [26]. Moreover,
since the compression-based bounds do not explicitly depend on ddim, they can be used even when
ddim is infinite, as we do in Theorem 4 below. To underscore the subtle nature of Bayes-consistency,
we note that the proof technique given here does not carry to an earlier algorithm, suggested in [20,
Theorem 4], which also uses ?-nets. It is an open question whether the latter is Bayes-consistent.
Next, we study Bayes-consistency of KSU in infinite dimensions (i.e., with ddim = ?) ? in particular, in a setting where k-NN was shown by [9] not to be Bayes-consistent. Indeed, a straightforward
application of [9, Lemma A.1] yields the following result.
Theorem 3 (C?rou and Guyader [9]). There exists an infinite dimensional separable metric space
(X , ?) and a realizable distribution ?
? over X ? {0, 1} such that no kn -NN learner satisfying
kn /n ? 0 when n ? ? is Bayes-consistent under ?
?. In particular, this holds for any space and
realizable distribution ?
? that satisfy the following condition: The set C of points labeled 1 by ?
?
satisfies
?r (x))
?(C ? B
?(C) > 0
and
?x ? C, lim
= 0.
(3)
?
r?0
?(Br (x))
Since ?(C) > 0, Eq. (3) constitutes a violation of the Besicovitch covering property. In doubling
spaces, the Besicovitch covering theorem precludes such a violation [15]. In contrast, as [35, 36]
show, in infinite-dimensional spaces this violation can in fact occur. Moreover, this is not an isolated
pathology, as this property is shared by Gaussian Hilbert spaces [45].
At first sight, Eq. (3) might appear to thwart any 1-NN algorithm applied to such a distribution.
However, the following result shows that this is not the case: KSU is Bayes-consistent on a distribution
with this property.
Theorem 4. There is a metric space equipped with a realizable distribution for which KSU is weakly
Bayes-consistent, while any k-NN classifier necessarily is not.
The proof relies on a classic construction of Preiss [35] which satisfies Eq. (3). We show that the
structure of the construction, combined with the packing and covering properties of ?-nets, imply that
the majority-vote classifier induced by any ?-net with a sufficienlty small ? approaches the Bayes
error. To contrast with Theorem 4, we next show that on the same construction, not all majority-vote
Voronoi partitions succeed. Indeed, if the packing property of ?-nets is relaxed, partition sequences
obstructing Bayes-consistency exist.
Theorem 5. For the example constructed in Theorem 4, there exists a sequence of Voronoi partitions
with a vanishing diameter such that the induced true majority-vote classifiers are not Bayes consistent.
The above result also stands in contrast to [14, Theorem 21.2], showing that, unlike in finite dimensions, the partitions? vanishing diameter is insufficient to establish consistency when ddim = ?. We
conclude the main results by posing intriguing open problems.
6

Open problem 1. Does there exist a metric probability space on which some k-NN algorithm is
consistent while KSU is not? Does there exist any separable metric space on which KSU fails?
Open problem 2. C?rou and Guyader [9] distill a certain Besicovitch condition which is necessary
and sufficient for k-NN to be Bayes-consistent in a metric space. Our Theorem 4 shows that the
Besicovitch condition is not necessary for KSU to be Bayes-consistent. Is it sufficient? What is a
necessary condition?

5

Bayes-consistency of KSU in finite dimensions

In this section we give a high-level proof of Theorem 2, showing that KSU is strongly Bayesconsistent in finite-dimensional metric spaces. A fully detailed proof is given in Appendix B in
[26].
Recall the optimal empirical error ?n? ? ?(?n? ) and the optimal compression size m?n ? m(?n? ) as
computed by KSU. As shown in Proposition 1, the sub-sample Sn0 (?n? ) is an (?n? , 2m?n )-compression
of Sn . Abbreviate the compression-based generalization bound used in KSU by
Qn (?, m) := Q(n, ?, 2m, ?n ).
To show Bayes-consistency, we start by a standard decomposition of the excess error over the optimal
Bayes into two terms:


err(hSn0 (?n? ) ) ? R? = err(hSn0 (?n? ) ) ? Qn (?n? , m?n ) + Qn (?n? , m?n ) ? R? =: TI (n) + TII (n),
and show that each term decays to zero with probability one. For the first term, Property 1 for Q,
together with the Borel-Cantelli lemma, readily imply lim supn?? TI (n) ? 0 with probability one.
The main challenge is showing that lim supn?? TII (n) ? 0 with probability one. We do so in
several stages:
1. Loosely speaking, we first show (Lemma 10) that the Bayes error R? can be well approximated using 1-NN classifiers defined by the true (as opposed to empirical) majority-vote
labels over fine partitions of X . In particular, this holds for any partition induced by a ?-net
of X with a sufficiently small ? > 0. This approximation guarantee relies on the fact that in
finite-dimensional spaces, the class of continuous functions with compact support is dense
in L1 (?) (Lemma 9).
2. Fix ?? > 0 sufficiently small such that any true majority-vote classifier induced by a ?? -net
has a true error close to R? , as guaranteed by stage 1. Since for bounded subsets of finitedimensional spaces the size of any ?-net is finite, the empirical error of any majority-vote
?-net almost surely converges to its true majority-vote error as the sample size n ? ?. Let
n(?
? ) sufficiently large such that Qn(?? ) (?(?
? ), m(?
? )) as computed by KSU for a sample of
0
size n(?
? ) is a reliable estimate for the true error of hSn(?
(?
?).
?)
3. Let ?? and n(?
? ) be as in stage 2. Given a sample of size n = n(?
? ), recall that KSU
selects an optimal ? ? such that Qn (?(?), m(?)) is minimized over all ? > 0. For margins
?  ?? , which are prone to over-fitting, Qn (?(?), m(?)) is not a reliable estimate for
hSn0 (?) since compression may not yet taken place for samples of size n. Nevertheless, these
margins are discarded by KSU due to the penalty term in Q. On the other hand, for ?-nets
with margin ?  ?? , which are prone to under-fitting, the true error is well estimated by
Qn (?(?), m(?)). It follows that KSU selects ?n? ? ?? and Qn (?n? , m?n ) ? R? , implying
lim supn?? TII (n) ? 0 with probability one.
As one can see, the assumption that X is finite-dimensional plays a major role in the proof. A simple
argument shows that the family of continuous functions with compact support is no longer dense
in L1 in infinite-dimensional spaces. In addition, ?-nets of bounded subsets in infinite dimensional
spaces need no longer be finite.

6

On Bayes-consistency of NN algorithms in infinite dimensions

In this section we study the Bayes-consistency properties of 1-NN algorithms on a classic infinitedimensional construction of Preiss [35], which we describe below in detail. This construction was
7

z1:k?2
?k?1
z1:k?1
?k

?k
z1:k

?k

?k

?k

?k

z

C = Z?

?? (z) for some z ? C.
Figure 1: Preiss?s construction. Encircled is the closed ball B
k?1
first introduced as a concrete example showing that in infinite-dimensional spaces the Besicovich
covering theorem [15] can be strongly violated, as manifested in Eq. (3).
Example 1 (Preiss?s construction). The construction (see Figure 1) defines an infinite-dimensional
metric space (X , ?) and a realizable measure ?
? over X ? Y with the binary label set Y = {0, 1}.
It relies on two sequences: a sequence of natural numbers {Nk }k?N and a sequence of positive
numbers {ak }k?N . The two sequences should satisfy the following:
P?
limk?? ak N1 . . . Nk+1 = ?; and limk?? Nk = ?. (4)
k=1 ak N1 . . . Nk = 1;
Q
These properties are satisfied, for instance, by setting Nk := k! and ak := 2?k / i?[k] Ni . Let Z0
be the set of all finite sequences (z1 , . . . , zk )k?N of natural numbers such that zi ? Ni , and let Z?
be the set of all infinite sequences (z1 , z2 , . . . ) of natural numbers such that zi ? Ni .
Define the example space X := Z0 ? Z? and denote ?k := 2?k , where ?? := 0. The metric ? over
X is defined as follows: for x, y ? X , denote by x ? y their longest common prefix. Then,
?(x, y) = (?|x?y| ? ?|x| ) + (?|x?y| ? ?|y| ).
It can be shown (see [35]) that ?(x, y) is a metric; in fact, it embeds isometrically into the square
norm metric of a Hilbert space.
To define ?, the marginal measure over X , let ?? be the uniform product distribution measure
over Z? , that is: for all i ? N, each zi in the sequence z = (z1 , z2 , . . . ) ? Z? is independently
drawn from a uniform distribution over [Ni ]. Let ?0 be an atomic measure on Z0 such that for all
z ? Z0 , ?0 (z) = a|z| . Clearly, the first condition in Eq. (4) implies ?0 (Z0 ) = 1. Define the marginal
probability measure ? over X by
?A ? Z0 ? Z? ,

?(A) := ??? (A) + (1 ? ?)?0 (A).

In words, an infinite sequence is drawn with probability ? (and all such sequences are equally likely),
or else a finite sequence is drawn (and all finite sequences of the same length are equally likely).
Define the realizable distribution ?
? over X ? Y by setting the marginal over X to ?, and by setting
the label of z ? Z? to be 1 with probability 1 and the label of z ? Z0 to be 0 with probability 1.
As shown in [35], this construction satisfies Eq. (3) with C = Z? and ?(C) = ? > 0. It follows
from Theorem 3 that no k-NN algorithm is Bayes-consistent on it. In contrast, the following theorem
shows that KSU is weakly Bayes-consistent on this distribution. Theorem 4 immediately follows
from the this result.
Theorem 6. Assume (X , ?), Y and ?
? as in Example 1. KSU is weakly Bayes-consistent on ?
?.
The proof, provided in Appendix C in [26], first characterizes the Voronoi cells for which the true
majority-vote yields a significant error for the cell (Lemma 15). In finite-dimensional spaces, the total
measure of all such ?bad? cells can be made arbitrarily close to zero by taking ? to be sufficiently
small, as shown in Lemma 10 of Theorem 2. However, it is not immediately clear whether this can
be achieved for the infinite dimensional construction above.
Indeed, we expect such bad cells, due to the unintuitive property that for any x ? C, we have
?? (x) ? C)/?(B
?? (x)) ? 0 when ? ? 0, and yet ?(C) > 0. Thus, if for example a significant
?(B
8

?? (x) with
portion of the set C (whose label is 1) is covered by Voronoi cells of the form V = B
x ? C, then for all sufficiently small ?, each one of these cells will have a true majority-vote 0. Thus
a significant portion of C would be misclassified. However, we show that by the structure of the
construction, combined with the packing and covering properties of ?-nets, we have that in any ?-net,
the total measure of all these ?bad? cells goes to 0 when ? ? 0, thus yielding a consistent classifier.
Lastly, the following theorem shows that on the same construction above, when the Voronoi partitions
are allowed to violate the packing property of ?-nets, Bayes-consistency does not necessarily hold.
Theorem 5 immediately follows from the following result.
Theorem 7. Assume (X , ?), Y and ?
? as in Example 1. There exists a sequence of Voronoi partitions
(Pk )k?N of X with maxV ?Pk diam(V ) ? ?k such that the sequence of true majority-vote classifiers
(hPk )k?N induced by these partitions is not Bayes consistent: lim inf k?? err(hPk ) = ? > 0.
The proof, provided in Appendix D, constructs a sequence of Voronoi partitions, where each partition
Pk has all of its impure Voronoi cells (those with both 0 and 1 labels) being bad. In this case, C is
incorrectly classified by hPk , yielding a significant error. Thus, in infinite-dimensional metric spaces,
the shape of the Voronoi cells plays a fundamental role in the consistency of the partition.
Acknowledgments. We thank Fr?d?ric C?rou for the numerous fruitful discussions and helpful
feedback on an earlier draft. Aryeh Kontorovich was supported in part by the Israel Science
Foundation (grant No. 755/15), Paypal and IBM. Sivan Sabato was supported in part by the Israel
Science Foundation (grant No. 555/15).

References
[1] Christophe Abraham, G?rard Biau, and Beno?t Cadre. On the kernel rule for function classification. Ann. Inst. Statist. Math., 58(3):619?633, 2006.
[2] Daniel Berend and Aryeh Kontorovich. The missing mass problem. Statistics & Probability
Letters, 82(6):1102?1110, 2012.
[3] Daniel Berend and Aryeh Kontorovich. On the concentration of the missing mass. Electronic
Communications in Probability, 18(3):1?7, 2013.
[4] Alina Beygelzimer, Sham Kakade, and John Langford. Cover trees for nearest neighbor. In
ICML ?06: Proceedings of the 23rd international conference on Machine learning, pages
97?104, New York, NY, USA, 2006. ACM.
[5] G?rard Biau, Florentina Bunea, and Marten H. Wegkamp. Functional classification in Hilbert
spaces. IEEE Trans. Inform. Theory, 51(6):2163?2172, 2005.
[6] G?rard Biau, Fr?d?ric C?rou, and Arnaud Guyader. Rates of convergence of the functional
k-nearest neighbor estimate. IEEE Trans. Inform. Theory, 56(4):2034?2040, 2010.
[7] V. I. Bogachev. Measure theory. Vol. I, II. Springer-Verlag, Berlin, 2007.
[8] Oren Boiman, Eli Shechtman, and Michal Irani. In defense of nearest-neighbor based image
classification. In CVPR, 2008.
[9] Fr?d?ric C?rou and Arnaud Guyader. Nearest neighbor classification in infinite dimension.
ESAIM: Probability and Statistics, 10:340?355, 2006.
[10] Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for nearest neighbor classification. In NIPS, 2014.
[11] Thomas M. Cover and Peter E. Hart. Nearest neighbor pattern classification. IEEE Transactions
on Information Theory, 13:21?27, 1967.
[12] Luc Devroye. On the inequality of Cover and Hart in nearest neighbor discrimination. IEEE
Trans. Pattern Anal. Mach. Intell., 3(1):75?78, 1981.
[13] Luc Devroye and L?szl? Gy?rfi. Nonparametric density estimation: the L1 view. Wiley Series
in Probability and Mathematical Statistics: Tracts on Probability and Statistics. John Wiley &
Sons, Inc., New York, 1985.
9

[14] Luc Devroye, L?szl? Gy?rfi, and G?bor Lugosi. A probabilistic theory of pattern recognition,
volume 31. Springer Science & Business Media, 2013.
[15] Herbert Federer. Geometric measure theory. Die Grundlehren der mathematischen Wissenschaften, Band 153. Springer-Verlag New York Inc., New York, 1969.
[16] Evelyn Fix and Jr. Hodges, J. L. Discriminatory analysis. nonparametric discrimination:
Consistency properties. International Statistical Review / Revue Internationale de Statistique,
57(3):pp. 238?247, 1989.
[17] Sally Floyd and Manfred Warmuth. Sample compression, learnability, and the VapnikChervonenkis dimension. Machine learning, 21(3):269?304, 1995.
[18] Lee-Ad Gottlieb, Aryeh Kontorovich, and Robert Krauthgamer. Efficient classification for metric
data (extended abstract COLT 2010). IEEE Transactions on Information Theory, 60(9):5750?
5759, 2014.
[19] Lee-Ad Gottlieb, Aryeh Kontorovich, and Robert Krauthgamer. Adaptive metric dimensionality
reduction. Theoretical Computer Science, 620:105?118, 2016.
[20] Lee-Ad Gottlieb, Aryeh Kontorovich, and Pinhas Nisnevitch. Near-optimal sample compression
for nearest neighbors. In Neural Information Processing Systems (NIPS), 2014.
[21] Lee-Ad Gottlieb, Aryeh Kontorovich, and Pinhas Nisnevitch. Nearly optimal classification for
semimetrics (extended abstract AISTATS 2016). Journal of Machine Learning Research, 2017.
[22] Thore Graepel, Ralf Herbrich, and John Shawe-Taylor. PAC-Bayesian compression bounds on
the prediction error of learning algorithms for classification. Machine Learning, 59(1):55?76,
2005.
[23] Peter Hall and Kee-Hoon Kang. Bandwidth choice for nonparametric classification. Ann.
Statist., 33(1):284?306, 02 2005.
[24] Olav Kallenberg. Foundations of modern probability. Second edition. Probability and its
Applications. Springer-Verlag, 2002.
[25] Aryeh Kontorovich, Sivan Sabato, and Ruth Urner. Active nearest-neighbor learning in metric
spaces. In Advances in Neural Information Processing Systems, pages 856?864, 2016.
[26] Aryeh Kontorovich, Sivan Sabato, and Roi Weiss. Nearest-neighbor sample compression:
Efficiency, consistency, infinite dimensions. CoRR, abs/1705.08184, 2017.
[27] Aryeh Kontorovich and Roi Weiss. A Bayes consistent 1-NN classifier. In Artificial Intelligence
and Statistics (AISTATS 2015), 2014.
[28] Aryeh Kontorovich and Roi Weiss. Maximum margin multiclass nearest neighbors. In International Conference on Machine Learning (ICML 2014), 2014.
[29] Robert Krauthgamer and James R. Lee. Navigating nets: Simple algorithms for proximity
search. In 15th Annual ACM-SIAM Symposium on Discrete Algorithms, pages 791?801, January
2004.
[30] Sanjeev R. Kulkarni and Steven E. Posner. Rates of convergence of nearest neighbor estimation
under arbitrary sampling. IEEE Trans. Inform. Theory, 41(4):1028?1039, 1995.
[31] Nick Littlestone and Manfred K. Warmuth. Relating data compression and learnability. unpublished, 1986.
[32] James R. Munkres. Topology: a first course. Prentice-Hall, Inc., Englewood Cliffs, N.J., 1975.
[33] Vladimir Pestov. On the geometry of similarity search: dimensionality curse and concentration
of measure. Inform. Process. Lett., 73(1-2):47?51, 2000.
[34] Vladimir Pestov. Is the k-NN classifier in high dimensions affected by the curse of dimensionality? Comput. Math. Appl., 65(10):1427?1437, 2013.
10

[35] David Preiss. Invalid Vitali theorems. Abstracta. 7th Winter School on Abstract Analysis, pages
58?60, 1979.
[36] David Preiss. Gaussian measures and the density theorem. Comment. Math. Univ. Carolin.,
22(1):181?193, 1981.
[37] Demetri Psaltis, Robert R. Snapp, and Santosh S. Venkatesh. On the finite sample performance
of the nearest neighbor classifier. IEEE Transactions on Information Theory, 40(3):820?837,
1994.
[38] Walter Rudin. Principles of mathematical analysis. McGraw-Hill Book Co., New York, third
edition, 1976. International Series in Pure and Applied Mathematics.
[39] Walter Rudin. Real and Complex Analysis. McGraw-Hill, 1987.
[40] Richard J. Samworth. Optimal weighted nearest neighbour classifiers. Ann. Statist., 40(5):2733?
2763, 10 2012.
[41] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to
Algorithms. Cambridge University Press, 2014.
[42] John Shawe-Taylor, Peter L. Bartlett, Robert C. Williamson, and Martin Anthony. Structural
risk minimization over data-dependent hierarchies. IEEE Transactions on Information Theory,
44(5):1926?1940, 1998.
[43] Robert R. Snapp and Santosh S. Venkatesh. Asymptotic expansions of the k nearest neighbor
risk. Ann. Statist., 26(3):850?878, 1998.
[44] Charles J. Stone. Consistent nonparametric regression. The Annals of Statistics, 5(4):595?620,
1977.
[45] Jaroslav Ti?er. Vitali covering theorem in Hilbert space. Trans. Amer. Math. Soc., 355(8):3277?
3289, 2003.
[46] Kilian Q. Weinberger and Lawrence K. Saul. Distance metric learning for large margin nearest
neighbor classification. Journal of Machine Learning Research, 10:207?244, 2009.
[47] Lin Cheng Zhao. Exponential bounds of mean error for the nearest neighbor estimates of
regression functions. J. Multivariate Anal., 21(1):168?178, 1987.

11

"
2016,Efficient High-Order Interaction-Aware Feature Selection Based on Conditional Mutual Information,Poster,6584-efficient-high-order-interaction-aware-feature-selection-based-on-conditional-mutual-information.pdf,"This study introduces a novel feature selection approach CMICOT, which is a further evolution of filter methods with sequential  forward selection (SFS) whose scoring functions are based on conditional mutual information (MI). We state and study a novel saddle point (max-min) optimization problem to build a scoring function that is able to identify joint interactions between several  features. This method fills the gap of MI-based SFS techniques with high-order dependencies. In this high-dimensional case, the estimation of MI has prohibitively high sample complexity. We mitigate this cost using a greedy approximation and binary representatives what makes our technique able to be effectively used. The superiority of our approach is demonstrated by comparison with recently proposed interaction-aware filters and several interaction-agnostic state-of-the-art ones on ten publicly available benchmark datasets.","Efficient High-Order Interaction-Aware Feature
Selection Based on Conditional Mutual Information
Alexander Shishkin, Anastasia Bezzubtseva, Alexey Drutsa,
Ilia Shishkov, Ekaterina Gladkikh, Gleb Gusev, Pavel Serdyukov
Yandex; 16 Leo Tolstoy St., Moscow 119021, Russia
{sisoid,nstbezz,adrutsa,ishfb,kglad,gleb57,pavser}@yandex-team.ru

Abstract
This study introduces a novel feature selection approach CMICOT, which is a
further evolution of filter methods with sequential forward selection (SFS) whose
scoring functions are based on conditional mutual information (MI). We state and
study a novel saddle point (max-min) optimization problem to build a scoring
function that is able to identify joint interactions between several features. This
method fills the gap of MI-based SFS techniques with high-order dependencies.
In this high-dimensional case, the estimation of MI has prohibitively high sample
complexity. We mitigate this cost using a greedy approximation and binary representatives what makes our technique able to be effectively used. The superiority of
our approach is demonstrated by comparison with recently proposed interactionaware filters and several interaction-agnostic state-of-the-art ones on ten publicly
available benchmark datasets.

1

Introduction

Methods of feature selection is an important topic of machine learning [8, 2, 17], since they improve
performance of learning systems while reducing their computational costs. Feature selection methods
are usually grouped into three main categories: wrapper, embedded, and filter methods [8]. Filters are
computationally cheap and are independent of a particular learning model that make them popular
and broadly applicable. In this paper, we focus on most popular filters, which are based on mutual
information (MI) and apply the sequential forward selection (SFS) strategy to obtain an optimal
subset of features [17]. In such applications as web search, features may be highly relevant only
jointly (having a low relevance separately). A challenging task is to account for such interactions [17].
Existing SFS-based filters [18, 3, 24] are able to account for interactions of only up to 3 features.
In this study, we fill the gap in the absence of effective SFS-based filters accounting for feature
dependences of higher orders. A search of t-way interacting features is turned into a novel saddle
point (max-min) optimization problem for MI of the target variable and the candidate feature with
its complementary team conditioned on its opposing team of previously selected features. We show
that, on the one hand, the saddle value of this conditional MI is a low-dimensional approximation
of the CMI score1 and, on the other hand, solving that problem represents two practical challenges:
(a) prohibitively high computational complexity and (b) sample complexity, a larger number of
instances required to accurately estimate the MI. These issues are addressed by two novel techniques:
(a) a two stage greedy search for the approximate solution of the above-mentioned problem whose
computational complexity is O(i) at each i-th SFS iteration; and (b) binary representation of features
that reduces the dimension of the space of joint distributions by a factor of (q/2)2t for q-value
features. Being reasonable and intuitive, these techniques together constitute the main contribution of
our study: a novel SFS method CMICOT that is able to identify joint interactions between multiple
1

The CMI filter is believed to be a ?north star"" for vast majority of the state-of-the-art filters [2].

30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

features. We also empirically validate our approach with 3 state-of-the-art classification models on
10 publicly available benchmark datasets and compare it with known interaction-aware SFS-based
filters and several state-of-the-art ones.

2

Preliminaries and related work

Information-theoretic measures. The mutual information (MI) of two random variables f and
g is defined as I(f ; g) = H(f ) + H(g) ? H(f, g), where H(f ) = ?E [log P(f )] is Shannon?s
entropy [4]2 . The conditional mutual information of two random variables f and g given the variable
h is I(f ; g | h) = I(f ; g, h) ? I(f ; h). The conditional MI measures the amount of additional
information about the variable f carried by g compared to the variable h. Given sample data, entropy
(and, hence, MI and conditional MI) of discrete variables could be simply estimated using the
empirical frequencies (the point estimations) [15] or in a more sophisticated way (e.g., by means of
the Bayesian framework [10]). More details on different entropy estimators can be found in [15].
Background of the feature selection based on MI. Let F be a set of features that could be used by
a classifier to predict a variable c representing a class label. The objective of a feature selection (FS)
procedure is to find a feature subset S o ? F of a given size k ? N that maximizes its joint MI with
the class label c, i.e., S o = argmax{S:S?F,|S|?k} I(c; S). In our paper, we focus on this simple but
commonly studied FS objective in the context of MI-based filters [2], though there is a wide variety
of other definitions of optimal subset of features [17] (e.g., the all-relevant problem [13]).
In order to avoid an exhaustive search of an optimal subset S , most filters are based on sub-optimal
search strategies. The most popular one is the sequential forward selection (SFS) [20, 23, 17], which
starts with an empty set (S0 := ?) and iteratively increases it by adding one currently unselected
feature on each step (Si := Si?1 ? {fi }, i = 1, . . . , k, and S o := Sk ). The feature fi is usually
selected by maximizing a certain scoring function (also called score) Ji (f ) that is calculated with
respect to currently selected features Si?1 , i.e., fi := argmaxf ?F \Si?1 Ji (f ).
A trivial feature selection approach is to select top-k features in terms of their MI with the class label
c [12]. This technique is referred to as MIM [2] and is a particular case of the SFS strategy based
on score JiMIM (f ) := I(c; f ). Note that the resulting set may contain a lot of redundant features,
since the scoring function JiMIM (?) is independent from already selected features Si?1 . Among
methods that take into account the redundancy between features [2, 17], the most popular and widely
applicable ones are MIFS [1], JMI [21, 14], CMIM [6, 19], and mRMR [16]. Brown et al. [2] unified
these techniques under one framework, where they are different low-order approximations of CMI
feature selection approach. This method is based on the score equal to MI of the label with the
evaluated feature conditioned on already selected features:
JiCMI (f ) := I(c; f | Si?1 ).

(1)

The main drawback of CMI is the sample complexity, namely, the exponential growth of the dimension
of the distribution of the tuple (c, f, Si?1 ) with respect to i. The larger the dimension is, the larger
number of instances is required to accurately estimate the conditional MI in Eq. (1). Therefore, this
technique is not usable in the case of small samples and in the cases, when a large number of features
should be selected [2]. This is also observed in our experiment in Appendix.F2, where empirical
score estimated over high dimensions results in drastically low performance of CMI.
Thus, low-dimensional approximations of Eq. (1) are more preferable in practice. For instance, the
CMIM approach approximates Eq. (1) by
JiCMIM (f ) := min I(c; f | g),
g?Si?1

(2)

i.e., one replaces the redundancy of f with respect to the whole subset Si?1 by the worst redundancy
with respect to one feature from this subset. The other popular methods (mentioned above) are
particular cases of the following approximation of the I(c; f | Si?1 ):

X 
Ji?,? (f ) := I(c; f ) ?
?I(g; f ) ? ?I(g; f | c) ,
(3)
g?Si?1
2

From here on in the paper, variables separated by commas
 or a set of variables in MI expressions are treated as
one random vector variable, e.g., I(f ; g, h) := I f ; (g, h) and, for F = ?n
i=1 {fi }, I(f ; F ) := I(f ; f1 , .., fn ).

2

e.g., MIFS (? ? [0, 1], ? = 0), mRMR (? = |Si?1 |?1 , ? = 0), and JMI (? = ? = |Si?1 |?1 ).
An important but usually neglected aspect in FS methods is feature complementariness [8, 3] (also
known as synergy [24] and interaction [11]). In general, complementary features are those that
appear to have low relevance to the target class c individually, but whose combination is highly
relevant [25, 24]. In the next subsection, we provide a brief overview of existing studies on filters that
take into account feature interaction. A reader interested in a formalized concept of feature relevance,
redundancy, and interaction is referred to [11] and [24].
Related work on interaction-aware filters. To the best of our knowledge, existing interaction-aware
filters that utilize the pure SFS strategy with a MI-based scoring function are the following ones.
RelaxMRMR [18] is a modification of the mRMR method,Pwhose scoring function in Eq. (3) was
refined by adding the three-way feature interaction terms h,g?Si?1 ,h6=g I(f ; h | g). The method
RCDFS [3] is a special case of Eq. (3), where ? = ? are equal to a transformation of the standard
deviation of the set {I(f ; h)}h?Si?1 . The approach IWFS [24] is based on the following idea: at
each step i, for each unselected feature f ? F \ Si , one calculates the next step score Ji+1 (f ) as
the current score Ji (f ) multiplied by a certain measure of interaction between this feature f and the
feature fi selected at the current step. Both RCDFS and IWFS can catch dependences between no
more than 2 features, while RelaxMRMR is able to identify an interaction of up to 3 features, but
its score?s computational complexity is O(i2 ) what makes it unusable in real applications. All these
methods could not be straightforwardly improved to incorporate interactions of a higher order.
In our study, we propose a general methodology that fills the gap between the ideal (?oracle"") but
infeasible CMI method, which takes all interactions into account, and the above-described methods
that account for up to 3 interacting features. Our method can be effectively used in practice with its
score?s computational complexity of a linear growth O(i) (as in most state-of-the-art SFS-filters).

3

Proposed feature selection

In this section, we introduce a novel feature selection approach based on the SFS strategy whose
score is built by solving from a novel optimization problem and comprises two novel techniques that
makes the approach efficient and effective in practice.
3.1

Score with t-way interacted complementary and opposing teams

Our FS method has a parameter t ? N that is responsible for the desirable number of features whose
mutual interaction (referred to as a t-way feature interaction) should be taken into account by the
scoring function Ji (?). We build the scoring function according to the following intuitions.
First, the amount of relevant information carried by a t-way interaction of a candidate feature f has the
form I(c; f, H) for some set of features H of size |H| ? t ? 1. Second, we remove the redundant part
of this information w.r.t. the already selected features Si?1 and obtain the non-redundant information
part I(c; f, H | Si?1 ). Following the heuristic of the CMIM method, this could be approximated
by use of a small subset G ? Si?1 , |G| ? s ? N, i.e., by the low-dimensional approximation
min{G?Si?1 ,|G|?s} I(c; f, H | G) (assuming s  i). Third, since in the SFS strategy one has to
select only one feature at an iteration i, this approximated additional information of the candidate f
with H w.r.t. Si?1 will be gained by with the feature f at this SFS iteration only if all complementary
features H have been already selected (i.e., H ? Si?1 ). In this way, the score of the candidate f
should be equal to the maximal additional information estimated using above reasoning, i.e., we
come to the score which is a solution of the following saddle point (max-min) optimization problem
?

(t,s)

Ji

min I(c; f, H | G).

(f ) := max

H?Si?1 , G?Si?1 ,
|H|?t?1 |G|?s

(4)

We refer to the set {f } ? Hfo , where Hfo is an optimal set H in Eq. (4), as an optimal complementary
team of the feature f ? F \ Si?1 , while an optimal set G in Eq. (4) is referred to as an optimal
opposing team to this feature f (and, thus, to its complementary team as well) and is denoted by Gof .
The described approach is inspired by methods of greedy learning of ensembles of decision trees [7],
where an ensemble of trees is built by sequentially adding a decision tree that maximizes the gain in
learning quality. In this way, our complementary team corresponds to the features used in a candidate
3

decision tree, while our opposing team corresponds to the features used to build previous trees in the
ensemble. Since they are already selected by SFS, they are expectedly stronger than f and we can
assume that, at the early iterations, a greedy machine learning algorithm would more likely use these
features rather than the new feature f once we add it into the feature set. So, Eq. (4) tries to mimic
the maximal amount of information that feature f can provide additionally to the worst-case baseline
built on Si?1 .
?

(t,s)

from Eq. (4) is equal to the score JiCMI from Eq. (1).

Statement 1. For t, s + 1 ? i, the score Ji

?

(t,s)

The proof?s sketch is: (a) justify the identity Ji (f ) = maxH?Si?1 minG?Si?1 \H I(c; f | H, G)
for t, s + 1 ? i; (b) get a contradiction to the assumption that there are no optimal subsets H and G
such that Si?1 = H ? G. Detailed proof of Statement 1 is given in Appendix A. Thus, we argue that
?
(t,s)
the score Ji
from Eq. (4) is a low-dimensional approximation of the CMI score JiCMI .3 .
The score from Eq. (4) is of a general nature and reasonable, but, to the best of our knowledge, was
never considered in existing studies. However, this score is not suitable for effective application,
since it suffers from two practical issues:
(PI.a) computational complexity: efficient search of optimal sets Hfo and Gof in Eq. (4);
(PI.b) sample complexity: accurate estimation of the MI over features with a large dimension of its
joint distribution.
We address these research problems and propose the following solutions to them: in Sec. 3.2, the
issue (PI.a) is overcome in a greedy fashion, while, in Sec. 3.3,the issue (PI.b) is mitigated by means
of binary representatives.
3.2

Greedy approximation of the score

Note that an exhaustive search of a saddle point in Eq. (4) requires
?

i?1
t?1



i?1
s



MI calculations

(t,s)
Ji

that can make calculation of the scoring function
infeasible at a large iteration i even for low
team sizes t, s > 1. In order to overcome this issue, we propose the following greedy search for
sub-optimal complementary and opposing teams.
At the first stage, we start from a greedy search of a sub-optimal set H that cannot be done straightforwardly, since Eq. (4) comprises both max and min operators. The latter one requires a search of
an optimal G that we want do at the second stage (after H). Hence, the double optimization problem
needs to be replaced by a simpler one which does not utilize a search of G.
Proposition 1. (1) For any H ? Si?1 such that |H| ? s, the following holds
I(c; f, H | G) ? I(c; f | H).

min
G?Si?1 ,|G|?s

(5)

(2) If s ? t ? 1, then the score given by the following optimization problem
I(c; f | H),

max

(6)

H?Si?1 ,|H|?t?1
?

(t,s)

is an upper bound for the score Ji

from Eq. (4).

The optimization problem Eq. (6) seems reasonable due to the following properties: (a) in fact, the
search of H in Eq. (6) is maximization of the additional information carried out by the candidate f
w.r.t. no more than t ? 1 already selected features from Si?1 ; (b) if a candidate f is a combination of
features from H, then the right hand side in Eq. (5) is 0 and the inequality becomes an equality.
So, we greedily search the maximum in Eq. (6), obtaining the (greedy) complementary team {f }?Hf ,
where Hf := {h1 , . . . , ht?1 } is defined by4
hj := argmax I(c; f | h1 , . . . , hj?1 , h),

j = 1, . . . , t ? 1.

(7)

h?Si?1
3
4

Moreover, the CMIM score from Eq. (2) is a special case of Eq. (4) with s = t = 1 and restriction G 6= ?.
If several elements provide an optimum (the case of ties), then we randomly select one of them.

4

At the second stage, given the complementary team {f } ? Hf , we greedily search the (greedy)
opposing team Gf := {g1 , . . . , gs } in the following way:
gj := argmin I(c; f, h1 , . . . , hmin{j,t}?1 | g1 , . . . , gj?1 , g),

j = 1, . . . , s.

(8)

g?Si?1
?

(t,s)

Finally, given the teams {f } ? Hf and Gf , we get the following greedy approximation of Ji
(t,s)
Ji (f )

:= I(c; f, Hf | Gf ).

(f ):
(9)

This score requires (t + s ? 1)i MI calculations (see Eq. (7)?(9)), which is a linear dependence
on an iteration i as in the most state-of-the-art SFS-based filters [2]. Thus, we built an efficient
?
(t,s)
approximation of the score Ji
and resolve the issue (PI.a).
Note that we have two options on the minimization stage: either to search among all members of the
set Hf at each step (as in Eq. (A.7) in Appendix A.3), or (what we actually do in Eq. (8)) to use only
a few first members of Hf . The latter option demonstrates noticeably better MAUC performance and
also results in 0 score for a feature that is a copy of an already selected one (Proposition 2), while the
former does not (Remark A.2 in Appendix A.3). That is why we chose this option.
Proposition 2. Let s ? t and a candidate feature f ? F \ Si?1 be such that its copy f? ? f is
(t,s)
already selected f? ? Si?1 , then, in the absence of ties in Eq. (8) for j ? t, the score J
(f ) = 0.
i

(t,s)

Proposition 2 shows that the FS approach based on the greedy score Ji (f ) remains conservative,
i.e., a copy of an already selected feature will not be selected, despite that it exploits sub-optimal
?
(t,s)
teams in contrast to the FS approach based on the optimal score Ji (f ).
3.3

Binary representatives of features

As it is mentioned in Sec. 2, a FS method that is based on calculation of MI over more than three
features is usually not popular in practice, since a large number of features implies a large dimension
of their joint distribution that leads to a large number of instances required to accurately estimate the
?
(t,s)
(t,s)
MI [2]. Both our optimal score Ji
and our greedy one Ji
suffer from the same issue (PI.b) as
well, since they exploit high-dimensional MI in Eq.(4) and Eq. (7)?(9). For instance, if we deal with
binary classification and each feature in F has q unique values (e.g., continuous features are usually
preprocessed into discrete variables with q ? 5 [18]), then the dimension of the joint distribution of
features in Eq. (9) is equal to 2 ? q t+s (e.g., ? 4.9 ? 108 for t = s = 6, q = 5). In our method, we
cannot reduce the number of features used in MIs (since t-way interaction constitutes the key basis
of our approach), but we can mitigate the effect of the sample complexity by the following novel
(t,s)
technique, which we demonstrate on our greedy score Ji . Let F consists of discrete features5 .
Definition 1. For each discrete feature f ? F , we denote by B[f ] the binary transformation of f ,
i.e., the set of binary variables (referred to as the binary representatives (BR) of f ) that constitute all
together a vector containing the same information as f 6 . For S
any subset F 0 ? F , the set of binary
representatives of all features from F 0 is denoted by B[F 0 ] = f ?F 0 B[f ].
Then, we replace all features by their binary representatives at each stage of our score calculation.
Namely, in Eq. (7) and Eq. (8), (a) the searches are performed for each binary representative b ? B[f ]
instead of f ; (b) the set Hbbin of the complementary team is found among B[Si?1 ] ? B[f ]; while
(c) the opposing team Gbin
is found among B[Si?1 ] (exact formulas could be found in Algorithm 1,
b
lines 12 and 15). Finally, the score of a feature f in this FS approach based on binary representatives
is defined as the best score among the binary representatives B[f ] of the candidate f :
(t,s),bin

Ji

(f ) := max I(c; b, Hbbin | Gbin
b ).
b?B[f ]

(10)

Note that, in the previous example with a binary target variable c and q-value features, the dimension
(t,s),bin
of the joint distribution of binary representatives used to calculate MI in Ji
is equal to 21+t+s ,
5
If there is a non-discrete feature, then we apply a discretization (e.g., by equal-width, equal-frequency
binnings [5], MDL [22, 3], etc.), which is the state-of-the-art preprocessing of continuous features in filters.
6
For instance, for f with values in {xl }ql=1 , one could take B[f ] = {I{f =xl } }q?1
l=1 , where IX is X ?s indicator,
or take bits of a binary encoding of {xl }ql=1 that is a smallest set (i.e., |B[f ]| = dlog2 qe) among possible B[f ].

5

Algorithm 1 Pseudo-code of the CMICOT feature selection method (an implementation of this
algorithm is available at https://github.com/yandex/CMICOT).
1: Input: F ? the set of all features; B[f ], f ? F, ? set of binary representatives built on f ;
2: c ? the target variable; k ? N ? the number of features to be selected;
3: t ? N, s ? Z+ ? the team sizes (parameters of the algorithm);
4: Output: S ? the set of selected features;
5: Initialize:
6: fbest := argmaxf ?F maxb?B[f ] I(c; b);
// Select the first feature
7: S := {fbest }; S bin := B[fbest ];
8: while |S| < k and |F \ S| > 0 do
9:
for f ? F \ S do
10:
for b ? B[f ] do
11:
for j := 1 to t ? 1 do
12:
hj := argmaxh?S bin ?B[f ] I(c; b | h1 , .., hj?1 , h); // Search for complementary feat.
13:
end for
14:
for j := 1 to s do
15:
gj := argming?S bin I(c; b, h1 , .., hmin{j,t}?1 | g1 , .., gj?1 , g); // Search for opp. feat.
16:
end for
17:
Ji [b] := I(c; b, h1 , .., ht?1 | g1 , .., gs ); // Calculate the score of the binary rep. b
18:
end for
19:
Ji [f ] := maxb?B[f ] Ji [b]; // Calculate the score of the feature f
20:
end for
21:
fbest := argmaxf ?F \S Ji [f ];
// Select the best candidate feature at the current step
bin
bin
22:
S := S ? {fbest }; S
:= S ? B[fbest ];
23: end while
(t,s)

which is (q/2)t+s times smaller (the dimension reduction rate) than for the MI in Ji . For
instance, for t = s = 6, q = 5, the MI from Eq. (10) deals with ? 8.2 ? 103 dimensions, which is
? 6 ? 104 times lower than? 4.9 ? 108 ones for the MI from Eq. (9). The described technique has been
inspired by the intuition that probably two binary representatives of two different features interact on
average better than two binary representatives of one feature (see App. A.5.1). Therefore, we believe
that the BR modification retains the score?s awareness to the most interactions between features.
Surely, on the one hand, the BR technique can also be applied to any state-of-the-art SFS-filter [2] or
any existing interaction-aware one (RelaxMRMR [18], RCDSFS [3], and IWFS [24]), but the effect
on them will not be striking breakthrough, since these filters exploit no more than 3 features in one
MI, and the dimension reduction rate will thus be not more than (q/2)3 (e.g., ? 15.6 for q = 5). On
the other hand, this technique is of a general nature and represents a self-contained contribution to
ML community, since it may be applied with noticeable profit to SFS-based filters with MIs of higher
orders (possibly not yet invented).
3.4

CMICOT feature selection method

We summarize Sec. 3.1?Sec. 3.3 in our novel feature selection method that is based on sequential
forward selection strategy with the scoring function from Eq. (10). We refer to this FS method as
CMICOT (Conditional Mutual Information with Complementary and Opposing Teams) and present
its pseudo-code in Algorithm 1, which has a form of a SFS strategy with a specific algorithm to
calculate the score (lines 10?19). In order to benefit from Prop. 1 and 2, one has to select s ? t, and,
for simplicity, from here on in this paper we consider only equally limited teams, i.e., t = s.
Proposition 3. Let |B[f ]| ? ?, ?f ? F , |F | ? M , and entropies in MIs are calculated over
(t,t),bin
N instances, then O(i? 2 t2 N ) simple operations are needed to calculate the score Ji
and
2 2 2
O(k ? t M N ) simple operations are needed to select top-k features by CMICOT from Alg. 1.
Let us remind how each of our techniques contributes to the presented above computational complexity
of the score. First, the factor t2 is an expected payment for the ability to be aware of t-way interactions
(Sec. 3.1). Second, the two stage greedy technique from Sec. 3.2 makes the score? computational
complexity linearly depend on a SFS iteration i. Third, utilization of the BR technique from Sec. 3.3,
on the one hand, seems to increase the computational complexity by the factor ? 2 , but, on the other
6

hand, we know that it drastically reduces the sample complexity (i.e., the number of instances required
to accurately estimate the used MIs). For simplicity, let us assume that each feature has 2? values and
is transformed to ? binary ones. If we do not use the BR technique, the complexity will be lower by
the factor ? 2 for the same number of instances N , but estimation of the MIs will require (2? /2)2t
times more instances to achieve the same level of accuracy as with the BRs. Hence, the BR technique
actually reduces the computational complexity by the factor 22t(??1) /? 2 . Note that the team size
t can be used to trade off between the number of instances available in the sample dataset and the
maximal number of features whose joint interaction could be taken into account in a SFS manner.
Finally, for a given dataset and a given team size t, the score?s computational complexity linearly
depends on the i-th SFS iteration, on the one hand, as in most state-of-the-art SFS-filters [2] like
CMIM, MIFS, mRMR, JMI, etc. (see Eq. (2)?(3)). On the other hand, scores of existing interactionaware ones have either the same (O(i) for RCDFS [3]), or higher (O(M ? i) for IWFS [24] and
O(i2 ) for RelaxMRMR [18]) order of complexity w.r.t. i. Thus, we conclude that our FS method is
not inferior in efficiency to all baseline filters, but is able to identify feature dependences of higher
orders than these baselines.

4

Experimental evaluation

We compare our CMICOT approach with (a) all known interaction-aware SFS-based filters (RelaxMRMR [18], IWFS [24], and RCDFS [3]); (b) the state-of-the-art filters [2] (MIFS, mRMR, CMIM,
JMI, DISR, and FCBF (CBFS)); (c) and the idealistic but practically infeasible CMI method (see
Sec. 2 and [2]). In our experiments, we consider t = 1, . . . , 10 to validate that CMICOT is able to
detect interactions of a considerably higher order than its competitors.
Evaluation on synthetic data. First, we study the ability to detect high-order feature dependencies
using synthetic datasets where relevant and interacting features are a priory known. A synthetic
dataset has feature set F , which contains a group of jointly interacting relevant features Fint , and a its
target c is a deterministic function of Fint for half of examples (|F \Fint | = 15 and |Fint | = 2, . . . , 11
in our experiments). The smaller k0 = min{k | Fint ? Sk }, the more effective the considered FS
method, since it builds the smaller set of features needed to construct the best possible classifier.
We conduct an experiment where, first, we randomly sample 100 datasets from the predefined joint
distribution (more details in Appendix C). Second, we calculate k0 for each of studied FS methods
on these datasets. Finally, we average k0 over the datasets and present the results in Figure 1 (a). We
see, first, that CMICOT with t ? |Fint | significantly outperforms all baselines, except the idealistic
CMI method whose results are similar to CMICOT. This is expected, since CMI is infeasible only for
large k, and, in App. F.2, we show that CMICOT is the closest approximation of true CMI among
all baselines. Second, the team size t definitely responds to the number of interacted features, that
provides an experimental evidence for ability of CMICOT to identify high-order feature interactions.
Evaluation on benchmark real data. Following the state-of-the-art practice [6, 22, 2, 18, 24, 3],
we conduct an extensive empirical evaluation of the effectiveness of our CMICOT approach on
10 large public datasets from the UCI ML Repo (that include the NIPS?2003 FS competition) and
one private dataset from one of the most popular search engines7 . We employ three state-of-theart classifiers: Naive Bayes Classifier (NBC), k-Nearest Neighbor (kNN), and AdaBoost [6] (see
App. B). Their performance on a set of features is measured by means of AUC [2] (MAUC [9])
for a binary (multi-class) target variable. First, we apply each of the FS methods to select top-k
features Sk for each dataset and for k = 1, .., 50 [2, 24, 3]. Given k ? {1, .., 50}, a dataset, and a
certain classifier, we measure the performance of a FS method (1) in terms of the (M)AUC of the
classifier built on the selected features Sk (2) and in terms of the rank of the FS method among
the other FS methods w.r.t. (M)AUC. The resulting (M)AUC and rank averaged over all datasets
are shown in Fig. 1(b,c) for kNN and AdaBoost. From these figures we see that our CMICOT for
t = 68 method noticeably outperforms all baselines for the classification models kNN and AdaBoost9
starting from approximately k = 10. We reason this frontier by the size of the teams in CMICOT
7
The number of features, instances, and target classes varies from 85 to 5000, from 452 to 105 , and from 2
to 26 respectively. More datasets? characteristics and preprocessing can be found in Appendix D.
8
Our experimentation on CMICOT with different t = 1, . . . , 10 on our datasets showed that t = 5 and 6 are
the most reasonable in terms of classifier performance (see Appendix E.1.1).
9
The results of CMICOT on NBC classifier are similar to the ones of other baselines. This is expected
since NBC does not exploit high-order feature dependences, which is the key advantage of CMICOT. Note that

7

Figure 1: (a) Comparison of the performance of SFS-based filters in terms of average k0 on synthetic
datasets. (b) Average values of (M)AUC for compared FS methods and (c) their ranks w.r.t. (M)AUC
k = 1, .., 50 and for the kNN and AdaBoost classification models over all datasets (see also App. C,E).
method, which should select different teams more likely when |Si?1 | > 2t (= 12 for t = 6). The
curves on Fig. 1 (b,c) are obtained over a test set, while a 10-fold cross-validation [2, 18] is also
applied for several key points (e.g. k = 10, 20, 50) to estimate the significance of differences in
classification quality. The detailed results of this CV for k = 50 on representative datasets are given
in Appendix E.2. A more comprehensive details on these and other experiments are in App. E and F.
We find that our approach either significantly outperforms baselines (most one for kNN and AdaBoost),
or have non-significantly different difference with the other (most one for NBC). Note that the
interaction awareness of RelaxMRMR, RCDFS and IWFS is apparently not enough to outperform
CMIM, our strongest competitor. In fact, there is no comparison of RelaxMRMR and IWFS with
CMIM in [3, 24], while RCDFS is outperformed by CMIM on some datasets including the only one
utilized in both [18] and our work. One compares CMICOT with and without BR technique: on
the one hand, we observed that CMICOT without BRs loses in performance to the one with BRs
on the datasets with non-binary features, that emphasizes importance of the problem (PI.b); on the
other hand, results on binary datasets (poker, ranking, and semeion; see App. E), where the CMICOT
variants are the same, the effectiveness of our approach separately to the BR technique is established.

5

Conclusions

We proposed a novel feature selection method CMICOT that is based on sequential forward selection
and is able to identify high-order feature interactions. The technique based on a two stage greedy
search and binary representatives of features makes our approach able to be effectively used on
datasets of different sizes for restricted team sized t. We also empirically validated our approach
for t up to 10 by means of 3 state-of-the-art classification models (NBC, kNN, and AdaBoost) on
10 publicly available benchmark datasets and compared it with known interaction-aware SFS-based
filters (RelaxMRMR, IWFS, and RCDFS) and several state-of-the-art ones (CMIM, JMI, CBFS,
and others). We conclude that our FS algorithm, unlike all competitor methods, is capable to detect
interactions between up to t features. The overall performance of our algorithm is the best among the
state-of-the-art competitors.
Acknowledgments
We are grateful to Mikhail Parakhin for important remarks which resulted in significant improvement
of the paper presentation.
RelaxMRMR also showed its poorest performance on NBC in [18], while IWFS and RCDFS in [3, 24] didn?t
consider NBC at all.

8

References
[1] R. Battiti. Using mutual information for selecting features in supervised neural net learning. Neural
Networks, IEEE Transactions on, 5(4):537?550, 1994.
[2] G. Brown, A. Pocock, M.-J. Zhao, and M. Luj?n. Conditional likelihood maximisation: a unifying
framework for information theoretic feature selection. JMLR, 13(1):27?66, 2012.
[3] Z. Chen, C. Wu, Y. Zhang, Z. Huang, B. Ran, M. Zhong, and N. Lyu. Feature selection with redundancycomplementariness dispersion. arXiv preprint arXiv:1502.00231, 2015.
[4] T. M. Cover and J. A. Thomas. Elements of information theory. John Wiley & Sons, 2012.
[5] J. Dougherty, R. Kohavi, M. Sahami, et al. Supervised and unsupervised discretization of continuous
features. In ICML, volume 12, pages 194?202, 1995.
[6] F. Fleuret. Fast binary feature selection with conditional mutual information. JMLR, 5:1531?1555, 2004.
[7] J. H. Friedman. Greedy function approximation: a gradient boosting machine. Annals of Statistics, 2001.
[8] I. Guyon and A. Elisseeff. An introduction to variable and feature selection. JMLR, 3:1157?1182, 2003.
[9] D. J. Hand and R. J. Till. A simple generalisation of the area under the roc curve for multiple class
classification problems. Machine Learning, 2001.
[10] M. Hutter. Distribution of mutual information. NIPS, 1:399?406, 2002.
[11] A. Jakulin and I. Bratko. Analyzing attribute dependencies. Springer, 2003.
[12] D. D. Lewis. Feature selection and feature extraction for text categorization. In Proceedings of the
workshop on Speech and Natural Language, pages 212?217. ACL, 1992.
[13] J. Liu, C. Zhang, C. A. McCarty, P. L. Peissig, E. S. Burnside, and D. Page. High-dimensional structured
feature screening using binary markov random fields. In AISTATS, pages 712?721, 2012.
[14] P. E. Meyer, C. Schretter, and G. Bontempi. Information-theoretic feature selection in microarray data
using variable complementarity. IEEE Journal of STSP, 2(3):261?274, 2008.
[15] L. Paninski. Estimation of entropy and mutual information. Neural comput., 15(6):1191?1253, 2003.
[16] H. Peng, F. Long, and C. Ding. Feature selection based on mutual information criteria of max-dependency,
max-relevance, and min-redundancy. PAMI, 27(8):1226?1238, 2005.
[17] J. R. Vergara and P. A. Est?vez. A review of feature selection methods based on mutual information.
Neural Computing and Applications, 24(1):175?186, 2014.
[18] N. X. Vinh, S. Zhou, J. Chan, and J. Bailey. Can high-order dependencies improve mutual information
based feature selection? Pattern Recognition, 2015.
[19] G. Wang and F. H. Lochovsky. Feature selection with conditional mutual information maximin in text
categorization. In ACM CIKM, pages 342?349. ACM, 2004.
[20] A. W. Whitney. A direct method of nonparametric measurement selection. Computers, IEEE Transactions
on, 100(9):1100?1103, 1971.
[21] H. Yang and J. Moody. Feature selection based on joint mutual information. In Proceedings of international
ICSC symposium on advances in intelligent data analysis, pages 22?25. Citeseer, 1999.
[22] L. Yu and H. Liu. Efficient feature selection via analysis of relevance and redundancy. JMLR, 5:1205?1224,
2004.
[23] M. Zaffalon and M. Hutter. Robust feature selection by mutual information distributions. In UAI, pages
577?584. Morgan Kaufmann Publishers Inc., 2002.
[24] Z. Zeng, H. Zhang, R. Zhang, and C. Yin. A novel feature selection method considering feature interaction.
Pattern Recognition, 48(8):2656?2666, 2015.
[25] Z. Zhao and H. Liu. Searching for interacting features in subset selection. Intelligent Data Analysis,
13(2):207?228, 2009.

9

"
2017,Multi-output Polynomial Networks and Factorization Machines,Poster,6927-multi-output-polynomial-networks-and-factorization-machines.pdf,"Factorization machines and polynomial networks are supervised polynomial models based on an efficient low-rank decomposition. We extend these models to the multi-output setting, i.e., for learning vector-valued functions, with application to multi-class or multi-task problems. We cast this as the problem of learning a 3-way tensor whose slices share a common basis and propose a convex formulation of that problem. We then develop an efficient conditional gradient algorithm and prove its global convergence, despite the fact that it involves a non-convex basis selection step. On classification tasks, we show that our algorithm achieves excellent accuracy with much sparser models than existing methods. On recommendation system tasks, we show how to combine our algorithm with a reduction from ordinal regression to multi-output classification and show that the resulting algorithm outperforms simple baselines in terms of ranking accuracy.","Multi-output Polynomial Networks
and Factorization Machines
Mathieu Blondel
NTT Communication Science Laboratories
Kyoto, Japan
mathieu@mblondel.org
Takuma Otsuka
NTT Communication Science Laboratories
Kyoto, Japan
otsuka.takuma@lab.ntt.co.jp

Vlad Niculae?
Cornell University
Ithaca, NY
vlad@cs.cornell.edu

Naonori Ueda
NTT Communication Science Laboratories
RIKEN
Kyoto, Japan
ueda.naonori@lab.ntt.co.jp

Abstract
Factorization machines and polynomial networks are supervised polynomial models based on an efficient low-rank decomposition. We extend these models to the
multi-output setting, i.e., for learning vector-valued functions, with application to
multi-class or multi-task problems. We cast this as the problem of learning a 3-way
tensor whose slices share a common basis and propose a convex formulation of that
problem. We then develop an efficient conditional gradient algorithm and prove
its global convergence, despite the fact that it involves a non-convex basis selection step. On classification tasks, we show that our algorithm achieves excellent
accuracy with much sparser models than existing methods. On recommendation
system tasks, we show how to combine our algorithm with a reduction from ordinal
regression to multi-output classification and show that the resulting algorithm
outperforms simple baselines in terms of ranking accuracy.

1

Introduction

Interactions between features play an important role in many classification and regression tasks.
Classically, such interactions have been leveraged either explicitly, by mapping features to their
products (as in polynomial regression), or implicitly, through the use of the kernel trick. While fast
linear model solvers have been engineered for the explicit approach [9, 28], they are typically limited
to small numbers of features or low-order feature interactions, due to the fact that the number of
parameters that they need to learn scales as O(dt ), where d is the number of features and t is the order
of interactions considered. Models kernelized with the polynomial kernel do not suffer from this
problem; however, the cost of storing and evaluating these models grows linearly with the number of
training instances, a problem sometimes referred to as the curse of kernelization [30].
Factorization machines (FMs) [25] are a more recent approach that can use pairwise feature interactions efficiently even in very high-dimensional data. The key idea of FMs is to model the weights
of feature interactions using a low-rank matrix. Not only this idea offers clear benefits in terms of
model compression compared to the aforementioned approaches, it has also proved instrumental
in modeling interactions between categorical variables, converted to binary features via a one-hot
encoding. Such binary features are usually so sparse that many interactions are never observed in the
?

Work performed during an internship at NTT Commmunication Science Laboratories, Kyoto.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

training set, preventing classical approaches from capturing their relative importance. By imposing
a low rank on the feature interaction weight matrix, FMs encourage shared parameters between
interactions, allowing to estimate their weights even if they never occurred in the training set. This
property has been used in recommender systems to model interactions between user variables and
item variables, and is the basis of several industrial successes of FMs [32, 17].
Originally motivated as neural networks with a polynomial activation (instead of the classical
sigmoidal or rectifier activations), polynomial networks (PNs) [20] have been shown to be intimately
related to FMs and to only subtly differ in the non-linearity they use [5]. PNs achieve better
performance than rectifier networks on pedestrian detection [20] and on dependency parsing [10],
and outperform kernel approximations such as the Nystr?m method [5]. However, existing PN and
FM works have been limited to single-output models, i.e., they are designed to learn scalar-valued
functions, which restricts them to regression or binary classification problems.
Our contributions. In this paper, we generalize FMs and PNs to multi-output models, i.e., for
learning vector-valued functions, with application to multi-class or multi-task problems.
1) We cast learning multi-output FMs and PNs as learning a 3-way tensor, whose slices share a
common basis (each slice corresponds to one output). To obtain a convex formulation of that
problem, we propose to cast it as learning an infinite-dimensional but row-wise sparse matrix. This
can be achieved by using group-sparsity inducing penalties. (?3)
2) To solve the obtained optimization problem, we develop a variant of the conditional gradient
(a.k.a. Frank-Wolfe) algorithm [11, 15], which repeats the following two steps: i) select a new basis
vector to add to the model and ii) refit the model over the current basis vectors. (?4) We prove the
global convergence of this algorithm (Theorem 1), despite the fact that the basis selection step is
non-convex and more challenging in the shared basis setting. (?5)
3) On multi-class classification tasks, we show that our algorithm achieves comparable accuracy to
kernel SVMs but with much more compressed models than the Nystr?m method. On recommender
system tasks, where kernelized models cannot be used (since they do not generalize to unseen
user-item pairs), we demonstrate how our algorithm can be combined with a reduction from ordinal
regression to multi-output classification and show that the resulting algorithm outperforms singleoutput PNs and FMs both in terms of root mean squared error (RMSE) and ranking accuracy, as
measured by nDCG (normalized discounted cumulative gain) scores. (?6)

2

Background and related work

Notation. We denote the set {1, . . . , m} by [m]. Given a vector v ? Rk , we denote its elements
by vr ? R ?r ? [k]. Given a matrix V ? Rk?m , we denote its rows by vr ? Rm ?r ? [k] and its
columns by v:,c ?c ? [m]. We denote the lp norm of V by kV kp := k vec(V )kp and its lp /lq norm
1
P
k
p p
. The number of non-zero rows of V is denoted by kV k0,? .
by kV kp,q :=
r=1 kvr kq

Factorization machines (FMs). Given an input vector x ? Rd , FMs predict a scalar output by
X
wi,j xi xj ,
y?FM := wT x +
i<j

d

where w ? R contains feature weights and W ? Rd?d is a low-rank matrix that contains pairwise
feature interaction weights. To obtain a low-rank W , [25] originally proposed to use a change of
variable W = H T H, where H ? Rk?d (with k ? N+ a rank parameter) and to learn H instead.
Noting that this quadratic model results in a non-convex problem in H, [4, 31] proposed to convexify
the problem by learning W directly but to encourage low rank using a nuclear norm on W . For
learning, [4] proposed a conditional gradient like approach with global convergence guarantees.
Polynomial networks (PNs). PNs are a recently-proposed form of neural network where the usual
activation function is replaced with a squared activation. Formally, PNs predict a scalar output by
y?PN := wT x + v T ?(Hx) = wT x +

k
X

vr ?(hT
r x),

r=1

2

where ?(a) = a (evaluated element-wise) is the squared activation, v ? Rk is the output layer
vector, H ? Rk?d is the hidden layer matrix and k is the number of hidden units. Because the
2

hT
k

hT
1

m

+ ? ? ? + vk,m

v1,m

Wm
...

h1

W2
W1

hk
hT
k

hT
1

d
+ ? ? ? + vk,1

v1,1
d

hk

h1

Figure 1: Our multi-output PNs / FMs learn a tensor whose slices share a common basis {hr }kr=1 .
Pd
r.h.s term can be rewritten as xT W x = i,j=1 wi,j xi xj if we set W = H T diag(v)H, we see
that PNs are clearly a slight variation of FMs and that learning (v, H) can be recast as learning a
low-rank matrix W . Based on this observation, [20] proposed to use GECO [26], a greedy algorithm
for convex optimization with a low-rank constraint, similar to the conditional gradient algorithm. [13]
proposed a learning algorithm for PNs with global optimality guarantees but their theory imposes
non-negativity on the network parameters and they need one distinct hyper-parameter per hidden unit
to avoid trivial models. Other low-rank polynomial models were recently introduced in [29, 23] but
using a tensor network (a.k.a. tensor train) instead of the canonical polyadic (CP) decomposition.

3

A convex formulation of multi-output PNs and FMs

In this section, we generalize PNs and FMs to multi-output problems. For the sake of concreteness,
we focus on PNs for multi-class classification. The extensionPto FMs is straightforward and simply
requires to replace ?(hT x) = (hT x)2 by ?ANOVA (h, x) := i<j xi hi xj hj , as noted in [5].

The predictions of multi-class PNs can be naturally defined as y?MPN := argmaxc?[m] wcT x+xT Wc x,
where m is the number of classes, wc ? Rd and Wc ? Rd?d is low-rank. Following [5], we can
model the linear term directly in the quadratic term if we augment all data points with an extra feature
of value 1, i.e., xT ? [1, xT ]. We will therefore simply assume y?MPN = argmaxc?[m] xT Wc x
henceforth. Our main proposal in this paper is to decompose W1 , . . . , Wm using a shared basis:
Pk
Wc = H T diag(v:,c )H = r=1 vr,c hr hT
?c ? [m],
(1)
r
where, in neural network terminology, H ? Rk?d can be interpreted as a hidden layer matrix and
V ? Rk?m as an output layer matrix. Compared to the naive approach of decomposing each Wc as
Wc = HcT diag(v:,c )Hc , this reduces the number of parameters from m(dk + k) to dk + mk.

While a nuclear norm could be used to promote a low rank on each Wc , similarly as in [4, 31], this is
clearly not sufficient to impose a shared basis. A naive approach would be to use non-orthogonal
joint diagonalization as a post-processing. However, because this is a non-convex problem for which
no globally convergent algorithm is known [24], this would result in a loss of accuracy. Our key
idea is to cast the problem of learning a multi-output PN as that of learning an infinite but row-wise
sparse matrix. Without loss of generality, we assume that basis vectors (hidden units) lie in the unit
ball. We therefore denote the set of basis vectors by H := {h ? Rd : khk2 ? 1}. Let us denote this
infinite matrix by U ? R|H|?m (we use a discrete notation for simplicity). We can then write
X
?(hT x)uh ? Rm and
y?MPN = argmax o(x; U )c where o(x; U ) :=
c?[m]

h?H

m

uh ? RP denotes the weights of basis h across all classes (outputs). In this formulation, we have
Wc = h?H uh,c hhT and sharing a common basis (hidden units) amounts to encouraging the rows
of U , uh , to be either dense or entirely sparse. This can be naturally achieved using group-sparsity
inducing penalties. Intuitively, V in (1) can be thought as U restricted to its row support. Define the
training set by X ? Rn?d and y ? [m]n . We then propose to solve the convex problem
min F (U ) :=

?(U )??

n
X
i=1

3

? (yi , o(xi ; U )) ,

(2)

Table 1: Sparsity-inducing penalties considered in this paper. With some abuse of notation, we denote
by eh and ec standard basis vectors of dimension |H| and m, respectively. Selecting an optimal
basis vector h? to add is a non-convex optimization problem. The constant ? ? (0, 1) is the tolerance
parameter used for the power method and ? is the multiplicative approximation we guarantee.
?? (G)

?? ? ? ? ??? (G)

kU k1

kGk?

sign(gh? ,c? )eh? eT
c?

kU k1,2

kGk?,2

T
? e h? g h
? /kgh? k2

h? ? argmaxkgh k2

1??
?
m

kU k1,?

kGk?,1

? eh? sign(gh? )T

h? ? argmaxkgh k1

1??
m

?(U )
l1 (lasso)
l1 /l2 (group lasso)
l1 /l?

?

Subproblem
?

?

h ,c ?

argmax |gh,c |

h?H,c?[m]

h?H

h?H

?
1??

where ? is a smooth and convex multi-class loss function (cf. Appendix A for three common examples),
? is a sparsity-inducing penalty and ? > 0 is a hyper-parameter. In this paper, we focus on the l1
(lasso), l1 /l2 (group lasso) and l1 /l? penalties for ?, cf. Table 1. However, as we shall see, solving
(2) is more challenging with the l1 /l2 and l1 /l? penalties than with the l1 penalty. Although our
formulation is based on an infinite view, we next show that U ? has finite row support.
Proposition 1 Finite row support of U ? for multi-output PNs and FMs
Let U ? be an optimal solution of (2), where ? is one of the penalties in Table 1. Then,
kU ? k0,? ? nm + 1. If ?(?) = k ? k1 , we can tighten this bound to kU ? k0,? ? min(nm + 1, dm).
Proof is in Appendix B.1. It is open whether we can tighten this result when ? = k ? k1,2 or k ? k1,? .

4

A conditional gradient algorithm with approximate basis vector selection

At first glance, learning with an infinite number of basis vectors seems impossible. In this section,
we show how the well-known conditional gradient algorithm [11, 15] combined with group-sparsity
inducing penalties naturally leads to a greedy algorithm that selects and adds basis vectors that are
useful across all outputs. On every iteration, the conditional gradient algorithm performs updates
of the form U (t+1) = (1 ? ?)U (t) + ??? , where ? ? [0, 1] is a step size and ?? is obtained by
solving a linear approximation of the objective around the current iterate U (t) :
?? ? argminh?, ?F (U (t) )i = ? ? argmaxh?, ??F (U (t) )i.
?(?)??

(3)

?(?)?1

Let us denote the negative gradient ??F (U ) by G ? R|H|?m for short. Its elements are defined by
gh,c = ?

n
X

?(hT xi )?? (yi , o(xi ; U ))c ,

i=1

where ??(y, o) ? Rm is the gradient of ? w.r.t. o (cf. Appendix A). For ReLu activations, solving
(3) is known to be NP-hard [1]. Here, we focus on quadratic activations, for which we will be able to
provide approximation guarantees. Plugging the expression of ?, we get
gh,c = ?hT ?c h where ?c := X T Dc X (PN) or ?c :=

n

X
1 T
diag(xi )2 (FM)
X Dc X ? Dc
2
i=1

and Dc ? Rn?n is a diagonal matrix such that (Dc )i,i := ??(yi , o(xi ; U ))c . Let us recall the
definition of the dual norm of ?: ?? (G) := max?(?)?1 h?, Gi. By comparing this equation to (3),
we see that ?? is the argument that achieves the maximum in the dual norm ?? (G), up to a constant
factor ? . It is easy to verify that any element in the subdifferential of ?? (G), which we denote by
??? (G) ? R|H|?m , achieves that maximum, i.e., ?? ? ? ? ??? (G).
Basis selection. As shown in Table 1, elements of ??? (G) (subgradients) are |H| ? m matrices with
a single non-zero row indexed by h? , where h? is an optimal basis (hidden unit) selected by
h? ? argmax kgh kp ,
h?H

4

(4)

and where p = ? when ? = k ? k1 , p = 2 when ? = k.k1,2 and p = 1 when ? = k ? k1,? . We
call (4) a basis vector selection criterion. Although this selection criterion was derived from the
linearization of the objective, it is fairly natural: it chooses the basis vector with largest ?violation?,
as measured by the lp norm of the negative gradient row gh .
Multiplicative approximations. The key challenge in solving (3) or equivalently (4) arises from the
fact that G has infinitely many rows gh . We therefore cast basis vector selection as a continuous
optimization problem w.r.t. h. Surprisingly, although the entire objective (2) is convex, (4) is not.
? ? R|H|?m that satisfies
Instead of the exact maximum, we will therefore only require to find a ?
? ??
?(?)

and

? Gi ? ?h?? , Gi,
h?,

where ? ? (0, 1] is a multiplicative approximation (higher is better). It is easy to verify that this is
? ? H that satisfies kg? kp ? ?kgh? kp .
equivalent to replacing the optimal h? by an approximate h
h
Sparse case. When ?(?) = k ? k1 , we need to solve

max kgh k? = max max |hT ?c h| = max max |hT ?c h|.
h?H

h?H c?[m]

c?[m] h?H

It is well known that the optimal solution of maxh?H |hT ?c h| is the dominant eigenvector of ?c .
? as the hc
Therefore, we simply need to find the dominant eigenvector hc of each ?c and select h
T
with largest singular value |hc ?c hc |. Using the power method, we can find an hc that satisfies
T
|hT
c ?c hc | ? (1 ? ?) max |h ?c h|,

(5)

h?H

for some tolerance parameter ? ? (0, 1). The procedure takes O(Nc log(d)/?) time, where Nc is
the number of non-zero elements in ?c [26]. Taking the maximum w.r.t. c ? [m] on both sides of
(5) leads to kgh
? k? ? ?kgh? k? , where ? = 1 ? ?. However, using ? = k ? k1 does not encourage
?
selecting an h that is useful for all outputs. In fact, when ? = k ? k1 , our approach is equivalent to
imposing independent nuclear norms on W1 , . . . , Wm .
Group-sparse cases. When ?(?) = k.k1,2 or ?(?) = k.k1,? , we need to solve
max kgh k22 = max f2 (h) :=
h?H

h?H

m
X

(hT ?c h)2

or

c=1

max kgh k1 = max f1 (h) :=
h?H

h?H

m
X
c=1

|hT ?c h|,

respectively. Unlike the l1 -constrained case, we are clearly selecting a basis vector with largest violation across all outputs. However, we are now faced with a more difficult non-convex optimization
problem. Our strategy is to first choose an initialization h(0) which guarantees a certain multiplicative
approximation ?, then refine the solution using a monotonically non-increasing iterative procedure.
Initialization. We simply choose h(0) as the approximate solution of the ? = k ? k1 case, i.e., we have
kgh(0) k? ? (1 ? ?) max kgh k? .
h?H

Now, using

?

mkxk? ? kxk2 ? kxk? and mkxk? ? kxk1 ? kxk? , this immediately implies
kgh(0) kp ? ? max kgh kp ,
h?H

with ? =

1??
?
m

if p = 2 and ? =

1??
m

if p = 1.

Refining the solution. We now apply another instance of the conditional gradient algorithm to solve
the subproblem maxkhk2 ?1 fp (h) itself, leading to the following iterates:
h(t+1) = (1 ? ?t )h(t) + ?t

?fp (h(t) )
,
k?fp (h(t) )k2

(6)

where ?t ? [0, 1]. Following [3, Section 2.2.2], if we use the Armijo rule to select ?t , every limit
point of the sequence {h(t) } is a stationary point of fp . In practice, we observe that ?t = 1 is almost
always selected. Note that when ?t = 1 and m = 1 (i.e., single-output case), our refining algorithm
recovers the power method. Generalized power methods were also studied for structured matrix
factorization [16, 21], but with different objectives and constraints. Since the conditional gradient
5

Algorithm 1 Multi-output PN/FM training
Input: X, y, k, ?
H ? [ ], V ? [ ]
for t := 1, . . . , k doP
T
Compute oi := t?1
r=1 ?(hr xi )vr ?i ? [n]
Let gh := [?hT ?1 h, . . . , ?hT ?m h]T
? ? argmax
Find h
h?H kgh kp
? to H and 0 to V
Append h
V ? argmin Ft (V , H)
?(V )??

Optional: V , H ?

argmin

Ft (V , H)

?(V )??
hr ?H ?r?[t]

end for
P
Output: V , H (equivalent to U = kt=1 eht vtT )

algorithm assumes a differentiable function, in the case p = 1, we replace the absolute function with
the Huber function |x| ? 21 x2 if |x| ? 1, |x| ? 12 otherwise.
Corrective refitting step. After t iterations, U (t) contains at most t non-zero rows. We can therefore
always store U (t) as V (t) ? Rt?m (the output layer matrix) and H (t) ? Rt?d (the basis vectors /
hidden units added so far).
order to improveaccuracy, on iteration t, we can then refit the objective
 In
Pn
Pt
Ft (V , H) := i=1 ? yi , r=1 ?(hT
r xi )vr . We consider two kinds of corrective steps, a convex
one that minimizes Ft (V , H (t) ) w.r.t. V ? Rt?m and an optional non-convex one that minimizes
Ft (V , H) w.r.t. both V ? Rt?m and H ? Rt?d . Refitting allows to remove previously-added
bad basis vectors, thanks to the use of sparsity-inducing penalties. Similar refitting procedures are
commonly used in matching pursuit [22]. The entire procedure is summarized in Algorithm 1 and
implementation details are given in Appendix D.

5

Analysis of Algorithm 1

The main difficulty in analyzing the convergence of Algorithm 1 stems from the fact that we cannot
solve the basis vector selection subproblem globally when ? = k ? k1,2 or k ? k1,? . Therefore, we
need to develop an analysis that can cope with the multiplicative approximation ?. Multiplicative
approximations were also considered in [18] but the condition they require is too stringent (cf.
Appendix B.2 for a detailed discussion). The next theorem guarantees the number of iterations needed
to output a multi-output network that achieves as small objective value as an optimal solution of (2).
Theorem 1 Convergence of Algorithm 1
Assume F is smooth with constant ?. Let U (t) be the output after t iterations of Algorithm 1 run with
8? 2 ?
constraint parameter ?? . Then, F (U (t) ) ? min F (U ) ? ? ?t ?
? 2.
?? 2
?(U )??
2 
In [20], single-output PNs were trained using GECO [26], a greedy algorithm with similar O ????2
guarantees. However, GECO is limited to learning infinite vectors (not matrices) and it does not
constrain its iterates like we do. Hence GECO cannot remove bad basis vectors. The proof of
Theorem 1 and a detailed comparison with GECO are given in Appendix B.2. Finally, we note
that the infinite dimensional view is also key to convex neural networks [2, 1]. However, to our
knowledge, we are the first to give an explicit multiplicative approximation guarantee for a non-linear
multi-output network.

6
6.1

Experimental results
Experimental setup

Datasets. For our multi-class experiments, we use four publicly-available datasets: segment (7
classes), vowel (11 classes), satimage (6 classes) and letter (26 classes) [12]. Quadratic models sub6

stantially improve over linear models on these datasets. For our recommendation system experiments,
we use the MovieLens 100k and 1M datasets [14]. See Appendix E for complete details.
Model validation. The greedy nature of Algorithm 1 allows us to easily interleave training with
model validation. Concretely, we use an outer loop (embarrassingly parallel) for iterating over the
range of possible regularization parameters, and an inner loop (Algorithm 1, sequential) for increasing
the number of basis vectors. Throughout our experiments, we use 50% of the data for training, 25%
for validation, and 25% for evaluation. Unless otherwise specified, we use a multi-class logistic loss.
6.2

Method comparison for the basis vector (hidden unit) selection subproblem

satimage
vowel
As we mentioned previously, the linearized subprob+ refine
lem (basis vector selection) for the l1 /l2 and l1 /l? 1 init
(proposed)
constrained cases involves a significantly more chal- random init
+refine
lenging non-convex optimization problem. In this
1 init
section, we compare different methods for obtaining
random
init
?
an approximate solution h to (4). We focus on the
best data
?1 /?? case, since we have a method for computing
the true global solution h? , albeit with exponential
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
complexity in m (cf. Appendix C). This allows us
to report the empirically observed multiplicative Figure 2: Empirically observed multiplicative
? 1 (h? ).
? 1 (h? ).
approximation factor ?? = f1 (h)/f
approximation factor ?? := f1 (h)/f
l

l

Compared methods. We compare l1 init + refine (proposed), random init + refine, l1 init (without
? = xi? /kxi? k2 where i? = argmax f1 (xi /kxi k2 ).
refine), random init and best data: h
i?[n]

Results. We report ?? in Figure 2. l1 init + refine achieves nearly the global maximum on both
datasets and outperforms random init + refine, showing the effectiveness of the proposed initialization
and that the iterative update (6) can get stuck in a bad local minimum if initialized badly. On the
other hand, l1 init + refine outperforms l1 init alone, showing the importance of iteratively refining
the solution. Best data, a heuristic similar to that of approximate kernel SVMs [7], is not competitive.
Sparsity-inducing penalty comparison

In this section, we compare the l1 , l1 /l2 and l1 /l? penalties for the
choice of ?, when varying the maximum number of basis vectors
(hidden units). Figure 3 indicates test set accuracy when using
output layer refitting. We also include linear logistic regression,
kernel SVMs and the Nystr?m method as baselines. For the latter
2
two, we use the quadratic kernel (xT
i xj + 1) . Hyper-parameters
are chosen so as to maximize validation set accuracy.
Results. On the vowel (11 classes) and letter (26 classes) datasets,
l1 /l2 and l1 /l? penalties outperform l1 norm starting from 20 and
75 hidden units, respectively. On satimage (6 classes) and segment
(7 classes), we observed that the three penalties are mostly similar
(not shown). We hypothesize that l1 /l2 and l1 /l? penalties make
a bigger difference when the number of classes is large. Multioutput PNs substantially outperform the Nystr?m method with
comparable number of basis vectors (hidden units). Multi-output
PNs reach the same test accuracy as kernel SVMs with very few
basis vectors on vowel and satimage but appear to require at least
100 basis vectors to reach good performance on letter. This is not
surprising, since kernel SVMs require 3,208 support vectors on
letter, as indicated in Table 2 below.
6.4

Test multi-class accuracy

6.3

0.94
0.92
0.90
0.88
0.86
0.70
0.50

letter

0

50
100
Max. hidden units

150

Figure 3: Penalty comparison.

Multi-class benchmark comparison

Compared methods. We compare the proposed conditional gradient algorithm with output layer
refitting only and with both output and hidden layer refitting; projected gradient descent (FISTA)
7

Table 2: Muli-class test accuracy and number of basis vectors / support vectors.
segment

vowel

satimage

Conditional gradient (full refitting, proposed)
87.83 (12)
89.80 (25)
l1
96.71 (41)
l1 /l2
96.71 (40)
89.57 (15)
89.08 (18)
l1 /l?
96.71 (24)
86.96 (15)
88.99 (20)

letter
92.29 (150)
91.81 (106)
92.35 (149)

Conditional gradient (output-layer refitting, proposed)
l1
97.05 (20)
80.00 (21)
89.71 (40)
91.01 (139)
l1 /l2
96.36 (21)
85.22 (15)
89.71 (50)
92.24 (150)
l1 /l?
96.19 (16)
86.96 (41)
89.35 (41)
91.68 (128)
Projected gradient descent (random init)
l1
96.88 (50)
79.13 (50)
89.53
l1 /l2
96.88 (50)
80.00 (48)
89.80
l1 /l?
96.71 (50)
83.48 (50)
89.08
l22
96.88 (50)
81.74 (50)
89.98
Baselines
Linear
Kernelized
OvR PN

92.55
96.71 (238)
94.63

60.00
85.22 (189)
73.91

(50)
(50)
(50)
(50)

83.03
89.53 (688)
89.44

88.45
88.45
88.45
88.45

(150)
(150)
(150)
(150)

71.17
93.73 (3208)
75.36

with random initialization; linear and kernelized models; one-vs-rest PNs (i.e., fit one PN per class).
We focus on PNs rather than FMs since they are known to work better on classification tasks [5].
Results are included in Table 2. From these results, we can make the following observations and
conclusions. When using output-layer refitting on vowel and letter (two datasets with more than 10
classes), group-sparsity inducing penalties lead to better test accuracy. This is to be expected, since
these penalties select basis vectors that are useful across all classes. When using full hidden layer and
output layer refitting, l1 catches up with l1 /l2 and l1 /l? on the vowel and letter datasets. Intuitively,
the basis vector selection becomes less important if we make more effort at every iteration by refitting
the basis vectors themselves. However, on vowel, l1 /l2 is still substantially better than l1 (89.57 vs.
87.83).
Compared to projected gradient descent with random initialization, our algorithm (for both output
and full refitting) is better on 3/4 (l1 ), 2/4 (l1 /l2 ) and 3/4 (l1 /l? ) of the datasets. In addition, with our
algorithm, the best model (chosen against the validation set) is substantially sparser. Multi-output
PNs substantially outperform OvR PNs. This is to be expected, since multi-output PNs learn to share
basis vectors across different classes.
6.5

Recommender system experiments using ordinal regression

A straightforward way to implement recommender systems consists in training a single-output model
to regress ratings from one-hot encoded user and item indices [25]. Instead of a single-output PN
or FM, we propose to use ordinal McRank, a reduction from ordinal regression to multi-output
binary classification, which is known to achieve good nDCG (normalized discounted cumulative
gain) scores [19]. This reduction involves training a probabilistic binary classifier for each of the m
relevance levels (for instance, m = 5 in the MovieLens datasets). The expected relevance of x (e.g.
the concatenation of the one-hot encoded user and item indices) is then computed by
m h
m
i
X
X
c p(y ? c | x) ? p(y ? c ? 1 | x) ,

c p(y = c | x) =

y? =

c=1

c=1

where we use the convention p(y ? 0 | x) = 0. Thus, all we need to do to use ordinal McRank is to
train a probabilistic binary classifier p(y ? c | x) for all c ? [m].

Our key proposal is to use a multi-output model to learn all m classifiers simultaneously, i.e., in a
multi-task fashion. Let xi be a vector representing a user-item pair with corresponding rating yi , for
8

RMSE

nDCG@1

1.00

nDCG@5

0.76

Movielens 100k

0.77
0.98

0.74

0.76

0.72

0.96

0.75

0.70

0.74

0.94
0.68
0

10

20

30

40

50

0.73
0

10

20

30

40

50

0

10

20

30

40

50

1.00
0.76
Movielens 1M

0.98

0.77
0.75

0.96

0.94

0.74

0.92

0.73

0.76

Single-output PN
Single-output FM
Ordinal McRank FM l1 /l2

0.75
0.90

Ordinal McRank FM l1 /l

0.72
0

10

20

30

40

50

0

10

Max. hidden units

20

30

Max. hidden units

40

50

0

10

20

30

40

50

Max. hidden units

Figure 4: Recommender system experiment: RMSE (lower is better) and nDCG (higher is better).
i ? [n]. We form a n ? m matrix Y such that yi,c = +1 if yi ? c and ?1 otherwise, and solve
!
n X
m
X
X
min
? yi,c ,
?ANOVA (h, xi )uh,c ,
?(U )??

i=1 c=1

h?H

where ? is set to the binary logistic loss, in order to be able to produce probabilities. After running
Algorithm 1 on that objective for k iterations, we obtain H ? Rk?d and V ? Rk?m . Because H is
shared across all outputs, the only small overhead of using the ordinal McRank reduction, compared
to a single-output regression model, therefore comes from learning V ? Rk?m instead of v ? Rk .

In this experiment, we focus on multi-output factorization machines (FMs), since FMs usually work
better than PNs for one-hot encoded data [5]. We show in Figure 4 the RMSE and nDCG (truncated
at 1 and 5) achieved when varying k (the maximum number of basis vectors / hidden units).
Results. When combined with the ordinal McRank reduction, we found that l1 /l2 and l1 /l? ?
constrained multi-output FMs substantially outperform single-output FMs and PNs on both RMSE
and nDCG measures. For instance, on MovieLens 100k and 1M, l1 /l? ?constrained multi-output
FMs achieve an nDCG@1 of 0.75 and 0.76, respectively, while single-output FMs only achieve 0.71
and 0.75. Similar trends are observed with nDCG@5. We believe that this reduction is more robust
to ranking performance measures such as nDCG thanks to its modelling of the expected relevance.

7

Conclusion and future directions

We defined the problem of learning multi-output PNs and FMs as that of learning a 3-way tensor
whose slices share a common basis. To obtain a convex optimization objective, we reformulated that
problem as that of learning an infinite but row-wise sparse matrix. To learn that matrix, we developed
a conditional gradient algorithm with corrective refitting, and were able to provide convergence
guarantees, despite the non-convexity of the basis vector (hidden unit) selection step.
Although not considered in this paper, our algorithm and its analysis can be modified to make
use of stochastic gradients. An open question remains whether a conditional gradient algorithm
with provable guarantees can be developed for training deep polynomial networks or factorization
machines. Such deep models could potentially represent high-degree polynomials with few basis
vectors. However, this would require the introduction of a new functional analysis framework.

9

References
[1] F. Bach. Breaking the curse of dimensionality with convex neural networks. JMLR, 2017.
[2] Y. Bengio, N. Le Roux, P. Vincent, O. Delalleau, and P. Marcotte. Convex neural networks. In
NIPS, 2005.
[3] D. P. Bertsekas. Nonlinear programming. Athena Scientific Belmont, 1999.
[4] M. Blondel, A. Fujino, and N. Ueda. Convex factorization machines. In ECML/PKDD, 2015.
[5] M. Blondel, M. Ishihata, A. Fujino, and N. Ueda. Polynomial networks and factorization
machines: New insights and efficient training algorithms. In ICML, 2016.
[6] M. Blondel, K. Seki, and K. Uehara. Block coordinate descent algorithms for large-scale sparse
multiclass classification. Machine Learning, 93(1):31?52, 2013.
[7] A. Bordes, S. Ertekin, J. Weston, and L. Bottou. Fast kernel classifiers with online and active
learning. JMLR, 6(Sep):1579?1619, 2005.
[8] V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The convex geometry of linear
inverse problems. Foundations of Computational Mathematics, 12(6):805?849, 2012.
[9] Y.-W. Chang, C.-J. Hsieh, K.-W. Chang, M. Ringgaard, and C.-J. Lin. Training and testing
low-degree polynomial data mappings via linear svm. Journal of Machine Learning Research,
11:1471?1490, 2010.
[10] D. Chen and C. D. Manning. A fast and accurate dependency parser using neural networks. In
EMNLP, 2014.
[11] J. C. Dunn and S. A. Harshbarger. Conditional gradient algorithms with open loop step size
rules. Journal of Mathematical Analysis and Applications, 62(2):432?444, 1978.
[12] R.-E. Fan and C.-J. Lin.
datasets/, 2011.

http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/

[13] A. Gautier, Q. N. Nguyen, and M. Hein. Globally optimal training of generalized polynomial
neural networks with nonlinear spectral methods. In NIPS, 2016.
[14] GroupLens. http://grouplens.org/datasets/movielens/, 1998.
[15] M. Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In ICML, 2013.
[16] M. Journ?e, Y. Nesterov, P. Richt?rik, and R. Sepulchre. Generalized power method for sparse
principal component analysis. Journal of Machine Learning Research, 11:517?553, 2010.
[17] Y. Juan, Y. Zhuang, W.-S. Chin, and C.-J. Lin. Field-aware factorization machines for CTR
prediction. In ACM Recsys, 2016.
[18] S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher. Block-coordinate Frank-Wolfe
optimization for structural SVMs. In ICML, 2012.
[19] P. Li, C. J. Burges, and Q. Wu. McRank: Learning to rank using multiple classification and
gradient boosting. In NIPS, 2007.
[20] R. Livni, S. Shalev-Shwartz, and O. Shamir. On the computational efficiency of training neural
networks. In NIPS, 2014.
[21] R. Luss and M. Teboulle. Conditional gradient algorithms for rank-one matrix approximations
with a sparsity constraint. SIAM Review, 55(1):65?98, 2013.
[22] S. G. Mallat and Z. Zhang. Matching pursuits with time-frequency dictionaries. IEEE Transactions on Signal Processing, 41(12):3397?3415, 1993.
[23] A. Novikov, M. Trofimov, and I. Oseledets.
arXiv:1605.03795, 2016.
10

Exponential machines.

arXiv preprint

[24] A. Podosinnikova, F. Bach, and S. Lacoste-Julien. Beyond CCA: Moment matching for multiview models. In ICML, 2016.
[25] S. Rendle. Factorization machines. In ICDM, 2010.
[26] S. Shalev-Shwartz, A. Gonen, and O. Shamir. Large-scale convex minimization with a low-rank
constraint. In ICML, 2011.
[27] S. Shalev-Shwartz, Y. Wexler, and A. Shashua. ShareBoost: Efficient multiclass learning with
feature sharing. In NIPS, 2011.
[28] S. Sonnenburg and V. Franc. Coffin: A computational framework for linear SVMs. In ICML,
2010.
[29] E. Stoudenmire and D. J. Schwab. Supervised learning with tensor networks. In NIPS, 2016.
[30] Z. Wang, K. Crammer, and S. Vucetic. Multi-class Pegasos on a budget. In ICML, 2010.
[31] M. Yamada, W. Lian, A. Goyal, J. Chen, K. Wimalawarne, S. A. Khan, S. Kaski, H. M.
Mamitsuka, and Y. Chang. Convex factorization machine for toxicogenomics prediction. In
KDD, 2017.
[32] E. Zhong, Y. Shi, N. Liu, and S. Rajan. Scaling factorization machines with parameter server.
In CIKM, 2016.

11

"
1993,Digital Boltzmann VLSI for constraint satisfaction and learning,,864-digital-boltzmann-vlsi-for-constraint-satisfaction-and-learning.pdf,Abstract Missing,"Digital Boltzmann VLSI for
constraint satisfaction and learning

Michael Murray t

Ming-Tak Leung t

Kan Boonyanit t

Kong Kritayakirana t

James B. Burrt*

Gregory J. Wolff+

Takahiro Watanabe+

Edward Schwartz+

David G. Storktt

Allen M. Petersont
t Department of Electrical Engineering
Stanford University
Stanford, CA 94305-4055
+Ricoh California Research Center
2882 Sand Hill Road Suite 115
Menlo Park, CA 94025-7022
and
*Sun Mlcrosystems
.
2550 Garcia Ave., MTV-29, room 203
Mountain View, CA 94043

Abstract
We built a high-speed, digital mean-field Boltzmann chip and SBus
board for general problems in constraint satjsfaction and learning.
Each chip has 32 neural processors and 4 weight update processors,
supporting an arbitrary topology of up to 160 functional neurons.
On-chip learning is at a theoretical maximum rate of 3.5 x 108 connection updates/sec; recall is 12000 patterns/sec for typical conditions. The chip's high speed is due to parallel computation of inner
products, limited (but adequate) precision for weights and activations (5 bits), fast clock (125 MHz), and several design insights.

896

Digital Boltzmann VLSI for Constraint Satisfaction and Learning

1

INTRODUCTION

A vast number of important problems can be cast into a form of constraint satisfaction. A crucial difficulty when solving such problems is the fact that there are local
minima in the solution space, and hence simple gradient descent methods rarely suffice. Simulated annealing via the Boltzmann algorithm (BA) is attractive because it
can avoid local minima better than many other methods (Aarts and Korst, 1989).
It is well known that the problem of learning also generally has local minima in
weight (parameter) space; a Boltzmann algorithm has been developed for learning
which is effective at avoiding local minima (Ackley and Hinton, 1985). The BA
has not received extensive attention, however, in part because of its slow operation
which is due to the annealing stages in which the network is allowed to slowly relax
into a state of low error. Consequently there is a great need for fast and efficient
special purpose VLSI hardware for implementing the algorithm. Analog Boltzmann
chips have been described by Alspector, Jayakumar and Luna (1992) and by Arima
et al. (1990); both implement stochastic BA. Our digital chip is the first to implement the deterministic mean field BA algorithm (Hinton, 1989), and although its
raw throughput is somewhat lower than the analog chips just mentioned, ours has
unique benefits in capacity, ease of interfacing and scalability (Burr, 1991, 1992).

2

BOLTZMANN THEORY

The problems of constraint satisfaction and of learning are unified through the
Boltzmann learning algorithm. Given a partial pattern and a set of constraints,
the BA completes the pattern by means of annealing (gradually lowering a computational ""temperature"" until the lowest energy state is found) - an example
of constraint satisfaction. Over a set of training patterns, the learning algorithm
modifies the constraints to model the relationships in the data.

2.1

CONSTRAINT SATISFACTION

A general constraint satisfaction problem over variables Xi (e.g., neural activations)
is to find the set Xi that minimize a global energy function E = -~ Lij WijXiXj,
where Wij are the (symmetric) connection weights between neurons i and j and
represent the problem constraints.
There are two versions of the BA approach to minimizing E. In one version - the
stochastic BA - each binary neuron Xi E {-I, I} is polled randomly, independently
and repeatedly, and its state is given a candidate perturbation. The probability of
acceptance of this perturbation depends upon the amount of the energy change
and the temperature. Early in the annealing schedule (Le., at high temperature)
the probability of acceptance is nearly independent of the change in energy; late in
annealing (Le., at low temperature), candidate changes that lead to lower energy
are accepted with higher probability.
In the deterministic mean field BA, each continuous valued neuron (-1 < Xi ::;
1) is updated simultaneously and in parallel, its new activation is set to Xi =
I(Lj WijXj), where 10 is a monotonic non-linearity, typically a sigmoid which
corresponds to a stochastic unit at a given temperature (assuming independent

897

898

Murray, Leung, Boonyanit, Kritayakirana, Burr, Wolff, Watanabe, Schwartz, Stork, and Peterson

inputs). The inverse slope of the non-linearity is proportional to the temperature; at
the end of the anneal the slope is very high and f (.) is effectively a step function. It
has been shown that if certain non-restrictive assump'tions hold, and if the annealing
schedule is sufficiently slow, then the final binary states (at 0 temperature) will be
those of minimum E (Hinton, 1989, Peterson and Hartman, 1989).

2.2

LEARNING

The problem of Boltzmann learning is the following: given a network topology
of input and output neurons, interconnected by hidden neurons, and given a set of
training patterns (input and desired output), find a set of weights that leads to high
probability of a desired output activations for the corresponding input activations.
In the Boltzmann algorithm such learning is achieved using two main phases the Teacher phase and the Student phase - followed by the actual Weight update.
During the Teacher phase the network is annealed with the inputs and outputs
clamped (held at the values provided by the omniscient teacher). During the anneal
of the Student phase, only the inputs are clamped - the outputs are allowed to
vary. The weights are updated according to:
D..Wij =

?( (x!x;) - (x:xj))

(1)

where ? is a learning rate and (x~x;) the coactivations of neurons i and j at the end
of the Teacher phase and (x:xj) in at the end of the Student phase (Ackley and
Hinton, 1985). Hinton (1989) has shown that Eq. 1 effectively performs gradient
descent on the cross-entropy distance between the probability of a state in the
Teacher (clamped) and the Student (free-running) phases.
Recent simulations by Galland (1993) have shown limitations of the deterministic
BA for learning in networks having hidden units directly connected to other hidden
units. While his results do not cast doubt on the deterministic BA for constraint
satisfaction, they do imply that the deterministic BA for learning is most successful
in networks of a single hidden layer. Fortunately, with enough hidden units this
topology has the expressive power to represent all but the most pathological inputoutput mappings.

3

FUNCTIONAL DESIGN AND CHIP OPERATION

Figure 1 shows the functional block diagram of our chip. The most important units
are the Weight memory, Neural processors, Weight update processors, Sigmoid and
Rotating Activation Storage (RAS), and their operation are best explained in terms
of constraint satisfaction and learning.

3.1

CONSTRAINT SATISFACTION

For constraint satisfaction, the weights (constraints) are loaded into the Weight
memory, the form of the transfer function is loaded into the Sigmoid Unit, and
the values and duration of the annealing temperatures (the annealing schedule) are
loaded into the Temperature Unit. Then an input pattern is loaded into a bank
of the RAS to be annealed. Such an anneal occurs as follows: At an initial high

Digital Boltzmann VLSI for Constraint Satisfaction and Learning

temperature, the 32 Neural processors compute Xi = Lj WijXj in parallel for the
hidden units. A 4 x multiplexing here permits networks of up to 128 neurons to
be annealed, with the remaining 32 neurons used as (non-annealed) inputs. Thus
our chip supports networks of up to 160 neurons total. These activations are then
stored in the Neural Processor Latch and then passed sequentially to the Sigmoid
unit, where they are multiplied by the reciprocal of the instantaneous temperature.
This Sigmoid unit employs a lookup table to convert the inputs to neural outputs
by means of non-linearity f(?). These outputs are sequentially loaded back into the
activation store. The temperature is lowered (according to the annealing schedule), and the new activations are calculated as before, and so on. The final set of
activations Xi (i.e., at the lowest temperature) represent the solution.
r-----t.... 4

Rotating
Activation

weight update processors
weight update cache

Weight
memory

1
32 Neural Processors (NP)

Sigmoid

Figure 1: Boltzmann VLSI block diagram. The rotating activation storage (black)
consists of three banks, which for learning problems contain the last pattern (already annealed), the current pattern (being annealed) and the next pattern (to be
annealed) read onto the chip through the external interface.

3.2

LEARNING

When the chip is used for learning, the weight memory is initialized with random
weights and the first, second and third training patterns are loaded into the RAS.
The three-bank RAS is crucial for our chip's speed because it allows a three-fold

899

900

Murray, Leung, Boonyanit, Kritayaldrana, Burr, Wolff, Watanabe, Schwartz, Stork, and Peterson

concurrency: 1) a current pattern of activations is annealed, while 2) the annealed
last pattern is used to update the weights, while 3) the next pattern is being loaded
from off-chip. The three banks form a circular buffer, each with a Student and a
Teacher activation store.
During the Teacher anneal phase (for the current pattern), activations of the input
and output neurons are held at the values given by the teacher, and the values of
the hidden units found by annealing (as described in the previous subsection). After
the last such annealling step (Le., at the lowest temperature), the final activations
are left in the Teacher activation store - the Teacher phase is then complete. The
annealing schedule is then reset to its initial temperature, and the above process is
then repeated for the Student phase; here only the input activations are clamped
to their values and the outputs are free to vary. At the end of this Student anneal,
the final activations are left in the Student activation storage.
In steady state, the MUX then rotates the storage banks of the RAS such that the
next, current, and last banks are now called the current, last, and next, respectively.
To update the weights, the activations in the Student and Teacher storage bank
for the pattern just annealed (now called the ""last"" pattern) are sent to the four
Weight update processors, along with the weights themselves. The Weight update
processors compute the updated weights according to Eq. 1, and write them back
to the Weight memory. While such weight update is occuring for the last pattern,
the current pattern is annealing and the next pattern is being loaded from off chip.
After the chip has been trained with all of the patterns, it is ready for use in
recall. During recall, a test pattern is loaded to the input units of an activation
bank (Student side), the machine performs a Student anneal and the final output
activations are placed in the Student activation store, then read off the chip to
the host computer as the result. In a constraint satisfaction problem, we merely
download the weights (constraints) and perform a Student anneal.

4

HARDWARE IMPLEMENTATION

Figure 2 shows the chip die. The four main blocks of the Weight memory are at
the top, surrounded by 32 Neural processors (above and below this memory), and
four Weight update processors (between the memory banks). The three banks of
the Rotating Activation Store are at the bottom of the chip. The Sigmoid processor
is at the lower left, and instruction cache and external interface at the lower right.
Most of the rest of the chip consists of clocking and control circuitry.

4.1

VLSI

The chip mixes dynamic and static memory on the same die. The Activation and
Temperature memories are static RAM (which needs no refresh circuitry) while the
Weight memory is dynamic (for area efficiency) . The system clock is distributed to
various local clock drivers in order to reduce the global clock capacitance and to selectively disable the clocks in inactive subsystems for reducing power consumption.
Each functional block has its own finite state machine control which communicates

Digital Boltzmann VLSI for Constraint Satisfaction and Learning

.. ""

._ ? ...- .

.. ....

? . -,

""o.t ' .

. IM

.... . .

'7 "","""",

Figure 2: Boltzmann VLSI chip die.

asynchronously. For diagnostic purposes, the State Machines and counters are observable through the External Interface. There is a Single Step mode which has
been very useful in verifying sub-system performance. Figure 3 shows the power
dissipation throughout a range of frequencies. Note that the power is less than
2 Watts throughout.
Extensive testing of the first silicon revealed two main classes of chip error: electrical
and circuit. Most of the electrical problems can be traced to fast edge rates on
the DRAM sense-amp equalization control signals, which cause inductive voltage
transients on the power supply rails of roughly 1 Volt. This appears to be at least
partly responsible for the occasional loss of data in dynamic storage nodes. There
also seems to be insufficient latchup protection in the pads, which is aggravated by
the on-chip voltage surges. The circuit problems can be traced to having to modify
the circuits used in the layout for full chip simulation.
In light of these problems, we have simulated the circuit in great detail in order to
explore possible corrective steps. We have modified the design to provide improved
electrical isolation, resized drivers and reduced the logic depth in several components. These corrections solve the problems in simulation, and give us confidence
that the next fab run will yield a fully working chip.

4.2

BOARD AND SBus INTERFACE

An SBus interface board was developed to allow the Boltzmann chip to be used
with a SparcStation host. The registers and memory in the chip can be memory
mapped so that they are directly accessible to user software. The board can support

901

902

Murray, Leung, Boonyanit, Kritayakirana, Burr, Wolff, Watanabe, Schwartz, Stork, and Peterson

Table 1: Boltzmann VLSI chip specifications
Architecture
Size
Neurons
Weight memory
Activation store
Technology
Transistors
Pins
Clock
I/O rate
Learning rate
Recall rate
Power dissipation

n-Iayer, arbitrary intercoItnnections
9.5 mm x 9.8 mm
32 processors --+ 160 virtual
20,480 5-bit weights (on chip)
3 banks, 160 teacher & 160 student values in each
1. 211m CMOS
400,000
84
125 MHz (on chip)
3 x 107 activations/sec (sustained)
3.5 x 108 connection updates/sec (on chip)
12000 patterns/sec
:::;2 Watts (see Figure 3)

20-bit transfers to the chip at a sustained rate in excess of 8 Mbytes/second. The
board uses reconfigurable Xilinx FPGAs (field-programmable gate arrays) to allow
flexibility for testing with and without the chip installed.

4.3

SOFTWARE

The chip control program is written in C (roughly 1,500 lines of code) and communicates to the Boltzmann interface card through the virtual memory. The user can
read/write to all activation and weight memory locations and all functions of the
chip (learning, recall, annealing, etc.) can thus be specified in software.

5

CONCLUSIONS AND FUTURE WORK

The chip was designed so that interchip communications could be easily incorporated by means of high-speed parallel busses. The SBus board, interface and software described above will require only minor changes to incorporate a multi-chip
module (MCM) containing several such chips (for instance 16). There is minimal

,--

2
1. 75
til
.w 1.5
.w 1. 25
111
2:
1
~
Q)
0.75
~
0
0.5
0.
0.25
0

i

i,

I

f--T;
!

i

-

,

----

i

,i
I

i
i
I

i
,

i
50

60

70

80

90

I
I
100 110

frequency, MHz

Figure 3: Power dissipation of the chip during full operation at 5 Volts.

Digital Boltzmann VLSI for Constraint Satisfaction and Learning

inter chip communication delay ? 3% overhead), and thus MCM versions of our
system promise to be extremely powerful learning systems for large neural network
problems (Murrayet al., 1992).
Acknowledgements

Thanks to Martin Boliek and Donald Wynn for assistance in design and construction of the SBus board. Research support by NASA through grant NAGW419 is
gratefully acknowledged; VLSI fabrication by MOSIS. Send reprint requests to Dr.
Stor k: stor k@crc.ricoh.com.
References

E. Aarts & J. Korst. (1989) Simulated Annealing and Boltzmann Machines: A
stochastic approach to combinatorial optimization and neural computing. New York:
Wiley.
D. H. Ackley & G. E. Hinton. (1985) A learning algorithm for Boltzmann machines.
Cognitive Science 9, 147-169.
J. Alspector, A. Jayakumar & S. Luna. (1992) ExpeJimental evaluation of learning
in a neural microsystem. Advances in Neural Information Processing Systems-4,
J. E. Moody, S. J. Hanson & R. P. Lippmann (eds.), San Mateo, CA: Morgan
Kaufmann, 871-878.
Y. Arima, K. Mashiko, K. Okada, T. Yamada, A. Maeda, H. Kondoh & S. Kayano.
(1990) A self-learning neural network chip with 125 neurons and 10K selforganization synapses. In Symposium on VLSI Circuits, Solid State Circuits Council
Staff, Los Alamitos, CA: IEEE Press, 63-64.
J. B. Burr. (1991) Digital Neural Network Implementations. Neural Networks:
Concepts, Applications, and Implementations, Volume 2, P. Antognetti & V. Milutinovic (eds.) 237-285, Englewood Cliffs, NJ: Prentice Hall.
J. B. Burr. (1992) Digital Neurochip Design. Digital Parallel Implementations of
Neural Networks. K. Wojtek Przytula & Viktor K. Prasanna (eds.), Englewood
Cliffs, N J: Prentice Hall.
C. C. Galland. (1993) The limitations of deterministic Boltzmann machine learning.
Network 4, 355-379.
G. E. Hinton. (1989) Deterministic Boltzmann learning performs steepest descent
in weight-space. Neural Computation 1, 143-150.
C. Peterson & E. Hartman. (1989) Explorations of the mean field theory learning
algorithm. Neural Networks 2, 475-494.
M. Murray, J. B. Burr, D. G. Stork, M.-T. Leung, K. Boonyanit, G. J. Wolff
& A. M. Peterson. (1992) Deterministic Boltzmann machine VLSI can be scaled
using multi-chip modules. Proc. of the International Conference on Application
Specific Array Processors. Berkeley, CA (August 4-7) Los Alamitos, CA: IEEE
Press, 206-217.

903

"
2003,No Unbiased Estimator of the Variance of K-Fold Cross-Validation,,2468-no-unbiased-estimator-of-the-variance-of-k-fold-cross-validation.pdf,Abstract Missing,"No Unbiased Estimator of the Variance of
K-Fold Cross-Validation
Yoshua Bengio and Yves Grandvalet
Dept. IRO, Universit?e de Montr?eal
C.P. 6128, Montreal, Qc, H3C 3J7, Canada
{bengioy,grandvay}@iro.umontreal.ca

Abstract
Most machine learning researchers perform quantitative experiments to
estimate generalization error and compare algorithm performances. In
order to draw statistically convincing conclusions, it is important to estimate the uncertainty of such estimates. This paper studies the estimation
of uncertainty around the K-fold cross-validation estimator. The main
theorem shows that there exists no universal unbiased estimator of the
variance of K-fold cross-validation. An analysis based on the eigendecomposition of the covariance matrix of errors helps to better understand
the nature of the problem and shows that naive estimators may grossly
underestimate variance, as con?rmed by numerical experiments.

1

Introduction

The standard measure of accuracy for trained models is the prediction error (PE), i.e. the
expected loss on future examples. Learning algorithms themselves are often compared on
their average performance, which estimates expected value of prediction error (EPE) over
training sets. If the amount of data is large enough, PE can be estimated by the mean
error over a hold-out test set. The hold-out technique does not account for the variance
with respect to the training set, and may thus be considered inappropriate for the purpose
of algorithm comparison [4]. Moreover, it makes an inef?cient use of data which forbids
its application to small sample sizes. In this situation, one resorts to computer intensive
resampling methods such as cross-validation or bootstrap to estimate PE or EPE. We
focus here on K-fold cross-validation. While it is known that cross-validation provides an
unbiased estimate of EPE, it is also known that its variance may be very large [2]. This
variance should be estimated to provide faithful con?dence intervals on PE or EPE, and
to test the signi?cance of observed differences between algorithms. This paper provides
theoretical arguments showing the dif?culty of this estimation.
The dif?culties of the variance estimation have already been addressed [4, 7, 8]. Some
distribution-free bounds on the deviations of cross-validation are available, but they are
speci?c to locally de?ned classi?ers, such as nearest neighbors [3]. This paper builds upon
the work of Nadeau and Bengio [8], which investigated in detail the theoretical and practical merits of several estimators of the variance of cross-validation. Our analysis departs
from this work in the sampling procedure de?ning the cross-validation estimate. While [8]
considers K independent training and test splits, we focus on the standard K-fold cross-

validation procedure, with no overlap between test sets: each example is used once and
only once as a test example.

2

General Framework

Formally, we have a training set D = {z1 , . . . , zn }, with zi ? Z, assumed independently
sampled from an unknown distribution P . We also have a learning algorithm A : Z ? ? F
which maps a data set to a function. Here we consider symmetric algorithms, i.e. A is
insensitive to the ordering of examples in the training set D. The discrepancy between
the prediction and the observation z is measured by a loss functional L : F ? Z ? R.
For example one may take in regression L(f, (x, y)) = (f (x) ? y)2 , and in classi?cation
L(f, (x, y)) = 1f (x)6=y .
Let f = A(D) be the function returned by algorithm A on the training set D. In
application-based evaluation, the goal of learning is usually stated as the minimization
of the expected loss of f = A(D) on future test examples:
PE(D) = E[L(f, z)] ,

(1)

where the expectation is taken with respect to z ? P . To evaluate and compare learning algorithms [4] we care about the expected performance of learning algorithm A over
different training sets:
EPE(n) = E[L(A(D), z)] ,
(2)
where the expectation is taken with respect to D ? z independently sampled from P n ? P .
When P is unknown, PE and EPE have to be estimated, and it is crucial to assess the
uncertainty attached to this estimation. Although this point is often overlooked, estimating
c and EPE
[ requires caution, as illustrated here.
the variance of the estimates PE
2.1

Hold-out estimates of performance

c is given by the
The mean error over a hold-out test set estimates PE, and the variance of PE
usual variance estimate for means of independent variables. However, this variance estima[ the test errors are correlated when the training set is considered as
tor is not suited to EPE:
a random variable.
Figure 1 illustrates how crucial it is to take these correlations into account. The average
ratio (estimator of variance/empirical variance) is displayed for two variance estimators, in
an ideal situation where 10 independent training and test sets are available. The average of
?b1 /?, the naive variance estimator ignoring correlations, shows that this estimate is highly
down-biased, even for large sample sizes.
1

0.8

0.6

0.4

100

200

300

400

500

600

Figure 1: Average ratio (estimator of variance/empirical variance) on 100 000 experiments:
?b1 /? (ignoring correlations, lower curve) and ?b2 /? (taking into account correlations, upper
curve) vs. sample size n. The error bars represent ?2 standard errors on the average value.

Experiment 1 Ideal hold-out estimate of EPE.
We have K = 10 independent training sets D1 , . . . , DK of n independent examples
zi = (xi , yi ), where xi = (xi1 , . .p
. , xid )0 is a d-dimensional centered, unit covariance
Pd
Gaussian variable (d = 30), yi = 3/d k=1
p xik + ?i with ?i being independent, centered, unit variance Gaussian variables (the 3/d factor provides R2 ' 3/4). We also
have K independent test sets T1 , . . . , TK of size n sampled from the same distribution.
The learning algorithm consists in ?tting a line by ordinary least squares, and the
? =
[ = L
estimate of EPE is the average quadratic loss on test examples EPE
PK 1 P
1
k=1 n
zi ?Tk Lki , where Lki = L(A(Dk ), zi ).
K
PK P
1
? 2 , which
[ is ?b1 =
The ?rst estimate of variance of EPE
(Lki ? L)
Kn(Kn?1)

k=1

i

is unbiased provided there is no correlation between test errors. The second estimate is
PK P
1
?
?
?b2 = K(K?1)n
2
k=1
i,j (Lki ? L)(Lkj ? L), which estimates correlations.

Note that Figure 1 suggests that the naive estimator of variance ?b1 asymptotically converges
to the true variance. This can be shown by taking advantage of the results in this paper, as
a.s.
long as the learning algorithm converges (PE(D) ? limn?? EPE(n)), i.e. provided that
[ is due to the ?nite test size.
the only source of variability of EPE
2.2

K-fold cross-validation estimates of performance

In K-fold cross-validation [9], the data set D is ?rst chunked into K disjoint subsets (or
blocks) of the same size m = n/K (to simplify the analysis below we assume that n is a
multiple of K). Let us write Tk for the k-th such block, and Dk the training set obtained
by removing the elements in Tk from D. The estimator is
CV =

K
1 X 1 X
L(A(Dk ), zi ) .
K
m
k=1

(3)

zi ?Tk

Under stability assumptions on A, CV estimates PE(D) at least as accurately as the
training error [6]. However, as CV is an average of unbiased estimates of PE(D 1 ),
. . . , PE(DK ), a more general statement is that CV estimates unbiasedly EPE(n?m).
Note that the forthcoming analysis also applies to the version of cross-validation dedicated
to comparing algorithms, using matched pairs
K
1 X 1 X
?CV =
L(A1 (Dk ), zi ) ? L(A2 (Dk ), zi ) ,
K
m
k=1

zi ?Tk

and to the delete-m jackknife estimate of PE(D) debiasing the training error (see e.g. [5]):
?
!
n
K X
n
X
1X
1
1X
JK =
L(A(D), zi )?(K?1)
L(A(Dk ), zi ) ?
L(A(D), zi ) .
n i=1
K(n ? m)
n i=1
k=1 zi ?Dk

In what follows, CV, ?CV and JK will generically be denoted by ?
?:
?
?

=

n
K
1 X 1 X
1X
ei =
ei ,
n i=1
K
m
k=1

i?Tk

where, slightly abusing notation, i ? Tk means zi ? Tk and
?
? L(A(Dk ), zi )
L(A1 (Dk ), zi ) ? L(A2 (Dk ), zi )
?i ? Tk , ei =
? KL(A(D), z ) ? P
i
`6=k L(A(D` ), zi )

for ?
? = CV ,
for ?
? = ?CV ,
for ?
? = JK .

Note that ?
? is the average of identically distributed (dependent) variables. Thus, it asymptotically converges to a normally distributed variable, which is completely characterized by
its expectation E[?
?] and its variance Var[?
?].

3

Structure of the Covariance Matrix

P
The variance of ?
? is ? = n12 i,j Cov(ei , ej ) . By using symmetry over permutations of
the examples in D, we show that the covariance matrix has a simple block structure.

Lemma 1 Using the notation introduced in section 2.2, 1) all ei are identically distributed;
2) all pairs (ei , ej ) belonging to the same test block are jointly identically distributed; 3)
all pairs (ei , ej ) belonging to different test blocks are jointly identically distributed;
Proof: derived immediately from the permutation-invariance of P (D) and the symmetry
of A. See [1] for details and the proofs not shown here for lack of space.
Corollary 1 The covariance matrix ? of cross-validation errors e = (e1 , . . . , en )0 has
the simple block structure depicted in Figure 2: 1) all diagonal elements are identical
?i, Cov(ei , ei ) = Var[ei ] = ? 2 ; 2) all the off-diagonal entries of the K m ? m diagonal
blocks are identical ?(i, j) ? Tk2 : j 6= i, T (j) = T (i), Cov(ei , ej ) = ?; 3) all the
remaining entries are identical ?i ? Tk , ?j ? T` : ` 6= k, Cov(ei , ej ) = ?.
n

z

| {z }

}|

{

m

Figure 2: Structure of the covariance matrix.
Corollary 2 The variance of the cross-validation estimator is a linear combination of three
moments:
1 X
1
m?1
n?m
? =
Cov(ei , ej ) = ? 2 +
?+
?
(4)
2
n i,j
n
n
n
Hence, the problem of estimating ? does not involve estimating n(n + 1)/2 covariances,
but it cannot be reduced to that of estimating a single variance parameter. Three components intervene, which may be interpreted as follows when ?
? is the K-fold cross-validation
estimate of EPE:
1. the variance ? 2 is the average (taken over training sets) variance of errors for
?true? test examples (i.e. sampled independently from the training sets) when
algorithm A is fed with training sets of size m(K ? 1);
2. the within-block covariance ? would also apply to these ?true? test examples; it
arises from the dependence of test errors stemming from the common training set.
3. the between-blocks covariance ? is due to the dependence of training sets (which
share n(K ? 2)/K examples) and the fact that test block Tk appears in all the
training sets D` for ` 6= k.

4

No Unbiased Estimator of Var[?
?] Exists

Consider a generic estimator ?? that depends on the sequence of cross-validation errors
e = (e1 , e2 , . . . , en )0 . Assuming ?? is analytic in e, consider its Taylor expansion:
X
X
X
?? = ?0 +
?1 (i)ei +
?2 (i, j)ei ej +
?3 (i, j, k)ei ej ek + . . .
(5)
i

i,j

i,j,k

? = Var[?
We ?rst show that for unbiased variance estimates (i.e. E[ ?]
?]), all the ?i coef?cients must vanish except for the second order coef?cients ? 2,i,j .

Lemma 2 There is no universal unbiased estimator of Var[?
?] that involves the e i in a
non-quadratic way.
Proof: Take the expected value of ?? expressed as in (5), and equate it with Var[?
?] (4).
Since estimators that include moments other than the second moments in their expectation
are biased, we now focus on estimators which are quadratic forms of the errors, i.e.
X
?? = e0 We =
Wij ei ej .
(6)
i,j

Lemma 3 The expectation of quadratic estimators ?? de?ned as in (6) is a linear combination of only three terms
? = a(? 2 + ?2 ) + b(? + ?2 ) + c(? + ?2 ) ,
E[?]
(7)
where (a, b, c) are de?ned as follows:
?
Pn
?
?
ii ,
? a = Pi=1 W
P
?
K P
b =
k=1
i?Tk
j?Tk :j6=i Wij ,
?
P
P
PK P
?
?
c =
k=1
`6=k
i?Tk
j?T` Wij .

A ?trivial? representer of estimators with this expected value is
?? = as1 + bs2 + cs3 ,

(8)

where (s1 , s2 , s3 ) are the only quadratic statistics of e that are invariants to the within
blocks and between blocks permutations described in Lemma 1:
?
Pn 2
?
1
?
ei ,
?
? s1 = n i=1P
P
?
K P
1
s2 = n(m?1) k=1 i?Tk j?Tk :j6=i ei ej ,
(9)
?
PK P
P
P
?
?
1
? s =
3
k=1
`6=k
i?Tk
j?T` ei ej .
n(n?m)
Proof: in (6), group the terms that have the same expected values (from Corollary 1).
Theorem 1 There exists no universally unbiased estimator of Var[?
?].
? = Var[?
Proof: thanks to Lemma 2 and 3, it is enough to show that E[?]
?] has no solution
for quadratic estimators:
1
m?1
n?m
? = Var[?
E[?]
?] ? a(? 2 + ?2 ) + b(? + ?2 ) + c(? + ?2 ) = ? 2 +
?+
? .
n
n
n
Finding (a, b, c) satisfying this equality for all admissible values of (?, ? 2 , ?, ?) is impossible since it is equivalent to solving the following overdetermined system:
?
a
= n1 ,
?
?
b
= m?1
,
n
(10)
n?m
c
=
,
?
?
n
Q.E.D.
a+b+c = 0

5

Eigenanalysis of the covariance matrix

One way to gain insight on the origin of the negative statement of Theorem 1 is via the
eigenanalysis of ?, the covariance of e. This decomposition can be performed analytically
thanks to the very particular block structure displayed in Figure 2.
Lemma 4 Let vk be the binary vector indicating the membership of each example to test
block k. The eigenvalues of ? are as follows:
? ?1 = ? 2 ? ? with multiplicity n ? K and eigenspace orthogonal to {vk }K
k=1 ;
? ?2 = ? 2 + (m ? 1)? ? m? with multiplicity K ? 1 and eigenspace de?ned in
the orthogonal of 1 by the basis {vk }K
k=1 ;
? ?3 = ? 2 + (m ? 1)? + (n ? m)? with eigenvector 1.
Lemma 4 states that the vector e can be decomposed into three uncorrelated parts: n ? K
projections to the subspace orthogonal to {vk }K
k=1 , K ? 1 projections to the subspace
in
the
orthogonal
of
1,
and
one projection on 1.
spanned by {vk }K
k=1
A single vector example with n independent elements can be seen as n independent examples. Similarly, the uncorrelated projections of e can be equivalently represented by
respectively n ? K, K ? 1 and one uncorrelated one-dimensional examples.
In particular, for the projection on 1, with a single example, the sample variance is null,
resulting in the absence of unbiased variance estimator of ?3 . The projection of e on
the eigenvector n1 1 is precisely ?
?. Hence there is no unbiased estimate of V ar[?
?] = ?n3
when we have only one realization of the vector e. For the same reason, even with simple
parametric assumptions on e (such as e Gaussian), the maximum likelihood estimate of ?
is not de?ned. Only ? 1 and ?2 can be estimated unbiasedly. Note that this problem cannot
be addressed by performing multiple K-fold splits of the data set. Such a procedure would
not provide independent realizations of e.

6

Possible values for ? and ?

Theorem 1 states that no estimator is unbiased, and in its demonstration, it is shown that
the bias of any quadratic estimator is a linear combination of ?2 , ? 2 , ? and ?. Regarding
estimation, it is thus interesting to see what constraints restrict their possible range.
Lemma 5 For ?
? = CV and ?
? = ?CV, the following inequalities hold:
?
0
? ? ?
?2
1
1
2
2
? ? m (? + (m ? 1)?)
? ? n?m (? + (m ? 1)?) ?
0
? ? ? ?2
?
m
? n?m
?2 ? ? ? ?2 .
The admissible (?, ?) region is very large, and there is no constraint linking ? to ? 2 . Hence,
we cannot propose a variance estimate with universally small bias.

7

Experiments

The bias of any quadratic estimator is a linear combination of ?2 , ? 2 , ? and ?. The admissible values provided earlier suggest that ? and ? cannot be proved to be negligible
compared to ? 2 . This section illustrates that in practice, the contribution to the variance of
?
? due to ? and ? (see Equation (4)) can be of same order as the one due ? 2 . This con?rms
that the estimators of ? should indeed take into account the correlations of ei .
Experiment 2 True variance of K-fold cross-validation.
We repeat the experimental setup of Experiment 1, except that only one sample of size n is
available. Since cross-validation is known to be sensitive to the instability of algorithms,

in addition to this standard setup, we also consider another one with outliers:
The input xi = (xi1 , . . . , xid )0 is still 30-dimensional, but it is now a mixture of two centered Gaussian: let ti be a binary variable, withpP (ti = 1) = p = 0.95; ti = 1 ? xi ?
Pd
N (0, I), ti = 0 ? xi ? N (0, 100I); yi = 3/(d(p + 100(1 ? p))) k=1 xik + ?i ;
ti = 1 ? ?i ? N (0, 1/(p + 100(1 ? p))), ti = 0 ? ?i ? N (0, 100/(p + 100(1 ? p))).

We now look at the variance of K-fold cross-validation (K = 10), and decompose in the
three orthogonal components ? 2 , ? and ?. The results are shown in Figure 3.
0.25

4

2

?
?
?

0.2

?

?

0.15

?2
?
?

3

2

0.1
1

0.05
0

60

0

80 100 120 160 220 280 360 460 600

n?m

60

80 100 120 160 220 280 360 460 600

n?m

no outliers

outliers

Figure 3: Contributions of (? 2 , ?, ?) to total variance V ar[CV ] vs. n ? m.
Without outliers, the contribution of ? is very important for small sample sizes. For large
sample sizes, the overall variance is considerably reduced and is mainly caused by ? 2
because the learning algorithm returns very similar answers for all training sets. When
there are outliers, the contribution of ? is of same order as the one of ? 2 even when the
ratio of examples to free parameters is large (here up to 20). Thus, in dif?cult situations,
where A(D) varies according to the realization of D, neglecting the effect of ? and ? can
be expected to introduce a bias of the order of the true variance.
It is also interesting to see how these quantities are affected by the number of folds K. The
decomposition of ? in ? 2 , ? and ? (4) does not imply that K should be set either to n or
to 2 (according to the sign of ? ??) in order to minimize the variance of ?
?. Modifying
K affects ? 2 , ? and ? through the size and overlaps of the training sets D1 , . . . , DK , as
illustrated in Figure 4. For a ?xed sample size, the variance of ?
? and the contribution of ? 2 ,
? and ? vary smoothly with K (of course, the mean of ?
? is also affected in the process).
2.5

0.25

2

?
?
?

0.2

1.5

?

?

0.15

2

0.1

1

0.05

0.5

0

2 3 4 5 6 8 10 12 15 20 24 30 40 60120

K

no outliers

0

2 3 4 5 6 8 10 12 15 20 24 30 40 60120

K

outliers

Figure 4: Contributions of (? 2 , ?, ?) to total variance V ar[CV ] vs. K for n = 120.

8

Discussion

The analysis presented in this paper for K-fold cross-validation can be instantiated to several interesting cases. First, when having K independent training and test sets (K = 1

is the realistic case), the structure of hold-out errors resemble the one of cross-validation
errors, with ? = 0. Knowing that allows to build the unbiased estimate ?b2 given in 2.1:
knowing that ? = 0 removes the third equation of system (10) in the proof of Theorem 1.
Two-fold cross-validation has been advocated to perform hypothesis testing [4]. It is a
special case of K-fold cross-validation where the training blocks are mutually independent
since they do not overlap. However, this independence does not modify the structure of e
in the sense that ? is not null. The between-block correlation stems from the fact that the
training block D1 is the test block T2 and vice-versa.
Finally, Leave-one-out cross validation is another particular case, with K = n. The
structure of the covariance matrix is simpli?ed, without diagonal blocks. The estimation
dif?culties however remain: even in this particular case, there is no unbiased estimate of
variance. From the de?nition of b in Lemma 3, we have b = 0, and with m = 1 the linear
system (10) still admits no solution.
To summarize, it is known that K-fold cross-validation may suffer from high variability,
which can be responsible for bad choices in model selection and erratic behavior in the
estimated expected prediction error [2, 4, 8]. This paper demonstrates that estimating the
variance of K-fold cross-validation is dif?cult. Not only there is no unbiased estimate of
this variance, but we have no theoretical result showing that this bias should be negligible in the non-asymptotical regime. The eigenanalysis of the covariance matrix of errors
traces the problem back to the dependencies between test-block errors, which induce the
absence of redundant pieces of information regarding the average test error. i.e. the K-fold
cross-validation estimate. It is clear that this absence of redundancy is bound to provide
dif?culties in the estimation of variance.
Our experiments show that the bias incurred by ignoring test errors dependencies can be
of the order of the variance itself, even for large sample sizes. Thus, the assessment of the
signi?cance of observed differences in cross-validation scores should be treated with much
caution. The next step of this study consists in building and comparing variance estimators
dedicated to the very speci?c structure of the test-block error dependencies.

References
[1] Y. Bengio and Y. Grandvalet. No unbiased estimator of the variance of K-fold cross-validation.
Journal of Machine Learning Research, 2003.
[2] L. Breiman. Heuristics of instability and stabilization in model selection. The Annals of Statistics,
24(6):2350?2383, 1996.
[3] L. Devroye, L. Gy?or?, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer,
1996.
[4] T. G. Dietterich. Approximate statistical tests for comparing supervised classi?cation learning
algorithms. Neural Computation, 10(7):1895?1924, 1999.
[5] B. Efron and R. J. Tibshirani. An Introduction to the Bootstrap, volume 57 of Monographs on
Statistics and Applied Probability. Chapman & Hall, 1993.
[6] M. Kearns and D. Ron. Algorithmic stability and sanity-check bounds for leave-one-out crossvalidation. Neural Computation, 11(6):1427?1453, 1996.
[7] R. Kohavi. A study of cross-validation and bootstrap for accuracy estimation and model selection. In Proceedings of the Fourteenth International Joint Conference on Arti?cial Intelligence,
pages 1137?1143, 1995.
[8] C. Nadeau and Y. Bengio. Inference for the generalization error. Machine Learning, 52(3):239?
281, 2003.
[9] M. Stone. Cross-validatory choice and assessment of statistical predictions. Journal of the Royal
Statistical Society, B, 36(1):111?147, 1974.

"
1989,Optimal Brain Damage,,250-optimal-brain-damage.pdf,Abstract Missing,"598

Le Cun, Denker and Solla

Optimal Brain Damage

Yann Le Cun, John S. Denker and Sara A. Sol1a
AT&T Bell Laboratories, Holmdel, N. J. 07733

ABSTRACT
We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural
network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer
training examples required, and improved speed of learning and/or
classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training
set error. Experiments confirm the usefulness of the methods on a
real-world application.

1

INTRODUCTION

Most successful applications of neural network learning to real-world problems have
been achieved using highly structured networks of rather large size [for example
(Waibel, 1989; Le Cun et al., 1990a)]. As applications become more complex, the
networks will presumably become even larger and more structured. Design tools
and techniques for comparing different architectures and minimizing the network
size will be needed. More importantly, as the number of parameters in the systems
increases, overfitting problems may arise, with devastating effects on the generalization performance. We introduce a new technique called Optimal Brain Damage
(OBD) for reducing the size of a learning network by selectively deleting weights.
We show that OBD can be used both as an automatic network minimization procedure and as an interactive tool to suggest better architectures.
The basic idea of OBD is that it is possible to take a perfectly reasonable network,
delete half (or more) of the weights and wind up with a network that works just as
well, or better. It can be applied in situations where a complicated problem must

Optimal Brain Damage

be solved, and the system must make optimal use of a limited amount of training
data. It is known from theory (Denker et al., 1987; Baum and Haussler, 1989; Solla
et al., 1990) and experience (Le Cun, 1989) that, for a fixed amount of training
data, networks with too many weights do not generalize well. On the other hand.
networks with too few weights will not have enough power to represent the data
accurately. The best generalization is obtained by trading off the training error and
the network complexity.
One technique to reach this tradeoff is to minimize a cost function composed of two
terms: the ordinary training error, plus some measure of the network complexity.
Several such schemes have been proposed in the statistical inference literature [see
(Akaike, 1986; Rissanen, 1989; Vapnik, 1989) and references therein] as well as in
the NN literature (Rumelhart, 1988; Chauvin, 1989; Hanson and Pratt, 1989; Mozer
and Smolensky, 1989).
Various complexity measures have been proposed, including Vapnik-Chervonenkis
dimensionality (Vapnik and Chervonenkis, 1971) and description length (Rissanen,
1989) . A time-honored (albeit inexact) measure of complexity is simply the number
of non-zero free parameters, which is the measure we choose to use in this paper
[but see (Denker, Le Cun and Solla, 1990)]. Free parameters are used rather than
connections, since in constrained networks, several connections can be controlled by
a single parameter.
In most cases in the statistical inference literature, there is some a priori or heuristic
information that dictates the order in which parameters should be deleted; for
example, in a family of polynomials, a smoothness heuristic may require high-order
terms to be deleted first. In a neural network, however, it is not at all obvious in
which order the parameters should be deleted.
A simple strategy consists in deleting parameters with small ""saliency"", i.e. those
whose deletion will have the least effect on the training error. Other things being equal, small-magnitude parameters will have the least saliency, so a reasonable
initial strategy is to train the network and delete small-magnitude parameters in
order. After deletion, the network should be retrained. Of course this procedure
can be iterated; in the limit it reduces to continuous weight-decay during training
(using disproportionately rapid decay of small-magnitude parameters). In fact, several network minimization schemes have been implemented using non-proportional
weight decay (Rumelhart, 1988; Chauvin, 1989; Hanson and Pratt, 1989), or ""gating coefficients"" (Mozer and Smolensky, 1989). Generalization performance has
been reported to increase significantly on the somewhat small problems examined.
Two drawbacks of these techniques are that they require fine-tuning of the ""pruning"" coefficients to avoid catastrophic effects, and also that the learning process
is significantly slowed down. Such methods include the implicit hypothesis that
the appropriate measure of network complexity is the number of parameters (or
sometimes the number of units) in the network.
One of the main points of this paper is to move beyond the approximation that
""magnitude equals saliency"" , and propose a theoretically justified saliency measure.

599

600

Le Cun, Denker and Solla

Our technique uses the second derivative of the objective function with respect to
the parameters to compute the saliencies. The method was ,,-alidated using our
handwritten digit recognition network trained with backpropagation (Le Cun et aI.,
1990b).

2

OPTIMAL BRAIN DAMAGE

Objective functions playa central role in this field; therefore it is more than reasonable to define the saliency of a parameter to be the change in the objective
function caused by deleting that parameter. It would be prohibiti,-ely laborious to
evaluate the saliency directly from this definition, i.e. by temporarily deleting each
parameter and reevaluating the objective function.
Fortunately, it is possible to construct a local model of the error function and
analytically predict the effect of perturbing the parameter vector. ""'e approximate
the objective function E by a Taylor series. A perturbation lL~ of the parameter
vector will change the objective function by
(1)

Here, the 6ui'S are the components of flJ, the gi's are the components of the gradient
G of E with respect to U, and the hi;'S are the elements of the Hessian matrix H
of E with respect to U:

8E

gi= -8
Ui

and

(2)

The goal is to find a set of parameters whose deletion will cause the least increase
of E . This problem is practically insoluble in the general case. One reason is
that the matrix H is enormous (6.5 x 10 6 terms for our 2600 parameter network),
and is very difficult to compute. Therefore we must introduce some simplifying
approximations. The ""diagonal"" approximation assumes that the 6E caused by
deleting several parameters is the sum of the 6E's caused by delet~ each parameter
individually; cross terms are neglected, so third term of the npt hand side of
equation 1 is discarded. The ""extremal"" approximation assumes that parameter
deletion will be performed after training has converged. The parameter vector is
then at a (local) minimum of E and the first term of the right hand side of equation 1
can be neglected. Furthermore, at a local minimum, all the hii's are non-negative,
so any perturbation of the parameters will cause E to increase or stay the same.
Thirdly, the ""quadratic"" approximation assumes that the cost fundion is nearly
quadratic 80 that the last term in the equation can be neglected. Equation 1 then
reduces to
6E=~~h""6u~
(3)
2L.i
"" ?
i

Optimal Brain Damage
2.1

COMPUTING THE SECOND DERIVATIVES

Now we need an efficient way of computing the diagonal second derivatives hi i .
Such a procedure was derived in (Le Cun, 1987), and was the basis of a fast backpropagation method used extensively in \1lrious applications (Becker and Le Cun,
1989; Le Cun, 1989; Le Cun et al., 1990a). The procedure is very similar to the
back-propagation algorithm used for computing the first derivatives. We will only
outline the proced ure; details can be found in the references.
We assume the objective function is the usual mean-squared error (MSE); generalization to other additive error measures is straightforward. The following expressions apply to a single input pattern; afterward E and H must be averaged over
the training set. The network state is computed using the standard formulae
and

ai

=L

WijZj

( 4)

j

where Zi is the state of unit i, ai its total input (weighted sum), ! the squashing
function and Wij is the connection going from unit j to unit i. In a shared-weight
network like ours, a single parameter Uk can control one or more connections: Wij
Uk for all (i, j) E Vk, where Vk is a set of index pairs. By the chain rule, the diagonal
terms of H are given by

=

hu =

{)2E

L

(i,j)EV.

{)w~,

(5)

.,

The summand can be expanded (using the basic network equations 4) as:
{J2E

lP E

.

2

--=-z?

.,

{Jw~.

{Ja~'

(6)

The second derivatives are back-propagated from layer to layer:

(7)
We also need the boundary condition at the output layer, specifying the second
derivative of E with respect to the last-layer weighted BUms:
{J{J2

~ = 2!'(ai)2 -

ai

2(di - Zi)!""(ai)

(8)

for all units i in the output layer.
As can be seen, computing the diagonal Hessian is of the same order of complexity
as computing the gradient. In some cases, the second term of the right hand side of
the last two equations (involving the second derivative of I) can be neglected. This
corresponds to the well-known Levenberg-Marquardt approximation, and has the
interesting property of giving guaranteed positive estimates of the second derivative.

601

602

Le Cun, Denker and Solla
2.2

THE RECIPE

The OBO procedure can be carried out as follows:
1. Choose a reasonable network architecture

2.
3.
4.
5.
6.

Train the network until a reasonable solution is obtained
Compute the second derivatives hu for each parameter
Compute the saliencies for each parameter: Sk = huu~/2
Sort the parameters by saliency and delete some low-saliency parameters
Iterate to step 2

Deleting a parameter is defined as setting it to 0 and freezing it there. Several
variants of the procedure can be devised, such as decreasing the ...41ues of the lowsaliency parameters instead of simply setting them to 0, or allowing the deleted
parameters to adapt again after they have been set to o.

2.3

EXPERIMENTS

The simulation results given in this section were obtained using back-propagation
applied to handwritten digit recognition. The initial network was highly constrained
and sparsely connected, having 10 5 connections controlled by 2578 free parameters.
It was trained on a database of segmented handwritten zip code digits and printed
digits containing approximately 9300 training examples and 3350 t.est examples.
More details can be obtained from the companion paper (Le Cun et al., 1990b).
16

16

<a>

14
1
10
pJ 8

Magnitude

~6

~6

b04

b04

-o
o

OBD

.9

o

~~--~--~---+--~----~

o

(b)

14
1
10
pJ 8

500

1000 1500 2000 2SOO

Parameters

-2~

o

__~__~__-+________~
SOO 1000 1500 laX) 2SOO

Parameters

Figure 1: (a) Objective function (in dB) versus number of paramet.ers for OBn
(lower curve) and magnitude-based parameter deletion (upper curve). (b) Predicted
and actual objective function versus number of parameters. The predicted value
(lower curve) is the sum of the saliencies of the deleted parameters.
Figure la shows how the objective function increases (from right to left) as the
number of remaining parameters decreases. It is clear that deletin~ parameters by

Optimal Brain Damage

order of saliency causes a significantly smaller increase of the objective function than
deleting them according to their magnitude. Random deletions were also tested for
the sake of comparison, but the performance was so bad that the curves cannot be
shown on the same scale.
Figure 1b shows how the objective function increases (from right to left) as the number of remaining parameters decreases, compared to the increase predicted by the
Quadratic-Extremum-Diagonal approximation. Good agrement is obtained for up
to approximately 800 deleted parameters (approximately 30% of the parameters).
Beyond that point, the curves begin to split, for several reasons: the off-diagonal
terms in equation 1 become disproportionately more important as the number of
deleted parameters increases, and higher-than-quadratic terms become more important when larger-valued parameters are deleted.
'
16
14
1

16
14
1

<a)

10

10

UJ 8

UJ 8

~

6

~

~4

~

-o
o

6
4.

~~

-2~--4----+----~--~--~

o

(b)

SOO

1000 1500 2000 2500

Parameters

-2~I--~,~
o

500

__

~, ~I

~I

+,____ __ ____
1000 1500 2000 2500

Parameters

Figure 2: Objective function (in dB) versus number of parameters, without retraining (upper curve), and after retraining (lower curve). Curves are given for the
training set (a) and the test set (b).
Figure 2 shows the log-MSE on the training set and the on the test set before and
after retraining. The performance on the training set and on the test set (after
retraining) stays almost the same when up to 1500 parameters (60% of the total)
are deleted.
We have also used OBn as an interactive tool for network design and analysis.
This contrasts with the usual view of weight deletion as a more-or-Iess automatic
procedure. Specifically, we prepared charts depicting the saliency of the 10,000
parameters in the digit recognition network reported last year (Le Cun et aI., 1990b).
To our surprise, several large groups of parameters were expendable. We were
able to excise the second-to-Iast layer, thereby reducing the number of parameters
by a factor of two. The training set MSE increased by a factor of 10, and the
generalization MSE increased by only 50%. The 10-category classification error
on the test set actually decreased (which indicates that MSE is not the optimal

603

604

Le Cun, Denker and Solla

objective function for this task). OBD motivated other architectural changes, as
can be seen by comparing the 2600-parameter network in (Le Cun et aI., 1990a) to
the 1O,OOO-parameter network in (Le Cun et aI., 1990b).

3

CONCLUSIONS AND OUTLOOK

We have used Optimal Brain Damage interactively to reduce the number of parameters in a practical neural network by a factor of four. We obtained an additional
factor of more than two by using OBD to delete parameters automatically. The network's speed improved significantly, and its recognition accuracy increased slightly.
We emphasize that the starting point was a state-of-the-art network. It would be
too easy to start with a foolish network and make large improvements: a technique
that can help improve an already-good network is particularly valuable.
We believe that the techniques presented here only scratch the surface of the applications where second-derivative information can and should be used. In particular,
we have also been able to move beyond the approximation that ""complexity equals
number of free parameters"" by using second-derivative information. In (Denker, Le
Cun and Solla, 1990), we use it to to derive an improved measure of the network's
information content, or complexity. This allows us to compare network architectures on a given task, and makes contact with the notion of Minimum Description
Length (MDL) (Rissanen, 1989). The main idea is that a ""simple"" network whose
description needs a small number of bits is more likely to generalize correctly than
a more complex network, because it presumably has extracted the essence of the
data and removed the redundancy from it.
Acknowledgments
We thank the US Postal Service and its contractors for providing us with the
database. We also thank Rich Howard and Larry Jackel for their helpful comments
and encouragements. We especially thank David Rumelhart for sharing unpublished
ideas.

References
Akaike, H. (1986). Use of Statistical Models for Time Series Analysis. In Proceedings
ICASSP 86, pages 3147-3155, Tokyo. IEEE.
Baum, E. B. and Haussler, D. (1989). What Size Net Gives Valid Generaliztion?
Neural Computation, 1:151-160.
Becker, S. and Le Cun, Y. (1989). Improving the Convergence of Back-Propagation
Learning with Second-Order Methods. In Touretzky, D., Hinton, G., and Sejnowski, T., editors, Proc. of the 1988 Connectionist Model& S.mmer School,
pages 29-37, San Mateo. Morgan Kaufman.
Chauvin, Y. (1989). A Back-Propagation Algorithm with Optimal Use of Hidden Units. In Touretzky, D., editor, Neural Information Proce$$ing S,&tems,
volume 1, Denver, 1988. Morgan Kaufmann.

Optimal Brain Damage

Denker, J., Schwartz, D., Wittner, B., Solla, S. A., Howard, R., Jackel, L., and
Hopfield, J. (1987). Large Automatic Learning, Rule Extraction and Generalization. Complex Systems, 1:877-922.
Denker, J. S., Le Cun, Y., and Solla, S. A. (1990). Optimal Brain Damage. To
appear in Computer and System Sciences.
Hanson, S. J. and Pratt, L. Y. (1989). Some Comparisons of Constraints for Minimal Network Construction with Back-Propagation. In Touretzky, D., editor,
Neural Information Processing Systems, volume 1, Denver, 1988. Morgan Kaufmann.
Le Cun, Y. (1987). Modeles Connexionnistes de l'Apprentissage. PhD thesis, Universite Pierre et Marie Curie, Paris, France.
Le Cun, Y. (1989). Generalization and Network Design Strategies. In Pfeifer, R.,
Schreter, Z., Fogelman, F., and Steels, L., editors, Connectionism in Perspective, Zurich, Switzerland. Elsevier.
Le Cun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard,
W., and Jackel, L. D. (1990a) . Handwritten Digit Recognition with a BackPropagation Network. In Touretzky, D., editor, Neural Information Processing
Systems, volume 2, Denver, 1989. Morgan Kaufman.
Le Cun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W.,
and Jackel, L. D. (1990b). Back-Propagation Applied to Handwritten Zipcode
Recognition. Neural Computation, 1{ 4).
Mozer, M. C. and Smolensky, P. (1989). Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment. In Touretzky, D.,
editor, Neural Information Processing Systefn$, volume 1, Denver, 1988. Morgan Kaufmann.
Rissanen, J. (1989). Stochastic Complexity in Statistical Inquiry. World Scientific,
Singapore.
Rumeihart, D. E. (1988). personal communication.
Solla, S. A., Schwartz, D. B., Tishby, N., and Levin, E. (1990). Supervised Learning: a Theoretical Framework. In Touretzky, D., editor, Neural Information
Processing Systems, volume 2, Denver, 1989. Morgan Kaufman.
Vapnik, V. N. (1989). Inductive Principles of the Search for Empirical Dependences.
In Proceedings of the second annual Workshop on Computational Learning Theory, pages 3-21. Morgan Kaufmann.
Vapnik, V. N. and Chervonenkis, A. Y. (1971). On the Uniform Convergence of
Relative Frequencies of Events to Their Probabilities. Th. Pro6. and its Applications, 17(2):264-280.
Waibel, A. (1989). Consonant Recognition by Modular Construction of Large
Phonemic Time-Delay Neural Networks. In Touretzky, D., editor, Neural Information Processing Systems, volume 1, pages 215-223, Denver, 1988. Morgan
Kaufmann.

605

"
2017,Unbounded cache model for online language modeling with open vocabulary,Poster,7185-unbounded-cache-model-for-online-language-modeling-with-open-vocabulary.pdf,"Recently, continuous cache models were proposed as extensions to recurrent neural network language models, to adapt their predictions to local changes in the data distribution. These models only capture the local context, of up to a few thousands tokens. In this paper, we propose an extension of continuous cache models, which can scale to larger contexts. In particular, we use a large scale non-parametric memory component that stores all the hidden activations seen in the past. We leverage recent advances in approximate nearest neighbor search and quantization algorithms to store millions of representations while searching them efficiently. We conduct extensive experiments showing that our approach significantly improves the perplexity of pre-trained language models on new distributions, and can scale efficiently to much larger contexts than previously proposed local cache models.","Unbounded cache model for online language
modeling with open vocabulary

Edouard Grave
Facebook AI Research
egrave@fb.com

Moustapha Cisse
Facebook AI Research
moustaphacisse@fb.com

Armand Joulin
Facebook AI Research
ajoulin@fb.com

Abstract
Recently, continuous cache models were proposed as extensions to recurrent neural
network language models, to adapt their predictions to local changes in the data
distribution. These models only capture the local context, of up to a few thousands
tokens. In this paper, we propose an extension of continuous cache models, which
can scale to larger contexts. In particular, we use a large scale non-parametric
memory component that stores all the hidden activations seen in the past. We
leverage recent advances in approximate nearest neighbor search and quantization
algorithms to store millions of representations while searching them efficiently. We
conduct extensive experiments showing that our approach significantly improves
the perplexity of pre-trained language models on new distributions, and can scale
efficiently to much larger contexts than previously proposed local cache models.

1

Introduction

Language models are a core component of many natural language processing applications such
as machine translation [3], speech recognition [2] or dialogue agents [50]. In recent years, deep
learning has led to remarkable progress in this domain, reaching state of the art performance on many
challenging benchmarks [31]. These models are known to be over-parametrized, and large quantities
of data are needed for them to reach their full potential [12]. Consequently, the training time can be
very long (up to weeks) even when vast computational resources are available [31]. Unfortunately, in
many real-world scenarios, either such quantity of data is not available, or the distribution of the data
changes too rapidly to permit very long training. A common strategy to circumvent these problems is
to use a pre-trained model and slowly finetune it on the new source of data. Such adaptive strategy is
also time-consuming for parametric models since the specificities of the new dataset must be slowly
encoded in the parameters of the model. Additionally, such strategy is also prone to overfitting and
dramatic forgetting of crucial information from the original dataset. These difficulties directly result
from the nature of parametric models.
In contrast, non-parametric approaches do not require retraining and can efficiently incorporate
new information without damaging the original model. This makes them particularly suitable for
settings requiring rapid adaptation to a changing distribution or to novel examples. However, nonparametric models perform significantly worse than fully trained deep models [12]. In this work,
we are interested in building a language model that combines the best of both non-parametric and
parametric approaches: a deep language model to model most of the distribution and a non-parametric
one to adapt it to the change of distribution.
This solution has been used in speech recognition under the name of cache models [36, 37]. Cache
models exploit the unigram distribution of a recent context to improve the predictive ability of the
model. Recently, Grave et al. [22] and Merity et al. [43] showed that this solution could be applied to
neural networks. However, cache models depend on the local context. Hence, they can only adapt a
parametric model to a local change in the distribution. These specificities limit their usefulness when
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

the context is unavailable (e.g., tweets) or is enormous (e.g., book reading). This work overcomes
this limitation by introducing a fast non-parametric retrieval system into the hybrid approach. We
demonstrate that this novel combination of a parametric neural language model with a non-parametric
retrieval system can smoothly adapt to changes in the distribution while remaining as consistent as
possible with the history of the data. Our approach is as a generalization of cache models which
scales to millions of examples.

2

Related work

This section reviews different settings that require models to adapt to changes in the data distribution,
like transfer learning or open set (continual) learning. We also discuss solutions specific to language
models, and we briefly explain large-scale retrieval methods.
Transfer Learning. Transfer learning [10] is a well-established component of machine learning
practitioners? toolbox. It exploits the commonalities between different tasks to improve the predictive
performance of the models trained to solve them. Notable variants of transfer learning are multitask
learning [10], domain adaptation [6], and curriculum learning [8]. Multitask learning jointly trains
several models to promote sharing of statistical strength. Domain adaptation reuses existing information about a given problem (e.g., data or model) to solve a new task. Curriculum learning takes
one step further by adapting an existing model across a (large) sequence of increasingly difficult
tasks. Models developed for these settings have proven useful in practice. However, they are chiefly
designed for supervised learning and do not scale to the size of the problem we consider in this work.
Class-incremental and Open Set Learning. These methods are concerned with problems where
the set of targets is not known in advance but instead, increases over time. The main difficulty
in this scenario lies in the deterioration of performance on previously seen classes when trying to
accommodate new ones. Kuzborskij et al. [39] proposed to reduce the loss of accuracy when adding
new classes by partly retraining the existing classifier. Muhlbaier et al. [47] introduced an ensemble
model to deal with an increasingly large number of concepts. However, their approach relies on
unrealistic assumptions on the data distribution. Zero-shot learning [41] can deal with new classes
but often requires additional descriptive information about them [1]. Scheirer et al. [49] proposed a
framework for open set recognition based on one-class SVMs.
Adaptive language models. Adaptive language models change their parameters according to the
recent history. Therefore, they implement a form of domain adaptation. A popular approach adds
a cache to the model and has shown early success in the context of speech recognition [36, 38, 37].
Jelinek et al. further extended this strategy [29] into a smoothed trigram language model, reporting a
reduction in both perplexity and word error rates. Della Pietra et al.[15] adapt the cache to a general
n-gram model such that it satisfies marginal constraints obtained from the current document. Closer
to our work, Grave et al. [21] have shown that this strategy can improve modern language models
like recurrent networks without retraining. However, their model assumes that the data distribution
changes smoothly over time, by using a context window to improve the performance. Merity et
al. [43] proposed a similar model, where the cache is jointly trained with the language model.
Other adaptive language models have been proposed in the past: Kneser and Steinbiss [35] and, Iyer
and Ostendorf [26] dynamically adapt the parameters of their model to recent history using different
weight interpolation schemes. Bellegarda [5] and Coccaro and Jurafsky [14] use latent semantic
analysis to adapt their models to current context. Similarly, topic features have been used with either
maximum entropy models [33] or recurrent networks [46, 53]. Finally, Lau et al. [42] propose to use
pairs of distant of words to capture long-range dependencies.
Large scale retrieval approaches. The standard method for large-scale retrieval is to compress
vectors and query them using a standard efficient algorithm. One of the most popular strategies is
Locality-sensitive hashing (LSH) by Charikar [11], which uses random projections to approximate
the cosine similarity between vectors by a function related to the Hamming distance between their
corresponding binary codes. Several works have built on this initial binarization technique, such as
spectral hashing [54], or Iterative Quantization (ITQ) [19]. Product Quantization (PQ) [28] approximates the distances between vectors by simultaneously learning the codes and the centroids, using
2

k-means. In the context of text, several works have shown that compression does not significantly
reduce the performance of models [17, 24, 30].

3

Approach

In this section, we first briefly review language modeling and the use of recurrent networks for this
task. We then describe our model, called unbounded cache, and explain how to scale it to large
datasets with millions of words.
3.1

Language modeling

A language model evaluates the probability distribution of sequences of words. It is often framed
as learning the conditional probability of words, given their history [4]. Let V be the size of the
vocabulary; each word is represented by a one-hot encoding vector x in RV = V, corresponding
to its index in the dictionary. Using the chain rule, the probability assigned to a sequence of words
x1 , . . . , xT can be factorized as
p(x1 , ..., xT ) =

T
Y

p(xt | xt?1 , ..., x1 ).

(1)

t=1

This conditional probability is traditionally approximated with non-parametric models based on
counting statistics [20]. In particular, smoothed N-gram models [32, 34] have been the dominant type
of models historically, achieving good performance in practice [44]. While the use of parametric
models for language modeling is not new [48], their superiority has only been established with the
recent emergence of neural networks [7, 45]. In particular, recurrent networks are now the standard
approach, achieving state-of-the-art performances on several challenging benchmarks [31, 55].
3.2

Recurrent networks.

Recurrent networks are a special case of neural networks specifically designed for sequence modeling.
At each time step, they maintain a hidden representation of the past and make a prediction accordingly.
This representation is maintained by a continuous vector ht ? Rd encoding the history xt , ..., x1 .
The probability of the next word is then simply parametrized using this hidden vector, i.e.,
p(w | xt , ..., x1 ) ? exp(h>
t ow ).

(2)

The hidden vector ht is computed by recursively applying an update rule:
ht = ? (xt , ht?1 ) ,

(3)

where ? is a function depending on the architecture of the network. Depending on ?, the hidden
vectors may have a specific structure adapted to different sequence representation problems. Several
architectures for recurrent networks have been proposed, such as the Elman network [16], the long
short-term memory (LSTM) [25] or the gated recurrent unit (GRU) [13]. For example, the Elman
network [16] is defined by the following update rule
ht = ? (Lxt + Rht?1 ) ,

(4)

where ? is a non-linearity such as the logistic or tanh functions, L ? Rd?V is a word embedding
matrix and R ? Rd?d is the recurrent matrix. Empirical results have validated the effectiveness of
the LSTM architecture to natural language modeling [31]. We refer the reader to [23] for details on
this architecture. In the rest of this paper, we focus on this structure of recurrent networks.
Recurrent networks process a sentence one word at a time and update their weights by backpropagating
the error of the prediction to a fixed window size of past time steps. This training procedure
is computationally expensive, and often requires a significant amount of data to achieve good
performance. To circumvent the need of retraining such network for domain adaptation, we propose
to add a non-parametric model that takes care of the fluctuation in the data distribution.
3

3.3

Unbounded cache

An unbounded cache adds a non-parametric and unconstrained memory to a neural network. Our
approach is inspired by the cache model of Khun [36] and can be seen as an extension of Grave et
al. [22] to an unbounded memory structure tailored to deal with out-of-vocabulary and rare words.
Similar to Grave et al. [22], we extend a recurrent neural network with a key-value memory component, storing the pairs (hi , wi+1 ) of hidden representation and corresponding word. This memory
component also shares similarity with the parametric memory component of the pointer network
introduced by Vinyals et al. [52] and extended by Merity et al. [43]. As opposed to these models
and standard cache models, we do not restrict the cache component to recent history but store all
previously observed words. Using the information stored in the cache component, we can obtain a
probability distribution over the words observed up to time t using the kernel density estimator:


t?1
X
kht ? hi k
pcache (wt | w1 , ...wt?1 ) ?
1{w = wi }K
,
(5)
?
i=1
where K is a kernel, such as Epanechnikov or Gaussian, and ? is a smoothing parameter. If K is
the Gaussian kernel (K(x) = exp(?x2 /2)) and the hidden representations are normalized, this is
equivalent to the continuous cache model.
As the memory grows with the amount of data seen by the model, this probability distribution becomes
impossible to compute. Millions of words and their multiple associated context representations are
stored, and exact exhaustive matching is prohibitive. Instead, we use the approximate k-nearest
neighbors algorithm that is described below in Sec. 3.4 to estimate this probability distribution:


X
kht ? hi k
pcache (wt | w1 , ...wt?1 ) ?
,
(6)
1{w = wi }K
?(ht )
i?N (ht )

where N (ht ) is the set of nearest neighbors and ?(ht ) is the Euclidean distance from ht to its k-th
nearest neighbor. This estimator is known as variable kernel density estimation [51]. It should be
noted that if the kernel K is equal to zero outside of [?1, 1], taking the sum over the k nearest
neighbors is equivalent to taking the sum over the full data.
The distribution obtained using the estimator defined in Eq. 6 assigns non-zero probability to at
most k words, where k is the number of nearest neighbors used. In order to have non-zero probability
everywhere (and avoid getting infinite perplexity), we propose to linearly interpolate this distribution
with the one from the model:
p(wt | w1 , ...wt?1 ) = (1 ? ?)pmodel (wt | w1 , ...wt?1 ) + ?pcache (wt | w1 , ...wt?1 ).
3.4

Fast large scale retrieval

Fast computation of the probability of a rare word is crucial to make the cache grow to millions of
potential words. Their representation also needs to be stored with relatively low memory usage. In this
section, we briefly describe a scalable retrieval method introduced by Jegou et al. [27]. Their approach
called Inverted File System Product Quantization (IVFPQ) combines two methods, an inverted file
system [56] and a quantization method, called Product quantization (PQ) [28]. Combining these two
components offers a good compromise between a fast retrieval of approximate nearest neighbors and
a low memory footprint.
Inverted file system. Inverted file systems [56] are a core component of standard large-scale text
retrieval systems, like search engines. When a query x is compared to a set Y of potential elements,
an inverted file avoids an exhaustive search by providing a subset of possible matching candidates.
In the context of continuous vectors, this subset is obtained by measuring some distance between
the query and predefined vector representations of the set. More precisely, these candidates are
selected through ?coarse matching? by clustering all the elements in Y in c groups using k-means.
The centroids are used as the vector representations. Each element of the set Y is associated with
one centroid in an inverted table. The query x is then compared to each centroid and a subset of
them is selected according to their distance to the query. All the elements of Y associated with these
centroids are then compared to the query x. Typically, we take c centroids and keep the cc closest
centroids to a query.
4

This procedure is quite efficient but very memory consuming, as each vector in the set Y must be
stored. This can be drastically reduced by quantizing the vectors. Product Quantization (PQ) is
a popular quantization method that has shown competitive performance on many retrieval benchmarks [28]. Following Jegou et al. [28], we do not directly quantize the vector y but its residual r,
i.e., the difference between the vector and its associated centroids.
Product Quantization. Product quantization is a data-driven compression algorithm with no
overhead during search [28]. While PQ has been designed for image feature compression, Joulin
et al. [30] have demonstrated its effectiveness for text too. PQ compresses real-valued vector by
approximating them with the closest vector in a pre-defined structured set of centroids, called a
codebook. This codebook is obtained by splitting each residual vector r into k subvectors ri , each of
dimension d/k, and running a k-means algorithm with s centroids on each resulting subspace. The
resulting codebook contains cs elements which is too large to be enumerated, and is instead implicitly
defined by its structure: a d-dimensional vector x ? Rd is approximated as
x
?=

k
X

qi (x),

(7)

i=1

where qi (x) is the closest centroid to subvector xi . For each subspace, there are s = 2b centroids,
where b is the number of bits required to store the quantization index of the sub-quantizer. Note
that in PQ, the subspaces are aligned with the natural axis and improvements where made by Ge et
al. [18] to align the subspaces to principal axes in the data. The reconstructed vector can take 2kb
distinct reproduction values and is stored in kb bits.
PQ estimates the inner product in the compressed domain as
x> y ? x
?> y =

k
X

qi (xi )> y i .

(8)

i=1

In practice, the vector estimate x
? is trivially reconstructed from the codes, (i.e., from the quantization
indexes) by concatenating these centroids. PQ uses two parameters, namely the number of subquantizers k and the number of bits b per quantization index.

4

Experiments

In this section, we present evaluations of our unbounded cache model on different language modeling
tasks. We first briefly describe our experimental setting and the datasets we used, before presenting
the results.
4.1

Experimental setting

One of the motivations of our model is to be able to adapt to changing data distribution. In particular,
we want to incorporate new words in the vocabulary, as they appear in the test data. We thus consider
a setting where we do not replace any words by the <unk> token, and where the test set contains
out-of-vocabulary words (OOV) which were absent at train time. Since we use the perplexity as the
evaluation metric, we need to avoid probabilities equal to zero in the output of our models (which
would result in infinite perplexity). Thus, we always interpolate the probability distributions of the
various models with the uniform distribution over the full vocabulary:
puniform (wt ) =

1
.
|vocabulary|

This is a standard technique, which was previously used to compare language models trained on
datasets with different vocabularies [9].
Baselines We compare our unbounded cache model with the static model interpolated with uniform
distribution, as well as the static model interpolated with the unigram probability distribution observed
up to time t. Our proposal is a direct extension of the local cache model [22]. Therefore, we also
compare to it to highlight the settings where an unbounded cache model is preferable to a local one.
5

model

Size

OoV rate (%)

News 2008
News 2009
News 2010
News 2011

219,796
218,628
205,859
209,187

2.3%
2.4%
2.4%
2.5%

Commentary
Web
Wiki
Books

144,197
321,072
191,554
174,037

4.2%
5.9%
5.5%
3.7%

Table 1: Vocabulary size and out-of-vocabulary rate for various test sets (for a model trained on News
2007).

4.2

Implementation details

We train recurrent neural networks with 256 LSTM hidden units, using the Adagrad algorithm with a
learning rate of 0.2 and 10 epochs. We compute the gradients using backpropagation through time
(BPTT) over 20 timesteps. Because of the large vocabulary sizes, we use the adaptative softmax [21].
We use the IVFPQ implementation from the FAISS open source library.1 We use 4, 096 centroids
and 8 probes for the inverted file. Unless said otherwise, we query the 1, 024 nearest neighbors.
4.3

Datasets

Most commonly used benchmarks for evaluating language models propose to replace rare words
by the <unk> token. On the contrary, we are interested in open vocabulary settings, and therefore
decided to use datasets without <unk>. We performed experiments on data from the five following
domains:
? News Crawl2 is a dataset made of news articles, collected from various online publications.
There is one subset of the data for each year, from 2007 to 2011. This dataset will allow
testing the unbounded cache models on data whose distribution slowly changes over time.
The dataset is shuffled at the sentence level. In the following, we refer to this dataset as
news 2007-2011.
? News Commentary consists of political and economic commentaries from the website
https://www.project-syndicate.org/. This dataset is publicly available from the
Statistical Machine Translation workshop website. In the following, we refer to this dataset
as commentary.
? Common Crawl is a text dataset collected from diverse web sources. The dataset is shuffled
at the sentence level. In the following, we refer to this dataset as web.
? WikiText3 is a dataset derived from high quality English Wikipedia articles, introduced by
Merity et al. [43]. Since we do not to replace any tokens by <unk>, we use the raw version.
In the following, we refer to this dataset as wiki.
? The book Corpus This is a dataset of 3,036 English books, collected from the Project
Gutenberg4 [40]. We use a subset of the books, which have a length around 100,000 tokens.
In the following we refer to this dataset as books.
All these datasets are publicly available. Unless stated otherwise, we use 2 million tokens for training
the static models and 10 million tokens for evaluation. All datasets are lowercased and tokenized
using the europarl dataset tools.5
1

https://github.com/facebookresearch/faiss
http://www.statmt.org/wmt14/translation-task.html
3
https://metamind.io/research/the-wikitext-long-term-dependency-language-modeling-dataset/
4
http://www.gutenberg.org/
5
http://statmt.org/europarl/v7/tools.tgz
2

6

model

2007

2008

static
static + unigram
static + local cache
static + unbounded cache

220.9
220.3
218.9
166.5

237.6
235.9
234.5
191.4

Test set
2009 2010
256.2
252.6
250.5
202.6

259.7
256.1
256.2
204.8

2011
268.8
264.3
265.2
214.3

0.14
0.15
0.16
0.17
0.18
0.19
0.20
0.21
0.22
0.23

News 2008-2011

Entropy difference with baseline

Entropy difference with baseline

Table 2: Static model trained on news 2007 and tested on news 2007-2011.

news 2008
news 2009
news 2010
news 2011

200 400 600 800 1000
number k of nearest neighbors

0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60

Domain adaptation
commentary
web
wiki
books

200 400 600 800 1000
number k of nearest neighbors

Figure 1: Performance of our model, as a function of the number k of nearest neighbors, used to
estimate the probability of words in the unbounded cache. We report the entropy difference with the
static+unigram baseline.

Train domain
News

Web

Wiki

model

News

Test domain
Commentary
Web

Wiki

Books

static
static + unigram
static + local cache
static + unbounded cache

-

342.7
303.5
288.5
191.1

689.3
581.1
593.4
383.4

1003.2
609.4
316.5
337.4

687.1
349.1
240.3
237.2

static
static + unigram
static + local cache
static + unbounded cache

624.1
519.2
531.4
306.3

484.0
395.6
391.3
234.9

-

805.3
605.3
321.5
340.2

784.3
352.4
235.8
223.6

static
static + unigram
static + local cache
static + unbounded cache

638.1
537.9
532.8
318.7

626.3
462.2
436.7
255.3

901.0
688.5
694.3
456.1

-

654.6
346.9
228.8
223.8

Table 3: Static model trained on news 2007 and tested on data from other domains.

Dataset
News 2008
Commentary
Web
Wiki
Books

Static model

Local cache

Unbounded cache

82
78
85
87
81

664
613
668
637
626

433
494
502
540
562

Table 4: Computational time (in seconds) to process 10M tokens from different test sets for the static
language model, the local cache (size 10,000) and the unbounded cache.

7

Entropy difference with baseline
4.4

0.00
0.05
0.10
0.15
0.20
0.25 5
10

news 2008
news 2009
news 2010

107
106
Number of test examples (log scale)

Figure 2: Performance of the unbounded cache model, as a function
of the number of test examples. We
report the entropy difference with the
static+unigram baseline. We observe
that, as the number of test examples
increases (and thus, the information
stored in the cache), the performance
of the unbounded cache increases.

Results

We demonstrate the effectiveness of using an unbounded cache to complement a language model
as advocated in the previous sections model by performing two types of experiments representing a
near domain and far domain adaptation scenarios. In both experiments, we compare the unigram
static model, the unigram extension, and the unbounded cache model.
Local vs. Unbounded Cache We first study the impact of using an unbounded cache instead of a
local one. To that end, we compare the performance of the two models when trained and tested on
different combinations of the previously described datasets. These datasets can be categorized into
two groups according to their properties and the results obtained by the various models we use.
On the one hand, the Wiki and Books datasets are not shuffled. Hence, the recent history (up to a few
thousands words) contains a wealth of information that can be used by a local cache to reduce the
perplexity of a static model. Indeed, the local cache model achieves respectively 316.5 and 240.3
on the Wiki and Books datasets when trained on the News dataset. This corresponds to about 3?
reduction in perplexity on both datasets in comparison to the static model. A similar trend holds when
the training data is either Web or Wiki dataset. Surprisingly, the unbounded cache model performs
similarly to the cache model despite using orders of magnitude broader context. A static model
trained on News and augmented with an unbounded cache achieves respectively 337.4 and 237.2
of perplexity. It is also worth noting that our approach is more efficient than the local cache, while
storing a much larger number of elements. Thanks to the use of fast nearest neighbor algorithm,
it takes 502 seconds to process 10M tokens from the test set when using the unbounded cache.
Comparatively, it takes 668 seconds for a local cache model of size 10, 000 to perform a similar task.
The timing experiments, reported in Table 4.3, show a similar trend.
On the other hand, the Commentary and Web datasets are shuffled. Therefore, a local cache can
hardly capture the relevant statistics to significantly improve upon the static model interpolated with
the unigram distribution. Indeed, the perplexity of a local cache model on these datasets when the
static model is trained on the News dataset is respectively 288.5 and 593.4. In comparison, the
unbounded cache model achieves on the same datasets respectively a perplexity of 191.1 and 383.4.
That is an average improvement of about 50% over the local cache in both cases (see Table 3).
Near domain adaptation. We study the benefit of using an unbounded cache model when the test
domain is only slightly different from the source domain. We train the static model on news 2007
and test on the corpus news 2008 to news 2011. All the results are reported in Table 1.
We first observe that the unbounded cache brings a 24.6% improvement relative to the static model
on the in-domain news 2007 corpus by bringing the perplexity from 220.9 down to 166.5. In
comparison, neither using the unigram information nor using a local cache lead to significant
improvement. This result underlines two phenomena. First, the simple distributional information
captured by the unigram or the local cache is already captured by the static model. Second, the
unbounded cache enhances the discrimination capabilities of the static model by capturing useful
non-linearities thanks to the combination of the nearest neighbor and the representation extracted from
8

the static model. Interestingly, these observations remain consistent when we consider evaluations on
the test sets news 2008-2011. Indeed, the average improvement of unbounded cache relatively to
the static model on the corpus news 2008-2011 is 20.44% while the relative improvement of the
unigram cache is only 1.3%. Similarly to the in-domain experiment, the unigram brings little useful
information to the static model mainly because the source (news 2007) and the target distributions
(news 2008-2011) are very close. In contrast, the unbounded cache still complements the static
model with valuable non-linear information of the target distributions.
Far domain adaptation. Our second set of experiments is concerned with testing on different
domains from the one the static model is trained on. We use the News, Web and Wiki datasets as
source domains, and all five domains as target. The results are reported in Table 3.
First, we observe that the unigram, the local and the unbounded cache significantly help the static
model in all the far domain adaptation experiments. For example, when adapting the static model
from the News domain to the Commentary and Wiki domains, the unigram reduces the perplexity of
the static model by 39.2 and 393.8 in absolute value respectively. The unbounded cache significantly
improves upon the static model and the unigram on all the far domain adaptation experiment. The
smallest relative improvement compared to the static model and the unigram is achieved when
adapting from News to Web and is 79.7% and 51.6% respectively. The more the target domain is
different from the source one, the more interesting is the use of an unbounded cache mode. Indeed,
when adapting to the Books domain (which is the most different from the other domains) the average
improvement given by the unbounded cache relatively to the static model is 69.7%.
Number of nearest neighbors. Figure 1 shows the performance of our model with the number
of nearest neighbors per query. As observed previously by Grave et al [22], the performance of a
language model improves with the size of the context used in the cache. This context is, in some
sense, a constrained version of our set of retained nearest neighbors. Interestingly, we observe the
same phenomenon despite forming the set of possible predictions over a much broader set of potential
candidates than the immediate local context. Since IFVPQ has a linear complexity with the number
of nearest neighbors, setting the number of nearest neighbors to a thousand offers a good trade-off
between speed and accuracy.
Size of the cache. Figure 2 shows the gap between the performance of static language model with
and without the cache as the size of the test set increases. Despite having a much more significant set
of candidates to look from, our algorithm continues to select relevant information. As the test set is
explored, better representations for rare words are stored, explaining this constant improvement.

5

Conclusion

In this paper, we introduce an extension to recurrent networks for language modeling, which stores
past hidden activations and associated target words. This information can then be used to obtain a
probability distribution over the previous words, allowing the language models to adapt to the current
distribution of the data dynamically. We propose to scale this simple mechanism to large amounts of
data (millions of examples) by using fast approximate nearest neighbor search. We demonstrated on
several datasets that our unbounded cache is an efficient method to adapt a recurrent neural network
to new domains dynamically, and can scale to millions of examples.
Acknowledgements
We thank the anonymous reviewers for their insightful comments.

References
[1] I. Alabdulmohsin, M. Cisse, and X. Zhang. Is attribute-based zero-shot learning an ill-posed strategy? In
ECML-PKDD.
[2] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, J. Chen, M. Chrzanowski,
A. Coates, G. Diamos, et al. Deep speech 2: End-to-end speech recognition in English and Mandarin. In
ICML, 2016.

9

[3] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate.
In ICLR, 2015.
[4] L. R. Bahl, F. Jelinek, and R. L. Mercer. A maximum likelihood approach to continuous speech recognition.
PAMI, 1983.
[5] J. R. Bellegarda. Exploiting latent semantic information in statistical language modeling. Proceedings of
the IEEE, 2000.
[6] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan. A theory of learning
from different domains. Machine learning, 79(1), 2010.
[7] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language model. JMLR, 2003.
[8] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In ICML, 2009.
[9] C. Buck, K. Heafield, and B. van Ooyen. N-gram counts and language models from the common crawl. In
LREC, 2014.
[10] R. Caruana. Multitask learning. In Learning to learn. Springer, 1998.
[11] M. S. Charikar. Similarity estimation techniques from rounding algorithms. In STOC, 2002.
[12] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and T. Robinson. One billion word
benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005, 2013.
[13] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical evaluation of gated recurrent neural networks on
sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
[14] N. Coccaro and D. Jurafsky. Towards better integration of semantic predictors in statistical language
modeling. In ICSLP, 1998.
[15] S. Della Pietra, V. Della Pietra, R. L. Mercer, and S. Roukos. Adaptive language modeling using minimum
discriminant estimation. In Proceedings of the workshop on Speech and Natural Language, 1992.
[16] J. L. Elman. Finding structure in time. Cognitive science, 1990.
[17] M. Federico, N. Bertoldi, and M. Cettolo. Irstlm: an open source toolkit for handling large scale language
models. In INTERSPEECH, 2008.
[18] T. Ge, K. He, Q. Ke, and J. Sun. Optimized product quantization for approximate nearest neighbor search.
In CVPR, 2013.
[19] Y. Gong and S. Lazebnik. Iterative quantization: A procrustean approach to learning binary codes. In
CVPR, 2011.
[20] J. T. Goodman. A bit of progress in language modeling. Computer Speech & Language, 2001.
[21] E. Grave, A. Joulin, M. Ciss?, D. Grangier, and H. J?gou. Efficient softmax approximation for GPUs. In
ICML, 2017.
[22] E. Grave, A. Joulin, and N. Usunier. Improving neural language models with a continuous cache. In ICLR,
2017.
[23] A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In
ICASSP, 2013.
[24] K. Heafield. Kenlm: Faster and smaller language model queries. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, 2011.
[25] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 1997.
[26] R. M. Iyer and M. Ostendorf. Modeling long distance dependence in language: Topic mixtures versus
dynamic cache models. IEEE Transactions on speech and audio processing, 1999.
[27] H. Jegou, M. Douze, and C. Schmid. Hamming embedding and weak geometric consistency for large scale
image search. In ECCV, 2008.
[28] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. PAMI, 2011.

10

[29] F. Jelinek, B. Merialdo, S. Roukos, and M. Strauss. A dynamic language model for speech recognition. In
HLT, 1991.
[30] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. J?gou, and T. Mikolov. Fasttext.zip: Compressing text
classification models. arXiv preprint arXiv:1612.03651, 2016.
[31] R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer, and Y. Wu. Exploring the limits of language modeling.
arXiv preprint arXiv:1602.02410, 2016.
[32] S. M. Katz. Estimation of probabilities from sparse data for the language model component of a speech
recognizer. ICASSP, 1987.
[33] S. Khudanpur and J. Wu. Maximum entropy techniques for exploiting syntactic, semantic and collocational
dependencies in language modeling. Computer Speech & Language, 2000.
[34] R. Kneser and H. Ney. Improved backing-off for m-gram language modeling. In ICASSP, 1995.
[35] R. Kneser and V. Steinbiss. On the dynamic adaptation of stochastic language models. In ICASSP, 1993.
[36] R. Kuhn. Speech recognition and the frequency of recently used words: A modified markov model for
natural language. In Proceedings of the 12th conference on Computational linguistics-Volume 1, 1988.
[37] R. Kuhn and R. De Mori. A cache-based natural language model for speech recognition. PAMI, 1990.
[38] J. Kupiec. Probabilistic models of short and long distance word dependencies in running text. In
Proceedings of the workshop on Speech and Natural Language, 1989.
[39] I. Kuzborskij, F. Orabona, and B. Caputo. From n to n+ 1: Multiclass transfer incremental learning. In
CVPR, 2013.
[40] S. Lahiri. Complexity of word collocation networks: A preliminary structural analysis. In Proceedings of
the Student Research Workshop at the 14th Conference of the European Chapter of the Association for
Computational Linguistics, 2014.
[41] C. H. Lampert, H. Nickisch, and S. Harmeling. Attribute-based classification for zero-shot visual object
categorization. PAMI, 2014.
[42] R. Lau, R. Rosenfeld, and S. Roukos. Trigger-based language models: A maximum entropy approach. In
ICASSP, 1993.
[43] S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. In ICLR, 2017.
[44] T. Mikolov, A. Deoras, S. Kombrink, L. Burget, and J. Cernock`y. Empirical evaluation and combination of
advanced language modeling techniques. In INTERSPEECH, 2011.
[45] T. Mikolov, M. Karafi?t, L. Burget, J. Cernock`y, and S. Khudanpur. Recurrent neural network based
language model. In INTERSPEECH, 2010.
[46] T. Mikolov and G. Zweig. Context dependent recurrent neural network language model. In SLT, 2012.
[47] M. D. Muhlbaier, A. Topalis, and R. Polikar. Learn++ .NC: Combining ensemble of classifiers with
dynamically weighted consult-and-vote for efficient incremental learning of new classes. IEEE transactions
on neural networks, 20(1), 2009.
[48] R. Rosenfeld. A maximum entropy approach to adaptive statistical language modeling. Computer, Speech
and Language, 1996.
[49] W. J. Scheirer, A. de Rezende Rocha, A. Sapkota, and T. E. Boult. Toward open set recognition. PAMI,
2013.
[50] I. V. Serban, A. Sordoni, Y. Bengio, A. Courville, and J. Pineau. Building end-to-end dialogue systems
using generative hierarchical neural network models. In AAAI, 2016.
[51] G. R. Terrell and D. W. Scott. Variable kernel density estimation. The Annals of Statistics, 1992.
[52] O. Vinyals, M. Fortunato, and N. Jaitly. Pointer networks. In NIPS, 2015.
[53] T. Wang and K. Cho. Larger-context language modelling. In ACL, 2016.
[54] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In NIPS, 2009.
[55] J. G. Zilly, R. K. Srivastava, J. Koutn?k, and J. Schmidhuber. Recurrent highway networks. In ICML, 2017.
[56] J. Zobel and A. Moffat. Inverted files for text search engines. ACM computing surveys (CSUR), 2006.

11

"
2004,A Second Order Cone programming Formulation for Classifying Missing Data,,2670-a-second-order-cone-programming-formulation-for-classifying-missing-data.pdf,Abstract Missing,"A Second order Cone Programming
Formulation for Classifying Missing Data

Chiranjib Bhattacharyya
Department of Computer Science and Automation
Indian Institute of Science
Bangalore, 560 012, India
chiru@csa.iisc.ernet.in
Pannagadatta K. S.
Department of Electrical Engineering
Indian Institute of Science
Bangalore, 560 012, India
pannaga@ee.iisc.ernet.in

Alexander J. Smola
Machine Learning Program
National ICT Australia and ANU
Canberra, ACT 0200, Australia
Alex.Smola@anu.edu.au

Abstract
We propose a convex optimization based strategy to deal with uncertainty
in the observations of a classification problem. We assume that instead
of a sample (xi , yi ) a distribution over (xi , yi ) is specified. In particular, we derive a robust formulation when the distribution is given by a
normal distribution. It leads to Second Order Cone Programming formulation. Our method is applied to the problem of missing data, where it
outperforms direct imputation.

1

Introduction

Denote by (x, y) ? X ?Y patterns with corresponding labels. The typical machine learning
formulation only deals with the case where (x, y) are given exactly. Quite often, however,
this is not the case ? for instance in the case of missing values we may be able (using a
secondary estimation procedure) to estimate the values of the missing variables, albeit with
a certain degree of uncertainty. It is therefore only natural to take the decreased reliability of
such data into account and design estimators accordingly. What we propose in the present
paper goes beyond the traditional imputation strategy where missing values are estimated
and then used as if they had actually been observed. The key difference in what follows is
? i is drawn from a
that we will require that with high probability any (?
xi , yi ) pair, where x
distribution of possible xi , will be estimated correctly. For the sake of simplicity we limit
ourselves to the case of binary classification.
The paper is organized as follows: Section 2 introduces the problem of classification with
uncertain data. We solve the equations arising in the context of normal random variables
in Section 3 which leads to a Second Order Cone Program (SOCP). As an application
the problem of classification with missing variables is described in Section 4. We report
experimental results in Section 5.

2

Linear Classification using Convex Optimization

Assume we have m observations (xi , yi ) drawn iid (independently and identically distributed) from a distribution over X ? Y, where X is the set of patterns and Y = {?1} are
the labels (e.g. the absence/presence of a particular object). It is our goal to find a function
f : X ? Y which classifies observations x into classes +1 and ?1.
2.1

Classification with Certainty

Assume that X is a dot product space and f is a linear function
f (x) = sgn(hw, xi + b).

(1)

In the case of linearly separable datasets we can find (w, b) which separates the two classes.
Unfortunately, such separation is not always possible and we need to allow for slack in the
separation of the two sets. Consider the formulation
minimize
w,b,?

m
X

?i

(2a)

i=1

subject to yi (hw, xi i + b) ? 1 ? ?i , ?i ? 0, kwk ? W for all 1 ? i ? m

(2b)

It is well known that this problem minimizes an upper bound on the number of errors. The
latter occur
p whenever ?i ? 1, where ?i are the slack variables. The Euclidean norm of
kwk = hw, wi, is upper bounded by a user defined constant W . This is equivalent
to lower bounding the margin, or the separation between the two classes. The resulting
discriminant surface is called the generalized optimal hyperplane [9]. The statement of (2)
is slightly nonstandard. Typically one states the SVM optimization problem as follows [3]:
m

X
1
?i
minimize kwk2 + C
w,b,?
2
i=1

(3a)

subject to yi (hw, xi i + b) ? 1 ? ?i , ?i ? 0 for all 1 ? i ? m

(3b)

Instead of the user defined parameter W , the formulation (3) uses another parameter C.
For a proper choice of C and W the two formulations are equivalent. For the purpose of
the present paper, however, (2) will be much more easily amenable to modifications and to
cast the resulting problem as a second order cone program (SOCP).
2.2

Classification with Uncertainty

So far we assumed that the (xi , yi ) pairs are known with certainty. We now relax this to the
assumption that we only have a distribution over the xi , that is (Pi , yi ) at our disposition
(due to a sampling procedure, missing variables, etc.). Formally xi ? Pi . In this case it
makes sense to replace the constraints (2b) of the optimization problem (2) by
subject to Pr {yi (hw, xi i + b) ? 1 ? ?i } ? ?i , ?i ? 0, kwk ? W ? 1 ? i ? m

(4)

Here we replaced the linear classification constraint by a probabilistic one, which is required to hold with probability ?i ? (0, 1]. This means that by choosing a value of ?i close
to 1 we can find a conservative classifier which will classify even very infrequent (x i , yi )
pairs correctly. Hence ?i provides robustness of the estimate with respect to deviating xi .
It is clear that
Pmunless we impose further restrictions on Pi , it will be difficult to minimize the
objective i=1 ?i with the constraints (4) efficiently. In the following we will consider the
special cases of gaussian uncertainty for which a mathematical programming formulation
can be found.

3

Normal Distributions

For the purpose of this section we assume that Pi = N (?
xi , ?i ), i.e., xi is drawn from a
Gaussian distribution with mean x
?i and covariance ?i . We will not require that ?i has full
rank. This means that the uncertainty about xi may be limited to individual coordinates or
to a subspace of X . As we shall see, this problem can be posed as SOCP.
3.1

Robust Classification

Under the above assumptions, the probabilistic constraint (4) becomes
subject to Pr {yi (hw, xi i + b) ? 1 ? ?i } ? ?i where xi ? N (?
x i , ?i )
?i ? 0, kwk ? W for all 1 ? i ? m

The stochastic constraint can be restated as a deterministic optimization problem


zi ? z i
yi b + ? i ? 1 ? z i
? ?i
Pr
?
? zi
? zi

(5a)
(5b)

(6)

where zi := ?yi w> xi is a normal random variable with mean z?i and variance ?z2i :=
w> ?i w. Consequently (zi ? z?i )/?zi is a random variable with zero mean and unit variance
and we can compute the lhs of (6) by evaluating the cumulative distribution function for
normal distributions
Z u
s2
1
?(u) := ?
e? 2 ds.
2? ??
In summary, (6) is equivalent to the condition


yi b + ? i ? 1 ? z i
?
? ?i .
? zi
which can be solved (since ?(u) is monotonic and invertible), for the argument of ? and
obtain a condition on its argument
p
(7)
yi (w> x
?i + b) ? 1 ? ?i + ?i wT ?i w , ?i = ??1 (?i )
We now proceed to deriving a mathematical programming formulation.
3.2

Second Order Cone Programming Formulation

Depending on ?i we can distinguish between three different cases. First consider the case
where ?i = 0 or ?i = 0.5. This means that the second order cone part of the constraint (7)
reduces to the linear inequality of (2b). In other words, we recover the linear constraint of
a standard SVM.
Secondly consider the case ?i < 0 or ?i < 0.5. This means that the constraint (7) describes
a concave set, which turns the linear classification task into a hard optimization problem.
However, it is not very likely that anyone would like to impose such constraints which hold
only with low probability. After all, uncertain data requires the constraint to become more
restrictive in holding not only for a guaranteed point xi but rather for an entire set.
Lastly consider the case ?i > 0 or ?i > 0.5 second order cone constraint. In this case (7)
describes a convex set in in w, b, ?i . We obtain the following optimization problem:
m
X
minimize
?i
(8a)
w,b,?

i=1

1

subject to yi (w> xi + b) ? 1 ? ?i + ?i k?i2 wk and ?i ? 0 ? 1 ? i ? m
kwk ? W

(8b)
(8c)

These problems can be solved efficiently by publicly available codes: recent advances in
Interior point methods for convex nonlinear optimization [8] have made such problems
feasible. As a special case of convex nonlinear optimization SOCPs have gained much
attention in recent times. For a further discussion of efficient algorithms and applications
of SOCP see [6].
3.3

Worst Case Prediction

Note that if at optimality ?i > 0, the hyperplane intersects with the constraint set
B(xi , ?i , ?i ). Moreover, at a later stage we will need to predict the class label to asses
on which side of the hyperplane B lies. If the hyperplane intersects B we will end up with
different predictions for points in the different half spaces. In such a scenario a worst case
prediction, y can be
hw, xi i + b
y = sgn(z) sgn(h ? ?) where ? = ??1 (?), z = ?
and h = |z|.
(9)
w> ?w
Here sgn(z) gives us the sign of the point in the center of the ellipsoid and (h ? ?) is
the distance of z from the center. If the hyperplane intersects the ellipsoid, the worst case
prediction is then the prediction for all points which are in the opposite half space of the
center (xi ). Plugging ? = 0.5, i.e., ? = 0 into (9) yields the standard prediction (1).
In such a case h can serve as a measure of confidence as to how well the discriminating
hyperplane classifies the mean(xi ) correctly.
3.4

Set Constraints

The same problem as (8) can also be obtained by considering that the uncertainty in each
datapoint is characterized by an ellipsoid
2
B(xi , ?i , ?i ) = {x : (x ? xi )> ??1
(10)
i (x ? xi ) ? ?i }
in conjunction with the constraint
yi (hw, xi + b) ? 1 ? ?i for all x ? Si
(11)
where Si = B(xi , ?i , ?i ) As before ?i = ??1 (?i ) for ?i ? 0. In other words, we have
?i = 0 only when the hyperplane w> x + b = 0 does not intersect the ball B(xi , ?i , ?i ).

Note that this puts our optimization setting into the same category as the knowledge-based
SVM, and SDP for invariances as all three deal with the above type of constraint (11).
More to the point, in [5] Si = S(xi , ?) is a polynomial in ? which describes the set
of invariance transforms of xi (such as distortion or translation). [4] define Si to be a
polyhedral ?knowledge? set, specified by the intersection of linear constraints.
Such considerations suggest yet another optimization setting: instead of specifying a polyhedral set Si by constraints we can also specify it by its vertices. In particular, we may set
Si to be the convex hull of a set as in Si = co{xij for 1 ? j ? mi }. By the convexity of
the constraint set itself it follows that a necessary and sufficient condition for (11) to hold
is that the inequality holds for all x ? {xij for 1 ? j ? mi }. Consequently we can replace
(11) by yi (hw, xij i + b) ? 1 ? ?i Note that the index ranges over j rather than i. Such a
setting allows us to deal with uncertainties, e.g. regarding the range of variables, which are
just given by interval boundaries, etc. The table below summarizes the five cases:
Name
Plain SVM[3]
Knowledge Based SVM[4]
Invariances [5]
Normal Distribution
Convex Hull

Set Si
{xi }
Polyhedral set
trajectory of polynomial
B(xi , ?i , ?i )
co{xij ? 1 ? j ? mi }

Optimization Problem
Quadratic Program
Quadratic Program
Semidefinite Program
Second Order Cone Program
Quadratic Program

Clearly all the above constraints can be mixed and matched and it is likely that there will be
more additions to this table in the future. More central is the notion of stating the problems
via (11) as a starting point.

4

Missing Variables

In this section we discuss how to address the missing value problem. Key is how to obtain
estimates of the uncertainty in the missing variables. Since our optimization setting allows
for uncertainty in terms of a normal distribution we attempt to estimate the latter directly.
In other words, we assume that x|y is jointly normal with mean ?y and covariance ?y .
Hence we have the following two-stage procedure to deal with missing variables:
? Estimate ?y , ?y from incomplete data, e.g. by means of the EM algorithm.
? Use the conditionally normal estimates of xmissing |(xobserved , y) in the optimization problem. This can then be cast in terms of a SOCP as described in the previous
section.
Note that there is nothing to prevent us from using other estimates of uncertainty and use
e.g. the polyhedral constraints subsequently. However, for the sake of simplicity we focus
on normal distributions in this paper.
4.1

Estimation of the model parameters

We now detail the computation of the mean and covariance matrices for the datapoints
which have missing values. We just sketch the results, for a detailed derivation see e.g. [7].
Let x ? Rd , where xa ? Rda be the vector whose values are known, while xm ? Rd?da
be the vector consisting of missing variables. Assuming a jointly normal distribution in x
with mean ? and covariance ? it follows that
>
?1
xm |xa ? N (?m + ?am ??1
aa (xa ? ?a ), ?mm ? ?am ?aa ?am ).

Here we decomposed ?, ? according to (xa , xm ) into

?aa
? = (?a , ?m ) and ? =
?>
am

?am
?mm



.

(12)

(13)

Hence, knowing ?, ? we can estimate the missing variables and determine their degree of
uncertainty. One can show that [7] to obtain ?, ? the EM algorithm reads as follows:
1. Initialize ?, ?.
2. Estimate xm |xa for all observations using (12).
3. Recompute ?, ? using the completed data set and go to step 2.
4.2

Robust formulation for missing values

As stated above, we model the missing variables as Gaussian random variables, with its
mean and covariance given by the model described in the previous section. The standard
practice for imputation is to discard the covariance and treat the problem as a deterministic
problem, using the mean as surrogate. But using the robust formulation (8) one can as well
account for the covariance.
Let ma be number of datapoints for which all the values are available, while mm be the
number of datapoints containing missing values. Then the final optimization problem reads

as follows:
minimize
w,b,?

m
X

?i

(14)

i=1

subject to yi (hw, xi i + b) ? 1 ? ?i

1
2

yj (w> xj + b) ? 1 ? ?j + ??1 (?j )k?j wk
?i ? 0
kwk ? W

?1 ? i ? ma
?ma + 1 ? j ? ma + mm

?1 ? i ? ma + mm

The mean xj has two components; xaj has values available, while the imputed vector is
given by x
?mj , via (12). The matrix ?j has all entries zero except those involving the
missing values, given by Cj , computed via (12).
The formulation (14) is an optimization problem which involves minimizing a linear objective over linear and second order cone constraints. At optimality the values of w, b,
can be used to define a classifier (1). The resulting discriminator can be used to predict
the the class label of a test datapoint having missing variables by a process of conditional
imputation as follows.
Perform the imputation process assuming that the datapoint comes from class 1(class with
label y = 1). Specifically compute the mean and covariance, as outlined in section 4.1,
and denote them by ?1 and ?1 (see (13)) respectively. The training dataset of class 1 is to
be used in the computation of ?1 and ?1 . Using the estimated ?1 and ?1 compute h as
defined in (9), and denote it by h1 . Compute the label of ?1 with the rule (1), call it y1 .
Assuming that the test data comes from class 2 (with label y = ?1) redo the entire process
and denote the resulting mean, covariance, and h by ?2 , ?2 , h2 respectively. Denote by y2
the label of ?2 as predicted by (1). We decide that the observation belongs to class with
label y? as
y? = y2 if h1 < h2 and y? = y1 otherwise

(15)

The above rule chooses the prediction with higher h value or in other words the classifier
chooses the prediction about which it is more confident. Using y? , h1 , h2 as in (15), the
worst case prediction rule (9) can be modified as follows
y = y? sgn(h ? ?) where ? = ??1 (?) and h = max(h1 , h2 )

(16)

It is our hypothesis that the formulation (14) along with this decision rule is robust to
uncertainty in the data.

5

Experiments with the Robust formulation for missing values

Experiments were conducted to evaluate the proposed formulation (14), against the standard imputation strategy. The experiment methodology consisted of creating a dataset of
missing values from a completely specified dataset. The robust formulation (14) was used
to learn a classifier on the dataset having missing values. The resulting classifier was used
to give a worst case prediction (16), on the test data. Average number of disagreements was
taken as the error measure. In the following we describe the methodology in more detail.
Consider a fully specified dataset, D = {(xi , yi )|xi ? Rd , yi ? {?1}1 ? i ? N } having
N observations, each observation is a d dimensional vector (xi ) and labels yi . A certain
fraction(f ) of the observations were randomly chosen. For each of the chosen datapoints
dm (= 0.5d) entries were randomly deleted. This then creates a dataset having N datapoints
out of which Nm (= f N, 0 ? f ? 1) of them have missing values. This data is then

randomly partitioned into test set and training set in the ratio 1 : 9 respectively. We do this
exercise to generate 10 different datasets and all our results are averaged over them.
Assuming that the conditional probability distribution of the missing variables given the
? j ) can be estimated by the
other variables is a gaussian, the mean(xj ) and the covariance (C
methods described in (4.1). The robust optimization problem was then solved for different
values of ?. The parameter ?j (= ?) is set to the same value for all the Nm datapoints. For
each value of ? the worst case error is recorded.
Experimental results are reported for three public domain datasets downloaded from uci
repository ([2]). Pima(N = 768, d = 8), Heart ( N = 270, d = 13), and Ionosphere(N =
351, d = 34), were used for experiments.
Setting ? = 0.5, yields the generalized optimal hyperplane formulation, (2). The generalized optimal hyperplane will be referred to as the nominal classifier. The nominal classifier
considers the missing values are well approximated by the mean (xj ), and there is no uncertainty.
0.5

0.6

robust
nomwc
robustwc

robust
nomwc
robustwc

robust
nomwc
robustwc

0.55

0.5

0.4
0.45

0.4

0.4

0.35

0.3
0.3

0.25
0.5

0.6

0.7

0.8

0.9

1

0.2
0.5

0.41

0.6

0.7

0.8

0.9

1

0.4

0.42

0.2
0.5

0.6

0.7

0.8

0.9

1

0.6

0.7

0.8

0.9

1

0.45
robust
nomwc
robustwc

robust
nomwc
robustwc

robust
nomwc
robustwc

0.4

0.4

0.39
0.3

0.38

0.35

0.37

0.36

0.3

0.35
0.2

0.34

0.25

0.33
0.5

0.32
0.5

0.6

0.7

0.8

0.9

1

0.6

0.7

0.8

0.9

1

0.2
0.5

Figure 1: Performance of the robust programming solution for various datasets of the UCI
database. From left to right: Pima, Ionosphere, and Heart dataset. Top: small fraction
of data with missing variables (50%), Bottom: large number of observations with missing
variables (90%)
The experimental results are summarized by the graphs(1). The robust classifier almost
always outperforms the nominal classifier in the worst case sense (compare nomwc and
robustwc). Results are presented for low(f = 0.5), and high (f = 0.9) number of missing
values. The results show that for low number of missing values(f = 0.5) the robust classifier is marginally better than the nominal classifier the gain but for large f = 0.9 the gain

is significant. This confirms that the imputation strategy fails for high noise.
The standard misclassification error for the robust classifier, using the standard prediction
(1), is also shown in the graph with the legend robust. As expected the robust classifier
performance does not deteriorate in the standard misclassification sense as ? is increased.
In summary the results seems to suggest that for low noise level the nominal classifier
trained on imputed data performs as good as the robust formulation. But for high noise
level the robust formulation yields dividends in the worst case sense.

6

Conclusions

An SOCP formulation was proposed for classifying noisy observations and the resulting
formulation was applied to the missing data case. In the worst case sense the classifier
shows a better performance over the standard imputation strategy. Closely related to this
work is the Total Support Vector Classification(TSVC) formulation, presented in [1]. The
TSVC formulation tries to reconstruct the original maximal margin classifier in the presence of noisy data. Both TSVC formulation and the approach in this paper address the issue
of uncertainty in input data and it would be an important research direction to compare the
two approaches.
Acknowledgements CB was partly funded by ISRO-IISc Space technology cell (Grant
number IST/ECA/CB/152). National ICT Australia is funded through the Australian Government?s Backing Australia?s Ability initiative, in part through the Australian Research
Council. AS was supported by grants of the ARC. We thank Laurent ElGhaoui, Michael
Jordan, Gunnar R?atsch, and Frederik Schaffalitzky for helpful discussions and comments.

References
[1] J. Bi and T. Zhang. Support vector classification with input data uncertainty. In Advances in Neural Information Processing Systems. MIT Press, 2004.
[2] C. L. Blake and C. J. Merz. UCI repository of machine learning databases, 1998.
[3] C. Cortes and V. Vapnik. Support vector networks. Machine Learning, 20:273?297,
1995.
[4] G. Fung, O. L. Mangasarian, and Jude Shavlik. Knowledge-based support vector machine classifiers. In Advances in Neural Information Processing Systems. MIT Press,
2002.
[5] Thore Graepel and Ralf Herbrich. Invariant pattern recognition by semidefinite programming machines. In Advances in Neural Information Processing Systems 16, Cambridge, MA, 2003. MIT Press.
[6] M.S. Lobo, L. Vandenberghe, S. Boyd, and H. Lebret. Applications of second-order
cone programming. Linear Algebra and its Applications, 284(1?3):193?228, 1998.
[7] K. V. Mardia, J. T. Kent, and J. M. Bibby. Multivariate Analysis. Academic Press,
1979.
[8] Y. Nesterov and A. Nemirovskii. Interior Point Algorithms in Convex Programming.
Number 13 in Studies in Applied Mathematics. SIAM, Philadelphia, 1993.
[9] V. Vapnik. The Nature of Statistical Learning Theory. Springer, New York, 1995.

"
1995,Reorganisation of Somatosensory Cortex after Tactile Training,,1053-reorganisation-of-somatosensory-cortex-after-tactile-training.pdf,Abstract Missing,"Reorganisation of Somatosensory Cortex after
Tactile Training

Rasmus S. Petersen
John G. Taylor
Centre for Neural Networks, King's College London
Strand, London WC2R 2LS, UK

Abstract
Topographic maps in primary areas of mammalian cerebral cortex reorganise as a result of behavioural training. The nature of this reorganisation seems consistent with the behaviour of competitive neural networks, as has been demonstrated in the past by computer simulation.
We model tactile training on the hand representation in primate somatosensory cortex, using the Neural Field Theory of Amari and his colleagues. Expressions for changes in both receptive field size and magnification factor are derived, which are consistent with owl monkey experiments and make a prediction which goes beyond them.

1. INTRODUCTION
The primary cortical areas of mammals are now known to be plastic throughout life; reviewed recently by Kaas(1995). The problem of how and why the underlying learning
processes work is an exciting one, for which neural network modelling appears well
suited. In this contribution, we model the long-term effects of tactile training (Jenkins et
ai, 1990) on the functional organisation of monkey primary somatosensory cortex, by
perturbing a topographic net (Takeuchi and Amari, 1979).

1.1 ADAPTATION IN ADULT SOMATOSENSORY CORTEX
Light touch activates skin receptors which in primates are mapped, largely topographically, in area 3b. In a series of papers, Merzenich and colleagues describe how area 3b
becomes reorganised following peripheral nerve damage (Merzenich et ai, 1983a; 1983b)
or digit amputation (Merzenich et ai, 1984). The underlying learning processes may also
explain the phenomenon of phantom limb ""telescoping"" (Haber, 1955). Recent advances
in brain scanning are beginning to make them observable even in the human brain
(Mogilner et ai, 1993).

1.2 ADAPTATION ASSOCIATED WITH TACTILE TRAINING
Jenkins et al trained owl monkeys to maintain contact with a rotating disk. The apparatus
was arranged so that success eventually involved touching the disk with only the digit
tips. Hence these regions received selective stimulation. Some time after training had
been completed electro-physiological recordings were made from area 3b. These revealed an increase in Magnification Factor (MF) for the stimulated skin and a decrease in

83

Reorganization of Somatosensory Cortex after Tactile Training

the size of Receptive Fields (RFs) for that region. The net territory gained for light touch
of the digit tips came from area 3a and/or the face region of area 3b, but details of any
changes in these representations were not reported.

2. THEORETICAL FRAMEWORK
2.1 PREVIOUS WORK
Takeuchi and Amari(1979), Ritter and Schulten(1986), Pearson et al(1987) and Grajski
and Merzenich( 1990) have all modelled amputationldenervation by computer simulation
of competitive neural networks with various Hebbian weight dynamics. Grajski and
Merzenich(1990) also modelled the data of Jenkins et al. We build on this research
within the Neural Field Theory framework (Amari, 1977; Takeuchi and Amari, 1979;
Amari, 1980) of the Neural Activity Model of Willshaw and von der Malsburg(1976).

2.2 NEURAL ACTIVITY MODEL
Consider a ""cortical"" network of simple, laterally connected neurons. Neurons sum inputs linearly and output a sigmoidal function of this sum. The lateral connections are
excitatory at short distances and inhibitory at longer ones. Such a network is competitive: the steady state consists of blobs of activity centred around those neurons locally receiving the greatest afferent input (Amari, 1977). The range of the competition is limited
by the range of the lateral inhibition.
Suppose now that the afferent synapses adapt in a Hebbian manner to stimuli that are localised in the sensory array; the lateral ones are fixed. Willshaw and von der Malsburg(1976) showed by computer simulation that this network is able to form a topographic map of the sensory array. Takeuchi and Amari( 1979) amended the WillshawMalsburg model slightly: neurons possess an adaptive firing threshold in order to prevent
synaptic weight explosion, rather than the more usual mechanism of weight normalisation. They proved that a topographic mapping is stable under certain conditions.

2.3 TAKEUCHI-AMARI THEORY
Consider a one-dimensional model. The membrane dynamics are:

au(~y,t) = -u(x,y,t)+ f s(x,y' ,t)a(y- y')dy'-

(1)

f

so(x,t)ao + w(x-x')f[u(x' ,y,t)]dx'-h
Here u(x,y,t) is the membrane potential at time I for point x when a stimulus centred at y is
being presented; h is a positive resting potential; w(z) is the lateral inhibitory weight between two points in the neural field separated by a distance z - positive for small Izl and
negative for larger Izl; s(x,y,t) is the excitatory synaptic weight from y to x at time I and
sr/X,I) is an inhibitory weight from a tonically active inhibitory input aD to x at time t - it is
the adaptive firing threshold . f[u] is a binary threshold function that maps positive membrane potentials to 1 and non-positive ones to O.
Idealised, point-like stimuli are assumed, which ""spread out"" somewhat on the sensory
surface or subcortically. The spreading process is assumed to be independent of y and is
described in the same coordinates. It is represented by the function a(y-y'), which describes the effect of a point input at y spreading to the point y'. This is a decreasing, positive, symmetric function of Iy-y'l. With this type of input, the steady-state activity of the
network is a single blob, localised around the neuron with maximum afferent input.

R. S. PETERSEN, J. O. TAYLOR

84

The afferent synaptic weights adapt in a leaky Hebbian manner but with a time constant
much larger than that of the membrane dynamics (1). Effectively this means that learning
occurs on the steady state of the membrane dynamics. The following averaged weight
dynamics can be justified (Takeuchi and Amari, 1979; Geman 1979):

J) (

as( x,aty, t) =-s(x,y,t)+b p(y' a Y-Y')f [Au(x,y' )]dy'
(2)

aso(~y,t) =-so(x,y,t)+b' aoJ p(y')f[u(x,y')]dy'

where r1(x,y') is the steady-state of the membrane dynamics at x given a stimulus at y' and
p(y') is the probability of a stimulus at y '; b, b' are constants.
Empirically, the ""classical"" Receptive Field (RF) of a neuron is defined as the region of
the input field within which localised stimulation causes change in its activity. This concept can be modelled in neural field theory as: the RF of a neuron at x is the portion of the
input field within which a stimulus evokes a positive membrane potential (inhibitory RFs
are not considered). If the neural field is a continuous map of the sensory surface then the
RF of a neuron is fully described by its two borders rdx), rix), defined formally:
i

= 1,2

(3)

which are illustrated in figure 1.
Let RF size and RF position be denoted respectively by the functions rex) and m(x), which
represent experimentally measurable quantities. In terms of the border functions they can
be expressed:

r(x) = r2 (x) - r1 (x)

(4)

m(x) =-} (rl {x} + r2 (x))
y

~---------------------------

x

Figure 1:
RF
boundaries as a
function of position
in the neural field,
for a topographically ordered network. Only the region
in-between
rdx) and rix) has
positive
steadystate
membrane
potential
r1(x,y).
rdx) and rix) are
defined
by
the
condition
r1(x,r;(x))=O
for
i=J,2.

Using (1), (2) and the definition (3), Takeuchi and Amari(1979) derived dynamical equations for the change in RF borders due to learning. In the case of uniform stimulus probability, they found solutions for the steady-state RF border functions. With periodic
boundary conditions, the basic solution is a linear map with constant RF size:

85

Reorganization of Somatosensory Cortex after Tactile Training

r(x) = ro = const
m(x) = px ++ro

= px
r~tni (x) = px+ ro

r l uni ( x )

(5)

This means that both RF size and activity blob size are uniform across the network and
that RF position m(x) is a linear function of network location. (The value of p is determined by boundary conditions; ro is then determined from the joint equilibrium of (I),
(2?. The inverse of the RF position function, denoted by m-l(y), is the centre of the cortical active region caused by a stimulus centred at y. The change in m-l(y) over a unit interval in the input field is, by empirical definition, the cortical magnification factor (MF).
Here we model MF as the rate of change of m-l(y). The MF for the system described by
(5) is:
d
_I ( )
-m
y =p -I

(6)

dy

3. ANALYSIS OF TACTILE TRAINING
3.1 TRAINING MODEL AND ASSUMPTIONS
Jenkins et aI's training sessions caused an increase in the relative frequency of stimulation
to the finger tips, and hence a decrease in relative frequency of stimulation elsewhere.
Over a long time, we can express this fact as a localised change in stimulus probability
(figure 2). (This is not sufficient to cause cortical reorganisation - Recanzone et al( 1992)
showed that attention to the stimulation is vital. We consider only attended stimulation in
this model). To account for such data it is clearly necessary to analyse non-uniform
stimulus probabilities, which demands extending the results of Takeuchi and Amari. Unfortunately, it seems to be hard to obtain general results. However, a perturbation analysis around the uniform probability solution (5) is possible.
To proceed in this way, we must be able to assume that the change in the stimulus probability density function away from uniformity is small. This reasoning is expressed by the
following equation:

p(y) = Po + E p(y)

(7)

where pry) is the new stimulus probability in terms of the uniform one and a perturbation
due to training: E is a small constant. The effect of the perturbation is to ease the weight
dynamics (2) away from the solution (5) to a new steady-state. Our goal is to discover the
effect of this on the RF border functions, and hence for RF size and MF.

p(y)

Figure 2: The type
of
change
in
stimulus probability density that we
assume to model
the effects of behavioural training.

o

y

86

R. S. PETERSEN, J. G. TAYLOR

3.2 PERTURBATION ANALYSIS
3.2.1 General Case
For a small enough perturbation, the effect on the RF borders and on the activity blob size
ought also to be small. We consider effects to first order in E, seeking new solutions of
the form:
i

= 1,2

,{x} = r; {x} - ~ {x}
m{x} = +(~ (X}+'2 (x})

(8)

where the superscript peT denotes the new, perturbed equilibrium and uni denotes the unperturbed, uniform probability equilibrium. Using (1) and (2) in (3) for the post-training
RF borders, expanding to first order in E, a pair of difference equations may be obtained
for the changes in RF borders. It is convenient to define the following terms:

J

rt '(x)

o

r,""no (x)

ro

At (x) = p(y+ px)k(y)dy-b' a~
o

Jp(y)dy
r;-n' (x )

Jp(y + px + TO )k(y)dy - b' a~ Jp(y)dy
k(y) = bJa(y - y' )a(y' )dy'

A2 {x} =

(9)

B = b' a~p() -k(ro)po > 0
C=

w(p-tTo)p-t <0

where the signs of Band C arise due to stability conditions (Amari, 1977; Takeuchi and
Amari, 1979). In terms of RF size and RF position (4), the general result is:

= ~(~ + I)At (x) - M2 (x)
BC~2m{X) = (B- C -+ C~)(~+ I}A t (x) + (C- B++(C -2B)~)A2 (x)
B~2 ,(X}

(10)

where ~ is the difference operator:
~ f{ x) = f( x + p - t To) - f( x)

(11 )

3.2.2 Particular Case
The second order difference equations (l0) are rather opaque. This is partly due to coupling in y caused by the auto-correlation function key): (10) simplifies considerably if very
narrow stimuli are assumed - a(y)=O(y) (see also Amari, 1980). For periodic boundary
conditions:

(12)

where:

Reorganization of Somatosensory Cortex after Tactile Training

m -I P(W (y)

87

= m -I pre (y) + Em -I (y)
=p-l(y_+ro)+Em-l(y)

(13)

and we have used the crude approximation:

t;:

d _() 1
(
dx m x """" ~m x -

1

2"" P

_I

ro

)

(14)

which demands smoothness on the scale of 10 . However, for perturbations like that
sketched in figure 2, this is sufficient to tell us about the constant regions of MF. (We
would not expect to be able to model the data in the transition region in any case, as its
form is too dependent upon fine detail of the model).
Our results (12) show that the change in RF size of a neuron is simply minus the total
change in stimulus probability over its RF. Hence RF size decreases where p(y) increases
and vice versa. Conversely, the change in MF at a given stimulus location is roughly the
local average change in stimulus probability there. Note that changes in RF size correlate
inversely with changes in MF. Figure 3 is a sketch of these results for the perturbation of
figure 2.
MF

RF

o

o

y

I

\

I

L.J

Figure 3: Results of perturbation analysis for how behavioural training (figure 2) changes
RF size and MF respectively, in the case where stimulus width can be neglected. For MF
- due to the approximation (14) - predictions do not apply near the transitions.

4. DISCUSSION
Equations (12) are the results of our model for RF size and MF after area 3b has fully
adapted to the behavioural task, in the case where stimulus width can be neglected. They
appear to be fully consistent with the data of Jenkins et al described above: RF size decreases in the region of cortex selective for the stimulated body part and the MF for this
body part increases. Our analysis also makes a specific prediction that goes beyond
Jenkins et aI's data, directly due to the inverse relationship between changes in RF size
and those in MF. Within the regions that surrender territory to the entrained finger tips
(sometimes the face region), for which MF decreases, RF sizes should increase.
Surprisingly perhaps, these changes in RF size are not due to adaptation of the afferent
weights s(x,y). The changes are rather due to the adaptive threshold term six). This
point will be discussed more fully elsewhere.
A limitation of our analysis is the assumption that the change in stimulus probability is in
some sense small. Such an approximation may be reasonable for behavioural training but
seems less so as regards important experimental protocols like amputation or denervation.
Evidently a more general analysis would be highly desirable.

88

R. S. PETERSEN,J. O. TAYLOR

5. CONCLUSION
We have analysed a system with three interacting features: lateral inhibitory interactions;
Hebbian adaptivity of afferent synapses and an adaptive firing threshold. Our results indicate that such a system can account for the data of Jenkins et aI, concerning the response of adult somatosensory cortex to the changing environmental demands imposed by
tactile training. The analysis also brings out a prediction of the model, that may be testable.

Acknowledgements
RSP is very grateful for a travel stipend from the NIPS Foundation and for a Nick
Hughes bursary from the School of Physical Sciences and Engineering, King's College
London, that enabled him to participate in the conference.

References
Amari S. (1977) BioI. Cybern. 2777-87
Amari S. (1980) Bull. Math. Biology 42339-364
Geman S. (1979) SIAM 1. App. Math. 36 86-105
Grajski K.A., Merzenich M.M. (1990) in Neural Information Processing Systems 2
Touretzky D.S. (Ed) 52-59
HaberW.B. (1955)1. Psychol. 40115-123
Jenkins W.M ., Merzenich M.M., Ochs M.T., Allard T., Gufc-Robles E. (1990) 1. Neurophysiol. 63 82-104
Kaas J.H. (1995) in The Cognitive Neurosciences Gazzaniga M.S. (Ed ic) 51-71
Merzenich M.M., Kaas J.H., Wall J.T., Nelson R.J., Sur M., Felleman DJ. (1983a) Neuroscience 8 35-55
Merzenich M .M., Kaas J.H., Wall J.T., Sur M., Nelson R.I., Felleman DJ . (1983b) Neuroscience 10639-665
Merzenich M.M., Nelson R.I., Stryker M.P., Cynader M.S., Schoppmann A., Zook J.M.
(1984) 1. Compo Neural. 224591-605
Mogilner A., Grossman A.T., Ribrary V., Joliot M., Vol mann J., Rapaport D., Beasley R.,
L1inas R. (1993) Proc. Natl. Acad. Sci. USA 90 3593-3597
Pearson J.e., Finkel L.H., Edelman G.M. (1987) 1. Neurosci. 124209-4223
Recanzone G.H., Merzenich M.M., Jenkins W.M., Grajski K.A., Dinse H.R. (1992) 1.
Neurophysiol. 67 1031-1056
Ritter H., Schulten K. (1986) BioI. Cybern. 5499-106
Takeuchi A., Amari S. (1979) BioI. Cybern. 35 63-72
Willshaw DJ., von der Malsburg e. (1976) Proc. R. Soc. Lond. B194 203-243

"
1997,Gradients for Retinotectal Mapping,,1423-gradients-for-retinotectal-mapping.pdf,Abstract Missing,"Gradients for retinotectal mapping
Geoffrey J. Goodhill
Georgetown Institute for Cognitive and Computational Sciences
Georgetown University Medical Center
3970 Reservoir Road
Washington IX: 20007
geoff@giccs.georgetown.edu

Abstract
The initial activity-independent formation of a topographic map
in the retinotectal system has long been thought to rely on the
matching of molecular cues expressed in gradients in the retina
and the tectum. However, direct experimental evidence for the
existence of such gradients has only emerged since 1995. The new
data has provoked the discussion of a new set of models in the experimentalliterature. Here, the capabilities of these models are analyzed, and the gradient shapes they predict in vivo are derived.

1 Introduction
During the early development of the visual system in for instance rats, fish and
chickens, retinal axons grow across the surface of the optic tectum and establish
connections so as to form an ordered map. Although later neural activity refines
the map, it is not required to set up the initial topography (for reviews see Udin
& Fawcett (1988); Goodhill (1992?. A long-standing idea is that the initial topography is formed by matching gradients of receptor expression in the retina with
gradients of ligand expression in the tectum (Sperry, 1963). Particular versions of
this idea have been formalized in theoretical models such as those of Prestige &
Willshaw (1975), Willshaw & von der Malsburg (1979), Whitelaw & Cowan (1981),
and Gierer (1983;1987). However, these models were developed in the absence
of any direct experimental evidence for the existence of the necessary gradients.
Since 1995, major breakthroughs have occurred in this regard in the experimental
literature. These center around the Eph (Erythropoetin-producing hepatocellular)
subfamily of receptor tyrosine kinases. Eph receptors and their ligands have been
shown to be expressed in gradients in the developing retina and tectum respectively, and to playa role in guiding axons to appropriate positions. These exciting
new developments have led experimentalists to discuss theoretical models differ-

Gradients/or Retinotectal Mapping

153

ent from those previously proposed (e.g. Tessier-Lavigne (1995); Tessier-Lavigne
& Goodman (1996); Nakamoto et aI, (1996)). However, the mathematical consequences of these new models, for instance the precise gradient shapes they require,
have not been analyzed. In this paper, it is shown that only certain combinations
of gradients produce appropriate maps in these models, and that the validity of
these models is therefore experimentally testable.

2 Recent experimental data
Receptor tyrosine kinases are a diverse class of membrane-spanning proteins. The
Eph subfamily is the largest, with over a dozen members. Since 1990, many of the
genes encoding Eph receptors and their ligands have been shown to be expressed
in the developing brain (reviewed in Friedman & O'Leary, 1996). Ephrins, the
ligands for Eph receptors, are all membrane anchored. This is unlike the majority
of receptor tyrosine kinase ligands, which are usually soluble. The ephrins can be
separated into two distinct groups A and B, based on the type of membrane anchor.
These two groups bind to distinct sets of Eph receptors, which are thus also called
A and B, though receptor-ligand interaction is promiscuous within each subgroup.
Since many research groups discovered members of the Eph family independently,
each member originally had several names. However a new standardized notation
was recently introduced (Eph Nomenclature Committee, 1997), which is used in
this paper.
With regard to the mapping from the nasal-temporal axis of the retina to the
anterior-posterior axis of the tectum (figure 1), recent studies have shown the following (see Friedman & O'Leary (1996) and Tessier-Lavigne & Goodman (1996)
for reviews).
? EphA3 is expressed in an increasing nasal to temporal gradient in the
retina (Cheng et aI, 1995).
? EphA4 is expressed uniformly in the retina (Holash & Pasquale, 1995).
? Ephrin-A2, a ligand of both EphA3 and EphA4, is expressed in an increasing rostral to caudal gradient in the tectum (Cheng et aI, 1995).
? Ephrin-A5, another ligand of EphA3 and EphA4, is also expressed in an
increasing rostral to caudal gradient in the tectum, but at very low levels
in the rostral half of the tectum (Drescher et aI, 1995).
All of these interactions are repulsive. With regard to mapping along the complementary dimensions, EphB2 is expressed in a high ventral to low dorsal gradient
in the retina, while its ligand ephrin-B1 is expressed in a high dorsal to low ventral
gradient in the tectum (Braisted et aI, 1997). Members of the Eph family are also
beginning to be implicated in the formation of topographic projections between
many other pairs of structures in the brain (Renping Zhou, personal communication). For instance, EphA5 has been found in an increasing lateral to medial gradient in the hippocampus, and ephrin-A2 in an increasing dorsal to ventral gradient
in the septum, consistent with a role in establishing the topography of the map
between hippocampus and septum (Gao et aI, 1996).
The current paper focusses just on the paradigm case of the nasal-temporal to
anterior-posterior axis of the retinotectal mapping. Actual gradient shapes in this
system have not yet been quantified. The analysis below will assume that certain
gradients are linear, and derive the consequences for the other gradients.

G. J. Goodhill

154

TECTUM

RETINA

c

N

L(y)

R(x)

k:==::y

""'-----.......;;;~x

Figure 1: This shows the mapping that is normally set up from the retina to the
tectum. Distance along the nasal-temporal axis of the retina is referred to as x and
receptor concentration as R( x). Distance along the rostral-caudal axis of the tectum
is referred to as y and ligand concentration as L(y).

3

Mathematical models

Let R be the concentration of a receptor expressed on a growth cone or axon, and
L the concentration of a ligand present in the tectum. Refer to position along the
nasal-temporal axis of the retina as x, and position along the rostral-caudal axis of
the tectum as y, so that R = R(x) and L = L(y) (see figure 1). Gierer (1983; 1987)
discusses how topographic information could be signaled by interactions between
ligands and receptors. A particular type of interaction, proposed by Nakamoto et
al (1996), is that the concentration of a ""topographic signal"", the signal that tells
axons where to stop, is related to the concentration of receptor and ligand by the
law of mass action:
G(x, y) = kR(x)L(y)

(1)

where G(x, y) is the concentration of topographic signal produced within an axon
originating from position x in the retina when it is at position y in the tectum,
and k is a constant. In the general case of multiple receptors and ligands, with
promiscuous interactions between them, this equation becomes
G(x, y) =

L: kijRi(X)Lj(Y)

(2)

i,j

Whether each receptor-ligand interaction is attractive or repulsive is taken care of
by the sign of the relevant k ij ?
Two possibilities for how G(x, y) might produce a stop (or branch) signal in the
growth cone (or axon) are that this occurs when (1) a ""set point"" is reached (discussed in, for example, Tessier-Lavigne & Goodman (1996); Nakamoto et al (1996?
,i.e. G (x, y) = c where c is a constant, or (2) attraction (or repulsion) reaches a local
maximum (or minimum), i.e. &G~~,y) = 0 (Gierer, 1983; 1987). For a smooth, uni-

Gradients for Retinotectal Mapping

155

form mapping, one of these conditions must hold along a line y ex: x. For simplicity
assume the constant of proportionality is unity.
3.1

Set point rule

For one gradient in the retina and one gradient in the tectum (i.e. equation 1), this
requires that the ligand gradient be inversely proportional to the receptor gradient:
c
L(x) = R(x)
If R(x) is linear (c.f. the gradient of EphA3 in the retina), the ligand concentration
is required to go to infinity at one end of the tectum (see figure 2). One way round
this is to assume R(x) does not go to zero at x = 0: the experimental data is not
precise enough to decide on this point. However, the addition of a second receptor
gradient gives
c
L(x) = k1R1 (x) + k2R2(X)
If R1 (x) is linear and R 2(x) is flat (c.f. the gradient of EphA4 in the retina), then
L (y) is no longer required to go to infinity (see figure 2). For two receptor and two
ligand gradients many combinations of gradient shapes are possible. As a special
case, consider R1 (x) linear, R 2(x) flat, and L 1(y) linear (c.f. the gradient of Elfl in
the tectum). Then L2 is required to have the shape
L ( ) = ay2
2 Y
dy

+ by
+e

where a, b, d, e are constants. This shape depends on the values of the constants,
which depend on the relative strengths of binding between the different receptor
and ligand combinations. An interesting case is where R1 binds only to L1 and R2
binds only .to L 2 , i.e. there is no promiscuity. In this case we have

L 2(y) ex: y2
(see figure 2). This function somewhat resembles the shape of the gradient that
has been reported for ephrin-AS in the tectum. However, this model requires one
gradient to be attractive, whereas both are repulsive.
3.2

Local optimum rule

For one retinal and one tectal gradient we have the requirement
R(x) aL(y) = 0
ay

This is not generally true along the line y = x, therefore there is no map. The same
problem arises with two receptor gradients, whatever their shapes. For two receptor and two ligand gradients many combinations of gradient shapes are possible.
(Gierer (1983; 1987) investigated this case, but for a more complicated reaction law
for generating the topographic signal than mass action.) For the special case introduced above, L 2 (y) is required to have the shape
L2(y) = ay + blog(dy

+ e) + f

where a, b, d, e, and f are constants as before. Considering the case of no promiscuity, we again obtain
L 2(y) ex: y2
i.e. the same shape for L2 (y) as that specified by the set point rule.

G. 1. Goodhill

156

A

L

B

L

c

Figure 2: Three combinations of gradient shapes that are sufficient to produce a
smooth mapping with the mass action rule. In the left column the horizontal axis
is position in the retina while the vertical axis is the concentration of receptor. In
the right column the horizontal axis is position in the tectum while the vertical axis
is the concentration of ligand. Models A and B work with the set point but not the
local optimum rule, while model C works with both rules. For models B and C,
one gradient is negative and the other positive.

Gradients for Retinotectal Mapping

157

4 Discussion
For both rules, there is a set of gradient shapes for the mass-action model that is
consistent with the experimental data, except for the fact that they require one gradient in the tectum to be attractive. Both ephrin-A2 and ephrin-A5 have repulsive
effects on their receptors expressed in the retina, which is clearly a problem for
these models. The local optimum rule is more restrictive than the set point rule,
since it requires at least two ligand gradients in the tectum. However, unlike the set
point rule, it supplies directional information (in terms of an appropriate gradient
for the topographic signal) when the axon is not at the optimal location.
In conclusion, models based on the mass action assumption in conjunction with either a ""set point"" or ""local optimum"" rule can be true only if the relevant gradients
satisfy the quantitative relationships described above. A different theoretical approach, which analyzes gradients in terms of their ability to guide axons over the
maximum possible distance, also makes predictions about gradient shapes in the
retinotectal system (Goodhill & Baier, 1998). Advances in experimental technique
should enable a more quantitative analysis of the gradients in situ to be performed
shortly, allowing these predictions to be tested. In addition, analysis of particular
Eph and ephrin knockout mice (for instance ephrin-A5 (Yates et aI, 1997? is now
being performed, which should shed light on the role of these gradients in normal
map development.

Bibliography
Braisted, J.E., McLaughlin, T., Wang, H.U., Friedman, G.C, Anderson, D.J. &
O'Leary, D.D.M. (1997). Graded and lamina-specific distributions of ligands
of EphB receptor tyrosine kinases in the developing retinotectal system. Developmental Biology, 19114-28.
Cheng, H.J., Nakamoto, M., Bergemann, A.D & Flanagan, J.G. (1995). Complementary gradients in expression and binding of Elf-1 and Mek4 in development of
the topographic retinotectal projection map. Cell, 82,371-381.
Drescher, U., Kremoser, C, Handwerker, C, Loschinger, J., Noda, M. & Bonhoeffer,
F. (1995). In-vitro guidance of retinal ganglion-cell axons by RAGS, a 25 KDa
tectal protein related to ligands for Eph receptor tyrosine kinases. Cell, 82, 359370.
Eph Nomenclature Committee (1997). Unified nomenclature for Eph family receptors and their ligands, the ephrins. Cell, 90, 403-404.
Friedman, G.C & O'Leary, D.D.M. (1996). Eph receptor tyrosine kinases and their
ligands in neural development. Curro Opin. Neurobiol., 6, 127-133.
Gierer, A. (1983). Model for the retinotectal projection. Proc. Roy. Soc. Lond. B, 218,
77-93.
Gierer, A. (1987). Directional cues for growing axons forming the retinotectal projection. Development, 101,479-489.
Gao, P.-P., Zhang, J.-H., Yokoyama, M., Racey, R, Dreyfus, CF., Black, LR & Zhou,
R. (1996). Regulation of topographic projection in the brain: Elf-1 in the hippocampalseptal system. Proc. Nat. Acad. Sci. USA, 93, 11161-11166.
Goodhill, G.J. (1992). Correlations, Competition and Optimality: Modelling
the Development of Topography and Ocular Dominance. Cognitive Science Research Paper CSRP 226, University of Sussex. Available from
www.giccs.georgetown.edu/ ""'geoff

158

G. 1. Goodhill

Goodhill, G.J. & Baier, H. (1998). Axon guidance: stretching gradients to the limit.
Neural Computation, in press.
Holash, J.A. & Pasquale, E.B. (1995). Polarized expression of the receptor proteintyrosine kinase CekS in the developing avian visual system. Developmental Biology, 172, 683-693.
Nakamoto, M., Cheng H.J., Friedman, G.C, Mclaughlin, T., Hansen, M.J., Yoon,
CH., O'Leary, D.D.M. & Flanagan, J.G. (1996). Topographically specific effects
of ELF-Ion retinal axon guidance in-vitro and retinal axon mapping in-vivo.
Cell, 86, 755-766.
Prestige, M.C & Willshaw, D.J. (1975). On a role for competition in the formation
of patterned neural connexions. Proc. R. Soc. Lond. B, 190, 77-98.
Sperry, RW. (1963). Chemoaffinity in the orderly growth of nerve fiber patterns
and connections. Proc. Nat. Acad. Sci., U.S.A., 50, 703-710.
Tessier-Lavigne, M. (1995). Eph receptor tyrosine kinases, axon repulsion, and the
development of topographic maps. Cell, 82, 345-348.
Tessier-Lavigne, M. and Goodman, CS. (1996). The molecular biology of axon
guidance. Science, 274, 1123-1133.
Udin, S.B. & Fawcett, J.W. (1988). Formation of topographic maps. Ann. Rev. Neurosci., 11,289-327.
Whitelaw, V.A. & Cowan, J.D. (1981). Specificity and plasticity of retinotectal connections: a computational model. Jou. Neurosci., 1, 1369-1387.
Willshaw, D.J. & Malsburg, C von der (1979). A marker induction mechanism for
the establishment of ordered neural mappings: its application to the retinotectal problem. Phil. Trans. Roy. Soc. B, 287, 203-243.
Yates, P.A., McLaughlin, T., Friedman, G.C, Frisen, J., Barbacid, M. & O'Leary,
D.D.M. (1997). Retinal axon guidance defects in mice lacking ephrin-A5 (ALl/RAGS). Soc. Neurosci. Abstracts, 23, 324.

"
2012,Entropy Estimations Using Correlated Symmetric Stable Random Projections,,4667-entropy-estimations-using-correlated-symmetric-stable-random-projections.pdf,"Methods for efficiently estimating the Shannon entropy of data streams have important applications in learning, data mining, and network anomaly detections (e.g., the DDoS attacks). For nonnegative data streams, the method of Compressed Counting (CC) based on maximally-skewed stable random projections can provide accurate estimates of the Shannon entropy using small storage. However, CC is no longer applicable when entries of data streams can be below zero, which is a common scenario when comparing two streams. In this paper, we propose an algorithm for entropy estimation in general data streams which allow negative entries. In our method, the Shannon entropy is approximated by the finite difference of two correlated frequency moments estimated from correlated samples of symmetric stable random variables. Our experiments confirm that this method is able to substantially better approximate the Shannon entropy compared to the prior state-of-the-art.","Entropy Estimations Using Correlated Symmetric
Stable Random Projections
Ping Li
Department of Statistical Science
Cornell University
Ithaca, NY 14853
pingli@cornell.edu

Cun-Hui Zhang
Department of Statistics and Biostatistics
Rutgers University
New Brunswick, NJ 08901
czhang@stat.rutgers.edu

Abstract
Methods for efficiently estimating Shannon entropy of data streams have important applications in learning, data mining, and network anomaly detections (e.g.,
the DDoS attacks). For nonnegative data streams, the method of Compressed
Counting (CC) [11, 13] based on maximally-skewed stable random projections
can provide accurate estimates of the Shannon entropy using small storage. However, CC is no longer applicable when entries of data streams can be below zero,
which is a common scenario when comparing two streams. In this paper, we
propose an algorithm for entropy estimation in general data streams which allow
negative entries. In our method, the Shannon entropy is approximated by the finite difference of two correlated frequency moments estimated from correlated
samples of symmetric stable random variables. Interestingly, the estimator for the
moment we recommend for entropy estimation barely has bounded variance itself,
whereas the common geometric mean estimator (which has bounded higher-order
moments) is not sufficient for entropy estimation. Our experiments confirm that
this method is able to well approximate the Shannon entropy using small storage.

1 Introduction
Computing the Shannon entropy in massive data have important applications in neural computation [17], graph estimation [5], query logs analysis in Web search [14], network anomaly detection [21], etc. (See NIPS2003 workshop on entropy estimation www.menem.com/?ilya/
pages/NIPS03). In modern applications, as massive datasets are often generated in a streaming
fashion, entropy estimation in data streams has become a challenging and interesting problem.
1.1 Data Streams
Massive data generated in a streaming fashion are difficult to transmit and store [15], as the processing is often done on the fly in one-pass of the data. The problem of ?scaling up for high
dimensional data and high speed data streams? is among the ?ten challenging problems in data mining research? [20]. Mining data streams at petabyte scale has become an important research area [1],
as network data can easily reach that scale [20].
In the standard turnstile model [15], a data stream is a vector At of length D, where D = 264 or
even D = 2128 is possible in network applications, e.g., (a pair of) IP addresses + port numbers. At
time t, there is an input stream at = (it , It ), it ? [1, D] which updates At by a linear rule:
At [it ] = At?1 [it ] + It .

(1)

where It is the increment/decrement of package size at t. For network traffic, normally At [i] ? 0,
which is called the strict turnstile model and suffices for describing certain natural phenomena. On
the other hand, the general turnstile model (which allows At [i] < 0) is often used for comparing
two streams, e.g., in network OD (origin-destination) flow analysis [21].
An important task is to compute the ?-th frequency moment F(?) and the Shannon entropy H:
F(?) =

D
X

?

|At [i]| ,

H=?

i=1

D
X
|At [i]|
i=1

F1

log

|At [i]|
,
F1

(2)

The exact computation of these summary statistics is not feasible because to do so one has to store
the entire vector At of length D, as the entries are time-varying. Also, many applications (such as
anomaly detections of network traffic) require computing the summary statistics in real-time.
1

1.2 Network Measurement, Monitoring, and Anomaly Detection
Network traffic is a typical example of high-rate data streams. Industries are now prepared to move
to 100 Gbits/second or Terabit/second Ethernet. An effective and reliable measurement of network
traffic in real-time is crucial for anomaly detection and network diagnosis; and one such measurement metric is the Shannon entropy [4, 8, 19, 2, 9, 21]. The exact entropy measurement in real-time
on high-speed links is however computationally prohibitive.
The Distributed Denial of Service (DDoS) attack is a representative example of network anomalies. A DDoS attack attempts to make computers unavailable to intended
users, either by forcing users to reset the computers or
by exhausting the resources of service-hosting sites. For
example, hackers may maliciously saturate the victim
machines by sending many external communication requests. DDoS attacks typically target sites such as banks,
credit card payment gateways, or military sites. A DDoS
attack normally changes the statistical distribution of network traffic, which could be reliably captured by the abnormal variations in the measurements of Shannon entropy [4]. See Figure 1 for an illustration.

source IP address: entropy value
10
9
8
7
6
5
4
3
2
1
0
200

400
600
800 1000
packet counts (thousands)

1200

Figure 1: This plot is reproduced from
a DARPA conference [4]. One can view
x-axis as the surrogate for time. Y-axis
is the measured Shannon entropy, which
exhibited a sudden sharp change at the
time when an attack occurred.

Apparently, the entropy measurements do not have to be
?perfect? for detecting attacks. It is however crucial that
the algorithms should be computationally efficient (i.e., real-time and one-pass) at low memory cost,
because the traffic data generating by large high-speed networks are enormous and transient.

1.3 Symmetric Stable Random Projections and Entropy Estimation Using Moments
It turns out that, for 0 < ? ? 2, one can use stable random projections to compute F(?) efficiently
because the Turnstile model (1) is a linear model and the random projection operation is also linear
(i.e., vector-matrix multiplication) [7]. Conceptually, we multiply the data stream vector At ? RD
by a random matrix R ? RD?k , resulting in a vector X = At ? R ? Rk with entries
xj = [At ? R]j =

D
X

rij At [i], j = 1, 2, ..., k

i=1

where rij ? S(?, 1) is a symmetric ?-stable random variable with unit scale [3, 22]: E(erij t ) =
?
e?|t| . The standard normal (or Cauchy) distribution is a special case with ? = 2 (or ? = 1).
In data stream computations, the matrix R is not materialized. The standard procedure is to
(re)generate entries of R on-demand [7] using pseudo-random numbers [16]. Thus, we only need
to store X ? Rk . When a stream element at = (it , It ) arrives, one updates the entries of X:
xj ? xj + It rit j , j = 1, 2, ..., k.
By property of stable distributions, the samples xj , j = 1 to k, are also i.i.d. stable
!
?
D
D
X
X
?
xj =
rij At [i] ? S ?, F(?) =
|At [i]|
i=1

(3)

(4)

i=1

Therefore, the task boils down to estimating the scale parameter from k i.i.d. stable samples.
Because the Shannon entropy is essentially the derivative of the frequency moment at ? = 1, the
popular approach is to approximate the Shannon entropy by the Tsallis entropy [18]:
?
!
F(?)
1
1? ?
T? =
.
(5)
??1
F(1)
which approaches the Shannon entropy H as ? ? 1. [21] used a slight variant of (5) but the
difference is not essential.1 In their approach, F(?) and F(1) are first estimated separately from
1

F

?F

[21] used (1+?)2? (1??) and estimated the two frequency moments independently. The subtle difference
between the finite difference approximations is not essential. It is the correlation that plays the crucial role.

2

two independent sets of samples. The estimated moments are then plugged in (5) to estimate the
Shannon entropy H. Immediately, we can see the problem here: the variance of the estimated T(?)
1
1
2
might be proportional to (??1)
2 = ?2 . (Recall var(cX) = c var(X)).
One question is how to choose ? (i.e., ?). [6] proposed a conservative criterion by choosing ?
?7
according to the worst case bias |H ? T? |. One can verify
? 14that
? ? = |1 ? ?| < 10 is likely in
[6]. In other words, the required sample size could be O 10 . In practice, [21] exploited the biasvariance tradeoff but they still had to use an excessive number of samples, e.g., 106 . In comparison,
using our proposed approach, it appears that 100 ? 1000 samples might be sufficient.
1.4 Our Proposal
We have made two key contributions. Firstly, instead of estimating F(?) and F(1) separately using
two independent sets of samples, we make them highly positively correlated. Intuitively, if the two
consistent estimators, denoted by F?(?) and F?(1) respectively, are highly positively
correlated, then
?
?
possibly their ratio

F?(?)
F? ?

can be close to 1 with small variance. Ideally, if V ar
?

(1)

variance of the estimated Tsallis entropy T?? =

1
??1

?

It turns out that finding an estimator with V ar

?
F
(?)
??
F

1?

?
F
(?)
??
F

?

?
F
(?)
??
F

? ?
= O ?2 , the

(1)

will be essentially independent of ?.

(1)

?

? ?
= O ?2 was not straightforward. It is known

(1)

that around ? = 1, the geometric mean estimator [10] is nearly statistically optimal. Interestingly,
our analysis and
show that using the geometric mean estimator, we can essentially only
? simulation
?
?
F

achieve V ar F?(?)
= O (?), which, albeit a large improvement, is not small sufficient to cancel
?
(1)
? 1 ?
the O ?2 term. Therefore, our second key component is a new estimator of T? using a moment
estimator which does not have (or barely has) finite variance. Even though such an estimator is not
good for estimating the single moment compared to the geometric mean, due to the high correlation,
? ?
F?
the ratio F?(?)
is still very well-behaved and its variance is essentially O ?2 , as shown in our
?
(1)

theoretical analysis and experiments.
1.5 Compressed Counting (CC) for Nonnegative Data Streams
The recent work [13] on Compressed Counting (CC) [11] provides an ideal solution to the problem
of entropy estimation in nonnegative data streams. Basically, for nonnegative data streams, i.e.,
At [i] ? 0 at all times and all locations, we can compute the first moment easily, because
F(1) =

D
X

|At [i]| =

i=1

D
X
i=1

At [i] =

t
X

Is

(6)

s=0

where Is is the increment/decrement at time s. In other words, we just need a single counter to
accumulate all the increments Is . This observation lead to the conjecture that estimating F(?) should
be also easy if ? ? 1, which consequently lead to the development of Compressed Counting which
used maximally-skewed stable random projections instead of symmetric stable projections.? The
?
most recent work of CC [13] provided a new moment estimator to achieve the variance ? O ?2 .
Unfortunately, for general data streams where entries can be negative, we have to resort to symmetric stable random projections. Fundamentally, the reason that skewed projections work well on
nonnegative data streams is because the data themselves are skewed. However, when we compare
two streams, the data become more or less symmetric and hence we must use symmetric projections.
1.6 Why Comparing the Difference of Two Streams?
In machine learning research and practice, people routinely use the difference between feature vectors. [21] used the difference between data streams from a slightly different motivation.
The goal of [21] is to measure the entropies of all OD pairs (origin-destination) in a network, because
entropy measurements are crucial for detecting anomaly events such as DDoS attacks and network
failures. They argued that the change of entropy of the traffic distribution may be invisible (i.e., too
small to be detected) in the traditional volume matrix even during the time when an attack occurs.
Instead, they proposed to measure the entropy from a number of locations across the network, i.e.,
3

by examining the entropy of every OD flow in the network. In a similar argument, a DDoS attack
may be invisible in terms of the traffic volume change, if the attack is launched outside the network.
While [21] successfully demonstrated that measuring the Shannon entropy of OD flows is effective
for detecting anomaly events, at that time they did not have the tools for efficiently estimating the
entropy. Using symmetric stable random projections and independent samples,? they
? needed a large
number of samples (e.g., 106 ) because their variance blows up at the rate of O ?12 .
For anomaly detection, reducing the sample size (k) is crucial because k determines the storage
and estimation speed; and it is often required to detect the events at real time. In addition, the
pseudo-random numbers have to be (re)-generated on the fly, at a cost proportional to k.

2 Our Proposed Algorithm
Recall that a data stream is a long vector At [i], i = 1 to D. At time t, an incoming element
at = (it , It ) updates one entry: At [it ] ? At?1 [it ] + It . Conceptually, we generate a random
matrix R ? RD?k whose entries are sampled from a stable distribution and multiply it with At :
X = At ? R. The matrix multiplication is linear and can be conducted incrementally as the new
stream elements arrive. R is not materialized; its entries are re-generated on demand using pseudorandom numbers, as the standard practice in data stream computations [7]. Our method does not
require At [i] ? 0 and hence it can handle the difference between two streams (e.g., the OD flows).
2.1 The Symmetric Stable Law
Our work utilizes the symmetric stable distribution. We adopt the standard approach [3] to sample
from the stable law S(?, 1) with index ? and unit scale. We generate two independent random
variables: w ? exp(1) and u ? unif om(??/2, ?/2) and feed them to a nonlinear transformation:
Z(?) = g(w, u, ?) =

sin(?u) h cos(u ? ?u) i(1??)/?
? S(?, 1),
w
(cos u)1/?

(7)

to obtain a sample from S(?, 1). An important property is that, for ?1 < ? < ?, the moment exists:
E|Z|? = (2/?)?(1 ? ?/?)?(?) sin(??/2). For convenience, we define
G(?, ?) = E|g(w, u, ?)|? =

2
?(1 ? ?/?)?(?) sin (??/2)
?

(8)

2.2 Our Recommended Estimator
Conceptually, we have two matrices of i.i.d. random numbers:
wij ? exp(1),

uij ? unif orm(??/2, ?/2),

i = 1, 2, ..., D, j = 1, 2, ..., k,

(9)

As new stream elements arrive, we incrementally maintain two sets of samples, i.e., for i = 1 to k,
xj =

D
X

At [i]g(wij , uij , 1),

yj =

i=1

D
X

At [i]g(wij , uij , ?)

(10)

i=1

Note that xj and yj are highly correlated because they are generated using the same random numbers
(with different ?). However, xi and yj are independent if i 6= j.
Our recommended estimator of the Tsallis entropy T? is
?
?
!2? ?
Pk p
?
|yj |
1
?
j=1
?1 ?
?
?
? Pk p
T??,0.5 =
1
??1
? 1 ? 2?
|x
|
j
j=1

(11)

where ? = 1 + ? > 1 and the meaning of 0.5 will soon be clear. When ? is sufficiently small, the
estimated Tsallis entropy will be sufficiently close to the Shannon entropy. A nice property is that its
1
variance is free of ?
or ?12 terms. While it is intuitively clear that it is beneficial to make xj and yj
highly correlated for the sake of reducing the variance, it might not be as intuitive why T??,0.5 (11)
is a good estimator for the entropy. We will explain why the obvious geometric mean estimator [10]
is not sufficient for entropy estimation.
4

3

The Geometric Mean Estimator

For estimating F(?) , the geometric mean estimator [10] is close to be statistically optimal (efficiency
? 80%) at ? ? 1. Thus, it was our first attempt to test the following estimator of the Tsallis entropy:
?
!
Qk
Qk
1/k
?(?),gm
|yj |?/k
F
1
j=1
j=1 |xj |
T??,gm =
1? ?
, where F?(?),gm = k
, F?(1),gm =
,
??1
G (?, ?/k)
Gk (1, 1/k)
F?(1),gm
where G() is defined in (8). After simplification, we obtain:
?
#?
""? ?
k
Y
? yj ??/k G(1, 1/k)
1
? ?
?1 ?
?.
T??,gm =
? xj ?
??1
G(?,
?/k)
j=1
3.1

(12)

Theoretical Analysis

The theoretical analysis of T?(?),gm , however, turns out to be difficult, as it requires computing
??
?
?
""? ?
#
? PD A [i]g(w , u , ?) ?s?/k
? yj ?s?/k
?
?
t
ij
ij
?,
E ?? ??
= E ?? Pi=1
s = 1, 2,
(13)
?
? D At [i]g(wij , uij , 1) ?
xj
i=1

where g() is defined in (7). We first provide the following Lemma:
Lemma 1 Let w ? exp(1) and u ? unif orm(??/2, ?/2) be two independent variables. Let
? = 1 + ? > 1, for small ? > 0. Then, for ? > ?1,
?
?
? g(w, u, ?) ??
?
?
E?
g(w, u, 1) ?

?
?
?
?
=1 ? 0.5772?? + 0.5772??2 ? 1.6386??3 + 1.6822? 2 ?2 + O ??4 + O ? 2 ?3

? (14)

Note that we need to keep higher order terms in order to prove Lemma 2, to show the properties of
the geometric mean estimator, when D = 1 (i.e., a stream with only one element).
Lemma 2 If D = 1, then
? 2?
? ?
?
? 1 ?2
? ?
?
?
1
E T??,gm =
? 2.0935 + 1.0614?2 + O ?3 + O
+O
k 2
k
k
k2
? ?
? ?
?
? 3.3645
?
1
V ar T??,gm =
+O
+O
k
k
k2

(15)
?

(16)

When D = 1, we know T? = H = 0. In this case, the geometric mean estimator T??,gm is
1
, which is very encouraging.
asymptotically unbiased with variance essentially free of ?
Will this result in Lemma 2 extend to general D? The answer is no, even for D = 2, i.e.,
yj
At [1]g(w1j , u1j , ?) + At [2]g(w2j , u2j , ?)
=
xj
At [1]g(w1j , u1j , 1) + At [2]g(w2j , u2j , 1)
Because g() is symmetric, it is possible that the denominator At [1]g(w1j , u1j , 1) +
At [2]g(w2j , u2j , 1) might be very small while the numerator At [1]g(w1j , u1j , ?) +
At [2]g(w2j , u2j , ?) is not too small. In other words, there will be more variations when
D > 1. In fact, our experiments in Sec. 3.2 and the theoretical analysis
? 1 ?of a more general estimator
in Sec. 4 both reveal that the variance of T??,gm
is
essentially
O
? , which is of course still a
? ?
substantial improvement over the previous O ?12 solution.
3.2

Experiments on the Geometric Mean Estimator (Correlated vs. Independent samples)

We present some experimental results for evaluating T??,gm , to demonstrate that (i) using correlation
does substantially reduce variance and hence reduces the? required
sample size, and (ii) the variance
?
1
(or MSE, the mean square error) of T??,gm is roughly O ?
.
5

We follow [13] by using static data to evaluate the accuracies of the estimators. The projected vector
X = At ? R is the same at the end of the stream, regardless of whether it is computed at once (i.e.,
static) or incrementally (i.e., dynamic). Following [13], we selected 4 word vectors from a chunk of
Web crawl data. For example, the entries for vector ?REVIEW? are the numbers of occurrences of
the word ?REVIEW? in each document. We group these 4 vectors into 2 pairs: ?THIS-HAVE? and
?DO-REVIEW? and we estimate the Shannon entropies of the two resultant difference vectors.
Figure 2 presents the mean square errors (MSE) of the estimated Shannon entropy, i.e., E(T?? ?H)2 ,
normalized by the truth (H 2 ). The left panels contain the results using independently sampling
(i.e., the prior work [21]) and the geometric mean estimator. The middle panels contain the results
using correlated sampling (i.e., this paper) and the geometric mean estimator (12). The right panels
multiply the results of the middle panels by ?? to
? illustrate that the variance of the geometric mean
1
estimator for entropy T??,gm is essentially O ?
. See more experiments in Figure 3.
6

6

100

2

10

0

10

k = 1000

?2

4

10

k = 10

2

10

k = 100

0

10

k = 1000

?2

10

10

THIS?HAVE : GM + Indep.
?5

10

?4

10

?3

?2

10
10
?=??1

0

10

?5

10

10

?4

?3

10

?2

10
10
?=??1

?1

10

10

6

Normalized MSE

2

100

10

0

10

k = 1000
?2

10

2

10

k = 10

0

10

k = 100

?2

?5

?4

10

?3

?2

10
10
?=??1

k = 100

?4

10

10

DO?REVIEW : GM + Corr.

4

k = 1000

10

DO?REVIEW : GM + Indep.

10

10

k = 1000
MSE ? ?
?4

10

?3

?2

10
10
?=??1

?1

0

10

0

10

DO?REVIEW : GM + Corr.

?2

k = 10

10

?3

k = 100

10

?4

k = 1000

10

MSE ? ?

?5

10

?1

10

?1

10
k = 10

10

k = 10

?3

10 ?5
10

0

Normalized MSE ? ?

6

4

THIS?HAVE : GM + Corr.

?2

10

?5

?1

10
Normalized MSE

10
THIS?HAVE : GM + Corr.

Normalized MSE ? ?

10

10

?1

10
k = 10

4

Normalized MSE

Normalized MSE

10

?5

10

?4

?3

10

?2

10
10
?=??1

?1

0

10

10

10 ?5
10

?4

10

?3

?2

10
10
?=??1

?1

10

0

10

Figure 2: Two pairs of word vectors were selected. We conducted symmetric random projections
using both independent sampling (left panels, as in [21]) and correlated sampling (middle panels, as
our proposal). The Tsallis entropy (of the difference vector) is estimated using the geometric mean
estimator (12) with three sample sizes k = 10, 100, and 1000. The normalized mean square errors
(MSE: E|T??,gm ? H|2 /H 2 ) verify that correlated sampling reduces the errors substantially.

4

The General Estimator

Since the geometric mean estimator could not satisfactorily solve the entropy estimation problem,
we resort to estimators which behave dramatically different from the geometric mean. Our recommended estimator T??,0.5 as in (11) is a special case (for ? = 0.5) of a more general family of
estimators [12], parameterized by ? ? (0, 1):
?
!
? Pk
!?/?
? Pk
!1/?
?(?),?
|yj |?
|xj |?
F
1
j=1
j=1
T??,? =
1? ?
, F?(?),? =
, F?(1),? =
??1
kG(?, ?)
kG(1, ?)
F?(1),?
which, after simplification, becomes
T??,? =

?

1 ?
1?
??1

Recall G(?, ?) is defined in (8), and

G(1,0.5)
G(?,0.5)

? Pk

?

j=1 |yj | G(1, ?)
Pk
? G(?, ?)
j=1 |xj |

=

!?/? ?
?

(17)

?
?
.
1
?(1? 2?
)

?
To better understand
F?(?),?
? , recall if Z ? S(?, 1), then E|Z| = G(?, ?) < ? if ?1 < ? < ?.
?

Therefore,

Pk

|yj |?
kG(?,?)
j=1

?/?

is an unbiased estimate of F(?) . To recover F(?) , we need to apply the

?
power ?/? operation.
? Thus, it is clear that, as long as 0 < ? < 1, F(?),? is a consistent estimator of
?
F(?) and E F?(?),? is finite. In particular, the variance of F?(?),? is bounded if 0 < ? < 0.5:
? ?
?
1
?
E F(?),? = F(?) + O
,
k
?

? ?
? F 2 ?2 G(?, 2?) ? G2 (?, ?)
1
(?)
?
V ar F(?),? =
+O
k ?2
G2 (?, ?)
k2
?

6

The variance is unbounded if ? = 0.5 and ? = 1, because G(1, 1) = ? (?(0) = ?). Interestingly,
when ? ? 0 and ? = 1, the asymptotic variance reaches the minimum. In fact, when ? ? 0, F?(?),?
converges to the geometric mean estimator F?(?),gm . A variant of F?(?),? was discussed in [12].
4.1

Theoretical Analysis

Based on Lemma 3 and Lemma 4 (which
? is?a fairly?technical
? proof), we know that the variance of the
?2??1
?
general estimator is essentially V ar T?,? = O
, for fixed ? ? (0, 1/2). In other words,
k
when ? is close to 0, the variance of the entropy estimator is essentially on the order of O (1/(k?)),
and while ? is close to 1/2, the variance is essentially O(1/k) as desired.
Lemma 3 For any fixed ? ? (0, 1),
?
?
? ?
?
?
?
? 2
O
E(|x
|
?
|y
|
)
1
1
1
1
+O
V ar T??,? = 2
?
k
k2

?

Lemma 4 Let 0 < ? ? 1/2 and ? = 1 + ?. Let ? ? (0, 1/2) and m be a positive integer no
smaller than 1/?. Then, there exists a universal constant M such that
?
?2
n
o.
e2
E |x1 |? ? |y1 |? ? M F 2? ?1+2??1/m m2 m + H
+ (1 ? 2?)?2
(1 ? 2?),
(2m)

(1)

?
e 2m = PD
where H
i=1

|At [i]| ?
[i]| 2m ?1/(2m)
log |AFt(1)
)
.
F(1)

?

We should clarify that our theoretical analysis is only applicable for fixed ? ? (0, 1/2). When
? = 0.5, the estimator T?(?),0.5 is still well-behaved, except we are unable to precisely analyze this
case. Also, since we do not compute the exact constant, it is possible that for some carefully chosen
? (data-dependent), T?(?),? with ? < 0.5 may exhibit smaller variance than T?(?),0.5 . We recommend
T?(?),0.5 for convenience because it essentially frees practitioners from carefully choosing ?.
4.2

Experimental Results

Figure 3 presents some empirical results, for testing the general estimator T??,? (17), using more
word vector pairs (including the same 2 pairs in Figure 2). We can see that when ? = 0.5, the
(normalized) MSEs become flat (as desired) as ? = ? ? 1 ? 0. When ? > 1/2, the MSEs increase
although the curves remain flat. When ? < 1/2, the MSEs blow up with increasing ?. Note that,
when ? < 1/2, it is possible to achieve smaller MSEs if we carefully choose ?.
How many samples (k) are needed? If the goal is to estimate the Shannon entropy?within a few
percentages of the the true value, then k = 100 ? 1000 should be sufficient, because M SE/H <
0.1 when k ? 100 as shown in Figure 3.

5 Conclusion
Entropy estimation is an important task in machine learning, data mining, network measurement,
anomaly detection, neural computations, etc. In modern applications, the data are often generated
in a streaming fashion and many operations on the streams can only be conducted in one-pass of the
data. It has been a challenging problem to estimate the Shannon entropy of data streams.
The prior work [21] achieved some success in entropy estimation using symmetric stable random
projections. However, even after aggressively exploiting the bias-variance tradeoff, they still need
to a large number of samples, e.g., 106 , which is prohibitive in both time and space, especially
considering that in streaming applications the pseudo-random numbers have to be re-generated on
the fly, the cost of which is directly proportional to the sample size.
In our approach, we approximate the Shannon entropy using two high correlated estimates of the
frequent comments. The positive correlation can substantially reduce the variance of the Shannon
entropy estimate. However, finding the appropriate estimator of the frequency moment is another
challenging task. We successfully find such an estimator and show that its variance (of the Shannon
entropy estimate) is very small. Experimental results demonstrate that about 100 ? 1000 samples
should be sufficient for achieving high accuracies.
7

Acknowledgement
The research of Ping Li is partially supported by NSF-IIS-1249316, NSF-DMS-0808864, NSF-SES1131848, and ONR-YIP-N000140910911. The research of Cun-Hui Zhang is partially supported
by NSF-DMS-0906420, NSF-DMS-1106753, NSF-DMS-1209014, and NSA-H98230-11-1-0205.

Normalized MSE

2

100

10

0

10

k = 1000
?2

10

?4

10

?3

?2

10
10
?=??1

?1

10

0

2

k = 10

0

k = 100

10
10

k = 1000
?5

10

10

Normalized MSE

100

2

10

0

10

k = 1000

?5

?4

10

?3

?2

10
10
?=??1

?1

10

0

10

10

k = 100

0

k = 1000
?4

?3

10

?2

10
10
?=??1

?1

10

0

10

6

0

10

k = 1000

2

10

k = 10

0

10

k = 100

10

FOOD?LOVE : GM + Indep.
?5

10

?4

10

?3

?2

10
10
?=??1

?1

10

0

10

?4

10

?3

?2

10
10
?=??1

?1

10

0

10

6

Normalized MSE

k = 10

10

100

2

10

0

10

k = 1000
?2

10

?5

?4

10

?3

?2

10
10
?=??1

?1

10

DATA?PAPER : GM + Corr.

k = 10

2

10

0

k = 100

?2

k = 1000

10

0

?5

10

10

?4

10

?3

?2

10
10
?=??1

?1

10

6

10

Normalized MSE

100

2

10

0

10

k = 1000

?2

10

?4

10

?3

?2

10
10
?=??1

2

k = 10

10

0

k = 100

10

?2

10

NEWS?WASHINGTON : GM + Indep.
?5

10

?1

10

?4

10

?3

?2

10
10
?=??1

?1

10

6

10

0

10

k = 1000
?2

k = 10

2

10

k = 100

0

10

?2

10

?4

10

?3

?2

10
10
?=??1

?1

10

k = 1000

10

MACHINE?LEARN : GM + Indep.
?5

k = 100
?2

10

k = 1000
?3

10

0

10

DATA?PAPER : Corr.? = 0.3

?5

10

?4

10

?3

?2

10
10
?=??1

?1

10

0

10

Normalized MSE
Normalized MSE
10

?4

10

?3

?2

10
10
?=??1

?1

10

10

k = 10
?2

10

k = 100
k = 1000

?3

10

k = 100

?2

10

k = 1000

?3

10

NEWS?WASHINGTON : Corr. ? = 0.3
?4

10

?3

?2

10
10
?=??1

?1

10

?3

?2

10
10
?=??1

?1

10

0

10

k = 10
k = 100

?2

10

k = 1000

?3

10

k = 100

10

k = 1000

10

?4 MACHINE?LEARN : Corr. ? = 0.3
10 ?5
?4
?3
?2
?1
0
10
10
10
10
10
10
?=??1

?3

?2

10
10
?=??1

?1

10

0

10

A?THE : Corr. ? = 0.7

?1

k = 10
k = 100

10

?2

k = 1000

10

?3

10

?4

10

?3

?2

10
10
?=??1

?1

10

0

10

FOOD?LOVE : Corr. ? = 0.7

?1

k = 10
k = 100

?2

k = 1000

10
10

?3

10

?3

?2

10
10
?=??1

?1

10

10 ?5
10

0

10

?4

10

?3

?2

10
10
?=??1

?1

10

0

10

0

10

DATA?PAPER : Corr.? = 0.5

?1

k = 10

?2

k = 100

10
10

k = 1000

?3

10

?4

10

?3

?2

10
10
?=??1

?1

10

?1

k = 10
k = 100

?2

k = 1000

10
10

?3

10

DATA?PAPER : Corr.? = 0.7
?4
10 ?5
?4
?3
?2
?1
10
10
10
10
10
?=??1

0

10

0

10

0

10

NEWS?WASHINGTON : Corr. ? = 0.5

?1

10

k = 10
k = 100

?2

10

k = 1000
?3

10

10

?4

10

?4

?4

10

?1

?3

?2

10
10
?=??1

?1

10

k = 10
k = 100

10

?2

10

k = 1000

?3

10

?4

?4

10

NEWS?WASHINGTON : Corr. ? = 0.7

10 ?5
10

0

10

0

?1

10

10

?4

10

?3

?4

?4

10

FOOD?LOVE : Corr. ? = 0.5

?1

10 ?5
10

0

10

0

10

0

10

10
k = 10

?1

10

k = 1000

?2

10 ?5
10

0

?1

?2

UNITED?STATES : Corr.? = 0.7
k = 10
k = 100

10

10

A?THE : Corr. ? = 0.5

10 ?5
10

0

10

?1

10

10 ?5
10

0

?4

10

?3

10
10
?=??1

?1

0

?1

?2

?2

?1

k = 10

10

10

?3

?4

10

?3

10
10
?=??1

?4

?4

10

10

?4

10

0

10 ?5
10

0

10

10

4

Normalized MSE

Normalized MSE

100
2

k = 1000

?3

10

MACHINE?LEARN : GM + Corr.

10

10

k = 100
?2

10

0

10
k = 10

4

10

10

0

10 ?5
10

0

10

k = 1000

?3

10

?1

k = 1000
?5

10

6

0

10

10

?4

0

10

?1

k = 1000

?3

10

10

?4

10

k = 10
k = 100

?2

10

10 ?5
10

0

10

k = 100

?2

10 ?5
10

10

NEWS?WASHINGTON : GM + Corr.

4

10

?1

10

?4

0

10
k = 10

4

k = 1000

?3

10

10 ?5
10

0

10

Normalized MSE

6

10

k = 100
?2

10

?4

?2

k = 10

10

10

?1

DO?REVIEW : Corr. ? = 0.7

?1

10

0

10

k = 10

4

?3

10
10
?=??1

0

10

0

10

?4

?4

10

UNITED?STATES : Corr.? = 0.5

?1

10 ?5
10

10

10

10

DATA?PAPER : GM + Indep.

10

10

0

10

Normalized MSE

6

10

4

k = 1000
?3

?4 FOOD?LOVE : Corr. ? = 0.3
10 ?5
?4
?3
?2
?1
10
10
10
10
10
?=??1

k = 1000
?5

10

?3

10

k = 10

4

10

?2

?2

10

k = 100

?2

10

0

Normalized MSE

Normalized MSE

2

10

?2

0

10

k = 1000

?3

10

10

k = 10
k = 100
k = 1000

10

10

k = 10
k = 100

?2

10

THIS?HAVE : Corr. ? = 0.7
?4
10 ?5
?4
?3
?2
?1
10
10
10
10
10
?=??1

0

10

DO?REVIEW : Corr. ? = 0.6

?1

10 ?5
10

0

10

k = 10

?1

FOOD?LOVE : GM + Corr.

100

?1

10

10

k = 10
4

?2

?4 A?THE : Corr. ? = 0.3
10 ?5
?4
?3
?2
10
10
10
10
?=??1

10

10

?3

10
10
?=??1

?1

10

0

10

k = 10

10

?1

10

?4

?4

10

10

k = 10

2

10

10

10

k = 1000
?3

10

?4 UNITED?STATES : Corr.? = 0.3
10 ?5
?4
?3
?2
?1
0
10
10
10
10
10
10
?=??1

0

A?THE : GM + Corr.

?5

10

6

Normalized MSE

10
10
?=??1

?1

4

10

A?THE : GM + Indep.

10

Normalized MSE

10

?2

10

?2

10

k = 100

?2

10

0

10
k = 10

4

10

?2

Normalized MSE

?3

6

10

Normalized MSE

?4

Normalized MSE

6

k = 10

0

10

UNITED?STATES : GM + Indep.
?5

DO?REVIEW : Corr. ? = 0.5

?1

10

10

UNITED?STATES : GM + Corr.

4

10

?2

10

10

?4

6

10
k = 10

4

10

?2

0

10 ?5
10

0

10

?3

10
10
?=??1

Normalized MSE

k = 1000

?4

10

Normalized MSE

?3

10

Normalized MSE

6

10

10 ?5
10

0

10

Normalized MSE

k = 100

k = 1000

?3

10

Normalized MSE

?2

10

DO?REVIEW : Corr. ? = 0.4
?4
10 ?5
?4
?3
?2
?1
10
10
10
10
10
?=??1

0

10

?1

10

Normalized MSE

k = 10

k = 10
k = 100

?2

10

Normalized MSE

DO?REVIEW : Corr. ? = 0.3
?4
10 ?5
?4
?3
?2
?1
10
10
10
10
10
?=??1

?2

Normalized MSE

k = 1000

?3

10

?3

10
10
?=??1

Normalized MSE

k = 100

?2

10

?1

10

10

?4

?4

10

0

10
Normalized MSE

Normalized MSE

k = 10

k = 1000
?3

10

?4

0

?1

k = 100

10 ?5
10

0

10

10

10

k = 10

?2

10

0

10

THIS?HAVE : Corr. ? = 0.6

?1

?4

10

?3

?2

10
10
?=??1

?1

10

0

10

0

10

MACHINE?LEARN : Corr. ? = 0.5

?1

10

?2

Normalized MSE

k = 1000

10

Normalized MSE

0

Normalized MSE

?3

10

THIS?HAVE : Corr. ? = 0.4
?4
10 ?5
?4
?3
?2
?1
10
10
10
10
10
?=??1

0

10

10

Normalized MSE

k = 100

10

Normalized MSE

k = 1000

?3

10

?2

10

0

10

THIS?HAVE : Corr. ? = 0.5

?1

Normalized MSE

k = 100

?2

10

k = 10

Normalized MSE

Normalized MSE

Normalized MSE

10

THIS?HAVE : Corr. ? = 0.3
?4
10 ?5
?4
?3
?2
?1
10
10
10
10
10
?=??1

Normalized MSE

0

10

?1

Normalized MSE

0

10
k = 10

?1

Normalized MSE

0

10

k = 10
k = 100

10

k = 1000
?3

10

?4

10 ?5
10

?4

10

?3

?2

10
10
?=??1

?1

10

0

10

?1

k = 10
k = 100

?2

k = 1000

10
10

?3

10

MACHINE?LEARN : Corr. ? = 0.7
?4
10 ?5
?4
?3
?2
?1
0
10
10
10
10
10
10
?=??1

Figure 3: The first two rows are the normalized MSEs for same two vectors used in Figure 2, for
estimating Shannon entropy using the general estimator T??,? with ? = 0.3, 0.4, 0.5, 0.6, 0.7. For
the rest of the rows, the leftmost panels are the results of using independent samples (i.e., the prior
work [21]) and the geometric mean estimator. The second column of panels are the results of using
correlated samples and the geometric mean estimator. The right three columns of panels are for the
proposed general estimator T??,? with ? = 0.3, 0.5, 0.7. We recommend ? = 0.5.
8

References
[1] Brian Babcock, Shivnath Babu, Mayur Datar, Rajeev Motwani, and Jennifer Widom. Models and issues
in data stream systems. In PODS, pages 1?16, Madison, WI, 2002.
[2] Daniela Brauckhoff, Bernhard Tellenbach, Arno Wagner, Martin May, and Anukool Lakhina. Impact of
packet sampling on anomaly detection metrics. In IMC, pages 159?164, Rio de Janeriro, Brazil, 2006.
[3] John M. Chambers, C. L. Mallows, and B. W. Stuck. A method for simulating stable random variables.
Journal of the American Statistical Association, 71(354):340?344, 1976.
[4] Laura Feinstein, Dan Schnackenberg, Ravindra Balupari, and Darrell Kindred. Statistical approaches to
DDoS attack detection and response. In DARPA Information Survivability Conference and Exposition,
pages 303?314, 2003.
[5] Anupam Gupta, John D. Lafferty, Han Liu, Larry A. Wasserman, and Min Xu. Forest density estimation.
In COLT, pages 394?406, Haifa, Israel, 2010.
[6] Nicholas J. A. Harvey, Jelani Nelson, and Krzysztof Onak. Streaming algorithms for estimating entropy.
In ITW, 2008.
[7] Piotr Indyk. Stable distributions, pseudorandom generators, embeddings, and data stream computation.
Journal of ACM, 53(3):307?323, 2006.
[8] Anukool Lakhina, Mark Crovella, and Christophe Diot. Mining anomalies using traffic feature distributions. In SIGCOMM, pages 217?228, Philadelphia, PA, 2005.
[9] Ashwin Lall, Vyas Sekar, Mitsunori Ogihara, Jun Xu, and Hui Zhang. Data streaming algorithms for
estimating entropy of network traffic. In SIGMETRICS, pages 145?156, Saint Malo, France, 2006.
[10] Ping Li. Estimators and tail bounds for dimension reduction in l? (0 < ? ? 2) using stable random
projections. In SODA, pages 10 ? 19, San Francisco, CA, 2008.
[11] Ping Li. Compressed counting. In SODA, New York, NY, 2009.
[12] Ping Li and Trevor J. Hastie. A unified near-optimal estimator for dimension reduction in l? (0 < ? ? 2)
using stable random projections. In NIPS, Vancouver, BC, Canada, 2007.
[13] Ping Li and Cun-Hui Zhang. A new algorithm for compressed counting with applications in shannon
entropy estimation in dynamic data. In COLT, 2011.
[14] Qiaozhu Mei and Kenneth Church. Entropy of search logs: How hard is search? with personalization?
with backoff? In WSDM, pages 45 ? 54, Palo Alto, CA, 2008.
[15] S. Muthukrishnan. Data streams: Algorithms and applications. Foundations and Trends in Theoretical
Computer Science, 1:117?236, 2 2005.
[16] Noam Nisan. Pseudorandom generators for space-bounded computations. In Proceedings of the twentysecond annual ACM symposium on Theory of computing, STOC, pages 204?212, 1990.
[17] Liam Paninski. Estimation of entropy and mutual information. Neural Comput., 15(6):1191?1253, 2003.
[18] Constantino Tsallis. Possible generalization of boltzmann-gibbs statistics. Journal of Statistical Physics,
52:479?487, 1988.
[19] Kuai Xu, Zhi-Li Zhang, and Supratik Bhattacharyya. Profiling internet backbone traffic: behavior models
and applications. In SIGCOMM ?05: Proceedings of the 2005 conference on Applications, technologies,
architectures, and protocols for computer communications, pages 169?180, Philadelphia, Pennsylvania,
USA, 2005.
[20] Qiang Yang and Xindong Wu. 10 challeng problems in data mining research. International Journal of
Information Technology and Decision Making, 5(4):597?604, 2006.
[21] Haiquan Zhao, Ashwin Lall, Mitsunori Ogihara, Oliver Spatscheck, Jia Wang, and Jun Xu. A data streaming algorithm for estimating entropies of od flows. In IMC, San Diego, CA, 2007.
[22] Vladimir M. Zolotarev. One-dimensional Stable Distributions. American Mathematical Society, Providence, RI, 1986.

9

"
2001,Activity Driven Adaptive Stochastic Resonance,,2112-activity-driven-adaptive-stochastic-resonance.pdf,Abstract Missing,"Activity Driven Adaptive Stochastic
Resonance

Gregor Wenning and Klaus Oberrnayer

Department of Electrical Engineering and Computer Science
Technical University of Berlin
Franklinstr. 28/29 , 10587 Berlin
{grewe , oby}@cs.tu-berlin.de

Abstract
Cortical neurons might be considered as threshold elements integrating in parallel many excitatory and inhibitory inputs. Due to
the apparent variability of cortical spike trains this yields a strongly
fluctuating membrane potential, such that threshold crossings are
highly irregular. Here we study how a neuron could maximize its
sensitivity w.r.t. a relatively small subset of excitatory input. Weak
signals embedded in fluctuations is the natural realm of stochastic
resonance. The neuron's response is described in a hazard-function
approximation applied to an Ornstein-Uhlenbeck process. We analytically derive an optimality criterium and give a learning rule
for the adjustment of the membrane fluctuations, such that the
sensitivity is maximal exploiting stochastic resonance. We show
that adaptation depends only on quantities that could easily be
estimated locally (in space and time) by the neuron. The main
results are compared with simulations of a biophysically more realistic neuron model.

1

Introduction

Energetical considerations [1] and measurements [2] suggest , that sub-threshold
inputs, i.e. inputs which on their own are not capable of driving a neuron , play an
important role in information processing. This implies that measures must be taken,
such that the relevant information which is contained in the inputs is amplified in
order to be transmitted. One way to increase the sensitivity of a threshold device is
the addition of noise. This phenomenon is called stochastic resonance (see [3] for a
review) , and has already been investigated and experimentally demonstrated in the
context of neural systems (e.g. [3, 4]). The optimal noise level, however , depends
on the distribution of the input signals, hence neurons must adapt their internal
noise levels when the statistics of the input is changing. Here we derive and explore
an activity depend ent learning rule which is intuitive and which only depends on
quantities (input and output rates) which a neuron could - in principle - estimate.
The paper is structured as follows. In section 2 we describe the neuron model and we
introduce the m embrane potential dynamics in its hazard function approximation.

In section 3 we characterize stochastic resonance in this model system and we calculate the optimal noise level as a function of t he input and output rates. In section 4
we introduce an activity dependent learning rule for optimally adjusting the internal noise level, demonstrate its usefulness by applying it to t he Ornstein-Uhlenbeck
neuron and relate the phenomenon of stochastic resonance to its experimentally
accessible signature: the adaptation of the neuron 's transfer function . Section 5
contains a comparison to the results from a biophysically more realistic neuron
model. Section 6, finally, concludes with a brief discussion.

2

The abstract Neuron Model

Figure 1 a) shows the basic model setup. A leaky integrate-and-fire neuron receives
."",

a)

~;="".~

train with rate As

""0

>8 >
{5

-5""
'-<

O/l

b)

0.8

c

0.7

;::

0.6

.~

rateAo

8

0.9

""

0.5

E
~

0.4

""-'
0

0.2

.0

0.1

~

/'IWn

I

.S

2 N balanced Poisson
spike trains with rates
As

;::

0.

0.3

00

0.2

0.4

0.6

0.8

average membrane potential

Figure 1: a)The basic model setup. For explanation see text . b) A family of
Arrhenius type hazard functions for different noise levels. 1 corresponds to the
threshold and values below 1 are subthreshold .

e

a ""signal"" input , which we assume to be a Poisson distributed spike train with a rate
As. The rate As is low enough , so that the membrane potential V of the neuron
remains sub-threshold and no output spikes are generated . For the following we
assume that the information the input and output of the neuron convey is coded by
its input and output rates As and Ao only. Sensitivity is then increased by adding
2N balanced excitatory and inhibitory ""noise"" inputs (N inputs each) with rates
An and Poisson distributed spikes . Balanced inputs [5, 6] were chosen , because they
do not affect t he average membrane potential and allow to separate the effect of
decreasing the distance of the neuron's operating point to the threshold potential
from the effect of increasing the variance of the noise. Signal and noise inputs
are coupled to t he neuron via synaptic weights Ws and Wn for the signal and noise
inputs . The threshold of the neuron is denoted bye. Without loss of generality the
membrane time-constant, the neuron 's resting potential, and the neuron 's threshold
are set to one, zero , and one , respectively.
If the total rate 2N An of incoming spikes on t he ""noise"" channel is large and the
individual coupling constants Wn are small , the dynamics of the m embrane potential
can b e approximated by an Ornstein-Uhlenbeck process,

dV

=-V

dt

+ J.l

dt

+

(J""

dW,

(1)

where drift J.l and variance (J"" are given by J.l = wsA s and (J""2 = w1A s + 2NwYvAN,
and where dW describes a Gaussian noise process with m ean zero and variance
one [8]. Spike initiation is included by inserting an absorbing boundary with reset.
Equation (1) can b e solved an alytically for special cases [8], but here we opt for

a more versatile approximation (cf. [7]). In this approximation, the probability of
crossing the threshold , which is proportional to the instantaneous output rate of
the neuron , is described by an effective transfer function. In [7] several transfer
functions were compared in their performance, from which we choose an Arrheniustype function ,

Ao(t) = c exp{ _ (e - ~(t))2},

(2)

cr

e-

where
x(t) is the distance in voltage between the noise free trajectory of the
membrane potential x(t) and the threshold x(t) is calculated from eq. (1) without
its diffusion term. Note that x(t) is a function of As, c is a constant. Figure 1 b)
shows a family of Arrhenius type transfer functions for different noise levels cr.

3

e,

Stochastic Resonance in an Ornstein- Uhlenbeck Neuron

Several measures can be used to quantify the impact of noise on the quality of signal
transmission through threshold devices . A natural choice is the mutual information
[9] between the distributions p( As) and p( Ao) of input and output rates, which we
will discuss in section 4, see also figure 3f. In order to keep the analysis and the
derivation of the learning rule simple , however, we first consider a scenario, in which
a neuron should distinguish between two sub-threshold input rates As and As + ~s.
Optimal distinguishability is achieved if the difference ~o of the corresponding
output rates is maximal, i.e. if
~o =

/(As

+ ~ s) - /(As)

(3)

= max ,

where / is the transfer function given by eq. (2). Obviously there is a close connection between these two measures , because increasing both of them leads to an
increase in the entropy of p( Ao) .
Fig. 2 shows plots of the difference
0.16

~o

of output rates vs. the level of noise, cr , for
0.4

~s

0.14

~s

0.35

AS= 7

0.12

:5

0
<:::]

:5
0

0.1

<:::]

0.08
0.06
0.04
0.02

0.05

00

50

0 2

100

[per cent]

50

100

[per cent]

Figure 2: ~ o vs. cr 2 for two different base rates As = 2 (left) and 7 (right) and 10
different values of ~ s = 0.01 , 0.02 , ... , 0.1. cr 2 is given in per cent of the maximum
cr 2 = 2N W;An. The arrows above t he x-axis indicate the position of the maximum
according to eq. (3), the arrowh eads below the x-axis indicate the optimal value
computed using eq. (5) (67% and 25%). Parameters were: N = la , An = 7,
Ws = 0.1 , and Wn E [0, 0.1].
different rates As and different values of ~ s . All curves show a clear maximum at a

particular noise level. The optimal noise level increases wit h decreasing t he input
rate As, but is roughly independent of the difference ~ s as long as ~ s is small.
Therefore, one optimal noise level holds even if a neuron has to distinguish several
sub-threshold input rates - as long as these rates are clustered around a given base
rate As.
The optimal noise level for constant As (stationary states) is given by the condition
d

d(j2 (f(A s + ~ s) - f(As)) = 0 ,

(4)

where f is given by eq. (2). Equation (4) can be evaluated in the limit of small
values of ~ s using a Taylor expansion up to the second order. We obtain

(j;pt

=

2(1 - ws As)2

(5)

if the main part of the variance of the membrane potential is a result of the balanced
.
'"" 2N WNAN
2 , (f
2 -- - (1log(Ao/C)
- W, A, )2 , eq. (2) , eq. (5)
mput
, l.""
e.f
1 (j 2 '""
c . eq. (1)) . S'mce (jopt
is equivalent to 1 + 2 log( Ao (A; ;0""2)) = O. This shows that the optimal noise level
depends either only on As or on Ao(As; (j2), both are quantities which are locally
available at the cell.

4

Adaptive Stochastic Resonance

We now consider the case , that a neuron needs to adapt its internal noise level
because the base input rate As changes. A simple learning rule which converges to
the optimal noise level is given by
~(j2

=

- f

(j2
log( - 2-) ,
(j opt

(6)

where the learning parameter f determines the time-scale of adaptation . Inserting
the corresponding expressions for the actual and the optimal variance we obtain a
learning rule for the weights W n ,
~wn =

-f

I og ( ( 2NAnw; )2 ) .
2 1 - ws As

(7)

Note, t hat equivalent learning rules (in the sense of eq. (6)) can be formulat ed for
the number N of the noise inputs and for their rates An as well. The r.h. s. of eqs .
(6) and (7) depend only on quantities which are locally available at the neuron.
Fig. 3ab shows the stochastic adaptation of the noise level, using eq.
randomly distributed As which are clustered around a base rate.

(7) , to

Fig. 3c-f shows an application ofthe learning rule, eq. (7) to an Ornstein-Uhlenbeck
neuron whose noise level needs to adapt to three different base input rates. T he
figure shows t he base input rate As (Fig. 3a). In fig. 3b the adaptation of Wn
according to eq. (7) is shown (solid line), for comparison t he Wn which maximizes
eq. (3) is also displayed (dash ed dotted lin e). Mutual information was calculated
between a distribution of randomly chosen input rates which are clustered around
the base rate As. The Wn that maximizes mutual Information between input and
output rates is displayed in fig. 3d (dashed lin e). Fig. 3e shows the ratio ~ o / ~ s
computed by using eq. (3) and the Wn calculated with eq. (8) (dashed dotted line)
and the same ratio for the quadratic approximation. Fig. 3f shows the mutual
information between the input and output rates as a function of the changing w n .

I[n~[ :~--/
0
0

2500

3000

1500

2000

2500

3000

0~
500 1000 1500
0

2000

1000

1500

~I
I,rJ

2000

500

?

0.1 5

w

110

b)

n 0.1

I1S

0

0.05
00
10

AS

o:i,ek '

500

1000

1500

2000

2500

3000

W

O'h

n 0.1

a)

500

I
1000

d)

..
?

5

00

As
500

1000

1500

2000

2500

3000

':1C)
0
0

I
500

1000

?

1500

ri""
I I
2500

3000

2000 2500 3000

?

time[updatesteps1

time [update steps]

Figure 3: a) Input rates As are evenly distributed around a base rate with width
0.5, in each time step one As is presented . b) Application of the learning rule eq.
(7) to t he rates shown in a). Adaptation of the noise level to t hree different input
base rates As. c ) The three base rates As. d) Wn as a function of time according
to eq. (7) (solid line) , the optimal Wn that maximizes eq. (3) (dashed dotted line)
and the optimal Wn that maximizes the m ut ual information between t he input and
output rates (dashed). T he opt imal values of Wn as the quadratic approximation,
eq. (5) yield are indicated by the black arrows. e ) The ratio b.. o / b.. s computed
from eq. (3) (dashed dotted line) and t he quadratic approximation (solid line) . f)
Mut ual information between input and output rates as a function of base rate and
changing synaptic coupling constant W n . For calculating the mutual information
the input rates were chosen randomly from the interval [As - 0.25 , As + 0.25] in each
time step . Parameters as in fig . 2.

T he fig ure shows , that the learning rule, eq. (7) in t he quadratic approximation
leads to values for () which are near-optimal, and that optimizing the difference of
output rates leads to results similar to t he optimization of the m ut ual information .

5

Conductance based Model Neuron

To check if and how t he results from the abstract model carryover to a biophysically
mode realistic one we explore a modified Hodgkin-Huxley point neuron with an
additional A-Current (a slow potassium current) as in [11] . T he dynamics of the
membrane potential V is described by t he following equation

C~~

- gL(V(t ) - EL) -

!iNam~ h(t)(V -

ENa)

- !iKn(t)4(V - EK) - !iAa ~ b(t)(V - EK)
+ l syn

+ la pp,

(8)

the parameters can be found in the appendix. All parameters are kept fixed through-

a
a)

II~
""}

'N

?::l

10

c

tS

80 ,---------------------------------,
b)

70

60
50

::I

~

o

~ 5
~

peak
conductances

8

40

~
.~

30

.5

20

<t>

U

00'----- 0 ,""""
2 =:::...0-.4""""""""'-=--,L---~--,~
,2---""1.4

~

~
'i3

10
00

2

4

6

B

10

noiselevel in multiples of peak conductances

Figure 4: a) Transfer function for the conductance based model neuron with additional balanced input , a = 1, 2, 3, 4 b ) Demonstration of SR for the conductance
based model neuron. The plot shows the resonance for two different base currents
lapp = 0.7 and lapp = 0.2 and a E [0, 10].

~

-I
~

&
'0

-~
E

90 ,------ - - - - - - - - -- -- - - - - - - - - - - ---,

7

a)

E3 -

-B
~
~

5

--&.,

4-

b

:2

.j

1

70 -

~ 60 -

~

3

=

0)

gp ao -

50 40 -

1G

_~30 ~

20 -

-~

10 -

:;::

0 0~----------~~------~~~1
0.5
I drift [n~]

?0~----------0
~
.5
~--~------~
s""U-othresl""""1<:>ld p<:>te:n.t:ial ( 8 =

1

)

Figure 5: a) Optimal noise-level as a function ofthe base current in the conductance
based model. b) Optimal noise-level as a function of the noise-free membrane
potential in the abstract model.

out all shown data. As balanced input we choose an excitatory Poisson spike train
with rate Ane = 1750 Hz and an inhibitory spike train with rate Ani = 750 Hz .
These spike trains are coupled to the neuron via synapses resulting in a synaptic
current as in [12]

ls yn = ge(V(t) - E e) + gi(V(t) - Ei)).

(9)

Every time a spike arrives at the synapse the conductance is increased by its peak
conductance ge i and decreases afterwards exponentially like exp{ - _t_
, } . The corT e, t
I

responding parameters are ge = a * 0.02 * gL , gi = a * 0.0615 * gL. The common
factor a is varied in the simulations and adjusts the height of the peak conductances,
gL is the leak conductance given above . Excitatory and inhibitory input are called
balanced if the impact of a spike-train at threshold is the same for excitation and
inhibition

TegeAne(Ee - B)
with Te i
I

= - TigiAni(Ei -

B)

= ~ J!fooo ge,i(t)dt . Note that the factor a does cancel in eq .
ge,t

(10)
(10).

Fig. 4a displays transfer funct ions in the conductance b ased setting with balanced
input. A family of functions with varying p eak conduct ances for the balanced input
is drawn .

~

100 r-----~----~----~----~----~-----.

d)

rJJ

~
~

50
?OL'~--~--~----~--~~--~--~

50

'-J

100

150

200

250

300

,-...,

i':k""-:-..:~. :
'""

0

50

1~-:o

50

100

150

200

250

300

100

150

200

250

300

Figure 6: Adaptive SR in the conductance based model. a) Currents drawn from
a uniform distribution of width 0.2 nA centered around base currents of 3, 8, 1 nA
respectively. b) Optimal noise-level in terms of a. Optimality refers to a semi-linear
fit to the data of fig. 5a. c) adapting the peak conductances using a in a learning
rule like eg. (8). d) Difference in spike count , for base currents I ? 0.1 nA and
using a as specified in c) .

For studying SR in t he conductance based framework , we apply the same paradigm
as in the abstract model. Given a certain average membrane potential, which is
adjusted via injecting a current I (in nA), we calculate the difference in the output
rate given a certain difference in the average membrane potential (mediated via the
injected current) I ? t:.I. A demonstration of stochastic resonance in the conductance based neuron can be seen in fig. 4b. In fig. 5a the optimal noise-level, in
terms of multiples a of the peak conductances , is plotted versus all currents that
yield a sub-threshold membrane voltage. For comparison we give the corresponding
relationship for the abstract model in fig. 5b .
Fig. 6 shows the performance of the conductance based model using a learning
rule like eg. (7). Since we do not have an analytically derived expression for (J opt
in the conductance based case, the relation (Jopt (I) , necessary for using eg. (7),
corresponds to a semi-linear fit to the (a opt , I) relation in fig. 5a.

6

Conclusion and future directions

In our contribution we have shown , that a simple and activity driven learning rule
can be given for the adaptation of the optimal noise level in a stochastic resonance
setting. Th e results from the abstract framework are compared with results from a
conductance based model neuron. A biological plausible m echanism for implem enting adaptive stochastic resonance in conductance based neurons is currently under
investigation.

Acknowledgments
Supported by: Wellcome Trust (061113/Z/00)
App e ndix: Parame t ers for the conductance based mode l n e uron

=

:':2

=

somatic conductances/ion-channel properties: em
1.0
,gL 0.05 ~ ,gNa
100 ~,gJ( = 40 ~,gA = 20 ~,EL = -65 mV,ENa = 55 mV, EJ(

- 80 mV, TA

=

= 20 ms ,

synaptic coupling : Ee = 0 mV, Ei = -80 mV, Te = 5 ms , Ti = 10 ms ,
spike initiation: dh = ~ dn = ngo - n <jQ = ~
mCO = <>m<>-ti3m'

dt

Th

O:m

= -O.l(V

55)/18),
h oo = <>h"":i3h'
n co = <>n+i3 n '

O:h
O:n

'

dt

Tn'

dt

TA

'

+ 30)/(exp( -O .l(V + 30)) -

1), f3m = 4exp( -(V

+

= 0.07exp(-(V + 44)/20), f3h = l/( exp(-O. l(V + 14)) + 1) ,
= -O.Ol(V + 34)/(exp( -O.l(V + 34)) -1) , f3n = 0.125exp( -(V +

44)/80)
a oo = l/(exp( -(V + 50)/20) + 1) , boo = l/(exp((V
Th = in/(O:h + f3h) , Tn = in/(O:n + f3n), in = 0.1

+ 80)/6) + 1),

References
[1] S. B. Laughlin, RR de Ruyter van Steveninck and J.C. Anderson, The metabolic
cost of neural information, Nature Neuroscience, 1(1) , 1998, p.36-41
[2] J. S. Anderson, I. Lampl, D. C. Gillespie and D. Ferster, The Contribution of Noise to
Contrast Invariance of Orientation Tuning in Cat Visua l Cortex, Science, 290 , 2000,
p.1968-1972
[3] L. Gammaitoni, P. Hanggi, P. Jung and F. Marchesoni, Stochastic Resonance R eviews
of modern Physics, 70(1) , 1998, p.223-287
[4] D . F. Russel, L. A. Wilkens and F. Moss, Use of behavioral stochastic resonance by
paddle fish for feeding, Nature, 402 , 1999, p.291-294
[5] M. N . Shadlen, and W. T. Newsome, The Variable Discharge of Cortical Neurons:
Implications for Connec tivity, Computation, and Information Coding, The Journal of
Neuroscience, 18 (10), 1998, p.3870-3896
[6] M.V. Tsodyks and T. Sejnowski, Rapid state switching in balanced cortical network
models, Network: Computation in Neural Systems, 6 , 1995, p.I11-124
[7] H. E. Plesser and W. Gerstner, Noise in Integrate-and-Fire Neurons: From Stochastic
Input to Escape Rates, Neural Computation, 12 , 2000, p.367-384
[8] H. C. Tuckwell, Introduction to theoretical neurobiology: volume 2 nonlinear and
stochastic theories, Cambridge University Press, 1998
[9] T. M. Cover and J. A. Thomas, Elements of Information Theory, Wiley Series in
Telecommunications, 1991, 2nd edition
[10] A.R Bulsara, and A. Zador, Threshold detection of wide band signals: a noise-induced
maximum in the mutual information., PRE, 54(3), 1996, R2185-2188
[11] O. Shriki , D. Hansel and H. Sompolinsky, Modeling neuronal networks in cortex by rate
models using th e current-frequency respons e properties of cortical cells, Soc. Neurosci.
Abstr., 24 , p.143, 1998
[12] E. Salinas and T .J. Sejnowski, Impa ct of Co rrelated Synaptic Input on Output Firing
Rate and Variability in Simple Neurona l Mode ls J. Neurosci. 20 , 2000, p.6193-6209

"
1988,A Low-Power CMOS Circuit Which Emulates Temporal Electrical Properties of Neurons,,172-a-low-power-cmos-circuit-which-emulates-temporal-electrical-properties-of-neurons.pdf,Abstract Missing,"678

ALOW-POWER CMOS CIRCUIT WHICH EMULATES
TEMWORALELECTIDCALPROPERTIES OF NEURONS
Jack L. Meador and Clint S. Cole
Electrical and Computer Engineering Dept.
Washington State University
Pullman WA. 99164-2752

ABSTRACf
This paper describes a CMOS artificial neuron. The circuit is

directly derived from the voltage-gated channel model of neural
membrane, has low power dissipation, and small layout geometry.
The principal motivations behind this work include a desire for high
performance, more accurate neuron emulation, and the need for
higher density in practical neural network implementations.

INTRODUCTION
Popular neuron models are based upon some statistical measure of known natural
behavior. Whether that measure is expressed in terms of average firing rate or a
firing probability, the instantaneous neuron activation is only represented in an
abstract sense. Artificial electronic neurons derived from these models represent this
excitation level as a binary code or a continuous voltage at the output of a summing
amplifier. While such models have been shown to perform well for many applications, and form an integral part of much current work, they only partially emulate the
manner in which natural neural networks operate. They ignore, for example,
differences in relative arrival times of neighboring action potentials -- an important
characteristic known to exist in natural auditory and visual networks {Sejnowski,
1986}. They are also less adaptable to fme-grained, neuron-centered learning, like
the post-tetanic facilitation observed in natural neurons. We are investigating the
implementation and application of neuron circuits which better approximate natural
neuron function.

BACKGROUND

The major temporal artifacts associated with natural neuron function include the
spacio-temporal integration of synaptic activity, the generation of an action potential
(AP), and the post-AP hyperpolarization (refractory) period (Figure 1). Integration,
manifested as a gradual membrane depolarization, occurs when the neuron accumulates sodium ions which migrate through pores in its cellular membrane. The rate of
ion migration is related to the level of presynaptic AP bombardment, and is also
known to be a non-linear function of transmembrane potential. Efferent AP generation occurs when the voltage-sensitive membrane of the axosomal hillock reaches
some threshold potential whereupon a rapid increase in sodium permeability leads to

A Low-Power CMOS Circuit Which Emulates Neurons

complete depolarization. Immediately thereafter, sodium pores ""close"" simultaneously
with increased potassium permeability, thereby repolarizing the membrane toward its
resting potential. The high potassium permeability during AP generation leads to the
transient post-AP hyperpolarization state known as the refractory period.
v

Actl vat Ion
Threshold
( 1)

( 3)

Figure 1. Temporal artifacts associated with neuron function. (1) gradual
depolarization, (2) AP generation, (3) refractory period.
Several analytic and electronic neural models have been proposed which embody
these characteristics at varying levels of detail. These neuromimes have been used to
good advantage in studying neuron behavior. However, with the advent of artificial
neural networks (ANN) for computing, emphasis has switched from modeling neurons for physiologic studies to developing practical neural network implementations.
As the desire for high performance ANNs grows, models amenable to hardware
implementation become more attractive.
The general idea behind electronic neuromimes is not new. Beginning in 1937 with
work by Harmon {Harmon, 1937},lIectronic circuits have been used to model and
study neuronal behavior. In the late 196(Ys, Lewis {Lewis, 1968} developed a circuit
which simulated the Hodgkin-Huxley model for a single neuron, followed by
MacQregor's circuit {MacGregor, 1973} in the early 1970's which modelled a group
of 50 neurons. With the availability of VLSI in the 1980's, electronic neural implementations have largely moved to the realm of integrated circuits. Two different strategies have been documented: analog implementations employing operational
amplifiers {Graf, et at, 1987,1988; Sivilotti, et at, 1986; Raffel, 1988; Schwartz, et al,
1988}; and digital implementations such as systolic arrays {Kung, 1988}.
More recently, impulse neural implementations are receiving increased attention.
like other models, these neuromimes generate outputs based on some non-linear
function of the weighted net inputs. However, interneuron communication is realized
through impulse streams rather than continuous voltages or binary numbers {Murray,
1988; N. El-Leithy, 1987}. Impulse networks communicate neuron activation as variable pulse repetition rates. The impulse neuron circuits which shall be discussed offer
both small geometry and low power dissipation as well as a closer approximation to
natural neuron function.

679

680

Meador and Cole

-

A CMOS IMPULSE NEURON
An impulse neuron circuit developed for use in CMOS neural networks is shown in
Figure 2. In this circuit, membrane ion current is modeled by charge flowing to and
from Ca. Potassium and sodium influx is represented by current flow from Vdd to the
capacitor, and ion efflux by flow from the capacitor to ground. The Field EffectTransistors (FETs) connected between Vdd, Vsr , and the capacitor emulate voltageand chemically-gated ion channels found in natural neural membrane. In the Figure,
PET 1 corresponds to the post-synaptic chemicaIly-gated ion channels associated with
one synapse. PETs 2, 3, and 4 emulate the voltage-gated channels distributed
throughout a neuron membrane. The following equations summarize circuit operation:

Ca dVa/dt=/31E (Vr,Va)+/3:uF(Va)-/34G (Va)

(1)

E(Vr,Va) = (Vr-Va-V"",)(Vdd-Va)-(Vdd-Va)2 /2

(2)

F(Y. ) ={(Vdd-Vtp) (Va-Vdd)-(Va-Vdd)2 /2 if g(t) ~O

(3)

0

a

otherwlSe

G(V)={(Vdd-V"",)Va-Va2/2 if h(t~)=O
0

a

otherwlSe

g(t) =h (t)(l-h (t-C))

(4)
(5)

0 if Va(t) > Vth;

1

h()Vtl<Va(t)<Vth and h (Va(t-e))=O
t - 1 if Va(t)<Vtl ;
Vtl<Va(t)<Vth and h (~(t-e))=l

(6)

Vdd
{52

Exci tator y
Synapse

10

{53
{51

+

Axon

Vs
+

Va

Ca

1

/5.

Figure 2. A CMOS impulse neuron with one excitatory synapse-PET.

A Low-Power CMOS Circuit Which Emulates Neurons

Equation (1) expresses how changes in Va (which emulates instantaneous neuron
excitation) depend upon the sum of three current components controlled by these
PETs. E, F, and G in equations (2) through (4) express PET drain-source currents as
functions terminal voltages. Equations (3) and (5) rely upon the assumption that PET
2 and PET 3 are implemented as a single dual-gate device where the transconductance f3n=fJ2f33/f.P2+/33). Non-saturated PET operation is assumed for these equations even though the PETs will momentarily pass through saturation at the onset of
conduction in the actual circuit.
The Schmitt trigger circuit establishes a nonlinear positive feedback path responsible
for action potential initiation. The upper threshold of the trigger (VIII) emulates the
natural neuron activation threshold while the lower threshold (Va) emulates the maximum hyperpolarization voltage. Equation (6) expresses the hysterisis present in the
Schmitt trigger transfer characteristic. When Vs reaches the upper Schmitt threshold,
PET 3 turns on, creating a current path from Vtid to Cs , and emulating the upswing of
a natural action potential spike. A moment later, PET 2 turns off, starting the action
potential downswing. Simultaneously, PET 4 turns on, begining the absolute refractory period where Cs is discharged toward the maximum hyperpolarization potential.
When that potential is reached, the Schmitt trigger turns off PET 4 and the impulse
firing cycle is complete.
The capacitor terminal voltage Va emulates all gross temporal artifacts associated
with membrane potential, including spacio-temporal integration, the action potential
spike, and a refractory period. The instantaneous net excitation to the neuron is
represented by the total current flowing into the summing node on the floating plate
of the capacitor. Charge packets are transferred from Vtid to the capacitor by the
excitatory synapse PET. Excitatory packet magnitude is dependent upon the transconductance PI. Inhibitory synapses (not shown) operate similarly, but instead
reduce capacitor voltage by drawing charge to Va. A buffered action potential signal
useful for driving many synapse PETs is available at the axon output.
The membrane potential components (E,F,and G) of the circuit equations describe
nonlinear relationships between post-synaptic excitation (E), membrane potential (F
and G), and membrane ion currents. The functional forms of these components are
equivalent to those found between terminal voltages and currents in non-saturated
PETs. It is notable that natural voltage-gated channels do not necessarily follow the
same current-voltage relationship of a PET. Even though more accurate models and
emulations of natural membrane conductance exist, it seems unlikely at this time that
they would help further improve neural network implementation. There is little doubt
that more complex circuitry would be required to better approximate the true nonlinear relationship found in the biochemistry of natural neural membrane. That need
conflicts directly with the goal of high-density integration.

IMPULSE NEURAL NE1WORKS

~

Organizing a collection of neuron circuits into a useful network confIguration requires
some weight specification method. Weight values can be either directly specified by
the designer or learned by the network. A method particularly suited for use with the
fIXed PET-synapses of the foregoing circuit is to fust learn weights using an ""off-line""

681

682

Meador and Cole

simulation, then translate the numerical results to physical FET transconductances.
To do this, the activation function of an impulse neuron is derived and used in a
modified back-propagation learning procedure.

IMPULSE NEURON ACfIVATION FUNCTION
Learning algorithms typically require some expression of the neuron activation function. Neuron activation can be expressed as a numerical value, a binary pattern, or a
circuit voltage. In an impulse neuron, activation is expressed in terms of firing rate.
The more frequently an impulse neuron circuit fues, the greater its activation.
Impulse neuron activation is a nonlinear function of the excitation imparted through
its synapse connections. An analytical expression of this nonlinear function can be
derived using a rectangular approximation of neuron impulse waveforms.
It is fust necessary to defme a unit-impulse as one impulse conducted by a synapse
FET having some pre-determined reference transconductance (f3~). In Figure 3, To
represents an invariant activation impulse width which is assumed to be identical for
all neurons. T 1 represents the variable time period required for the neuron to accumulate the equivalent of K unit-impulses input excitation prior to firing. It can be
assumed that net input comes from a single excitatory synapse with no other excitation. It shall also be assumed that impulses arrive at a constant rate, so

(7)

T 1 =K /W;jR;

where R/ is the firing rate of the source neuron and
connecting neuron; and neuron j.
The firing rate of the receiving neuron will be Rj
this becomes:

W;j

is the weight of the synapse

=1/(To + T 1). Substituting for

T1
(8)

F""tgure 3 compares this function with the logistic activation function. The impulse
activation function approaches zero at the rate of 1/K when R; approaches zero. The
function also approaches an asymptote of Rj =l/T0 as R; increases without bound.
Any non-synaptic source which causes current flow from Vtit to Co will shift the curve
to the left, and reflect a spontaneous firing rate at zero input excitation. A similar
current source to VoU will shift the function to the right, reflecting a positive firingonset threshold. Circuit-level simulations show a clear correspondence to these
analytical results. This functional form is also evident in activation curves experimentally observed with natural neurons {Guyton, 1986}. Various natural neurons are
known to exhibit both spontaneous firing and fuing-onset thresholds as well.
The impulse activation function constant, K, is determined by several factors including
fJ~, Co, and To. Assuming that To? T h no leakage current exists, and that a FET
conducting in its non-saturated region can be approximated by a resistor, the following expression for K is obtained:
(9)

A Low-Power CMOS Circuit Which Emulates Neurons

where

Rchon =l/pteJ<Vdd -V"".),
Ca is the summing capacitance, Va Vth are the low and high threshold voltages of the
Schmitt trigger, and V"". is the gate threshold voltage for an excitatory PET-synapse.
A more accurate K value can be obtained by using the non-saturated PET current
equation and solving a nonlinear differential equation.

1.0

rj~

~

tTo~

1,
1

ri

j

rectangUlar Impulse Train

J

0 .5

Logistic
Activation

Impulse
Activation

o ~--~----------------------------------~---------------------------------------------ri
o
Figure 3. Rectangular impulse train approximation for impulse activation
function derivation. Unlike the logistic function which asymptotically
approaches zero, impulse activation is equal to zero over a range of net
excitation.

BACK?PROPAGATION IN IMPULSE NE1WORKS
A back-propagation algorithm has been used to learn connection weights for impulse
neural networks. At this time, weight values are non-adaptive (they are fixed at circuit fabrication) because they are implemented as invariant PET transconductances.
Adaptive synapses compatible with impulse neuron circuits are in the early stages of
development, but are not available at this time. Much can be learned about these networks using non-adaptive prototypes, however. As a result, weight learning is performed offline as part of the network design process. The back-propagation procedure used to learn weights for impulse networks differs from the generalized delta
rule {Rumelhart, 1986} in two ways.
The fust difference is the use of the impulse activation function instead of the logistic
function. Any activation nonlinearity is a viable candidate for use with the generalized
delta rule as long as it is differentiable. This is where difficulties mount with the
impulse activation function. First of all, it is not differentiable at zero. What seems to
be more important, however, is that its first derivative equals zero over a range of

683

684

Meador and Cole

inputs. Examination of the generalized delta rule (which performs gradient-descent)
reveals that when the fust derivative of neuron activation becomes zero, connections
associated with that-neuron will cease to adapt. Once this happens, the procedure will
most probably never arrive at a problem solution.
To work around this problem, a second deviation from the generalized delta rule was
implemented. This involves a departure from using the true first derivative when the
impulse activation becomes zero. A small constant can be used to guarantee that
learning continues even though the associated neuron activation is zero:
Act =l/(To +K /Net)
A

,_{(l/(To+K/Net)' if Net >0
ct - e
otherwise

(10)
(11)

The use of these equations yields a back-propagation algorithm for impulse networks
which does not perform true gradient descent, yet which so far has been observed to
learn solutions to logic problems such as XOR and the 4-2-4 encoder. Investigation
of other offline learning algorithms for impulse networks continues. Currently, this
algorithm fulfills the immediate need for an offline procedure which can be used in
the design of multi-layer impulse neural networks.

IMPLEMENTATION
Two requirements for high density integration are low power dissipation and small
circuit geometry. CMOS impulse neurons use switching circuits having no continuous
power dissipation. A conventional op-amp circuit must draw constant current to
achieve linear bias. An op-amp also requires larger circuit geometries for gain accuracy over typical fabrication process variations. Such is not the case for nonlinear
switching circuits. As a result, these neurons and others like them are expected to
help improve analog neural network integration density.
An impulse neuron circuit has been designed which eliminates FETs 2 and 3 of Figure
2 in exchange for reduced layout area. In this circuit, Va no longer exhibits an activation potential spike. This spike seems irrelevant given the buffered impulse available
at the axon output. The modified neuron circuit occupies 200 X 25 lambda chip area.
A fIXed PET-synapse occupies a 16 by 18 lambda rectangle. With these dimensions a
full-interconnect layout containing 40 neurons and 1600 fIXed connections will fit on a
MOSIS 2-micron tiny chip. XOR and 4-2-4 networks of these circuits are being
developed for 2-micron CMOS.

CONCLUSION
The motivation of this work is to improve neural network implementation technology
by designing CMOS circuits derived from the temporal characteristics of natural neurons. The results obtained thus far include:
Two CMOS circuits which closely correspond to the voltage-gat ed-channel
model of natural neural membrane.

A Low-Power CMOS Circuit Which Emulates Neurons

Simulations which show that these impulse neurons emulate gross artifacts of
natural neuron function.
Initial work on a back-propagation algorithm which learns logic solutions using
the impulse neuron activation function.
The development of prototype impulse network I.Cs.
Future goals involve extending this investigation to plastic synapse and neuron circuits, alternate algorithms for both offline and online learning, and practical implementations.

Rererences
H. P. Graf W. Hubbard L. D. Jackel P. G. N. deVegvar. A CMOS associative
Memory Chip. IEEE ICNN Con. Proc., pp. 461-468, (1987).
H. P. Graf L. D. Jackel W. E. Hubbard. VLSI Implementation of a Neural Network
Model. IEEE Computer, pp. 41-49, (1988).
A.C. Guyton. Chapt. 10. Organization of the Nervous System: Basic Functions of
Synapses. Textbook o[ Physiology, p.l36. (1986)
N. EI-Liethy, R.W. Newcomb, M. Zaghlou. A Basic MOS Neural-Type Junction A
Perspective on Neural-Type Microsystems. IEEE ICNN Con. Proc., pp. 469-477,
(1987).
E. R. Lewis. Using Electronic Circuits to Model Simple Neuroelectric Interactions.
Proc. IEEE 56, pp. 931-949, (1968).
R. J. MacGregor R. M. Oliver. A General-Purpose Electronic Model for Arbitrary
Configurations of Neurons. 1. Theor. Bioi. 38, pp. 527-538 (1973).
S. Y. Kung. and J. N. Hwang. Parallel Achitectures for Artificial Neural Nets. IEEE
ICNN Con. Proc., pp. 11-165 to 11-172, (1988).
J. Mann R. Lippman B. Berger J. Raffel. A Self-Organizing Neural Net Chip. IEEE
Cust.Integr. Ckts. Conf"" pp. 10.3.1-10.3.5 (1988).
A. F. Murray A. V. W. Smith. Asynchronous VLSI Neural Networks Using PulseStream Arithmetic. IEEE lnl. of Sol. St. Phys. 23, pp. 688-697, (1988).
J. I. Raffel. Electronic Implementation of Neuromorphic Systems. IEEE Cust. Integr.
Ckts. Conf"" pp. 10.1.1-10.1.7, (1988).
D. Rumelhart, G.E. Hinton, and RJ. Williams. Learning Internal Representations by
Error Propagation. Parallel Distributed Processing, Vol 1: Foundations, pp. 318-364,
(1986).
O. H. Schmitt. Mechanical Solution of the Equations of Nerve Impulse Propagation.
Am. 1. Physiol. 119, pp. 399-400, (1937).
D. B. Schwartz R. E. Howard. A Programmable Analog Neural Network Chip. IEEE
Cust. Integr. Ckts. Conf., pp. 10.2.1-1.2.4, (1988).

685

686

Meador and Cole

T J. Sejnowski. Open Questions About Computation in Cerebral Cortex. Parallel
Distributed Processing Vol. 2:Psychological and Biological Models, pp. 378-385, (1986).
M. A. Sivilotti M. R. Emerling C. A. Mead. VISI Architectures for Implementation
of Neural Networks. Am. Ins. of Phys., 408-413, (1986).

"
1995,Harmony Networks Do Not Work,,1160-harmony-networks-do-not-work.pdf,Abstract Missing,"Harmony Networks Do Not Work

Rene Gourley
School of Computing Science
Simon Fraser University
Burnaby, B.C., V5A 1S6, Canada
gourley@mprgate.mpr.ca

Abstract
Harmony networks have been proposed as a means by which connectionist models can perform symbolic computation. Indeed, proponents claim that a harmony network can be built that constructs
parse trees for strings in a context free language. This paper shows
that harmony networks do not work in the following sense: they
construct many outputs that are not valid parse trees.
In order to show that the notion of systematicity is compatible with connectionism,
Paul Smolensky, Geraldine Legendre and Yoshiro Miyata (Smolensky, Legendre,
and Miyata 1992; Smolen sky 1993; Smolen sky, Legendre, and Miyata 1994) proposed a mechanism, ""Harmony Theory,"" by which connectionist models purportedly
perform structure sensitive operations without implementing classical algorithms.
Harmony theory describes a ""harmony network"" which, in the course of reaching a
stable equilibrium, apparently computes parse trees that are valid according to the
rules of a particular context-free grammar.
Harmony networks consist of four major components which will be explained in
detail in Section 1. The four components are,
Tensor Representation: A means to interpret the activation vector of a connectionist system as a parse tree for a string in a context-free language.
Harmony: A function that maps all possible parse trees to the non-positive integers so that a parse tree is valid if and only if its harmony is zero.
Energy: A function that maps the set of activation vectors to the real numbers
and which is minimized by certain connectionist networks!.
Recursive Construction: A system for determining the weight matrix of a connectionist network so that if its activation vector is interpreted as a parse
1 Smolensky, Legendre and Miyata use the term ""harmony"" to refer to both energy and
harmony. To distinguish between them, we will use the term that is often used to describe
the Lyapunov function of dynamic systems, ""energy"" (see for example Golden 1986).

R. GOURLEY

32

tree, then the network's energy is the negation of the harmony of that parse
tree.
Smolen sky et al. contend that, in the process of minimizing their energy values,
harmony networks implicitly maximize the harmony of the parse tree represented by
their activation vector. Thus, if the harmony network reaches a stable equilibrium
where the energy is equal to zero, the parse tree that is represented by the activation
vector must be a valid parse tree:
When the lower-level description of the activation-spreading process satisfies certain mathematical properties, this process can be
analyzed on a higher level as the construction of that structure
including the given input structure which maximizes Harmony.
(Smolensky 1993, p848, emphasis is original)
Unfortunately, harmony networks do not work - they do not always construct
maximum-harmony parse trees. The problem is that the energy function is defined
on the values of the activation vector. By contrast, the harmony function is defined
on possible parse trees. Section 2 of this paper shows that these two domains are
not equal, that is, there are some activation vectors that do not represent any parse
tree.
The recursive construction merely guarantees that the energy function passes
through zero at the appropriate points; its minima are unrestricted . So, while
it may be the case that the energy and harmony functions are negations of one
another, it is not always the case that a local minimum of one is a local maximum
of the other. More succinctly, the harmony network will find minima that are not
even trees, let alone valid parse trees.
The reason why harmony networks do not work is straightforward. Section 3 shows
that the weight matrix must have only negative eigenvalues, for otherwise the network constructs structures which are not valid trees. Section 4 shows that if the
weight matrix has only negative eigenvalues, then the energy function admits only
a single zero - the origin. Furthermore, we show that the origin cannot be interpreted as a valid parse tree. Thus, the stable points of a harmony network are not
valid parse trees.

1
1.1

HARMONY NETWORKS
TENSOR REPRESENTATION

Harmony theory makes use of tensor products (Smolensky 1990; Smolensky, Legendre, and Miyata 1992; Legendre, Miyata, and Smolensky 1991) to convolve symbols
with their roles. The resulting products are then added to represent a labelled tree
using the harmony network's activation vector. The particular tensor product used
is very simple:
(aI, a2,? ? ?, an) <8> (b l , b2,.?., bm ) =
(alb l , a l b2, ... , a}b m , a2bl, a2 b2, ... , a2bm,

anb m )
If two tensors of differing dimensions are to be added , then they are essentially
concatenated.
.. . ,

Binary trees are represented with this tensor product using the following recursive
rules:
1. The tensor representation of a tree containing no vertices is O.

33

Harmony Networks Do Not Work

Table 1: Rules for determining harmony and the weight matrix. Let G = (V, E, P, S)
be a context-free grammar of the type suggested in section 1.2. The rules for
determining the harmony of a tree labelled with V and E are shown in the second
column. The rules for determining the system of equations for recursive construction
are shown in the third column. (Smolensky, Legendre, and Miyata 1992; Smolensky

1993)
Grammar
Element
S
xEE

x E
{S}

V\

--

x
yz
or x
yE P

Harmony Rule

Energy Equation

For every node labelled
S add -1 to H(T).
For every node labelled
x add -1 to H(T).
For every node labelled
x add -2 or -3 to H(T)
depending on whether
or not x appears on
the left of a production with two symbols
on the right.
For every edge where
x is the parent and y
is the left child add 2.
Similarly, add 2 every
time z is the right child
of x.

Include (S+00r,)Wroot(S+00rr) = 2
in the system of equations
Include (x +60r,)Wroot (x +60r,) = 2
in the system of equations

=

Include (x+60r,)Wroot(x+00r,)
4
or 6 in the system of equations, depending on whether or not x appears on the
left of a production with two symbols
on the right.
Include in the system of equations,
(x + 60 r,)Wroot (6 + y 0 r,) = -2
(0 + y 0 r,)Wroot(x + 60 r,) = -2
(x + 60 r,)Wroot(O + z 0 r,) = -2
(6 + z 0 r,)Wroot(x + 6? r,) = -2

2. If A is the root of a tree, and TL, TR are the tensor product representations
of its left subtree and right subtree respectively, then A + TL 0 r, + TR 0 rr
is the tensor representation of the whole tree.
The vectors, r"" and rr are called ""role vectors"" and indicate the roles of left child
and right child.

1.2

HARMONY

Harmony (Legendre, Miyata, and Smolensky 1990; Smolensky, Legendre, and Miyata 1992) describes a way to determine the well-formedness of a potential parse tree
with respect to a particular context free grammar. Without loss of generality, we
can assume that the right-hand side of each production has at most two symbols,
and if a production has two symbols on the right, then it is the only production for
the variable on its left side. For a given binary tree, T, we compute the harmony
of T, H(T) by first adding the negative contributions of all the nodes according to
their labels, and then adding the contributions of the edges (see first two columns
of table 1).

34

1.3

R.GOURLEY

ENERGY

Under certain conditions, some connectionist models are known to admit the following energy or Lyapunov function (see Legendre, Miyata, and Smolensky 1991):
1

E(a) = --atWa
2
Here, W is the weight matrix of the connectionist network, and a is its activation
vector. Every non-equilibrium change in the activation vector results in a strict
decrease in the network's energy. In effect, the connectionist network serves to
minimize its energy as it moves towards equilibrium.

1.4

RECURSIVE CONSTRUCTION

Smolensky, Legendre, and Miyata (1992) proposed that the recursive structure of
their tensor representations together with the local nature of the harmony calculation could be used to construct the weight matrix for a network whose energy
function is the negation of the harmony of the tree represented by the activation
vector. First construct a matrix W root which satisfies a system of equations. The
system of equations is found by including equations for every symbol and production in the grammar, as shown in column three of table 1. Gourley (1995) shows
that if W is constructed from copies of W root according to a particular formula, and
if aT is a tensor representation for a tree, T, then E(aT) = -H(T).

2

SOME ACTIVATIONS ARE NOT TREES

As noted above, the reason why harmony networks do not work is that they seek
minima in their state space which may not coincide with parse tree representations.
One way to amelioarate this would be to make every possible activation vector
represent some parse tree. If every activation vector represents some parse tree,
then the rules that determine the weight matrix will ensure that the energy minima
agree with the valid parse trees. Unfortunately, in that case, the system of equations
used to determine W root has no solution.

If every activation vector is to represent some parse tree, and the symbols of the
grammar are two dimensional, then there are symbols represented by each vector,
(Xl, xt), (Xl, X2), (X2' xt), and (X2' X2), where Xl 1= X2 . These symbols must satisfy
the equations given in table 1 , and so,

XiWrootll

X~Wrootll

Xi{Wrootll
Wroot12
Wroot~l
XIX2 W root12
XIX2 W root:n
XIX2Wrootl~ XIX2 W root :n

+
+

x~(Wrootll

+

+
+

+ Wroot12

+ Wroot~~)
+ x~Wroot:n
+ xiWroot~2
+ Wroot~l + Wrootn)
+

Because hi E {2, 4, 6}, there must be a pair hi, hj which are equal. In that
case, it can be shown using Gaussian elimination that there is no solution for
Wrootll , Wrootl~' Wroot~l , Wroot~~. Similarly, if the symbols are represented by vectors of dimension three or greater, the same contradiction occurs.
Thus there are some activation vectors that do not represent any tree - valid or
invalid. The question now becomes one of determining whether all of the harmony
network's stable equilibria are valid parse trees.

35

Harmony Networks Do Not Work

b

a

Figure 1: Energy functions of two-dimensional harmony networks. In each case, the
points i and f respectively represent an initial and a final state of the network. In
a, one eigenvector is positive and the other is negative; the hashed plane represents
the plane E
0 which intersects the energy function and the vertical axis at the
origin. In b, one eigenvalue is negative while the other is zero; The heavy line
represents the intersection of the surface with the plane E 0 and it intersects the
vertical axis at the origin.

=

=

3

NON-NEGATIVE EIGENVECTORS YIELD
NON-TREES

If any of the eigenvalues of the weight matrix, W, is positive, then it is easy to show
that the harmony network will seek a stable equilibrium that does not represent
a parse tree at all. Let A > 0 be a positive eigenvalue of W, and let e be an
eigenvector, corresponding to A, that falls within the state space. Then,

E(e)

1
1
= --etWe
= --Aete
2
2

< O.

Because the energy drops below zero, the harmony network would have to undergo
an energy increase in order to find a zero-energy stable equilibrium. This cannot
happen, and so, the network reaches an equilibrium with energy strictly less than
zero.
Figure la illustrates the energy function of a harmony network where one eigenvalue
is positive. Because harmony is the negation of energy, in this figure all the valid
parse trees rest on the hashed plane, and all the invalid parse trees are above it. As
we can see, the harmony network with positive eigenvalues will certainly find stable
equilibria which are not valid parse tree representations.
Now, suppose W, the weight matrix, has a zero eigenvalue. If e is an eigenvector
corresponding to that eigenvalue, then for every real a, aWe = O. Consequently,
one of the following must be true:
1. ae is not a stable equilibrium. In that case, the energy function must drop

below zero, yielding a sub-zero stable equilibrium - a stable equilibrium
that does not represent any tree.
2. ae is a stable equilibrium.
Then for every a, ae must be a
valid tree representation.
Such a situation is represented in fig-

R. GOURLEY

36

Figure 2: The energy function of a two-dimensional harmony network where both
eigenvalues are negative. The vertical axis pierces the surface at the origin , and the
points i and f respectively represent an initial and a final state of the network .
ure Ib where the set of all points ae is represented by the heavy
line. This implies that there is a symbol, (al, a2, . . . , an), such that
Ckl(al , a2, .. . ,an),Ck2(al,a2, . . . ,an), .. . ,an2+l(al,a2, ... , an) are also all
symbols. As before, this implies that W root must satisfy the equation,
?al, ... , an)

hi hi E
2"" ' {2 4 6}
a ,?
""

+ 0- ? r,) t Wroot?al, ... , an) + 0- 0 r,)

for i = 1 ... n 2 + 1. Again using Gaussian elimination, it can be shown that
there is no solution to this system of equations.

In either case, the harmony network admits stable equilibria that do not represent
any tree. Thus, the eigenvalues must all be negative.

4

NEGATIVE EIGENVECTORS YIELD NON-TREES

If all the eigenvalues of the weight matrix are negative, then the energy function has
a very special shape: it is a paraboloid centered on the origin and concave in the
direction of positive energy. This is easily seen by considering the first and second
derivatives of E:
8E(x) _ _ ~
8x; L..j

W, .. x .

'.1'

8 2 E(x) 8x;8x; -

-W,. .

'.1

Clearly, all the first derivatives are zero at the origin, and so, it is a critical point.
Now the origin is a strict minimum if all the roots of the following well-known
equation are positive:

0= det

= det I-W - All

det 1- W - All is the characteristic polynomial of -W . If A is a root then it is an
eigenvalue of - W, or equivalently, it is the negative of an eigenvalue of W . Because
all of W's eigenvalues are negative, the origin is a strict minimum, and indeed it is
the only minimum. Such a harmony network is illustrated in Figure 2.

37

Hannony Networks Do Not Work

Thus the origin is the only stable point where the energy is zero, but it cannot
represent a parse tree which is valid for the grammar. If it does, then

S + TL 0 r,

+ TR (9 rr =

(0, . . . ,0)

where TL, TR are appropriate left and right subtree representations, and S is the
start symbol of the grammar. Because each of the subtrees is multiplied by either
or rr, they are not the same dimension as S, and are consequently concatenated
instead of added. Therefore S = O. But then, Wroot must satisfy the equation

r,

(0 + 0(9 r,)Wroot(O + 0 (9 r,) =-2
This is impossible, and so, the origin is not a valid tree representation.

5

CONCLUSION

This paper has shown that in every case, a harmony network will reach stable
equilibria that are not valid parse trees. This is not unexpected. Because the
energy function is a very simple function, it would be more surprising if such a
connectionist system could construct complicated structures such as parse trees for
a context free grammar.
Acknowledgements
The author thanks Dr. Robert Hadley and Dr. Arvind Gupta, both of Simon Fraser
University, for their invaluable comments on a draft of this paper.

References
Golden, R. (1986). The 'brain-state-in-a-box' neural model is a gradient descent
algorithm. Journal of Mathematical Psychology 30, 73-80.
Gourley, R. (1995) . Tensor represenations and harmony theory: A critical analysis.
Master's thesis, Simon Fraser University, Burnaby, Canada. In preparation.
Legendre, G., Y. Miyata, and P. Smolensky (1990). Harmonic grammar - a formal
multi-level connectionist theory of linguistic well-formedness: Theoretical foundations. In Proceedings of the Twelfth National Conference on Cognitive Science,
Cambridge, MA, pp . 385- 395. Lawrence Erlbaum.
Legendre, G ., Y. Miyata, and P. Smolensky (1991) . Distributedrecursive structure
processing. In B. Mayoh (Ed.), Proceedings of the 1991 Scandinavian Conference
on Artificial Intelligence , Amsterdam, pp. 47-53. lOS Press.
Smolensky, P. (1990) . Tensor product variable binding and the representation of
symbolic structures in connectionist systems. Artificial Intelligence 46, 159-216.
Smolensky, P. (1993). Harmonic grammars for formal languages. In S. Hanson,
J. Cowan, and C. Giles (Eds.), Advances in Neural Information Processing Systems
5, pp. 847-854 . San Mateo: Morgan Kauffman .
Smolensky, P., G. Legendre, and Y. Miyata (1992). Principles for an integrated
connectionist/symbolic theory of higher cognition. Technical Report CU-CS-60092, University of Colorado Computer Science Department.
Smolensky, P., G. Legendre, and Y . Miyata (1994) . Integrating connectionist and
symbolic computation for the theory of language. In V. Honavar and L. Uhr (Eds.),
Artificial Intelligence and Neural Networks : Steps Toward Principled Integration,
pp. 509-530. Boston: Academic Press.

"
1997,A Solution for Missing Data in Recurrent Neural Networks with an Application to Blood Glucose Prediction,,1348-a-solution-for-missing-data-in-recurrent-neural-networks-with-an-application-to-blood-glucose-prediction.pdf,Abstract Missing,"A Solution for Missing Data in Recurrent Neural
Networks With an Application to Blood Glucose
Prediction

Volker Tresp and Thomas Briegel *
Siemens AG
Corporate Technology
Otto-Hahn-Ring 6
81730 Miinchen, Germany

Abstract
We consider neural network models for stochastic nonlinear dynamical
systems where measurements of the variable of interest are only available at irregular intervals i.e. most realizations are missing. Difficulties
arise since the solutions for prediction and maximum likelihood learning with missing data lead to complex integrals, which even for simple
cases cannot be solved analytically. In this paper we propose a specific combination of a nonlinear recurrent neural predictive model and
a linear error model which leads to tractable prediction and maximum
likelihood adaptation rules. In particular, the recurrent neural network
can be trained using the real-time recurrent learning rule and the linear
error model can be trained by an EM adaptation rule, implemented using forward-backward Kalman filter equations. The model is applied to
predict the glucose/insulin metabolism of a diabetic patient where blood
glucose measurements are only available a few times a day at irregular
intervals. The new model shows considerable improvement with respect
to both recurrent neural networks trained with teacher forcing or in a free
running mode and various linear models.

1

INTRODUCTION

In many physiological dynamical systems measurements are acquired at irregular intervals.
Consider the case of blood glucose measurements of a diabetic who only measures blood
glucose levels a few times a day. At the same time physiological systems are typically
highly nonlinear and stochastic such that recurrent neural networks are suitable models.
Typically, such networks are either used purely free running in which the networks predictions are iterated, or in a teacher forcing mode in which actual measurements are substituted
? {volker.tresp, thomas.briegel} @mchp.siemens.de

V. Tresp and T. Briegel

972

if available. In Section 2 we show that both approaches are problematic for highly stochastic systems and if many realizations of the variable of interest are unknown. The traditional
solution is to use a stochastic model such as a nonlinear state space model. The problem
here is that prediction and training missing data lead to integrals which are usually considered intractable (Lewis, 1986). Alternatively, state dependent linearizations are used for
prediction and training, the most popular example being the extended Kalman filter. In this
paper we introduce a combination of a nonlinear recurrent neural predictive model and a
linear error model which leads to tractable prediction and maximum likelihood adaptation
rules. The recurrent neural network can be used in all generality to model the nonlinear
dynamics of the system. The only limitation is that the error model is linear which is not
a major constraint in many applications. The first advantage of the proposed model is that
for single or multiple step prediction we obtain simple iteration rules which are a combination of the output of the iterated neural network and a linear Kalman filter which is used
for updating the linear error model. The second advantage is that for maximum likelihood
learning the recurrent neural network can be trained using the real-time recurrent learning
rule RTRL and the linear error model can be trained by an EM adaptation rule, implemented using forward-backward Kalman filter equations. We apply our model to develop a
model of the glucose/insulin metabolism of a diabetic patient in which blood glucose measurements are only available a few times a day at irregular intervals and compare results
from our proposed model to recurrent neural networks trained and used in the free running
mode or in the teacher forcing mode as well as to various linear models.

2 RECURRENT SYSTEMS WITH MISSING DATA
reasonable estimate for 1
/

..

=:t.

6

I

/ncosuremcnt at (,me t=7 y.

Y,

?

0

~..'_'m~__

o

teacher forC ing

free runn ing

'..-

.

~CQsurcment

at

lIm~

1= 7

UIIIIdrrrrI
1

2

""

4

S

6

7

8

9

10

J I

12

13

Figure 1: A neural network predicts the next value of a time-series based on the latest two
previous measurements (left). As long as no measurements are available (t = 1 to t = 6),
the neural network is iterated (unfilled circles). In a free-running mode, the neural network
would ignore the measurement at time t = 7 to predict the time-series at time t = 8. In a
teacher forcing mode, it would substitute the measured value for one of the inputs and use
the iterated value for the other (unknown) input. This appears to be suboptimal since our
knowledge about the time-series at time t = 7 also provides us with information about the
time-series at time t = 6. For example the dotted circle might be a reasonable estimate. By
using the iterated value for the unknown input, the prediction of the teacher forced system
is not well defined and will in general lead to unsatisfactory results. A sensible response
is shown on the right where the first few predictions after the measurement are close to the
measurement. This can be achieved by including a proper error model (see text).
Consider a deterministic nonlinear dynamical model of the form
Yt

=

!w(Yt-l,"" ?,Yt-N,Ut)

of order N, with input Ut and where ! w (.) is a neural network model with parametervector w. Such a recurrent model is either used in a free running mode in which network
predictions are used in the input of the neural network or in a teacher forcing mode where
measurements are substituted in the input of the neural network whenever these are available.

Missing Data in RNNs with an Application to Blood Glucose Prediction

-

- ..-.:::...:..- -~-

973

-~

-- ---

Figure 2: Left: The proposed architecture. Right: Linear impulse response.
Both can lead to undesirable results when many realizations are missing and when the system is highly stochastic. Figure 1 (left) shows that a free running model basically ignores
the measurement for prediction and that the teacher forced model substitutes the measured
value but leaves the unknown states at their predicted values which also might lead to undesirable responses. The traditional solution is to include a model of the error which leads
to nonlinear stochastical models, the simplest being
Yt

= fw (Yt-l,.'""

Yt-N,

utJ + lOt

where lOt is assumed to be additive uncorrelated zero-mean noise with probability density P? (f) and represents unmodeled system dynamics. For prediction and learning with
missing values we have to integrate over the unknowns which leads to complex integrals
which, for nonlinear models, have to be approximated. for example, using Monte Carlo
integration. l In general, those integrals are computationally too expensive to solve and, in
practice, one relies on locally linearized approximations of the nonlinearities typically in
form of the extended Kalman filter. The extended Kalman filter is suboptimal and summarizes past data by an estimate of the means and the covariances of the variables involved
(Lewis, 1986).
In this paper we pursue an alternative approach. Consider the model with state updates
Yt*

(1)

fW(Y;-l""' "" Y;-N' ut}
K

Xt

2: (JiXt-i +

(2)

lOt

i=l
K

Yt

Y;

+ Xt

= fw (Y;-l' ... , Y;-N, ue) + L (JiXt-i +

lOt

(3)

i=l

and with measurement equation
Zt=Yt+Ot.

(4)

where lOt and Ot denote additive noise. The variable of interest Yt is now the sum of the
deterministic response of the recurrent neural network Y; and a linear system error model
Xt (Figure 2). Zt is a noisy measurement of Yt. In particular we are interested in the special
cases that Yt can be measured with certainty (variance of Ot is zero) or that a measurement
is missing (variance of Ot is infinity). The nice feature is now that Y; can be considered
a deterministic input to the state space model consisting of the equations (2)- (3). This
means that for optimal one-step or multiple-step prediction, we can use the linear Kalman
filter for equations (2)- (3) and measurement equation (4) by treating Y; as deterministic
input. Similarly, to train the parameters in the linear part of the system (i.e. {Oi }f:l) we can
use an EM adaptation rule, implemented using forward-backward Kalman filter equations
(see the Appendix). The deterministic recurrent neural network is adapted with the residual
error which cannot be explained by the linear model, i.e. target~nn = y"": - :W near
1 For maximum likelihood learning of linear models we obtain EM equations which can be solved
using forward-backward Kalman equations (see Appendix).

V. Tresp and T. Briegel

974

where Y~ is a measurement ofYt at time t and where f)/near is the estimate of the linear
model. After the recurrent neural network is adapted the linear model can be retrained
using the residual error which cannot be explained by the neural network. then again the
neural network is retrained and so on until no further improvement can be achieved.
The advantage of this approach is that all of the nonlinear interactions are modeled by
a recurrent neural network which can be trained deterministically. The linear model is
responsible for the noise model which can be trained using powerful learning algorithms
for linear systems. The constraint is that the error model cannot be nonlinear which often
might not be a major limitation.

3

BLOOD GLUCOSE PREDICTION OF A DIABETIC

The goal of this work is to develop a predictive model of the blood glucose ofa person with
type 1 Diabetes mellitus. Such a model can have several useful applications in therapy:
it can be used to warn a person of dangerous metabolic states, it can be used to make
recommendations to optimize the person's therapy and, finally, it can be used in the design
of a stabilizing control system for blood glucose regulation, a so-called ""artificial beta cell""
(Tresp, Moody and Delong, 1994). We want the model to be able to adapt using patient data
collected under normal every day conditions rather than the controlled conditions typical
of a clinic. In a non-clinical setting, only a few blood glucose measurements per day are
available.
Our data set consists of the protocol ofa diabetic over a period of almost six months. During that time period, times and dosages of insulin injections (basal insulin ut and normal
insulin u;), the times and amounts of food intake (fast u~, intermediate ut and slow u~
carbohydrates), the times and durations of exercise (regular u~ or intense ui) and the blood
glucose level Yt (measured a few times a day) were recorded. The u{, j = 1, ... ,7 are
equal to zero except if there is an event, such as food intake, insulin injection or exercise.
For our data set, inputs u{ were recorded with 15 minute time resolution. We used the first
43 days for training the model (containing 312 measurements of the blood glucose) and the
following 21 days for testing (containing 151 measurements of the blood glucose). This
means that we have to deal with approximately 93% of missing data during training.
The effects on insulin, food and exercise on the blood glucose are delayed and are approximated by linear response functions. v{ describes the effect of input u{ on glucose. As an
example, the response of normal insulin after injection is determined by the diffusion
of the subcutaneously injected insulin into the blood stream and can be modeled by three
first order compartments in series or, as we have done, by a response function of the form
= l:T g2(t withg 2(t) = a2t2e-b2t (see figure 2 for a typical impulse response).
The functional mappings gj (.) for the digestive tract and for exercise are less well known.
In our experiments we followed other authors and used response functions of the above
form.

vt

vt

u;

r)u;

The response functions 9j ( .) describe the delayed effect of the inputs on the blood glucose.
We assume that the functional form of gj (.) is sufficient to capture the various delays of the
inputs and can be tuned to the physiology of the patient by varying the parameters aj ,bj .
To be able to capture the highly nonlinear physiological interactions between the response
functions
and the blood glucose level Yt, which is measured only a few times a day, we
employ a neural network in combination with a linear error model as described in Section 2.
In our experiments fw (.) is a feedforward multi-layer perceptron with three hidden units.
The five inputs to the network were insulin (in; = vi + v;>, food (in; = vf + vt + vt),
exercise (inr = vf + vi) and the current and previous estimate of the blood glucose. To be
specific, the second order nonlinear neural network model is

vi

Yt*

* +f
= Yt-l

W

(.
*
. t1 ,zn
. 2 . 3)
Yt_llYt_2,ln
t ,znt

(5)

Missing Data in RNNs with an Application to Blood Glucose Prediction

975

For the linear error model we also use a model of order 2
(6)
Table 1 shows the explained variance of the test set for different predictive models.

2

In the first experiment (RNN-FR) we estimate the blood glucose at time t as the output
of the neural network Yt
The neural network is used in the free running mode for
training and prediction. We use RTRL to both adapt the weights in the neural network as
well as all parameters in the response functions 9j (.). The RNN-FR model explains 14.1
percent of the variance. The RNN-TF model is identical to the previous experiment except
that measurements are substituted whenever available. RNN-TF could explain more of the
variance (18.8%). The reason for the better performance is, of course, that information
about measurements of the blood glucose can be exploited.

= y;.

The model RNN-LEM2 (error model with order 2) corresponds to the combination of the
recurrent neural network and the linear error model as introduced in Section 2. Here,
Yt
Xt + Y; models the blood glucose and Zt
Yt + 8t is the measurement equation
where we set the variance of 8t = 0 for a measurement of the blood glucose at time t and
00 for missing values. For ft we assume Gaussian independent noise.
the variance of 8t
For prediction, equation (5) is iterated in the free running mode. The blood glucose at time
t is estimated using a linear Kalman filter, treating
as deterministic input in the state
space model Yt
x t + Y; ,Zt Yt + 8t . We adapt the parameters in the linear error model
(i.e. (h, O2 , the variance of ft) using an EM adaptation rule, implemented using forwardbackward Kalman filter equations (see Appendix). The parameters in the neural network
are adapted using RTRL exactly the same way as in the RNN-FR model, except that the
iftinear where
is a measurement of Yt at time t and
target is now target~nn =
where iftinear is the estimate of the linear error model (based on the linear Kalman filter).
The adaptation of the linear error model and the neural network are performed aIternatingly
until no significant further improvement in performance can be achieved.

=

=

=

=

Y;

=

yr -

yr

As indicated in Table 1, the RNN-LEM2 model achieves the best prediction performance
with an explained variance of 44.9% (first order error model RNN-LEMI: 43.7%). As
a comparison, we show the performance of just the linear error model LEM (this model
ignores all inputs), a linear model (LM-FR) without an error model trained with RTRL and
a linear model with an error model (LM-LEM). Interestingly, the linear error model which
does not see any of the inputs can explain more variance (12.9%) than the LM-FR model
(8.9%). The LM-LEM model, which can be considered a combination of both can explain more than the sum of the individual explained variances (31.5%) which indicates that
the combined training gives better perfonnance than training both submodels individually.
Note also, that the nonlinear models (RNN-FR, RNN-TF, RNN-LEM) give considerably
better results than their linear counterparts, confirming that the system is highly nonlinear.
Figure 3 (left) shows an example of the responses of some of the models. We see that
the free running neural network (dotted line) has relatively small amplitudes and cannot
predict the three measurements very well. The RNN-TF model (dashed line) shows a
better response to the measurements than the free running network. The best prediction of
all measurements is indeed achieved by the RNN-LEM model (continuous line).
Based on the linear iterated Kalman filter we can calculate the variance of the prediction.
As shown in Figure 3 (right) the standard deviation is small right after a measurement is
available and then converges to a constant value. Based on the prediction and the estimated
variance, it will be possible to do a risk analysis for the diabetic (i.e a warning of dangerous
metabolic states).
2MSPE(model) is the mean squared prediction error on the test set of the model and
MSPE( mean) is the mean squared prediction error of predicting the mean.

V. Tresp and T. Briegel

976
240~--~----~--~--~

230
220
..,-210

~
=-200

I

~ 190

1-g
~

lBO

p.

~\

1\\

I

,

I \
J

\

\

'.""

\

. '.
?

~

- ':
""

~

, ""-

170?

160

_ -

'.~'::""""........

::;-;~..-~..,,""-

???????????????

150
2 .5

5
time [hOurS]

2.5

7 .5

5
time [hOurS]

7.5

Figure 3: Left: Responses of some models to three measurements. Note, that the prediction
of the first measurement is bad for all models but that the RNN-LEM model (continuous
line) predicts the following measurements much better than both the RNN -FR (dotted) and
the RNN-TF (dashed) model. Right: Standard deviation of prediction error ofRNN-LEM.

Table 1: Explained variance on test set [in percent]: 100 .

MODEL
mean
LM
LEM
RNN-FR

4

%
0

8.9
12.9
14.1

(1 - ~;~~ ::~~ )

MODEL

%

RNN-TF
LM-LEM
RNN-LEMl
RNN-LEM2

18.8
3l.4
43.7
44.9

CONCLUSIONS

We introduced a combination of a nonlinear recurrent neural network and a linear error
model. Applied to blood glucose prediction it gave significantly better results than both
recurrent neural networks alone and various linear models. Further work might lead to a
predictive model which can be used by a diabetic on a daily bases. We believe that our results are very encouraging. We also expect that our specific model can find applications in
other stochastical nonlinear systems in which measurements are only available at irregular
intervals such that in wastewater treatment, chemical process control and various physiological systems. Further work will include error models for the input measurements (for
example, the number of food calories are typically estimated with great uncertainty).

Appendix: EM Adaptation Rules for Training the Linear Error Model
Model and observation equations of a general model are3
Xt

=

eXt-l

+ ft

Zt

= MtXt + 8t .

(7)

e

where is the K x K transition matrix ofthe K -order linear error model. The K x 1 noise
terms (t are zero-mean uncorrelated normal vectors with common covariance matrix Q. 8t
is m-dimensional 4 zero-mean uncorrelated normal noise vector with covariance matrix
R t ? Recall that we consider certain measurements and missing values as special cases of
3Note, that any linear system of order K can be transformed into a first order linear system of
dimension K.
4 m indicates the dimension of the output of the time-series.

Missing Data in RNNs with an Application to Blood Glucose Prediction

977

noisy measurements. The initial state of the system is assumed to be a normal vector with
mean Jl and covariance E.
We describe the EM equations for maximizing the likelihood of the model. Define the
estimated parameters at the (r+ l)st iterate of EM as the values Jl, E, e, Q which maximize
G(Jl, E, e, Q) = Er (log Llzl, ... , zn)
(8)
,
where log L is log-likelihood of the complete data Xo, Xl, ??? , X n ZI, ? .? , Zn and Er denotes the conditional expectation relative to a density containing the rth iterate values
Jl(r), E(r), e(r) and Q(r). Recall that missing targets are modeled implicitly by the definition of M t and R t .
For calculating the conditional expectation defined in (8) the following set of recursions
are used (using standard Kalman filtering results, see (Jazwinski, 1970)). First, we use the
forward recursion
e- x t-l
X t-l
t
t_ l
pt-l
ept-leT
+Q
t
t-l
Kt
P/-lMtT(MtP/-IMtT + R t )-1
(9)
t-l
+
r.' (*
M
t-l)
Xtt
xt
I'q Yt tXt
Pi
p tt-l - K t M t P;-1
where we take

xg = Jl and p3 = E. Next, we use the backward recursion
t-leT(pt-l)-l
Pt-l
t

J t-l
X~_l

x~=~ + Jt-l(X~ - ex~=D
(10)
Ptn_l
p:~t + Jt-dPr - p;-l)J?'_l
pr-l,t-2
P/~11JL2 + Jt-dPtt-l - ePtt~l)Jt~2
with initialization p;: n-l
(1 - KnMn)ep;:::l. One forward and one backward recursion completes the E-'step of the EM algorithm.

=

To derive the M-step first realize that the conditional expectations in (8) yield to the following equation:
G

=

-~ log IEI- !tr{E-l(Pon + (xo - Jl)(xo - Jl)T)}
-% log IQI-1tr{Q-l(C - BeT - eB T - eAeT)}
-% log IRtl- !tr{R;-l E~l[(Y; - Mtxt)(y; - Mtxd T + MtprMtT]}

(11 )
where tr{.} denotes the trace, A

= E~=l (pr-l + xr_lx~:d,

B = E~=l(Pt~t-l

+ X~X~:l) and C = E~=l (pr + x~x~ T).
e(r + 1) = BA- 1 and Q(r + 1) = n-1(C - BA- l BT) maximize the log-likelihood
equation (11). Jl (r + 1) is set to Xo and E may be fixed at some reasonable baseline level.
The derivation of these equations can be found in (Shumway & Stoffer, 1981).
The E- (forward and backward Kalman filter equations) and M-steps are alternated repeatedly until convergence to obtain the EM solution.
References
Jazwinski, A. H. (1970) Stochastic Processes and Filtering Theory, Academic Press, N.Y.
Lewis, F. L. (1986) Optimal Estimation, John Wiley, N.Y.
Shumway, R. H. and Stoffer, D. S. (1981) TIme Series Smoothing and Forecasting Using
the EM Algorithm, Technical Report No. 27, Division of Statistics, UC Davis.
Tresp, v., Moody, 1. and Delong, W.-R. (1994) Neural Modeling of Physiological Processes, in Comput. Leaming Theory and Natural Leaming Sys. 2, S. Hanson et al., eds.,
MIT Press.

"
2001,Direct value-approximation for factored MDPs,,1981-direct-value-approximation-for-factored-mdps.pdf,Abstract Missing,"Direct value-approxiIllation for factored MDPs

Dale Schuurmans and ReIn Patrascll
Department of Computer Science
University of Waterloo
{dale, rpatrasc} @cs.'Uwaterloo.ca

Abstract
We present a simple approach for computing reasonable policies
for factored Markov decision processes (MDPs), when the optimal value function can be approximated by a compact linear form.
Our method is based on solving a single linear program that approximates the best linear fit to the optimal value function. By
applying an efficient constraint generation procedure we obtain an
iterative solution method that tackles concise linear programs. This
direct linear programming approach experimentally yields a significant reduction in computation time over approximate value- and
policy-iteration methods (sometimes reducing several hours to a
few seconds). However, the quality of the solutions produced by
linear programming is weaker-usually about twice the approximation error for the same approximating class. Nevertheless, the
speed advantage allows one to use larger approximation classes to
achieve similar error in reasonable time.

1

Introduction

Markov decision processes (MDPs) form a foundation for control in uncertain and
stochastic environments and reinforcement learning. Standard methods such as
value-iteration, policy-iteration and linear programming can be used to produce
optimal control policies for MDPs that are expressed in explicit form; that is, the
policy, value function and state transition model are all represented in a tabular
manner that explicitly enumerates the state space. This renders the approaches
impractical for all but toy problems. The real goal is to achieve solution methods
that scale up reasonably in the size of the state description, not the size of the state
space itself (which is usually either exponential or infinite).
There are two basic premises on which solution methods can scale up: (1) exploiting
structure in the MDP model itself (i.e. structure in the reward function and the state
transition model); and (2) exploiting structure in an approximate representation of
the optimal value function (or policy). Most credible attempts at scaling-up have
generally had to exploit both types of structure. Even then, it is surprisingly difficult
to formulate an optimization method that can handle large state descriptions and
yet simultaneously produce value functions or policies with small approximation
errors, or errors that can be bounded tightly. In this paper we investigate a simple
approach to determining approximately .optimal policies based on a simple direct

linear programming approach. Specifically, the idea is to approximate the optimal
value function by formulating a single linear program and exploiting structure in the
MDP and the value function approximation to solve this linear program efficiently.

2

Preliminaries

We consider MDPs with finite state and action spaces and consider the goal of maximizing infinite horizon discounted reward. In this paper, states will be represented
by vectors x of length n, where for simplicity we assume the state variables Xl, ... , X n
are in {O, I}; hence the total nuniber of states is N == 2n . We also assume there
is a small finite set of actions A == {aI, ... , al}. An MDP is defined by: (1) a state
transition model P(x/lx, a) which specifies the probability of the next state Xl given
the current state x and action a; (2) a reward function R(x, a) which specifies the
immediate reward obtained by taking action a in state X; and (3) a discount factor
"" 0 :S , < 1. The problem is to determine an optimal control policy 1r* : X --7 A
that achieves maximum expected future discounted reward in every state.
To understand the standard solution methods it is useful to define some auxiliary
concepts. For any policy 1r, the value function V 7r : X --7 JR denotes the expected
future discounted reward achieved by policy 1r in each state x. It turns out that
V 7r satisfies a fixed point relationship between the value of current states and the
expected values of future states, given by a backup operator V 7r == B 7r V 7r , where
B 7r operates on arbitrary functions over the state space according to

(B f) (x) == R(x, 1r(x)) + ,
7r

E P(x'lx, 1r(x)) f(x /)
X'

Another important backup operator is defined with respect to a fixed action a

(B af) (x) == R(x, a)

+, E P(x/lx, a)f(x')
X'

The action-value function Q7r : X x A --7 JR denotes the expected future discounted
reward achieved by taking action a in state x and following policy 1r thereafter;
which must satisfy Q7r (x, a) == B a V 7r ? Given an arbitrary function f over states,
the greedy policy 1rgre (f) with respect to f is defined by
1rgre (I) (x)

== arg max
(B a f) (x)
a

Finally, if we let 1r* denote the optimal policy and
we have the relationship V* == B*V*, where (B* f)
addition, we define Q*(x,a) == BaV* then we also
arg maxa Q* (x, a). Given these definitions, the three
culating 1r* can be formulated as:

V* denote its value function,
(x) == maXa (Ba f) (x). If, in
have 1r*(x) == 1rgre (V*)(x) ==
fundamental methods for cal-

Policy iteration: Start with an arbitrary policy 1r(0). Iterate 1r(i+l)
until1r(i+l) == 1r(i). Return 1r* == 1r(i+I).

f-

1rgre (V 7r (i?)

Value iteration: Start with an arbitrary function f(O). Iterate f(i+l)
untilllf(i+l) - f(i) 1100 < tole Return 1r* == 1rgre (f(i+I)).
Linear programming: Calculate V* == arg min]
(B a f) (x) for all a and x. Return 1r* == 1rgre (V*).

I:x I(x)

f-

B* f(i)

subject to f(x) 2=:

All three methods can be shown to produce optimal policies for the given MDP
[1, 10] even though they do so in very different ways. However, all three approaches
share the same fundamental limitation that they do not scale up feasibly in n, the
size of the state descriptions. Instead, all of these approaches work with explicit
representations of the policies and value functions that are exponential in n.

3

Exploiting structure

To scale up to large state spaces it is necessary to exploit substantial structure in
the MDP while also adopting some form of approximation for the optimal value
function and policy. The two specific structural assumptions we consider in this
paper are (1) factored MDPs and (2) linear value function approximations. Neither
of these two assumptions alone is sufficient to permit efficient policy optimization for
large MDPs. However, combined, the two assumptions allow approximate solutions
to be obtained for problems involving trillions of states reasonably quickly.
3.1

Factored MDPs

In the spirit of [7, 8, 6] we define a factored MDP to be one that can be represented compactly by an additive reward function and a factored state transition model. Specifically, we assume the reward function decomposes as R(x, a) ==
E~=l Ra,r (xa,r) where each local reward function Ra,r is defined on a small set
of variables xa,r' We assume the state transition model P(x/lx, a) can be represented by a set of dynamic Bayesian networks (DBNs) on state variables-one for
each action-where each DBN defines a compact transition model on a directed
bipartite graph connecting state variables in consecutive time steps. Let Xa,i denote the parents of successor variable x~ in the DBN for action a. To allow efficient optimization we assume the patent set Xa,i contains a small number of state
variables from the previous time step. Given this model, the probability of a successor state Xl given a predecessor state x and action a is given by the product
P(x/lx, a) == Il7=1 P(X~IXa,i)'
The main benefit of this factored representation is that it allows large MDPs to
be encoded concisely: if the functions Ra,r(xa,r) and P(X~IXa,i) depend on a small
number of variables, they can be represented by small tables and efficiently combined to determine R(x, a) and P(x'lx, a). Unfortunately, as pointed out in [7],
a factored MDP does not by itself yield a feasible method to determining optimal
policies. The main problem is that, even if P and R are factored, the optimal value
function generally does not have a compact representation (nor does the optimal
policy). Therefore, obtaining an exact solution appears to require a return to explicit representations. However, it turns out that the factored MDP representation
interacts very well with linear value function approximations.
3.2

Linear approximation

One of the central tenets to scaling up is to approximate the optimal value function rather than calculate it exactly. Numerous schemes have been investigated for
approximating optimal value functions and policies in a compact representational
framework, including: hierarchical decompositions [5], decision trees and diagrams
[3, 12], generalized linear functions [1, 13, 4, 7, 8, 6], neural networks [2], and products of experts .[11]. However, the simplest of these is generalized linear functions,
which is the form we investigate below. In this case, we consider functions of the
form f(x)
2:;=1 wjbj(xj) where b1 , ??? , bk are a fixed set of basis functions, and Xj
denotes the variables on which basis bj depends. Combining linear functions with,
factored MDPs provides many opportunities for feasible approximation.

=

The first main benefit of combining linear approximation with factored MDPs is
that the result of applying the backup operator B a to a linear function results in
a compact representation for the action-value function. Specifically if we define

g(X, a) == (B a f) (x) then we can rewrite it as
m

k

g(X, a) == L:Ra,r(xa,r) + L:WjCa,j(Xa,j)
r=l

j=l

where
Ca,j(Xa,j) ==1'L:P(xjla,xa,j)bj (xj) andxa,j
xj

==

U Xa,i
x~Exj

That is, Xa,i are the parent variables of x~, and Xa,j is the union of the parent
variables of x~ E xj. Thus, ca,j expresses the fact that in a factored MDP the
expected future value of one component of the approximation depends only on the
current state variables Xa,j that are direct parents of the variables xj in bj ? If the
MDP is sparsely connected then the variable sets in 9 will not be much larger than
those in f. The ability to represent the state-action value function in a compact
linear form immediately provides a feasible implementation of the greedy policy for
f, since 1rgre (f) (x) == argmaXa g(~, a) by definition of 1rgre , and g(x, a) is efficiently
determinable for each x and a. However, it turns out that this is not enough
to permit feasible forms of approximate policy- and value-iteration to be easily
implemented.
The main problem is that even though Ba f has a factored form for fixed a, B* f does
not and (therefore) neither does 1rgre (f). In fact, even if a policy 1f were concisely
represented, B 1r f would not necessarily have a compact form because 1f usually
depends on all the state variables and thus P(x/lx, 1r(x)) == I17=1 P(x~IX1r(x),i) becomes a product of terms that depend on all the state variables. Here [8, 6] introduce
an additional assumption that there is a special ""default"" action ad for the MDP
such that all other actions a have a factored transition model P (?1?, a) that differs
from P(?I?, ad) only on a small number of state variables. This allows the greedy
policy 1rgre (f) to have a compact form and moreover allows B 1r gre(f) f to be concisely represented. With some effort, it then becomes possible to formulate feasible
versions of approximate policy- and value-iteration [8, 6].
Approximate policy iteration: Start with default policy 1r(O)(x) == ad. Iterate
f(i) +- arg minf maxx If(x) - (B 1r (i) f) (x) I , 1r(i+1) f- 1fgre (f(i)) until1r(i+1) == 1r(i).
Approximate value iteration: Start with arbitrary f(O). Iterate 1r(i) +1rgre (f(i)) ,f(i+1) +- argminf maxx 1!(x)-(B 1r (i) f)(x)1 until Ilf(i+1)_!(i) 1100 < tole
The most expensive part of these iterative algorithms is determining
arg minf maxx If(x) - (B7r(i) f) (x) I which involves solving a linear program minw,E E
subject to -E :S !w (x) - (B 7r fw) (x) :S E for all x. This linear program is problematic
because it involves an exponential number of constraints. A? central achievement of
[6] is to show that this system of constraints can be encoded by an equivalent system
of constraints that has a much more compact form. The idea behind this construction is to realize that searching for the max or a min of a linear function with a
compact basis can be conducted in an organized fashion, and such an organized
search can be encoded in an equally concise constraint system. This construction
allows approximate solutions to MDPs with up to n == 40 state variables (1 trillion
states) to be generated in under 7.5 hours using approximate policy iteration [6].1
1 It turns out that approximate value iteration is less effective because it takes more
iterations to converge, and in fact can diverge in theory [6, 13].

Our main observation is that if one has to solve linear programs to conduct the
approximate iterations anyway, then it might be much simpler and more efficient
to approximate the linear programming approach directly.

4

Approximate linear programming

Our first idea is simply to observe that a factored MDP and linear value approximation immediately allow one to directly solve the linear programming approximation
to the optimal value function, which is given by
IIjin

L f(x) subject to f(x) -

(B a f) (x) ;::: 0 for all x and a

x

where f is restricted to a linear form over a fixed basis. In fact, it is well known [1, 2]
that this yields a linear program in the basis weights w. However, what had not
been previously shown is that given a factored MDP, an equivalent linear program
of feasible size could be formulated. Given the results of [6] outlined above this is
now easy to do. First, one can show that the minimization objective can be encoded
compactly
k

Lf(x)
x

LLWjbj(xj)
x

j=l

k

LWjYj

where Yj

== 2n-lxjl Lbj(xj)
~

j=l

Here the Yj components can be easily precomputed by enumerating assignments
to the small sets of variables in basis functions. Second, as we have seen, the
exponentially many constraints have a structured form. Specifically f (x) - (B a f) (X)
can be represented as
k

f(x) - (B a f) (x)

L
j=l

Wj

(b j (Xj) -

Ca,j (xa,j))

-

L

Ra,r (Xa,r)

r

which has a simple basis representation that allows the technique of [6] to be used
to encode a constraint system that enforces f(x) - (B a f) (x) 2:: 0 for all x and a
without enumerating the state space for each action.
We implemented this approach and tested it on some of the test problems from [6].
In these problems there is a directed network of computer systems Xl, ??? , X n where
each system is either up (Xi == 1) or down (Xi == 0). Systems can spontaneously
go down with some probability at each step, but this probability is increased if an
immediately preceding machine in the network is down. There are n + 1 actions:
do nothing (the default) and reboot machine i. The reward in a state is simply the
sum of systems that are up, with a bonus reward of 1 if system 1 (the server) is
up. I.e., R(x) == 2Xl + 2:7=2 Xi. We considered the network architectures shown in
Figure 1 and used the transition probabilities P(x~ == llxi, parent(Xi) , a == i) == 0.95
and P(x~ == 11Xi, parent(Xi) , a I- i) == 0.9 if Xi == parent(Xi) == 1; 0.67 if Xi == 1 and
parent(xi) == 0; and 0.01 if Xi == o. The discount factor was 'I == 0.95. The first basis
functions we considered were just the indicators on each variable Xi plus a constant
basis function (as reported in [6]).
The results for two network architectures are shown in Figure 1. Our approximate
linear programming method is labeled ALP and is compared to the approxi.mate

server

0

n=
N=
API[6]
APIgen
time
ALP
ALPgen
ALPgen2
APIgen
constraints ALP
ALPgen
ALPgen2
API[6]
DB Bellman APIgen
ALP
(gen)
/ Rmax
ALPgen2

time

constraints

DB Bellman
/ Rmax

12
4e3
7m
39s
4.5s
0.7s
14s
420
1131
38
166
0.3Q'
0.36
0.85
0.12

n=
N=
API[6]
APIgen
ALP
ALPgen
ALPgen2
APIgen
ALP
ALPgen
ALPgen2
API[6]
APIgen
ALP(gen)
ALPgen2

16
6e4
30m
1.'5m
23s
1.2s
37s
777
2023
50
321
, 0.33
0.34
0.82
0.14

13
8e4
5m
28s
0.7s
0.7s
17s
363
729
50
261
0.27
0.50
0.96
0.21

20
1e6
50m
2.3m
1.4m
1.8s
102m
921
3171
62
514
0.34
0.33
0.80
0.08

16
6e4
15m
106m
1.6s
LOs
338
952
1089
69
381
0.29
0.46
0.82
0.22

24
2e7
1.3h
4.0m
4.1m
2.6s
2.8m
1270
4575
74
914
0.35
0.33
0.78
0.08

22
4e6
50m
3.9m
6.0s
1.5s
1.9m
1699
2025
90
826
0.32
0.42
0.78
0.15

28
3e8
1.9h
6.5m
10m
3.5s
4.7m
1591
6235
86
1223
0.36
0.32
0.78
0.10

28
3e8
l.3h
12m
20s
2.4s
5.4m
3792
3249
114
1505
0.34
0.39
0.78
0.06

32
4e9
3h
13m
23m
4.5s
6.4m
2747
8151
98
1433
0.36
0.32
0.77
0.08

34
2e10
2.Th
23m
56s
3.4s
9.6m
6196
4761
135
1925
0.35
0.38
0.77
0.07

36
7e10
4.5h
22m
47m
5.9s
12m
4325
10K
110
1951
0.37
0.32
0.76
0.07

40
1e12
7.5h
28m
2.4h
7.0s
17m
4438
13K
122
2310
0.38
0.31
0.76
0.07

40
1e12
5h
33m
2.2m
4.7s
23m
7636
6561
162
3034
0.36
0.37
0.76
0.03

Figure 1: Experimental results (timings on a 750MHz PIlI processor, except

2)

policy iteration strategy API described in [6]. Since we did not have the specific
probabilities used in [6] and could only estimate the numbers for API from graphs
presented in the paper, this comparison is only meant to be loosely indicative of
the general run times of the two methods on such problems. (Perturbing the probability values did not significantly affect our results, but we implemented APlgen
for comparison.) As in [6] our implementation is based on Matlab, using CPLEX
to solve linear programs. Our preliminary results appear to support the hypothesis that direct linear programming can be more efficient than approximate policy
iteration on problems of this type. A further advantage of the linear programming approach is that it is simpler to program and involves solving only one LP.
More importantly, the direct LP approach does not require the MDP to have a special default action since the action-value function can be directly extracted using
7r gre (f)(x) == argma:xay(x,a) and g is easily recoverable from f.
Before discussing drawbacks, we note that it is possible to solve the linear program
even more efficiently by iteratively generating constraints as needed. This is now
possible because factored MDPs and linear value approximations allow an efficient
search for the maximally violated constraints in the linear program, which provides
an effective way of generating concise linear programs that can be solved much more
efficiently than those formulated above. Specifically, the procedure ALPgen exploits
the feasible search techniques for minimizing linear functions discussed previously
to efficiently generate a small set of critical constraints, which is iteratively grown
until the final solution is identified; see Figure 2.
2These numbers are estimated from graphs in [6]. The exact probabilities and computer
used for the simulations were not reported in that paper, so we cannot assert an exact
comparison. However, perturbed probabilities have little effect .on the performance of the
methods we tried, and it seems that overall this is a loosely representative comparison of
the general performance of the' various algorithms on these problems.

ALPgen
Start with f(O) = 0 and constraints = 0
Loop
For each a E A, compute x a t- arg minx f(i) (x) - (B a f(i)) (x)
I

constraints t- constraints U {constraint(x a1 ), ??? , constraint(x ak ) }
Solve f(i~l) t- minJ 2:~ f(x) subject to constraints
Until minx f(~)(x) - (Baf(~})(x) ~ 0 - tot for all a
Return g(., a) B a f for each a, to represent the greedy policy

=

Figure 2: ALPgen procedure

The rationale for this procedure is that the main bottleneck in the previous methods is generating the constraints, not solving the linear programs [6]. Since only a
small number of constraints are active at a solution and these are likely t.o be the
most violated near the solution, adding only most violated constraints appears to
be a useful way to proceed. Indeed, Figure 1 shows that ALPgen produces the same
approximate solutions as ALP in a tiny fraction of the time. In the most extreme
case ALPgen produces an approximate solution in 7 seconds while other methods
take several hours on the same problem. The reason for this speedup is explained
by the results which show the numbers of constraints generated by each method.
Further investigation is also required to fully outline the robustness of the constraint generation method. In fact, one cannot guarantee that a greedy constraint
generation scheme like the one proposed here will always produce a feasible number
of constraints [9]. Nevertheless, the potential benefits of conservatively generating
constraints as needed seem to be clear. Of course, the main drawback of the direct
linear programming approach over approximate policy iteration is that ALP incurs
larger approximation errors than API.

5

Bounding approximation error

It turns out that neither API nor ALP are guaranteed to return the best linear approximation to the true value function. Nevertheless, it is possible to efficiently calculate bounds on the approximation errors of these methods, again
by exploiting the structure of the problem: A well known result [14] asserts that
maxx V* (x) - V 7rgre (J) (x) :S 1 2, maxx f(x) - (B* f) (x) (where in our case f ~ V*).
This upper bound can in turn be bounded by a quantity that is feasible to calculate:
maxx f(x)-(B* f) (x) = maxxmina f(x)-(Ba f) (x) :S min a maxx f(x)-(B af)(x).
Thus an upper bound on the error from the optimal value function can be calculated
by performing an efficient search for maxx f(x) - (Baf) (x) for each a.
Figure 1 shows that the measurable error quantity, maxx f(x) - (B a f) (x) (reported
as UB Bellman) is about a factor of two larger for the linear programming approach
than for approximate policy iteration on the same basis. In this respect, API appears to have an inherent advantage (although in the limit of an exhaustive basis
both approaches converge to the same optimal value). To get an indication of the
computational cost required for ALPgen to achieve a similar bound on approximation error, we repeated the same experiments with a larger basis set that included all
four indicators between pairs of connected variables. The results for this model are
reported as ALPgen2, and Figure 1 shows that, indeed, the bound on approximation error is reduced substantially-but at the predictable cost of a sizable increase
in computation time. However, the run times are still appreciably smaller than the
policy iteration methods.

Paradoxically, linear programming seems to offer computational advantages over
policy and value iteration in the context of approximation, even though it is widely
held to be an inferior solution strategy for explicitly represented MDPs.

References
[1] D. Bertsekas. Dynamic Programming and Optimal Control, volume 2. Athena
Scientific, 1995.

[2] D. Bertsekas and J. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific,
1996.

[3] C. Boutilier, R. Dearden, and M. Goldszmidt. Stochastic dynamic programming with factored representations. Artificial Intelligence, 2000.
[4] J. Boyan. Least-squares temporal difference learning. In Proceedings ICML,
1999.
[5J T. Dietterich.Hierarchical reinforcement learning vlith the 1\1AXQ value function decomposition. JAIR, 13:227-303,2000.
[6] C. Guestrin, D. Koller, and R. Parr. Max-norm projection for factored MDPs.
?In Proceedings IJCAI, 2001.
[7] D. Koller and R. Parr. Computing factored value functions for policies in
structured MDPs. In Proceedings IJCAI, 1999.
[8] D. Koller and R. Parr. Policy iteration for factored MDPs. In Proceedings
UAI,2000.
.
[9] R. Martin. Large Scale Linear and Integer Optimization. Kluwer, 1999.
[10] M. Puterman. Markov Decision Processes: Discrete Dynamic Programming.
Wiley, 1994.

[IIJ B. Sallans and G. Hinton. Using free energies to represent Q-values in a multiagent reinforcement learning task. In Proceedings NIPS, 2000.
[12] R. St-Aubin, J. Hoey, and C. Boutilier. APRICODD: Approximating policy
construction using decision diagrams. In Proceedings NIPS, 2000.
[13J B. Van Roy. Learning and value function approximation in complex decision
processes. PhD thesis, MIT, EECS, 1998.
[14J R. Williams and L. Baird. Tight performance bounds on greedy policies based
_on imperfect value functions. Technical report, Northeastern University, 1993.

"
2001,The Infinite Hidden Markov Model,,1956-the-infinite-hidden-markov-model.pdf,Abstract Missing,"The Infinite Hidden Markov Model
Matthew J. Beal

Zoubin Ghahramani

Carl Edward Rasmussen

Gatsby Computational Neuroscience Unit
University College London
17 Queen Square, London WC1N 3AR, England
http://www.gatsby.ucl.ac.uk
m.beal,zoubin,edward @gatsby.ucl.ac.uk



Abstract
We show that it is possible to extend hidden Markov models to have
a countably infinite number of hidden states. By using the theory of
Dirichlet processes we can implicitly integrate out the infinitely many
transition parameters, leaving only three hyperparameters which can be
learned from data. These three hyperparameters define a hierarchical
Dirichlet process capable of capturing a rich set of transition dynamics.
The three hyperparameters control the time scale of the dynamics, the
sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a finite sequence. In this framework it
is also natural to allow the alphabet of emitted symbols to be infinite?
consider, for example, symbols being possible words appearing in English text.

1 Introduction
Hidden Markov models (HMMs) are one of the most popular methods in machine
learning and statistics for modelling sequences such as speech and proteins. An
HMM defines a probability distribution over sequences of observations (symbols)
by invoking another sequence of unobserved, or hidden, discrete
state variables
. The basic idea in an HMM is that the seis independent of
quence of hidden states has Markov dynamics?i.e. given ,
for all
?and that the observations
are independent of all other variables
given . The model is defined in terms of two sets of parameters, the transition matrix
whose
element is
and the emission matrix whose
element
is
. The usual procedure for estimating the parameters of an HMM is
the Baum-Welch algorithm, a special case of EM, which estimates expected values of two
matrices and corresponding to counts of transitions and emissions respectively, where
the expectation is taken over the posterior probability of hidden state sequences [6].


	
			    		  		 


'

! #""$&%  *12  
.0/  (*);+-, 9<4  (=7 .0/ 3)54 6(87
> ?




  

(:9+-,

Both the standard estimation procedure and the model definition for HMMs suffer from
important limitations. First, maximum likelihood estimation procedures do not consider
the complexity of the model, making it hard to avoid over or underfitting. Second, the
model structure has to be specified in advance. Motivated in part by these problems there
have been attempts to approximate a full Bayesian analysis of HMMs which integrates over,
rather than optimises, the parameters. It has been proposed to approximate such Bayesian
integration both using variational methods [3] and by conditioning on a single most likely
hidden state sequence [8].

In this paper we start from the point of view that the basic modelling assumption of
HMMs?that the data was generated by some discrete state variable which can take on
one of several values?is unreasonable for most real-world problems. Instead we formulate the idea of HMMs with a countably infinite number of hidden states. In principle,
such models have infinitely many parameters in the state transition matrix. Obviously it
would not be sensible to optimise these parameters; instead we use the theory of Dirichlet
processes (DPs) [2, 1] to implicitly integrate them out, leaving just three hyperparameters
defining the prior over transition dynamics.
The idea of using DPs to define mixture models with infinite number of components has
been previously explored in [5] and [7]. This simple form of the DP turns out to be inadequate for HMMs.1 Because of this we have extended the notion of a DP to a two-stage hierarchical process which couples transitions between different states. It should be stressed
that Dirichlet distributions have been used extensively both as priors for mixing proportions and to smooth n-gram models over finite alphabets [4], which differs considerably
from the model presented here. To our knowledge no one has studied inference in discrete
infinite-state HMMs.
We begin with a review of Dirichlet processes in section 2 which we will use as the basis
for the notion of a hierarchical Dirichlet process (HDP) described in section 3. We explore
properties of the HDP prior, showing that it can generate interesting hidden state sequences
and that it can also be used as an emission model for an infinite alphabet of symbols. This
infinite emission model is controlled by two additional hyperparameters. In section 4 we
describe the procedures for inference (Gibbs sampling the hidden states), learning (optimising the hyperparameters), and likelihood evaluation (infinite-state particle filtering).
We present experimental results in section 5 and conclude in section 6.

  (

2 Properties of the Dirichlet Process

 *12
*12

Let us examine in detail the statistics of hidden state transitions
 from a particular state
to
, with the number of hidden states finite and equal to . The transition probabilities
row of the transition
matrix can be interpreted as mixing proportions for
given in the

that we call 
.

( +-,  		 

	 	  from a discrete indicator variable which can take
Imagine drawing
samples
>


	






	
on values
 with proportions given by . The joint distribution of these indicators
is multinomial
.0/  		 4 7   	 with >   / 8	 )7
(1)
	 7 iff  , and otherwise)
where we have used the Kronecker-delta
function ( /

*

2
1

0) has been drawn. Let us see what happens to
to count the number of times > that
the distribution of these indicators when we integrate out the mixing proportions under a



	












 
















 
 

   






 















conjugate prior. We give the mixing proportions a symmetric Dirichlet prior with positive
concentration hyperparameter 


.0/ 4 27





/ 	 	 7$ / / 7 7 

""!$#&%'#)(*+&,.- 0/



0/



1

2

1
0/

 


43
5


 	

76

(2)

where  is restricted to be on the simplex of mixing proportions that sum to 1. We can
analytically integrate out  under this prior to yield:
1
That is, if we only applied the mechanism described in section 2, then state trajectories under the
prior would never visit the same state twice; since each new state will have no previous transitions
from it, the DP would choose randomly between all infinitely many states, therefore transitioning to
another new state with probability 1.



0. /  		 4 7  .0/  		 4 78.0/ 4 27  /->/ 27 27  /*> /  7 7  (3)
Thus
of a particular sequence of indicators is only a function of the counts
	the 	 probability
>indicators
> (denoted
 . The conditional
probability of an indicator given the setting of all other
 	 ) is given by
	
.0/   )54  
 	 27  > > 
   	
(4)

 is the counts as in (1) with the '+-, indicator removed. Note the self-reinforcing
where >

property of (4): is more likely to choose an already popular state. A key property of DPs,


' 








' 




2



1



1









1

1

0/

0/






6



6





6

6





0/









1 

which is at the very heart of the model in this paper, is the expression for (4) when we take
tends to infinity:
the limit as the number of hidden 
 states


) '	 	  i.e. represented

3

6
 	
0. /  )54  27  
 63 1 3 for all unrepresented ) , combined
(5)
 is the number of represented states (i.e. for which > 6	   ), which cannot
where

 6



	 	 > 



be infinite
of
  since.  is finite.  can be interpreted as the number of pseudo-observations

/
/
, i.e. the strength of belief in the symmetric prior.2 In the infinite limit
 acts as an ?innovation? parameter, controlling the tendency for the model to populate a
previously unrepresented state.

3 Hierarchical Dirichlet Process (HDP)
We now consider modelling each row of the transition and emission matrices of an HMM as
a DP. Two key results from the previous section form the basis of the HDP model for infinite
HMMs. The first is that we can integrate out the infinite number of transition parameters,
and represent the process with a finite number of indicator variables. The second is that
under a DP there is a natural tendency to use existing transitions in proportion to their
previous usage, which gives rise to typical trajectories. In sections 3.1 and 3.2 we describe
in detail the HDP model for transitions and emissions for an infinite-state HMM.
3.1 Hidden state transition mechanism

"" >! 
""      8	  >   1  	
(
)
  (
 *12
/ 8( 7 / ) 7
( +-,
>
(6)
.0/ *12  )54   ( 	 > 	 27  $"" #  > %>    ) 	 	  
Note
do not sum to 1?under the DP there is a finite probability
</ "" that> the above7 ofprobabilities
not selecting
transitions. In this case, the model defaults
 *12one
 withof these
to a second different DP (5) on
parameter & whose counts are given by a vector
counts as the oracle. Given that we have
> ' . We refer to the default DP and its associated
defaulted to the oracle DP, the probabilities
now become
*) 	1 of1 transitioning
'

	





	

  i.e. ) represented 	
  +-, /.0 1 ) ) 


	
	
*

2
1


.0/  )54  ( > ' &
7 ( +-, /.0 ) 	1 1 3) 2 '	 	  i.e. ) is a new state  (7)

Imagine we have generated a hidden state sequence up to and including time , building
6
a table
of counts  for transitions that have occured so far from state to , i.e. 



. Given that we are in state
, we impose on state
a DP


(5) with parameter  whose counts are those entries in the
row of , i.e. we prefer to
reuse transitions we have used before and follow typical trajectories (see Figure 1):




0/

 

 

 



 











 


 




 



4

2
Under the infinite model, at any time, there are an infinite number of (indistinguishable) unrepresented states available, each of which have infinitesimal mass proportional to .

nii + ?

? nij + ? + ?
j

self
transition

nij

a)

b)

c)

d)

?

? nij + ? + ? ?nij + ? + ?
j

j

existing
transition

oracle

j=i

njo

?

? n jo + ?

? n jo + ?

existing
state

new
state

j

j

 (time along horizontal axis) from the HDP: we give examples of four modes of
	

 4

 , explores many states with a sparse transition matrix. (b)
 4

 , retraces multiple interacting trajectory segments. (c)  ! 4""# ,
switches between a few different states. (d) $
 4%&
#'&
 , has strict left-to-right transition
Figure 1:

(left) State transition generative mechanism. (right a-d) Sampled state trajectories

of length
behaviour. (a)

dynamics with long linger time.

&

Under the oracle, with probability proportional to an entirely new state is transitioned
to. This is the only mechanism for visiting new
 states from the infinitely many available to

us. After each transition we set 
and, if we transitioned
to the state via the


. If we transitioned to a new
oracle DP just described then in addition we set 
state then the size of and
will increase.

>

>'

>  )( >% 

)

>!' ( > ' 

Self-transitions are special because their probability defines a time scale over which the
dynamics of the hidden state evolves. We assign a finite prior mass to self transitions for
each state; this is the third hyperparameter in our model. Therefore, when first visited (via
in the HDP), its self-transition count is initialised to .

&

*

*

The full hidden state transition mechanism is a two-level DP hierarchy shown in decision
tree form in Figure 1. Alongside are shown typical state trajectories under the prior with
different hyperparameters. We can see that, with just three hyperparameters, there are a
wealth of types of possible trajectories. Note that controls the expected number of represented hidden states, and  influences the tendency to explore new transitions, corresponding to the size and density respectively of the resulting transition count matrix. Finally
controls the prior tendency to linger in a state.

&

*

The role of the oracle is two-fold. First it serves to couple the transition DPs from different
hidden states. Since a newly visited state has no previous transitions to existing states,
without an oracle (which necessarily has knowledge of all represented states as it created
them) it would transition to itself or yet another new state with probability 1. By consulting
the oracle, new states can have finite probability of transitioning to represented states. The
second role of the oracle is to allow some states to be more influential (more commonly
transitioned to) than others.

 ,+  

3.2 Emission mechanism

 -+  *12

The emission process
is identical to the transition process
in every
respect except that there is no concept analogous to a self-transition. Therefore we need
6
only introduce two further hyperparameters  and
for the emission HDP. Like for state
2
2

which is the number
transitions we keep a table of counts


of times before that state has emitted symbol , and
is the number of times symbol

""

(

/.   &).
? 10  ""  9  / ?  8	' (=7 /   8	 9 7
0

miq

?e

?miq + ?e

? miq + ?e

2

10

2500

2000

q

q

existing
emission

oracle

1500
1

10
1000

?e

mq

?q mqo + ?e
existing
symbol

? mqo + ?e
q

new
symbol

500

0
0

0.5

1

1.5

2

0

2.5
4

x 10

10

0

20

40

60

80

100

Figure 2:

(left) State emission generative mechanism. (middle) Word occurence for entire Alice
novel: each word is assigned a unique integer identity as it appears. Word identity (vertical) is plotted
against the word position (horizontal) in the text. (right) (Exp 1) Evolution of number of represented
(vertical), plotted against iterations of Gibbs sweeps (horizontal) during learning of the
states
ascending-descending sequence which requires exactly 10 states to model the data perfectly. Each
line represents initialising the hidden state to a random sequence containing
distinct represented states. (Hyperparameters are not optimised.)

""
 !


9

has been emitted using the emission oracle.

For some applications the training sequence is not expected to contain all possible observation symbols. Consider the occurence of words in natural text e.g. as shown in Figure 2
(middle) for the Alice novel. The upper envelope demonstrates that new words continue to
appear in the novel. A property of the DP is that the expected number of distinct symbols
(i.e. words here) increases as the logarithm of the sequence length. The combination of
an HDP for both hidden states and emissions may well be able to capture the somewhat
super-logarithmic word generation found in Alice.

4 Inference, learning and likelihoods

 	 	
 

	 	& 	 .	& .

Given a sequence of observations, there are two sets of unknowns in the infinite HMM:
, and the five hyperparameters
the hidden state sequence


defining the transition and emission HDPs. Note that by using HDPs for both states and
observations, we have implicitly integrated out the infinitely many transition and emission
parameters. Making an analogy with non-parametric models such as Gaussian Processes,
we define a learned model as a set of counts
and optimised hyperparameters


.

*

> 	 >!' 	 ? 	 ?3'

* 	 	 & 	 . 	 & .

We first describe an approximate Gibbs sampling procedure for inferring the posterior over
the hidden state sequence. We then describe hyperparameter optimisation. Lastly, for calculating the likelihood we introduce an infinite-state particle filter. The following algorithm
summarises the learning procedure:
1. Instantiate a random hidden state sequence
2. For

  
#




	
 .

- Gibbs sample given hyperparameter settings, count matrices, and observations.
- Update count matrices to reflect new ; this may change , the number of represented hidden states.
3. End
4. Update hyperparameters
given hidden state statistics.
5. Goto step 2.






  4 #  4 #


4.1 Gibbs sampling the hidden state sequence

>

?  

> ' > ?  ' ?

Define and as the results of removing from and the transition and emission counts
contributed by . Define similar items
and
related to the transition and emission

/ 7
 ""  	 	

 		*12  / 7
	>  ?  	 >  ' 	 ?3 '  
In order to facilitate hyperparameter learning and improve the mixing
time
of the Gibbs



	

*

2
1


	

sampler,
we
also
sample
a
set
of
auxiliary
indicator
variables
.  alongside
 ; 	each
is a binary variable denoting whether the oracle was used to generate
 *12 	of  these
 respectively.






oracle vectors. An exact Gibbs sweep of the hidden state from
takes 
operations, since under the HDP generative process changing affects the probability of
all subsequent hidden state transitions and emissions.3 However this computation can be
reasonably approximated
in 
, by basing the Gibbs update for only on the state of
6
its neigbours
and the total counts
.4




	 	 & 	 . 	 )& .

4.2 Hyperparameter optimisation

*





We place vague Gamma priors5 on the hyperparameters
. We derive an
approximate form for the hyperparameter posteriors from (3) by treating each level of the
HDPs separately. The following expressions for the posterior for ,  , and  are accurate
for large , while the expressions for and
are exact:

&

*

&

'.

& . 6
# #
.0/ * 	  4 7 / 	' 7 / 3 	 3 7     1 / * / *7  27 / "" 1  /*>%>      ** 7  27 	
# 1  . # 1/2 .7 	

	

	
.0/2 . 4  27 / 3 3 7    / "" 0 ?  1 0   . 7
1
# / &
7 	
#
' 1
	
1
&
.0/ & 4 7 / 7 / ' 1  &7 .0/ & . 4  	 27 / 1 	' 1 7 & / ' . 1  / &)&. 7 . 7
1
  is the number of1 represented states that are transitioned to from
( (includwhere
  is the number of possible emissions from state (state
. ' and ' .
ing itself); similarly .


	





 


 












	





 

 

	 . 



are the number of times the oracle has been
for the transition and emission processes,
 used

calculated from the indicator variables
. We solve for the maximum a posteriori
(MAP) setting for each hyperparameter; for example  MAP is obtained as the solution to
following equation using gradient following techniques such as Newton-Raphson:

/.

"" #    .  /	 .  / . 7!
 / "" 0 ?3 0   . 7 


 

MAP



!

MAP

MAP #""



3 

 / 3 
  '7 /	 .   


MAP

4.3 Infinite-state particle filter
The likelihood for a particular observable sequence of symbols involves intractable sums
over the possible hidden state trajectories. Integrating out the parameters in any HMM
induces long range dependencies between states. In particular, in the DP, making the transition
makes that transition more likely later on in the sequence, so we cannot use
standard tricks like dynamic programming. Furthermore, the number of distinct states can
grow with the sequence length as new states are generated. If the chain starts with distinct states, at time there could be
possible distinct states making the total number
%$ /
$.
of trajectories over the entire length of the sequence

(+ )

  ""

""

/  7 



3
Although the hidden states in an HMM satisfy the Markov condition, integrating out the parameters induces these long-range dependencies.
4
This approximation can be motivated in the following way. Consider sampling parameters &
from the posterior distribution '()&+* , .-%/ of parameter matrices, which will depend on the count
matrices. By the Markov property, for a given & , the probability of only depends on 10 , 2 and
43 , and can therefore be computed without considering its effect on future states.
0 .E 0GF1H
5 567
(48 :9;/ <9=?>A@B(48C/+D 5 =
, with 8 and 9 the shape and inverse-scale parameters.

 	



 )

	



 	 

We propose estimating the likelihood of a test sequence given a learned model using particle
filtering. The idea is to start with some number of particles distributed on the represented
hidden states according to the final state marginal from the training sequence
(some of the

may fall onto new states).6 Starting from the set of particles
, the tables from
 the recursive procedure is as specified
the training sequences
,
and

6
 
below, where
:


  		  

> 	 	>  '  
	 ?  	 ?3'
  "" ""    	 



	






/ 7
.0/ 4
7

     
  #
 	   
 	

  #

 
 	
  	       
  

Compute 	 <'(42 *

 / for each particle  .
  '(42 * 2
( >
 /

:2 10 / .
Calculate 

Resample  particles 
 6 ( %>
	 /

  (
 / .


Update transition and emission tables   ,   for each particle.
6 '( 43 *
For each  sample forward dynamics: 
 3
   / ; this may
cause particles to land on novel states. Update  and  .
6. If !
, Goto 1 with
"" .

1.
2.
3.
4.
5.



"" 



The log likelihood of the test sequence is computed as
+#%$& . Since it is a discrete
state space, with much of the probability mass concentrated on the represented states, it is
feasible to use '
particles.

/ 7

5 Synthetic experiments
Exp 1: Discovering the number of hidden states We applied the infinite HMM inference algorithm to the ascending-descending observation sequence consisting of 30 concatenated copies of (*),+.-0/213/4-5+.) . The most parsimonious HMM which models this
data perfectly has exactly 10 hidden states. The infinite HMM was initialised with a random hidden state sequence, containing distinct represented states. In Figure 2 (right) we
show how the number of represented states evolves with successive Gibbs sweeps, starting
from a variety of initial . In all cases converges to 10, while occasionally exploring 9
and 11.









Exp 2: Expansive A sequence of length 76   was generated from a 4-state 8-symbol
HMM with the transition and emission probabilities as shown in Figure 3 (top left).



Exp 3: Compressive A sequence of length 86   was generated from a 4-state 3-symbol
HMM with the transition and emission probabilities as shown in Figure 3 (bottom left).

 

In both Exp 2 and Exp 3 the infinite HMM was initialised with a hidden state sequence
with :9 distinct states. Figure 3 shows that, over successive Gibbs sweeps and hyperparameter learning, the count matrices for the infinite HMM converge to resemble the true
probability matrices as shown on the far left.

6 Discussion
We have shown how a two-level Hierarchical Dirichlet Process can be used to define a nonparametric Bayesian HMM. The HDP implicity integrates out the transition and emission
parameters of the HMM. An advantage of this is that it is no longer necessary to constrain
the HMM to have finitely many states and observation symbols. The prior over hidden state
transitions defined by the HDP is capable of producing a wealth of interesting trajectories
by varying the three hyperparameters that control it.
We have presented the necessary tools for using the infinite HMM, namely a linear-time
approximate Gibbs sampler for inference, equations for hyperparameter learning, and a
particle filter for likelihood evaluation.
6
Different particle initialisations apply if we do not assume that the test sequence immediately
follows the training sequence.

True transition and
emission probability
matrices used for Exp 2

0

0

 
True transition and
emission probability
matrices used for Exp 3

0

 

 

0



 







0













0







00



0



 





00






0







0



 



0







	
  	
 

Figure 3:

The far left pair of Hinton diagrams represent the true transition and emission probabilities used to generate the data for each experiment 2 and 3 (up to a permutation of the hidden
states; lighter boxes correspond to higher values). (top row) Exp 2: Expansive HMM. Count matrix
are displayed after
sweeps of Gibbs sampling. (bottom
row) Exp 3:
pairs  



sweeps of
Compressive HMM. Similar to top row displaying count matrices after
Gibbs sampling. In both rows the display after a single Gibbs sweep has been reduced in size for
clarity.

 ) 

 
 !

!
   

""

!
 !   

On synthetic data we have shown that the infinite HMM discovers both the appropriate
number of states required to model the data and the structure of the emission and transition
matrices. It is important to emphasise that although the count matrices found by the infinite
HMM resemble point estimates of HMM parameters (e.g. Figure 3), they are better thought
of as the sufficient statistics for the HDP posterior distribution over parameters.
We believe that for many problems the infinite HMM?s flexibile nature and its ability to
automatically determine the required number of hidden states make it superior to the conventional treatment of HMMs with its associated difficult model selection problem. While
the results in this paper are promising, they are limited to synthetic data; in future we hope
to explore the potential of this model on real-world problems.
Acknowledgements
The authors would like to thank David Mackay for suggesting the use of an oracle, and
Quaid Morris for his Perl expertise.

References
[1] C. E. Antoniak. Mixtures of Dirichlet processes with applications to Bayesian nonparametric
problems. Annals of Statistics, 2(6):1152?1174, 1974.
[2] T. S. Ferguson. A Bayesian analysis of some nonparametric problems. Annals of Statistics,
1(2):209?230, March 1973.
[3] D. J. C. MacKay. Ensemble learning for hidden Markov models. Technical report, Cavendish
Laboratory, University of Cambridge, 1997.
[4] D. J. C. MacKay and L. C. Peto. A hierarchical Dirichlet language model. Natural Language
Engineering, 1(3):1?19, 1995.
[5] R. M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Technical
Report 9815, Dept. of Statistics, University of Toronto, 1998.
[6] L. R. Rabiner and B. H. Juang. An introduction to hidden Markov models. IEEE Acoustics,
Speech & Signal Processing Magazine, 3:4?16, 1986.
[7] C. E. Rasmussen. The infinite Gaussian mixture model. In Advances in Neural Information
Processing Systems 12, Cambridge, MA, 2000. MIT Press.
[8] A. Stolcke and S. Omohundro. Hidden Markov model induction by Bayesian model merging. In
S. J. Hanson, J. D. Cowan, and C. L. Giles, editors, Advances in Neural Information Processing
Systems 5, pages 11?18, San Francisco, CA, 1993. Morgan Kaufmann.

"
2014,Bandit Convex Optimization: Towards Tight Bounds,Poster,5377-bandit-convex-optimization-towards-tight-bounds.pdf,"Bandit Convex Optimization (BCO) is a fundamental framework for decision making under uncertainty, which generalizes many problems from the realm of online and statistical learning. While the special case of linear cost functions is well understood, a gap on the attainable regret for BCO with nonlinear losses remains an important open question. In this paper we take a step towards understanding the best attainable regret bounds for BCO: we give an efficient and near-optimal regret algorithm for BCO with strongly-convex and smooth loss functions. In contrast to previous works on BCO that use time invariant exploration schemes, our method employs an exploration scheme that shrinks with time.","Bandit Convex Optimization: Towards Tight Bounds

Kfir Y. Levy
Technion?Israel Institute of Technology
Haifa 32000, Israel
kfiryl@tx.technion.ac.il

Elad Hazan
Technion?Israel Institute of Technology
Haifa 32000, Israel
ehazan@ie.technion.ac.il

Abstract
Bandit Convex Optimization (BCO) is a fundamental framework for decision
making under uncertainty, which generalizes many problems from the realm of online and statistical learning. While the special case of linear cost functions is well
understood, a gap on the attainable regret for BCO with nonlinear losses remains
an important open question. In this paper we take a step towards understanding
the best attainable regret bounds for BCO: we give an efficient and near-optimal
regret algorithm for BCO with strongly-convex and smooth loss functions. In contrast to previous works on BCO that use time invariant exploration schemes, our
method employs an exploration scheme that shrinks with time.

1

Introduction

The power of Online Convex Optimization (OCO) framework is in its ability to generalize many
problems from the realm of online and statistical learning, and supply universal tools to solving
them. Extensive investigation throughout the last decade has yield efficient algorithms with worst
case guarantees. This has lead many practitioners to embrace the OCO framework in modeling and
solving real world problems.
One of the greatest challenges in OCO is finding tight bounds to the problem of Bandit Convex
Optimization (BCO). In this ?bandit? setting the learner observes the loss function only at the point
that she has chosen. Hence, the learner has to balance between exploiting the information she has
gathered and between exploring the new data. The seminal work of [5] elegantly resolves this
?exploration-exploitation? dilemma by devising a combined explore-exploit gradient descent algorithm. They obtain a bound of O(T 3/4 ) on the expected regret for the general case of an adversary
playing bounded and Lipschitz-continuous convex losses.
In this paper we investigate the BCO setting assuming that the adversary is limited to inflicting
strongly-convex and smooth losses and the player may choose points from a constrained
decision
?
? T ). This rate is
set. In this setting we devise an efficient algorithm that achieves a regret of O(
the best possible up?to logarithmic factors as implied by a recent work of [11], cleverly obtaining a
lower bound of ?( T ) for the same setting.
During our analysis, we develop a full-information algorithm that takes advantage of the strongconvexity of loss functions and uses a self-concordant barrier as a regularization term. This algorithm enables us to perform ?shrinking exploration? which is a key ingredient in our BCO algorithm.
Conversely, all previous works on BCO use a time invariant exploration scheme.
This paper is organized as follows. In Section 2 we introduce our setting and review necessary
preliminaries regarding self-concordant barriers. In Section 3 we discuss schemes to perform single1

Setting
Full-Info.
BCO

Convex
? 3/4 )
O(T

Linear
?
?( ?T )
? T)
O(

Smooth

Str.-Convex

? 2/3 )
O(T
?
?( T )

Str.-Convex & Smooth
?(log
? T)
? T ) [Thm. 10]
O(

Table 1: Known regret bounds in the Full-Info./ BCO setting. Our new result is highlighted, and
? 2/3 ) bound.
improves upon the previous O(T
point gradient estimations, then we define first-order online methods and analyze the performance
of such methods receiving noisy gradient estimates. Our main result is described and analyzed in
Section 4; Section 5 concludes.
1.1

Prior work

For BCO with general convex loss functions, almost simultaneously to [5], a bound of O(T 3/4 )
was also obtained by [7] for the setting of?
Lipschitz-continuous convex losses. Conversely, the best
known lower bound for this problem is ?( T ) proved for the easier full-information setting.
In case the adversary is limited to using linear losses, it can be shown that the player does not
?pay? for exploration; this property was
? used by [4] to devise the Geometric Hedge algorithm that
? T ). Later [1], inspired by interior point methods, devised the
achieves an optimal regret rate of O(
first efficient algorithm that attains the same nearly-optimal regret rate for this setup of bandit linear
optimization.
For some special classes of nonlinear convex losses, there are several works that lean on ideas
from [5] to achieve improved upper bounds for BCO. In the case of convex and smooth losses [9]
? 2/3 ). The same regret rate of O(T
? 2/3 ) was achieved by [2] in the
attained an upper bound of O(T
case of strongly-convex losses. For the special?case of unconstrained BCO with strongly-convex
? T ). A recent paper by Shamir [11], significantly
and smooth losses, [2] obtained a regret of O(
?
advanced our understanding of BCO by devising a lower bound of ?( T ) for the setting of stronglyconvex and smooth BCO. The latter implies the tightness of our bound.
A comprehensive survey by Bubeck and Cesa-Bianchi [3], provides a review of the bandit optimization literature in both stochastic and online setting.

2

Setting and Background

Notation: During this paper we denote by || ? || the `2 norm when referring to vectors, and use
the same notation for the spectral norm when referring to matrices. We denote by Bn and Sn the
n-dimensional euclidean unit ball and unit sphere, and by v ? Bn and u ? Sn random variables
chosen uniformly from these sets. The symbol I is used for the identity matrix (its dimension will
be clear from the context). For a positive definite matrix A  0 we denote by A1/2 the matrix B
such that B > B = A, and by A?1/2 the inverse of B. Finally, we denote [N ] := {1, . . . , N }.
2.1

Bandit Convex Optimization

We consider a repeated game of T rounds between a player and an adversary, at each round t ?
[T ]
1. player chooses a point xt ? K.

2. adversary independently chooses a loss function ft ? F.
3. player suffers a loss ft (xt ) and receives a feedback Ft .
2

In the OCO (Online Convex Optimization) framework we assume that the decision set K is convex and that all functions in F are convex. Our paper focuses on adversaries limited to choosing
functions from the set F?,? ; the set off all ?-strongly-convex and ?-smooth functions.
We also limit ourselves to oblivious adversaries where the loss sequence {ft }Tt=1 is predetermined
and is therefore independent of the player?s choices. Mind that in this case the best point in hindsight
is also independent of the player?s choices. We also assume that the loss functions are defined over
the entire space Rn and are strongly-convex and smooth there; yet the player may only choose points
from a constrained set K.
Let us define the regret of A, and its regret with respect to a comparator w ? K:
RegretA
T =

T
X
t=1

ft (xt ) ? min
?

w ?K

T
X

RegretA
T (w) =

ft (w? ),

t=1

T
X
t=1

ft (xt ) ?

T
X

ft (w)

t=1

A player aims at minimizing his regret, and we are interested in players that ensure an o(T ) regret
for any loss sequence that the adversary may choose.
The player learns through the feedback Ft received in response to his actions. In the full informations
setting, he receives the loss function ft itself as a feedback, usually by means of a gradient oracle i.e. the decision maker has access to the gradient of the loss function at any point in the decision set.
Conversely, in the BCO setting the given feedback is ft (xt ), i.e., the loss function
only
 at the point

that he has chosen; and the player aims at minimizing his expected regret, E RegretA
T .
2.2

Strong Convexity and Smoothness

As mentioned in the last subsection we consider an adversary limited to choosing loss functions
from the set F?,? , the set of ?-strongly convex and ?-smooth functions, here we define these properties.
Definition 1. (Strong Convexity) We say that a function f : Rn ? R is ?-strongly convex over the
set K if for all x, y ? K it holds that,
f (y) ? f (x) + ?f (x)> (y ? x) +

?
||x ? y||2
2

(1)

Definition 2. (Smoothness) We say that a convex function f : Rn ? R is ?-smooth over the set K
if the following holds:
f (y) ? f (x) + ?f (x)> (y ? x) +
2.3

?
||x ? y||2 ,
2

?x, y ? K

(2)

Self Concordant Barriers

Interior point methods are polynomial time algorithms to solving constrained convex optimization
programs. The main tool in these methods is a barrier function that encodes the constrained set and
enables the use of a fast unconstrained optimization machinery. More on this subject can be found
in [8].
Let K ? Rn be a convex set with a non empty interior int(K)
Definition 3. A function R : int(K) ? R is called ?-self-concordant if:
1. R is three times continuously differentiable and convex, and approaches infinity along any
sequence of points approaching the boundary of K.
2. For every h ? Rn and x ? int(K) the following holds:
|?3 R(x)[h, h, h]| ? 2(?2 R(x)[h, h])3/2
3

and

|?R(x)[h]| ? ? 1/2 (?2 R(x)[h, h])1/2

here, ?3 R(x)[h, h, h] :=

?3
?t1 ?t2 ?t3 R(x



+ t1 h + t2 h + t3 h)

t1 =t2 =t3 =0

.

?
Our algorithm requires a ?-self-concordant barrier over K, and its regret depends on ?. It is well
n
known that any convex set in R admits a ? = O(n) such barrier (? might be much smaller), and that
most interesting convex sets admit a self-concordant barrier that is efficiently represented.
The Hessian of a self-concordant barrier induces a local norm at every x ? int(K), we denote this
norm by || ? ||x and its dual by || ? ||?x and define ?h ? Rn :
q
q
||h||x = h> ?2 R(x)h,
||h||?x = h> (?2 R(x))?1 h
we assume that ?2 R(x) always has a full rank.

The following fact is a key ingredient in the sampling scheme of BCO algorithms [1, 9]. Let R is
be self-concordant barrier and x ? int(K) then the Dikin Ellipsoide,
W1 (x) := {y ? Rn : ||y ? x||x ? 1}

(3)

i.e. the || ? ||x -unit ball centered around x, is completely contained in K.

Our regret analysis requires a bound on R(y) ? R(x); hence, we will find the following lemma
useful:
Lemma 4. Let R be a ?-self-concordant function over K, then:
R(y) ? R(x) ? ? log

1
,
1 ? ?x (y)

where ?x (y) = inf{t ? 0 : x + t?1 (y ? x) ? K},

?x, y ? int(K)

?x, y ? int(K)

Note that ?x (y) is called the Minkowsky function and it is always in [0, 1]. Moreover, as y approaches the boundary of K then ?x (y) ? 1.

3
3.1

Single Point Gradient Estimation and Noisy First-Order Methods
Single Point Gradient Estimation

A main component of BCO algorithms is a randomized sampling scheme for constructing gradient estimates. Here, we survey the previous schemes as well as the more general scheme that we
use.
Spherical estimators: Flaxman et al. [5] introduced a method that produces single point gradient
estimates through spherical sampling. These estimates are then inserted into a full-information procedure that chooses the next decision point for the player. Interestingly, these gradient estimates are
unbiased predictions for the gradients of a smoothed version function which we next define.
Let ? > 0 and v ? Bn , the smoothed version of a function f : Rn ? R is defined as follows:
f?(x) = E[f (x + ?v)]

(4)

The next lemma of [5] ties between the gradients of f? and an estimate based on samples of f :
Lemma 5. Let u ? Sn , and consider the smoothed version f? defined in Equation (4), then the
following applies:
n
(5)
?f?(x) = E[ f (x + ?u)u]
?
Therefore, n? f (x + ?u)u is an unbiased estimator for the gradients of the smoothed version.
4

x

x

x
t

K

K

(a) Eigenpoles Sampling

(b) Continuous Sampling

K

(c) Shrinking Sampling

Figure 1: Dikin Ellipsoide Sampling Schemes
Ellipsoidal estimators: Abernethy et al. [1] introduced the idea of sampling from an ellipsoid
(specifically the Dikin ellipsoid) rather than a sphere in the context of BCO. They restricted the
sampling to the eigenpoles of the ellipsoid (Fig. 1a). A more general method of sampling continuously from an ellipsoid was introduced in [9] (Fig. 1b). We shall see later that our?algorithm
? T ) regret
uses a ?shrinking-sampling? scheme (Fig. 1c), which is crucial in achieving the O(
bound.
The following lemma of [9] shows that we can sample f non uniformly over all directions and create
an unbiased gradient estimate of a respective smoothed version:
Corollary 6. Let f : Rn ? R be a continuous function, let A ? Rn?n be invertible, and v ? Bn ,
u ? Sn . Define the smoothed version of f with respect to A:
f?(x) = E[f (x + Av)]
(6)
Then the following holds:
?f?(x) = E[nf (x + Au)A?1 u]

(7)

Note that if A  0 then {Au : u ? Sn } is an ellipsoid?s boundary.

Our next lemma shows that the smoothed version preserves the strong-convexity of f , and that we
can measure the distance between f? and f using the spectral norm of A2 :
Lemma 7. Consider a function f : Rn ? R, and a positive definite matrix A ? Rn?n . Let f? be
the smoothed version of f with respect to A as defined in Equation (6). Then the following holds:
? If f is ?-strongly convex then so is f?.

? If f is convex and ?-smooth, and ?max be the largest eigenvalue of A then:
?
?
0 ? f?(x) ? f (x) ? ||A2 ||2 = ?2max
2
2

(8)

Remark: Lemma 7 also holds if we define the smoothed version of f as f?(x) = Eu?Sn [f (x + Au)]
i.e. an average of the original function values over the unit sphere rather than the unit ball as defined
in Equation (6). Proof is similar to the one of Lemma 7.
3.2

Noisy First-Order Methods

Our algorithm utilizes a full-information online algorithm, but instead of providing this method with
exact gradient values we insert noisy estimates of the gradients. In what follows we define first-order
online algorithms, and present a lemma that analyses the regret of such algorithm receiving noisy
gradients.
5

Definition 8. (First-Order Online Algorithm) Let A be an OCO algorithm receiving an arbitrary
sequence of differential convex loss functions f1 , . . . , fT , and providing points x1 ? A and xt ?
A(f1 , . . . , ft?1 ). Given that A requires all loss functions to belong to some set F0 . Then A is called
first-order online algorithm if the following holds:
? Adding a linear function to a member of F0 remains in F0 ; i.e., for every f ? F0 and
a ? Rn then also f + a> x ? F0

? The algorithm?s choices depend only on its gradient values taken in the past choices of A,
i.e. :
A(f1 , . . . , ft?1 ) = A(?f1 (x1 ), . . . , ?ft?1 (xt?1 )),
?t ? [T ]

The following is a generalization of Lemma 3.1 from [5]:
Lemma 9. Let w be a fixed point in K. Let A be a first-order online algorithm receiving a sequence
of differential convex loss functions f1 , . . . , fT : K ? R (ft+1 possibly depending on z1 , . . . zt ).
Where z1 . . . zT are defined as follows: z1 ? A, zt ? A(g1 , . . . , gt?1 ) where gt ?s are vector valued
random variables such that:

E[gt z1 , f1 , . . . , zt , ft ] = ?ft (zt )
Then if A ensures a regret bound of the form: RegretA
T ? BA (?f1 (x1 ), . . . , ?fT (xT )) in the full
information case then, in the case of noisy gradients it ensures the following bound:
T
T
X
X
E[
ft (zt )] ?
ft (w) ? E[BA (g1 , . . . , gT )]
t=1

4

t=1

Main Result and Analysis

Following is the main theorem of this paper:
Theorem 10. Let K be a convex set with diameter DK and R be a ?-self-concordant barrier over
K. Then in the BCO setting where the adversary is limited to choosing ?-smooth and ?-stronglyconvex
functions and |ft (x)| ? L, ?x ? K, then the expected regret of Algorithm 1 with ? =
q
(?+2?/?) log T
2n2 L2 T

is upper bounded as
s

E[RegretT ] ? 4nL

2?
?+
?



2
?DK
T log T + 2L +
=O
2

r

??
T log T
?

!

whenever T / log T ? 2 (? + 2?/?).
Algorithm 1 BCO Algorithm for Str.-convex & Smooth losses
Input: ? > 0, ? > 0, ?-self-concordant barrier R
Choose x1 = arg minx?K R(x)
for t = 1, 2 . . . T do
?1/2
Define Bt = ?2 R(xt ) + ??tI
Draw u ? Sn
Play yt = xt + Bt u
Observe ft (xt + Bt u) and define gt= nft (xt + Bt u)Bt?1
	 u ?1
Pt
?
>
2
Update xt+1 = arg minx?K ? =1 g? x + 2 ||x ? x? || + ? R(x)
end for
Algorithm 1 shrinks the exploration magnitude with time (Fig. 1c); this is enabled thanks to the
strong-convexity of the losses. It also updates according to a full-information first-order algorithm
6

denoted FTARL-?, which is defined below. This algorithm is a variant of the FTRL methodology
as defined in [6, 10].
Algorithm 2 FTARL-?
Input: ? > 0, ?-self concordant barrier R
Choose x1 = arg minx?K R(x)
for t = 1, 2 . . . T do
Receive ?ht (xt )
	
Pt 
Output xt+1 = arg minx?K ? =1 ?h? (x? )> x + ?2 ||x ? x? ||2 + ? ?1 R(x)
end for

Next we give a proof sketch of Theorem 10
Proof sketch of Therorem 10. Let us decompose the expected regret of Algorithm 1 with respect to
w ? K:
PT
E [RegretT (w)] := t=1 E [ft (yt ) ? ft (w)]
PT
= t=1 E [ft (yt ) ? ft (xt )]
(9)
h
i
PT
+ t=1 E ft (xt ) ? f?t (xt )
(10)
h
i
PT
? t=1 E ft (w) ? f?t (w)
(11)
h
i
PT
+ t=1 E f?t (xt ) ? f?t (w)
(12)
where expectation is taken with respect to the player?s choices, and f?t is defined as
f?t (x) = E[ft (x + Bt v)],

?x ? K

here v ? Bn and the smoothing matrix Bt is defined in Algorithm 1.
The sampling scheme used by Algorithm 1 yields an unbiased gradient estimate gt of the smoothed
version f?t , which is then inserted to FTARL-? (Algorithm 2). We can therefore interpret Algorithm 1 as performing noisy first-order method (FTARL-?) over the smoothed versions. The xt ?s
in Algorithm 1 are the outputs of FTARL-?, thus the term in Equation (12) is associated with ?exploitation?. The other terms in Equations (9)-(11) measure the cost of sampling away from xt , and
the distance between the smoothed version and the original function, hence these term are associated
with ?exploration?.
In what follows we analyze these terms separately and show that Algorithm 1
?
? T ) regret.
achieves O(
The Exploration Terms: The next hold by the remark that follows Lemma 7 and by the lemma
itself:
 



E[ft (yt ) ? ft (xt )] = E Eu [ft (xt + Bt u)] ? ft (xt )xt ] ? 0.5?E ||Bt2 ||2 ? ?/2??t (13)
h
 i


? E[ft (w) ? f?t (w)] = E E[f?t (w) ? ft (w)xt ] ? 0.5?E ||Bt2 ||2 ? ?/2??t
(14)
h
 i
E[ft (xt ) ? f?t (xt )] = E E[ft (xt ) ? f?t (xt )xt ] ? 0
(15)
where ||Bt2 ||2 ? 1/??t follows by the definition of Bt and by the fact that ?2 R(xt ) is positive
definite.
7

The Exploitation Term: The next Lemma bounds the regret of FTARL-? in the full-information
setting:
Lemma 11. Let R be a self-concordant barrier over a convex set K, and ? > 0. Consider an
online player receiving ?-strongly-convex loss functions h1 , . . . , hT and choosing points according
to FTARL-? (Algorithm 2), and ?||?ht (xt )||?t ? 1/2, ?t ? [T ]. Then the player?s regret is upper
bounded as follows:
T
X
t=1

ht (xt ) ?

T
X
t=1

ht (w) ? 2?

T
X
t=1

2

(||?ht (xt )||?t ) + ? ?1 R(w),

?z ? K

here (||a||?t )2 = aT (?2 R(xt ) + ??tI)?1 a
Note that Algorithm 1 uses the estimates gt as inputs into FTARL-?. Using Corollary 6 we can
show that the gt ?s are unbiased estimates for the gradients of the smoothed versions f?t ?s. Using the
regret bound of the above lemma, and the unbiasedness of the gt ?s, Lemma 9 ensures us:
T
X
t=1

T
i
X
?
?
E ft (xt ) ? ft (w) ? 2?
E[(||gt ||?t )2 ] + ? ?1 R(w)

h

(16)

t=1

By the definitions of gt and Bt , and recalling |ft (x)| ? L, ?x ? K, we can bound:
h

?1 ?1  i
2
E[(||gt ||?t )2 xt ] = E n2 (ft (xt + Bt u)) u> Bt?1 ?2 R(xt ) + ??tI
Bt uxt ? (nL)2
Concluding: Plugging the latter into Equation (16) and combining Equations (9)-(16) we get:

E[RegretT (w)] ? 2?(nL)2 T + ? ?1 R(w) + 2?? ?1 log T
(17)
Recall that x1 = arg minx?K R(x) and assume w.l.o.g. that R(x1 ) = 0 (we can always add
R a constant). Thus, for a point w ? K such that ?x1 (w) ? 1 ? T ?1 Lemma 4 ensures us that
R(w) ? ? log T . Combining the latter
p with Equation (17) and the choice of ? in Theorem 10 assures
an expected regret bounded by 4nL (? + 2?? ?1 ) T log T . For w ? K such that ?x1 (w) > 1?T ?1
we can always find w0 ? K such that ||w ? w0 || ? O(T ?1 ) and ?x1 (w0 ) ? 1 ? T ?1 , using the
Lipschitzness of the ft ?s, Theorem 10 holds.
Correctness:
Note that Algorithm 1 chooses points from the set {xt +
?1/2
2
? R(xt ) + ??tI
u, u ? Sn } which is inside the Dikin ellipsoid and therefore belongs to K
(the Dikin Eliipsoid is always in K).

5

Summary and open questions

We have presented an efficient algorithm that attains near optimal regret for the setting of BCO with
strongly-convex and smooth losses, advancing our understanding of optimal regret rates for bandit
learning.
Perhaps the most important question in bandit learning remains the resolution of the attainable regret
bounds for smooth but non-strongly-convex, or vice versa, and generally convex cost functions (see
Table 1). Ideally, this should be accompanied by an efficient algorithm, although understanding the
optimal rates up to polylogarithmic factors would be a significant advancement by itself.
Acknowledgements
The research leading to these results has received funding from the European Union?s Seventh Framework Programme (FP7/2007-2013) under grant agreement n? 336078 ? ERCSUBLRN.
8

References
[1] Jacob Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient
algorithm for bandit linear optimization. In COLT, pages 263?274, 2008.
[2] Alekh Agarwal, Ofer Dekel, and Lin Xiao. Optimal algorithms for online convex optimization
with multi-point bandit feedback. In COLT, pages 28?40, 2010.
[3] S?ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1?122,
2012.
[4] Varsha Dani, Thomas P. Hayes, and Sham Kakade. The price of bandit information for online
optimization. In NIPS, 2007.
[5] Abraham Flaxman, Adam Tauman Kalai, and H. Brendan McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. In SODA, pages 385?394,
2005.
[6] Elad Hazan. A survey: The convex optimization approach to regret minimization. In Suvrit
Sra, Sebastian Nowozin, and Stephen J. Wright, editors, Optimization for Machine Learning,
pages 287?302. MIT Press, 2011.
[7] Robert D Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In NIPS,
volume 17, pages 697?704, 2004.
[8] Arkadii Nemirovskii. Interior point polynomial time methods in convex programming. Lecture
Notes, 2004.
[9] Ankan Saha and Ambuj Tewari. Improved regret guarantees for online smooth convex optimization with bandit feedback. In AISTATS, pages 636?642, 2011.
[10] Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends
in Machine Learning, 4(2):107?194, 2011.
[11] Ohad Shamir. On the complexity of bandit and derivative-free stochastic convex optimization.
In Conference on Learning Theory, pages 3?24, 2013.

9

"
2010,Heavy-Tailed Process Priors for Selective Shrinkage,,3996-heavy-tailed-process-priors-for-selective-shrinkage.pdf,"Heavy-tailed distributions are often used to enhance the robustness of regression and classification methods to outliers in output space.  Often, however, we are confronted with ``outliers'' in input space, which are isolated observations in sparsely populated regions. We show that heavy-tailed process priors (which we construct from Gaussian processes via a copula), can be used to improve robustness of regression and classification estimators to such outliers by selectively shrinking them more strongly in sparse regions than in dense regions. We carry out a theoretical analysis to show that selective shrinkage occurs provided the marginals of the heavy-tailed process have sufficiently heavy tails. The analysis is complemented by experiments on biological data which indicate significant improvements of estimates in sparse regions while producing competitive results in dense regions.","Heavy-Tailed Process Priors for Selective Shrinkage

Michael I. Jordan
University of California, Berkeley
jordan@cs.berkeley.edu

Fabian L. Wauthier
University of California, Berkeley
flw@cs.berkeley.edu

Abstract
Heavy-tailed distributions are often used to enhance the robustness of regression
and classification methods to outliers in output space. Often, however, we are confronted with ?outliers? in input space, which are isolated observations in sparsely
populated regions. We show that heavy-tailed stochastic processes (which we construct from Gaussian processes via a copula), can be used to improve robustness
of regression and classification estimators to such outliers by selectively shrinking
them more strongly in sparse regions than in dense regions. We carry out a theoretical analysis to show that selective shrinkage occurs when the marginals of the
heavy-tailed process have sufficiently heavy tails. The analysis is complemented
by experiments on biological data which indicate significant improvements of estimates in sparse regions while producing competitive results in dense regions.

1

Introduction

Gaussian process classifiers (GPCs) [12] provide a Bayesian approach to nonparametric classification with the key advantage of producing predictive class probabilities. Unfortunately, when training
data are unevenly sampled in input space, GPCs tend to overfit in the sparsely populated regions.
Our work is motivated by an application to protein folding where this presents a major difficulty.
In particular, while Nature provides samples of protein configurations near the global minima of
free energy functions, protein-folding algorithms, which imitate Nature by minimizing an estimated
energy function, necessarily explore regions far from the minimum. If the estimate of free energy is
poor in those sparsely-sampled regions then the algorithm has a poor guide towards the minimum.
More generally this problem can be viewed as one of ?covariate shift,? where the sampling pattern
differs in the training and testing phase.
In this paper we investigate a GPC-based approach that addresses overfitting by shrinking predictive
class probabilities towards conservative values. For an unevenly sampled input space it is natural
to consider a selective shrinkage strategy: we wish to shrink probability estimates more strongly in
sparse regions than in dense regions. To this end several approaches could be considered. If sparse
regions can be readily identified, selective shrinkage could be induced by tailoring the Gaussian
process (GP) kernel to reflect that information. In the absence of such knowledge, Goldberg and
Williams [5] showed that Gaussian process regression (GPR) can be augmented with a GP on the
log noise level. More recent work has focused on partitioning input space into discrete regions
and defining different kernel functions on each. Treed Gaussian process regression [6] and Treed
Gaussian process classification [1] represent advanced variations of this theme that define a prior
distribution over partitions and their respective kernel hyperparameters. Another line of research
which could be adapted to this problem posits that the covariate space is a nonlinear deformation
of another space on which a Gaussian process prior is placed [3, 13]. Instead of directly modifying
the kernel matrix, the observed non-uniformity of measurements is interpreted as being caused by
the spatial deformation. A difficulty with all these approaches is that posterior inference is based on
MCMC, which can be overly slow for the large-scale problems that we aim to address.
1

This paper shows that selective shrinkage can be more elegantly introduced by replacing the Gaussian process underlying GPC with a stochastic process that has heavy-tailed marginals (e.g., Laplace,
hyperbolic secant, or Student-t). While heavy-tailed marginals are generally viewed as providing robustness to outliers in the output space (i.e., the response space), selective shrinkage can be viewed
as a form of robustness to outliers in the input space (i.e., the covariate space). Indeed, selective
shrinkage means the data points that are far from other data points in the input space are regularized
more strongly. We provide a theoretical analysis and empirical results to show that inference based
on stochastic processes with heavy-tailed marginals yields precisely this kind of shrinkage.
The paper is structured as follows: Section 2 provides background on GPCs and highlights how
selective shrinkage can arise. We present a construction of heavy-tailed processes in Section 3 and
show that inference reduces to standard computations in a Gaussian process. An analysis of our
approach is presented in Section 4 and details on inference algorithms are presented in Section 5.
Experiments on biological data in Section 6 demonstrate that heavy-tailed process classification
substantially outperforms GPC in sparse regions while performing competitively in dense regions.
The paper concludes with an overview of related research and final remarks in Sections 7 and 8.

2

Gaussian process classification and shrinkage

A Gaussian process (GP) [12] is a prior on functions z : X ? R defined through a mean function
(usually identically zero) and a symmetric positive semidefinite kernel k(?, ?). For a finite set of
locations X = (x1 , . . . , xn ) we write z(X) ? p(z(X)) = N (0, K(X, X)) as a random variable
distributed according to the GP with finite-dimensional kernel matrix [K(X, X)]i,j = k(xi , xj ). Let
y denote an n-vector of binary class labels associated with measurement locations X 1 . For Gaussian
process classification (GPC) [12] the probability that a test point x? is labeled as class y? = 1, given
training data (X, y), is computed as


1
p(y? = 1|X, y, x? ) = Ep(z(x? )|X,y,x? )
(1)
1 + exp{?z(x? )}
Z
p(z(x? )|X, y, x? ) = p(z(x? )|X, z(X), x? )p(z(X)|X, y)dz(X).
The predictive distribution p(z(x? )|X, y, x? ) represents a regression on z(x? ) with a complicated
observation model y|z. The central observation from Eq. (1) is that we could selectively shrink
the prediction p(y? = 1|X, y, x? ) towards a conservative value 1/2 by selectively shrinking
p(z(x? )|X, y, x? ) closer to a point mass at zero.

3

Heavy-tailed process priors via the Gaussian copula

In this section we construct the heavy-tailed stochastic process by transforming a GP. As with the
GP, we will treat the new process as a prior on functions. Suppose that diag (K(X, X)) = ? 2 1. We
define the heavy-tailed process f (X) with marginal c.d.f. Gb as
z(X) ? N (0, K(X, X))
u(X) = ?0,?2 (z(X))

(2)
(3)

?1
f (X) = G?1
b (u(X)) = Gb (?0,? 2 (z(X))).

Here the function ?0,?2 (?) is the c.d.f. of a centered Gaussian with variance ? 2 . Presently, we
only consider the case when Gb is the (continuous) c.d.f. of a heavy-tailed density gb with scale
parameter b that is symmetric about the origin. Examples include the Laplace, hyperbolic secant
and Student-t distribution. We note that other authors have considered asymmetric or even discrete
distributions [2, 11, 16] while Snelson et al. [15] use arbitrary monotonic transformations in place
of Gb?1 (?0,?2 (?)). The process u(X) has the density of a Gaussian copula [10, 16] and is critical
in transferring the correlation structure encoded by K(X, X) from z(X) to f (X). If we define
1
To improve the clarity of exposition, we only deal with binary classification for now. A full multiclass
classification model is used in our experiments.

2

z(f (X)) = ??1
0,? 2 (Gb (f (X))), it is well known [7, 9, 11, 15, 16] that the density of f (X) satisfies
Q




1
I
>
?1
i=1 gb (f (xi ))
p(f (X)) =
z(f
(X))
. (4)
exp
?
z(f
(X))
K(X,
X)
?
2
?2
|K(X, X)/? 2 |1/2
Q
Observe that if K(X, X) = ? 2 I then p(f (X)) = i=1 gb (f (xi )). Also note that if Gb were
chosen to be Gaussian, we would recover the Gaussian process. The predictive distribution
p(f (x? )|X, f (X), x? ) can be interpreted as a Heavy-tailed process regression (HPR). It is easy to
see that its computation can be reduced to standard computations in a Gaussian model by nonlinearly
transforming observations f (X) into z-space. The predictive distribution in z-space satisfies
p(z(x? )|X, f (X), x? ) = N (?? , ?? )
(5)
?? = K(x? , X)K(X, X)?1 z(f (X))

(6)
?1

?? = K(x? , x? ) ? K(x? , X)K(X, X) K(X, x? ).
(7)
The corresponding distribution in f -space follows by another change of variables. Having defined
the heavy-tailed stochastic process in general we now turn to an analysis of its shrinkage properties.

4

Selective shrinkage

By ?selective shrinkage? we mean that the degree of shrinkage applied to a collection of estimators
varies across estimators. As motivated in Section 2, we are specifically interested in selectively
shrinking posterior distributions near isolated observations more strongly than in dense regions.
This section shows that we can achieve this by changing the form of prior marginals (heavy-tailed
instead of Gaussian) and that this induces stronger selective shrinkage than any GPR could induce.
Since HPR uses a GP in its construction, which can induce some selective shrinkage on its own, care
must be taken to investigate only the additional benefits the transformation G?1
b (?0,? 2 (?)) has on
shrinkage. For this reason we assume a particular GP prior which leads to a special type of shrinkage
in GPR and then check how an HPR model built on top of that GP changes the observed behavior.
In this section we provide an idealized analysis that allows us to compare the selective shrinkage
obtained by GPR and HPR. Note that we focus on regression in this section so that we can obtain
analytical results. We work with n measurement locations, X = (x1 , . . . , xn ), whose index set
{1, . . . , n} can be partitioned into a ?dense? set D with |D| = n ? 1 and a single ?sparse? index s ?
/
? d , xd0 ) =
D. Assume that xd = xd0 , ?d, d0 ? D, so that we may let (without loss of generality) K(x
? d , xs ) = K(x
? s , xd ) = 0 ?d ? D.
1, ?d 6= d0 ? D. We also assert that xd 6= xs ?d ? D and let K(x
?
Assuming that n > 2 we fix the remaining entry K(xs , xs ) = /( + n ? 2), for some  > 0. We
? + I.
interpret  as a noise variance and let K = K
Denote any distributions computed under the GPR model by pgp (?) and those computed in HPR
by php (?). Using K(X, X) = K, define z(X) as in Eq. (2). Let y denote a vector of real-valued
measurements for a regression task. The posterior distribution of z(xi ) given y, with xi ? X, is
derived by standard Gaussian computations as

pgp (z(xi )|X, y) = N ?i , ?i2
? i , X)K(X, X)?1 y
?i = K(x
? i , X)K(X, X)?1 K(X,
?
?i2 = K(xi , xi ) ? K(x
xi ).
For our choice of K(X, X) one can show that ?d2 = ?s2 for d ? D. To ensure that the posterior
distributions agree at the two locations we require ?d = ?s , which holds if measurements y satisfy
( 
)
X
n 

o

?1
?
?
y ? Ygp , y| K(xd , X) ? K(xs , X) K(X, X) y = 0 = y 
yd = ys .

d?D

A similar analysis can be carried out for the induced HPR model. By Eqs. (5)?(7) HPR inference
leads to identical distributions php (z(xd )|X, y 0 ) = php (z(xs )|X, y 0 ) with d ? D if measurements
y 0 in f -space satisfy
n 

o
? d , X) ? K(x
? s , X) K(X, X)?1 ??1 2 (Gb (y 0 )) = 0
y 0 ? Yhp , y 0 | K(x
0,?

	
= y 0 = G?1
b (?0,? 2 (y))|y ? Ygp .
3

?5
?10

(a)

0
x

n
o
1 exp ? |x|
gb (x) = 2b
b

10

0

0

b

G?1
(?(x))
b

G?1
(?(x))
b

0

5
G?1(?(x))

5

5

?5
?10

(b)

0
x

1 sech
gb (x) = 2b

10


?x
2b



?5
?10

(c)

gb (x) =

0
x

10

1

3/2
b 2+(x/b)2

2
Figure 1: Illustration of G?1
b (?0,? 2 (x)), for ? = 1.0 with Gb the c.d.f. of (a) the Laplace distribution (b) the hyperbolic secant distribution (c) a Student-t inspired distribution, all with scale
parameter b. Each plot shows three samples?dotted, dashed, solid?for growing b. As b increases
the distributions become heavy-tailed and the gradient of G?1
b (?0,? 2 (x)) increases.

To compare the shrinkage properties of GPR and HPR we analyze select pairs of measurements
in Ygp and Yhp . The derivation requires that G?1
b (?0,? 2 (?)) is strongly concave on (??, 0],
strongly convex on [0, +?) and has gradient > 1 on R. To see intuitively why this should hold,
note that for Gb with fatter tails than a Gaussian, |G?1
b (?0,? 2 (x))| should eventually dominate
2
|??1
(?
(x))|
=
(b/?)|x|.
Figure
1
demonstrates
graphically
that the assumption holds for sev0,?
0,b2
eral choices of Gb , provided b is large enough, i.e., that gb has sufficiently heavy tails. Indeed, it can
be shown that for scale parameters b > 0, the first and second derivatives of G?1
b (?0,? 2 (?)) scale linearly with b. Consider a measurement 0 6= y ? Ygp with sign (y(xd )) = sign (y(xd0 )) , ?d, d0 ? D.
Analyzing such y is relevant, as we are most interested in comparing how multiple reinforcing observations at clustered locations and a single isolated observation are absorbed during inference. By
definition of Ygp , for d? = argmaxd?D |yd | we have |yd? | < |ys | as long as n > 2. The corresponding element y 0 = G?1
b (?0,? 2 (y)) ? Yhp then satisfies
 


  y 0 (x ? )

 ?1
  G?1
?
 
d
0
b (?0,? 2 (y(xd )))


(8)
y(xs ) = 
y(xs ) .
|y (xs )| = Gb (?0,?2 (y(xs ))) > 


y(xd? )
y(xd? )
Thus HPR inference leads to identical predictive distributions in f -space at the two locations even
though the isolated observation y 0 (xs ) has disproportionately larger magnitude than y 0 (xd? ), relative
to the GPR measurements y(xs ) and y(xd? ). As this statement holds for any y ? Ygp satisfying
our earlier sign requirement, it indicates that HPR systematically shrinks isolated observations more
strongly than GPR. Since the second derivative of G?1
b (?0,? 2 (?)) scales linearly with scale b > 0,
an intuitive connection suggests itself when looking at inequality (8): the heavier the marginal tails,
the stronger the inequality and thus the stronger the selective shrinkage effect.
The previous derivation exemplifies in an idealized setting that HPR leads to improved shrinkage of
predictive distributions near isolated observations. More generally, because GPR transforms measurements only linearly, while HPR additionally pre-transforms measurements nonlinearly, our analysis suggests that for any GPR we can find an HPR model which leads to stronger selective shrinkage. The result has intuitive parallels to the parametric case: just as `1 -regularization improves
shrinkage of parametric estimators, heavy-tailed processes improve shrinkage of nonparametric estimators. We note that although our analysis kept K(X, X) fixed for GPR and HPR, in practice we
are free to tune the kernel to yield a desired scale of predictive distributions. The above analysis
has been carried out for regression, but motivates us to now explore heavy-tailed processes in the
classification case.

5

Heavy-tailed process classification

The derivation of heavy-tailed process classification (HPC) is similar to that of standard multiclass
GPC with Laplace approximation in Rasmussen and Williams [12]. However, due to the nonlinear
transformations involved, some nice properties of their derivation are lost. We revert notation and
let y denote a vector of class labels. For a C-class classification problem with n training points we
4

introduce a vector of nC latent function measurements (f11 , . . . , fn1 , f12 , . . . , fn2 , . . . , f1C , . . . , fnC )> .
For each block c ? {1, . . . , C} of n variables we define an independent heavy-tailed process prior
using Eq. (4) with kernel matrix Kc . Equivalently, we can define the prior jointly on f by letting
K be a block-diagonal kernel matrix with blocks K1 , . . . , KC . Each kernel matrix Kc is defined
by a (possibly different) symmetric positive semidefinite kernel with its own set of parameters. The
following construction relaxes the earlier condition that diag (K) = ? 2 1 and instead views ?0,?2 (?)
as some nonlinear transformation with parameter ? 2 . By this relaxation we effectively adopt Liu et
al.?s [9] interpretation that Eq. (4) defines the copula. The scale parameters b could in principle vary
across the nC variables, but we keep them constant at least within each block of n. Labels y are
represented in a 1-of-n form and generated by the following observation model
exp{fic }
.
c0
c0 exp{fi }

p(yic = 1|fi ) = ?ic = P

(9)

For inference we are ultimately interested in computing
p(y?c = 1|X, y, x? ) = Ep(f? |X,y,x? )



exp{f?c }
c0
c0 exp{f? }

P


,

(10)

where f? = (f?1 , . . . , f?C )> . The previous section motivates that improved selective shrinkage will
occur in p(f? |X, y, x? ), provided the prior marginals have sufficiently heavy tails.
5.1

Inference

As in GPC, most of the intractability lies in computing the predictive distribution p(f? |X, y, x? ). We
use the Laplace approximation to address this issue: a Gaussian approximation to p(z|X, y) is found
and then combined with the Gaussian p(z? |X, z, x? ) to give us an approximation to p(z? |X, y, x? ).
This is then transformed to a (typically non-Gaussian) distribution in f -space using a change of
variables. Hence we first seek to find a mode and corresponding Hessian matrix of the log posterior
log p(z|X, y). Recalling the relation f = G?1
b (?0,? 2 (z)), the log posterior can be written as
J(z) , log p(y|z) + log p(z) = y > f ?

X
i

log

X
c

1
1
exp {fic )} ? z > K ?1 z ? log |K| + const.
2
2

Let ? be an nC ? n matrix of stacked diagonal matrices diag (? c ) for n-subvectors ? c of ?. With
W = diag (?) ? ??> , the gradients are
 
df
?J(z) = diag
(y ? ?) ? K ?1 z
dz
 2 
 
 
d f
df
df
?2 J(z) = diag
diag
(y
?
?)
?
diag
W
diag
? K ?1 .
2
dz
dz
dz
Unlike in Rasmussen and Williams [12], ??2 J(z) is not generally positive definite owing to its first
term. For that reason we cannot use a Newton step to find the mode and instead resort to a simpler
gradient method. Once the mode z? has been found we approximate the posterior as

p(z|X, y) ? q(z|X, y) = N z?, ??2 J(?
z )?1 ,
and use this to approximate the predictive distribution by
Z
q(z? |X, y, x? ) = p(z? |X, z, x? )q(z|X, y)df.
Since we arranged for both distributions in the integral to be Gaussian, the resulting Gaussian can
be straightforwardly evaluated. Finally, to approximate the one-dimensional integral with respect
to p(f? |X, y, x? ) in Eq. (10) we could either use a quadrature method, or generate samples from
q(z? |X, y, x? ), convert them to f -space using G?1
b (?0,? 2 (?)) and then approximate the expectation
by an average. We have compared predictions of the latter method with those of a Gibbs sampler;
the Laplace approximation matched Gibbs results well, while being much faster to compute.
5

pi
r=1
r=2
r=3

O
C

H

?

0

pi/2

Rotamer r ? {1, 2, 3}

C?

?

?

Residue

ue

id
Res

Re
sid
ue

0

N

C0

N

?pi/2

H
H

O
?pi
?pi

?pi/2

0
?

pi/2

pi

(b)

(a)

Figure 2: (a) Schematic of a protein segment. The backbone is the sequence of C 0 , N, C? , C 0 , N
atoms. An amino-acid-specific sidechain extends from the C? atom at one of three discrete angles known as ?rotamers.? (b) Ramachandran plot of 400 (?, ?) measurements and corresponding
rotamers (by shapes/colors) for amino-acid arginine (arg). The dark shading indicates the sparse
region we considered in producing results in Figure 3. Progressively lighter shadings indicate how
the sparse region was grown to produce Figure 4.
5.2

Parameter estimation

Using a derivation similar to that in [12], we have for f? = G?1
z )) that the Laplace approxb (?0,? 2 (?
imation of the marginal log likelihood is
1
log p(y|x) ? log q(y|x) = J(?
z ) ? log | ? 2??2 J(?
z )|
(11)
2
n o 1
X
X
1
1
= y > f? ?
log
exp f?ic ? z?> K ?1 z? ? log |K| ? log | ? ?2 J(?
z )| + const.
2
2
2
c
i
We optimize kernel parameters ? by taking gradient steps on log q(y|x). The derivative needs to
take into account that perturbing the parameters can also perturb the mode z? found for the Laplace
approximation. At an optimum ?J(?
z ) must be zero, so that
!
df?
z? = Kdiag
(y ? ?
? ),
(12)
d?
z
where ?
? is defined as in Eq. (9) but using f? rather than f . Taking derivatives of this equation allows
us to compute the gradient d?
z /d?. Differentiating the marginal likelihood we have
!
d log q(y|x)
df? d?
z
d?
z
1
dK ?1
>
= (y ? ?
? ) diag
? K ?1 z? + z?> K ?1
K z? ?
d?
d?
z d?
d?
2
d?




1
dK
1
d?2 J(?
z)
tr K ?1
? tr ?2 J(?
z )?1
.
2
d?
2
d?
The remaining gradient computations are straightforward, albeit tedious. In addition to optimizing
the kernel parameters, it may also be of interest to optimize the scale parameter b of marginals Gb .
Again, differentiating Eq. (12) with respect to b allows us to compute d?
z /db. We note that when
?
perturbing b we change f by changing the underlying mode z? as well as by changing the parameter
b which is used to compute f? from z?. Suppressing the detailed computations, the derivative of the
marginal log likelihood with respect to b is


2
? d?
d log q(y|x)
z > ?1
1
z)
> df
2
?1 d? J(?
= (y ? ?
?)
?
K z? ? tr ? J(?
z)
.
db
db
db
2
db
6

1

0.8

0.8
Prediction rate

Prediction rate

1

0.6
0.4
HPC Hyp. sec.
HPC Laplace
GPC

0.2
0

0.6
0.4
HPC Hyp. sec.
HPC Laplace
GPC

0.2
0

trp tyr ser phe glu asn leu thr his asp arg cys lys met gln ile val

(a)

trp tyr ser phe glu asn leu thr his asp arg cys lys met gln ile val

(b)

Figure 3: Rotamer prediction rates in percent in (a) sparse and (b) dense regions. Both flavors
of HPC (hyperbolic secant and Laplace marginals) significantly outperform GPC in sparse regions
while performing competitively in dense regions.

6

Experiments

To a first approximation, the three-dimensional structure of a folded protein is defined by pairs
of continuous backbone angles (?, ?), one pair for each amino-acid, as well as discrete angles,
so-called rotamers, that define the conformations of the amino-acid sidechains that extend from
the backbone. The geometry is outlined in Figure 2(a). There is a strong dependence between
backbone angles (?, ?) and rotamer values; this is illustrated in the ?Ramachandran plot? shown
in Figure 2(b), which plots the backbone angles for each rotamer (indicated by the shapes/colors).
The dependence is exploited in computational approaches to protein structure prediction, where
estimates of rotamer probabilities given backbone angles are used as one term in an energy function
that models native protein states as minima of the energy. Poor estimates of rotamer probabilities
in sparse regions can derail the prediction procedure. Indeed, sparsity has been a serious problem
in state-of-the-art rotamer models based on kernel density estimates (Roland Dunbrack, personal
communication). Unfortunately, we have found that GPC is not immune to the sparsity problem.
To evaluate our algorithm we consider rotamer-prediction tasks on the 17 amino-acids (out of 20)
that have three rotamers at the first dihedral angle along the sidechain2 . Our previous work thus
applies with the number of classes C = 3 and the covariates being (?, ?) angle pairs. Since the
input space is a torus we defined GPC and HPC using the following von Mises-inspired kernel for
d-dimensional angular data:
(
!
!)
d
X
2
k(xi , xj ) = ? exp ?
cos(xi,k ? xj,k ) ? d
,
k=1
2

3

where xi,k , xj,k ? [0, 2?] and ? , ? ? 0 . To find good GPC kernel parameters we optimize
an `2 -regularized version of the Laplace approximation to the log marginal likelihood reported in
Eq. 3.44 of [12]. For HPC we let Gb be either the centered Laplace distribution or the hyperbolic
secant distribution with scale parameter b. We estimate HPC kernel parameters as well as b by
similarly maximizing an `2 -regularized form of Eq. (11). In both cases we restricted the algorithms
to training sets of only 100 datapoints. Since good regularization parameters for the objectives are
not known a priori we train with and test them on a grid for each of the 17 rotameric residues in
ten-fold cross-validation. To find good regularization parameters for a particular residue we look up
that combination which, averaged over the ten folds of the remaining 16 residues, produced the best
test results. Having chosen the regularization constants we report average test results computed in
ten-fold cross validation.
We evaluate the algorithms on predefined sparse and dense regions in the Ramachandran plot, as
indicated by the background shading in Figure 2(b). Across 17 residues the sparse regions usually
contained more than 70 measurements (and often more than 150), each of which appears in one
of the 10 cross validations. Figure 3 compares the label prediction rates on the dense and sparse
2

Residues alanine and glycine are non-discrete while proline has two rotamers at the first dihedral angle.
The function cos(xi,k ? xj,k ) = [cos(xi.k ), sin(xi,k )][cos(xj.k ), sin(xj,k )]> is a symmetric positive
semi-definite kernel. By Propositions 3.22 (i) and (ii) and Proposition 3.25 in Shawe-Taylor and Cristianini [14], so is k(xi , xj ) above.
3

7

0.65

Prediction rate

0.6

0.55

HPC Hyp. sec.
HPC Laplace
CTGP
GPC

0.5

0.45
155

246

390

618
980
?Density of test data?

1554

2463

3906

Figure 4: Average rotamer prediction rate in the sparse region for two flavors of HPC, standard GPC
well as CTGP [1] as a function of the average number of points per residue in the sparse region.
regions. Averaged over all 17 residues HPC outperforms GPC by 5.79% with Laplace and 7.89%
with hyperbolic secant marginals. With Laplace marginals HPC underperforms GPC on only two
residues in sparse regions: by 8.22% on glutamine (gln), and by 2.53% on histidine (his). On
dense regions HPC lies within 0.5% on 16 residues and only degrades once by 3.64% on his.
Using hyperbolic secant marginals HPC often improves GPC by more than 10% on sparse regions
and degrades by more than 5% only on cysteine (cys) and his. On dense regions HPC usually
performs within 1.5% of GPC. In Figure 4 we show how the average rotamer prediction rate across
17 residues changes for HPC, GPC, as well as CTGP [1] as we grow the sparse region to include
more measurements from dense regions. The growth of the sparse region is indicated by progressively lighter shadings in Figure 2(b). As more points are included the significant advantage of HPC
lessens. Eventually GPC does marginally better than HPC and much better than CTGP. The values
reported in Figure 3 correspond to the dark shaded region, with an average of 155 measurements.

7

Related research

Copulas [10] allow convenient modelling of multivariate correlation structures as separate from
marginal distributions. Early work by Song [16] used the Gaussian copula to generate complex
multivariate distributions by complementing a simple copula form with marginal distributions of
choice. Popularity of the Gaussian copula in the financial literature is generally credited to Li [8]
who used it to model correlation structure for pairs of random variables with known marginals. More
recently, the Gaussian process has been modified in a similar way to ours by Snelson et al. [15].
They demonstrate that posterior distributions can better approximate the true noise distribution if
the transformation defining the warped process is learned. Jaimungal and Ng [7] have extended
this work to model multiple parallel time series with marginally non-Gaussian stochastic processes.
Their work uses a ?binding copula? to combine several subordinate copulas into a joint model.
Bayesian approaches focusing on estimation of the Gaussian copula covariance matrix for a given
dataset are given in [4, 11]. Research also focused on estimation in high-dimensional settings [9].

8

Conclusions

This paper analyzed learning scenarios where outliers are observed in the input space, rather than
the output space as commonly discussed in the literature. We illustrated heavy-tailed processes as
a straightforward extension of GPs and an economical way to improve the robustness of estimators
in sparse regions beyond those of GP-based methods. Importantly, because these processes are
based on a GP, they inherit many of its favorable computational properties; predictive inference
in regression, for instance, is straightforward. Moreover, because heavy-tailed processes have a
parsimonious representation, they can be used as building blocks in more complicated models where
currently GPs are used. In this way the benefits of heavy-tailed processes extend to any GP-based
model that struggles with covariate shift.
Acknowledgements
We thank Roland Dunbrack for helpful discussions and providing access to the rotamer datasets.
8

References
[1] Tamara Broderick and Robert B. Gramacy. Classification and Categorical Inputs with Treed
Gaussian Process Models. Journal of Classification. To appear.
[2] Wei Chu and Zoubin Ghahramani. Gaussian Processes for Ordinal Regression. Journal of
Machine Learning Research, 6:1019?1041, 2005.
[3] Doris Damian, Paul D. Sampson, and Peter Guttorp. Bayesian Estimation of Semi-Parametric
Non-Stationary Spatial Covariance Structures. Environmetrics, 12:161?178.
[4] Adrian Dobra and Alex Lenkoski. Copula Gaussian Graphical Models. Technical report,
Department of Statistics, University of Washington, 2009.
[5] Paul W. Goldberg, Christopher K. I. Williams, and Christopher M. Bishop. Regression with
Input-dependent Noise: A Gaussian Process Treatment. In Advances in Neural Information
Processing Systems, volume 10, pages 493?499. MIT Press, 1998.
[6] Robert B. Gramacy and Herbert K. H. Lee. Bayesian Treed Gaussian Process Models with an
Application to Computer Modeling. Journal of the American Statistical Association, 2007.
[7] Sebastian Jaimungal and Eddie K. Ng. Kernel-based Copula Processes. In Proceedings of the
European Conference on Machine Learning and Knowledge Discovery in Databases, pages
628?643. Springer-Verlag, 2009.
[8] David X. Li. On Default Correlation: A Copula Function Approach. Technical Report 99-07,
Riskmetrics Group, New York, April 2000.
[9] Han Liu, John Lafferty, and Larry Wasserman. The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs. Journal of Machine Learning Research,
10:1?37, 2009.
[10] Roger B. Nelsen. An Introduction to Copulas. Springer, 1999.
[11] Michael Pitt, David Chan, and Robert J. Kohn. Efficient Bayesian Inference for Gaussian
Copula Regression Models. Biometrika, 93(3):537?554, 2006.
[12] Carl E. Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning.
MIT Press, 2006.
[13] Alexandra M. Schmidt and Anthony O?Hagan. Bayesian Inference for Nonstationary Spatial Covariance Structure via Spatial Deformations. Journal of the Royal Statistical Society,
65(3):743?758, 2003. Ser. B.
[14] John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge
University Press, 2004.
[15] Ed Snelson, Carl E. Rasmussen, and Zoubin Ghahramani. Warped Gaussian Processes. In
Advances in Neural Information Processing Systems, volume 16, pages 337?344, 2004.
[16] Peter Xue-Kun Song. Multivariate Dispersion Models Generated From Gaussian Copula.
Scandinavian Journal of Statistics, 27(2):305?320, 2000.

9

"
2013,Graphical Models for Inference with Missing Data,Spotlight,4899-graphical-models-for-inference-with-missing-data.pdf,"We address the problem of deciding whether there exists a consistent estimator of a given relation Q, when data are missing not at random. We employ a formal representation called `Missingness Graphs' to explicitly portray the causal mechanisms responsible for missingness and to encode dependencies between these mechanisms and the variables being measured. Using this representation, we define the notion of \textit{recoverability} which ensures that, for a given missingness-graph $G$ and a given query $Q$ an algorithm exists such that in the limit of large samples, it produces an estimate of $Q$ \textit{as if} no data were missing. We further present conditions that the graph should satisfy in order for recoverability to hold and devise algorithms to detect the presence of these conditions.","Graphical Models for Inference with Missing Data

Karthika Mohan
Judea Pearl
Dept. of Computer Science
Dept. of Computer Science
Univ. of California, Los Angeles Univ. of California, Los Angeles
Los Angeles, CA 90095
Los Angeles, CA 90095
karthika@cs.ucla.edu
judea@cs.ucla.edu

Jin Tian
Dept. of Computer Science
Iowa State University
Ames, IA 50011
jtian@iastate.edu

Abstract
We address the problem of recoverability i.e. deciding whether there exists a consistent estimator of a given relation Q, when data are missing not at random. We
employ a formal representation called ?Missingness Graphs? to explicitly portray
the causal mechanisms responsible for missingness and to encode dependencies
between these mechanisms and the variables being measured. Using this representation, we derive conditions that the graph should satisfy to ensure recoverability
and devise algorithms to detect the presence of these conditions in the graph.

1

Introduction

The ?missing data? problem arises when values for one or more variables are missing from recorded
observations. The extent of the problem is evidenced from the vast literature on missing data in such
diverse fields as social science, epidemiology, statistics, biology and computer science. Missing
data could be caused by varied factors such as high cost involved in measuring variables, failure of
sensors, reluctance of respondents in answering certain questions or an ill-designed questionnaire.
Missing data also plays a major role in survival data analysis and has been treated primarily using
Kaplan-Meier estimation [30].
In machine learning, a typical example is the Recommender System [16] that automatically generates a list of products that are of potential interest to a given user from an incomplete dataset of
user ratings. Online portals such as Amazon and eBay employ such systems. Other areas such as
data mining [7], knowledge discovery [18] and network tomography [2] are also plagued by missing data problems. Missing data can have several harmful consequences [23, 26]. Firstly they can
significantly bias the outcome of research studies. This is mainly because the response profiles of
non-respondents and respondents can be significantly different from each other. Hence ignoring
the former distorts the true proportion in the population. Secondly, performing the analysis using
only complete cases and ignoring the cases with missing values can reduce the sample size thereby
substantially reducing estimation efficiency. Finally, many of the algorithms and statistical techniques are generally tailored to draw inferences from complete datasets. It may be difficult or even
inappropriate to apply these algorithms and statistical techniques on incomplete datasets.
1.1

Existing Methods for Handling Missing Data

There are several methods for handling missing data, described in a rich literature of books, articles
and software packages, which are briefly summarized here1 . Of these, listwise deletion and pairwise
deletion are used in approximately 96% of studies in the social and behavioral sciences [24].
Listwise deletion refers to a simple method in which cases with missing values are deleted [3]. Unless data are missing completely at random, listwise deletion can bias the outcome [31]. Pairwise
1

For detailed discussions we direct the reader to the books- [1, 6, 13, 17].

1

deletion (or ?available case?) is a deletion method used for estimating pairwise relations among variables. For example, to compute the covariance of variables X and Y , all those cases or observations
in which both X and Y are observed are used, regardless of whether other variables in the dataset
have missing values.
The expectation-maximization (EM) algorithm is a general technique for finding maximum likelihood (ML) estimates from incomplete data. It has been proven that likelihood-based inference
while ignoring the missing data mechanism, leads to unbiased estimates under the assumption of
missing at random (MAR) [13]. Most work in machine learning assumes MAR and proceeds with
ML or Bayesian inference. Exceptions are recent works on collaborative filtering and recommender
systems which develop probabilistic models that explicitly incorporate missing data mechanism
[16, 14, 15]. ML is often used in conjunction with imputation methods, which in layman terms,
substitutes a reasonable guess for each missing value [1]. A simple example is Mean Substitution, in
which all missing observations of variable X are substituted with the mean of all observed values of
X. Hot-deck imputation, cold-deck imputation [17] and Multiple Imputation [26, 27] are examples
of popular imputation procedures. Although these techniques work well in practice, performance
guarantees (eg: convergence and unbiasedness) are based primarily on simulation experiments.
Missing data discussed so far is a special case of coarse data, namely data that contains observations
made in the power set rather than the sample space of variables of interest [12]. The notion of coarsening at random (CAR) was introduced in [12] and identifies the condition under which coarsening
mechanism can be ignored while drawing inferences on the distribution of variables of interest [10].
The notion of sequential CAR has been discussed in [9]. For a detailed discussion on coarsened data
refer to [30].
Missing data literature leaves many unanswered questions with regard to theoretical guarantees for
the resulting estimates, the nature of the assumptions that must be made prior to employing various
procedures and whether the assumptions are testable. For a gentle introduction to the missing data
problem and the issue of testability refer to [22, 19]. This paper aims to illuminate missing data
problems using causal graphs [See Appendix 5.2 for justification]. The questions we pose are:
Given a target relation Q to be estimated and a set of assumptions about the missingness process
encoded in a graphical model, under what conditions does a consistent estimate exist and how can
we elicit it from the data available?
We answer these questions with the aid of Missingness Graphs (m-graphs in short) to be described
in Section 2. Furthermore, we review the traditional taxonomy of missing data problems and cast it
in graphical terms. In Section 3 we define the notion of recoverability - the existence of a consistent
estimate - and present graphical conditions for detecting recoverability of a given probabilistic query
Q. Conclusions are drawn in Section 4.

2

Graphical Representation of the Missingness Process

2.1

Missingness Graphs

X

Y
Ry

X

Rx

Ry

Y*
(a)

X

Y

Y
Ry

Y*

Rx

Y*

X*

(b)

X

(c)

Y
Ry

Y*

X*
(d)

Figure 1: m-graphs for data that are: (a) MCAR, (b) MAR, (c) & (d) MNAR; Hollow and solid
circles denote partially and fully observed variables respectively.
Graphical models such as DAGs (Directed Acyclic Graphs) can be used for encoding as well as
portraying conditional independencies and causal relations, and the graphical criterion called dseparation (refer Appendix-5.1, Definition-3) can be used to read them off the graph [21, 20]. Graphical Models have been used to analyze missing information in the form of missing cases (due to
sample selection bias)[4]. Using causal graphs, [8]- analyzes missingness due to attrition (partially
2

observed outcome) and [29]- cautions against the indiscriminate use of auxiliary variables. In both
papers missing values are associated with one variable and interactions among several missingness
mechanisms remain unexplored.
The need exists for a general approach capable of modeling an arbitrary data-generating process and
deciding whether (and how) missingness can be outmaneuvered in every dataset generated by that
process. Such a general approach should allow each variable to be governed by its own missingness
mechanism, and each mechanism to be triggered by other (potentially) partially observed variables
in the model. To achieve this flexibility we use a graphical model called ?missingness graph? (mgraph, for short) which is a DAG (Directed Acyclic Graph) defined as follows.
Let G(V, E) be the causal DAG where V = V ? U ? V ? ? R. V is the set of observable nodes.
Nodes in the graph correspond to variables in the data set. U is the set of unobserved nodes (also
called latent variables). E is the set of edges in the DAG. Oftentimes we use bi-directed edges as
a shorthand notation to denote the existence of a U variable as common parent of two variables in
Vo ? Vm ? R. V is partitioned into Vo and Vm such that Vo ? V is the set of variables that are
observed in all records in the population and Vm ? V is the set of variables that are missing in
at least one record. Variable X is termed as fully observed if X ? Vo and partially observed if
X ? Vm .
Associated with every partially observed variable Vi ? Vm are two other variables Rvi and Vi? ,
where Vi? is a proxy variable that is actually observed, and Rvi represents the status of the causal
mechanism responsible for the missingness of Vi? ; formally,

vi
if rvi = 0
?
vi = f (rvi , vi ) =
(1)
m
if rvi = 1
Contrary to conventional use, Rvi is not treated merely as the missingness indicator but as a driver
(or a switch) that enforces equality between Vi and Vi? . V ? is a set of all proxy variables and
R is the set of all causal mechanisms that are responsible for missingness. R variables may not
be parents of variables in V ? U . This graphical representation succinctly depicts both the causal
relationships among variables in V and the process that accounts for missingness in some of the
variables. We call this graphical representation Missingness Graph or m-graph for short. Since
every d-separation in the graph implies conditional independence in the distribution [21], the mgraph provides an effective way of representing the statistical properties of the missingness process
and, hence, the potential of recovering the statistics of variables in Vm from partially missing data.
2.2

Taxonomy of Missingness Mechanisms

It is common to classify missing data mechanisms into three types [25, 13]:
Missing Completely At Random (MCAR) : Data are MCAR if the probability that Vm is missing
is independent of Vm or any other variable in the study, as would be the case when respondents
decide to reveal their income levels based on coin-flips.
Missing At Random (MAR) : Data are MAR if for all data cases Y , P (R|Yobs , Ymis ) = P (R|Yobs )
where Yobs denotes the observed component of Y and Ymis , the missing component. Example:
Women in the population are more likely to not reveal their age.
Missing Not At Random (MNAR) or ?non-ignorable missing?: Data that are neither MAR nor
MCAR are termed as MNAR. Example: Online shoppers rate an item with a high probability either
if they love the item or if they loathe it. In other words, the probability that a shopper supplies a
rating is dependent on the shopper?s underlying liking [16].
Because it invokes specific values of the observed and unobserved variables, (i.e., Yobs and Ymis ),
many authors find Rubin?s definition difficult to apply in practice and prefer to work with definitions
expressed in terms of independencies among variables (see [28, 11, 6, 17]). In the graph-based
interpretation used in this paper, MCAR is defined as total independence between R and Vo ?Vm ?U
i.e. R?
?(Vo ? Vm ? U ), as depicted in Figure 1(a). MAR is defined as independence between R and
Vm ?U given Vo i.e. R?
?Vm ?U |Vo , as depicted in Figure 1(b). Finally if neither of these conditions
hold, data are termed MNAR, as depicted in Figure 1(c) and (d). This graph-based interpretation uses
slightly stronger assumptions than Rubin?s, with the advantage that the user can comprehend, encode
and communicate the assumptions that determine the classification of the problem. Additionally, the
conditional independencies that define each class are represented explicitly as separation conditions
3

in the corresponding m-graphs. We will use this taxonomy in the rest of the paper, and will label
data MCAR, MAR and MNAR according to whether the defining conditions, R??Vo ? Vm ? U (for
MCAR), R?
?Vm ? U |Vo (for MAR) are satisfied in the corresponding m-graphs.

3

Recoverability

In this section we will examine the conditions under which a bias-free estimate of a given probabilistic relation Q can be computed. We shall begin by defining the notion of recoverability.
Definition 1 (Recoverability). Given a m-graph G, and a target relation Q defined on the variables
in V , Q is said to be recoverable in G if there exists an algorithm that produces a consistent estimate
of Q for every dataset D such that P (D) is (1) compatible with G and (2) strictly positive over
complete cases i.e. P (Vo , Vm , R = 0) > 0.2
Here we assume that the observed distribution over complete cases P (Vo , Vm , R = 0) is strictly
positive, thereby rendering recoverability a property that can be ascertained exclusively from the
m-graph.
Corollary 1. A relation Q is recoverable in G if and only if Q can be expressed in terms of the
probability P (O) where O = {R, V ? , Vo } is the set of observable variables in G. In other words,
for any two models M1 and M2 inducing distributions P M1 and P M2 respectively, if P M1 (O) =
P M2 (O) > 0 then QM1 = QM2 .
Proof: (sketch) The corollary merely rephrases the requirement of obtaining a consistent estimate to
that of expressibility in terms of observables.
Practically, what recoverability means is that if the data D are generated by any process compatible
?
with G, a procedure exists that computes an estimator Q(D)
such that, in the limit of large samples,
?
Q(D) converges to Q. Such a procedure is called a ?consistent estimator.? Thus, recoverability is
the sole property of G and Q, not of the data available, or of any routine chosen to analyze or process
the data.
Recoverability when data are MCAR For MCAR data we have R??(Vo ? Vm ). Therefore, we
can write P (V ) = P (V |R) = P (Vo , V ? |R = 0). Since both R and V ? are observables, the joint
probability P (V ) is consistently estimable (hence recoverable) by considering complete cases only
(listwise deletion), as shown in the following example.
Example 1. Let X be the treatment and Y be the outcome as depicted in the m-graph in Fig. 1
(a). Let it be the case that we accidentally deleted the values of Y for a handful of samples, hence
Y ? Vm . Can we recover P (X, Y )?
From D, we can compute P (X, Y ? , Ry ). From the m-graph G, we know that Y ? is a collider and
hence by d-separation, (X ? Y )?
?Ry . Thus P (X, Y ) = P (X, Y |Ry ). In particular, P (X, Y ) =
P (X, Y |Ry = 0). When Ry = 0, by eq. (1), Y ? = Y . Hence,
P (X, Y ) = P (X, Y ? |Ry = 0)

(2)

The RHS of Eq. 2 is consistently estimable from D; hence P (X, Y ) is recoverable.
Recoverability when data are MAR When data are MAR, we have R??Vm |Vo . Therefore
P (V ) = P (Vm |Vo )P (Vo ) = P (Vm |Vo , R = 0)P (Vo ). Hence the joint distribution P (V ) is recoverable.
Example 2. Let X be the treatment and Y be the outcome as depicted in the m-graph in Fig. 1 (b).
Let it be the case that some patients who underwent treatment are not likely to report the outcome,
hence the arrow X ? Ry . Under the circumstances, can we recover P (X, Y )?
From D, we can compute P (X, Y ? , Ry ). From the m-graph G, we see that Y ? is a collider and X is
a fork. Hence by d-separation, Y ?
?Ry |X. Thus P (X, Y ) = P (Y |X)P (X) = P (Y |X, Ry )P (X).
2
In many applications such as truncation by death, the problem forbids certain combinations of events
from occurring, in which case the definition need be modified to accommodate such constraints as shown in
Appendix-5.3. Though this modification complicates the definition of ?recoverability?, it does not change the
basic results derived in this paper.

4

In particular, P (X, Y ) = P (Y |X, Ry = 0)P (X). When Ry = 0, by eq. (1), Y ? = Y . Hence,
P (X, Y ) = P (Y ? |X, Ry = 0)P (X)

(3)

and since X is fully observable, P (X, Y ) is recoverable.
Note that eq. (2) permits P (X, Y ) to be recovered by listwise deletion, while eq. (3) does not; it
requires that P (X) be estimated first over all samples, including those in which Y is missing. In
this paper we focus on recoverability under large sample assumption and will not be dealing with
the shrinking sample size issue.
Recoverability when data are MNAR Data that are neither MAR nor MCAR are termed MNAR.
Though it is generally believed that relations in MNAR datasets are not recoverable, the following
example demonstrates otherwise.
Example 3. Fig. 1 (d) depicts a study where (i) some units who underwent treatment (X = 1) did
not report the outcome (Y ) and (ii) we accidentally deleted the values of treatment for a handful
of cases. Thus we have missing values for both X and Y which renders the dataset MNAR. We
shall show that P (X, Y ) is recoverable. From D, we can compute P (X ? , Y ? , Rx , Ry ). From the
m-graph G, we see that X?
?Rx and Y ??(Rx ? Ry )|X. Thus P (X, Y ) = P (Y |X)P (X) =
P (Y |X, Ry = 0, Rx = 0)P (X|Rx = 0). When Ry = 0 and Rx = 0 we have (by Equation (1) ),
Y ? = Y and X ? = X. Hence,
P (X, Y ) = P (Y ? |X ? , Rx = 0, Ry = 0)P (X ? |Rx = 0)

(4)

Therefore, P (X, Y ) is recoverable.
The estimand in eq. (4) also dictates how P (X, Y ) should be estimated from the dataset. In the first
step, we delete all cases in which X is missing and create a new data set D0 from which we estimate
P (X). Dataset D0 is further pruned to form dataset D00 by removing all cases in which Y is missing.
P (Y |X) is then computed from D00 . Note that order matters; had we deleted cases in the reverse
order, Y and then X, the resulting estimate would be biased because the d-separations needed for
establishing the validity of the estimand: P (X|Y )P (Y ), are not supported by G. We will call this
sequence of deletions as deletion order.
Several features are worth noting regarding this graph-based taxonomy of missingness mechanisms.
First, although MCAR and MAR can be verified by inspecting the m-graph, they cannot, in general
be verified from the data alone. Second, the assumption of MCAR allows an estimation procedure
that amounts (asymptotically) to listwise deletion, while MAR dictates a procedure that amounts
to listwise deletion in every stratum of Vo . Applying MAR procedure to MCAR problem is safe,
because all conditional independencies required for recoverability under the MAR assumption also
hold in an MCAR problem, i.e. R?
?(Vo , Vm ) ? R??Vm |Vo . The converse, however, does not
hold, as can be seen in Fig. 1 (b). Applying listwise deletion is likely to result in bias, because the
necessary condition R?
?(Vo , Vm ) is violated in the graph. An interesting property which evolves
from this discussion is that recoverability of certain relations does not require RVi ??Vi |Vo ; a subset
of Vo would suffice as shown below.
Property 1. P (Vi ) is recoverable if ?W ? Vo such that RVi ??V |W .
P
Proof: P (Vi ) may be decomposed as: P (Vi ) = w P (Vi? |Rvi = 0, W )P (W ) since Vi ??RVi |W
and W ? Vo . Hence P (Vi ) is recoverable.
It is important to note that the recoverability of P (X, Y ) in Fig. 1(d) was feasible despite the fact
that the missingness model would not be considered Rubin?s MAR (as defined in [25]). In fact, an
overwhelming majority of the data generated by each one of our MNAR examples would be outside
Rubin?s MAR. For a brief discussion on these lines, refer to Appendix- 5.4.
Our next question is: how can we determine if a given relation is recoverable? The following
theorem provides a sufficient condition for recoverability.
3.1

Conditions for Recoverability

Theorem 1. A query Q defined over variables in Vo ? Vm is recoverable if it is decomposable into
terms of the form Qj = P (Sj |Tj ) such that Tj contains the missingness mechanism Rv = 0 of
every partially observed variable V that appears in Qj .
5

Proof: If such a decomposition exists, every Qj is estimable from the data, hence the entire expression for Q is recoverable.
Example 4. Equation (4) demonstrates a decomposition of Q = P (X, Y ) into a product of two
terms Q1 = P (Y |X, Rx = 0, Ry = 0) and Q2 = P (X|Rx = 0) that satisfy the condition of
Theorem 1. Hence Q is recoverable.
Example 5. Consider the problem of recovering Q = P (X, Y ) from the m-graph of Fig. 3(b).
Attempts to decompose Q by the chain rule, as was done in Eqs. (3) and (4) would not satisfy the
conditions of Theorem 1. To witness we write P (X, Y ) = P (Y |X)P (X) and note that the graph
does not permit us to augment any of the two terms with the necessary Rx or Ry terms; X is
independent of Rx only if we condition on Y , which is partially observed, and Y is independent of
Ry only if we condition on X which is also partially observed. This deadlock can be disentangled
however using a non-conventional decomposition:
P (Rx , Ry |X, Y )
Q = P (X, Y ) = P (X, Y )
P (Rx , Ry |X, Y )
P (Rx , Ry )P (X, Y |Rx , Ry )
=
(5)
P (Rx |Y, Ry )P (Ry |X, Rx )
where the denominator was obtained using the independencies Rx ??(X, Ry )|Y and
Ry ?
?(Y, Rx )|X shown in the graph. The final expression above satisfies Theorem 1 and
renders P (X, Y ) recoverable. This example again shows that recovery is feasible even when data
are MNAR.
Theorem 2 operationalizes the decomposability requirement of Theorem 1.
Theorem 2 (Recoverability of the Joint P (V )). Given a m-graph G with no edges between the R
variables and no latent variables as parents of R variables, a necessary and sufficient condition
for recovering the joint distribution P (V ) is that no variable X be a parent of its missingness
mechanism RX . Moreover, when recoverable, P (V ) is given by
P (R = 0, v)
,
(6)
P (v) = Q
o
m
m
i P (Ri = 0|pari , pari , RP ar = 0)
i

where P aori ? Vo and P am
ri ? Vm are the parents of Ri .
Proof. (sufficiency) The observed joint distribution may be decomposed according to G as
X
P (R = 0, v) =
P (v, u)P (R = 0|v, u)
u

= P (v)

Y

P (Ri = 0|paori , pam
ri ),

(7)

i

where we have used the facts that there are no edges between the R variables, and that there are no
latent variables as parents of R variables. If Vi is not a parent of Ri (i.e. Vi 6? P am
ri ), then we have
Ri ?
?RP am
|(P aori ? P am
ri ). Therefore,
r
i

o
m
P (Ri = 0|paori , pam
= 0).
ri ) = P (Ri = 0|pari , pari , RP am
r
i

(8)

Given strictly positive P (R = 0, Vm , Vo ), we have that all probabilities P (Ri =
0|paori , pam
= 0) are strictly positive. Using Equations (7) and (8) , we conclude that
ri , RP am
ri
P (V ) is recoverable as given by Eq. (6).
(necessity) If X is a parent of its missingness mechanism RX , then P (X) is not recoverable based
on Lemmas 3 and 4 in Appendix 5.5. Therefore the joint P (V ) is not recoverable.
The following theorem gives a sufficient condition for recovering the joint distribution in a Markovian model.
Theorem 3. Given a m-graph with no latent variables (i.e., Markovian) the joint distribution P (V )
is recoverable if no missingness mechanism RX is a descendant of its corresponding variable X.
Moreover, if recoverable, then P (V ) is given by
Y
Y
P (v) =
P (vi |paoi , pam
=
0)
P (vj |paoj , pam
= 0), (9)
i , RP am
j , RVj = 0, RP am
i
j
i,Vi ?Vo

j,Vj ?Vm

where P aoi ? Vo and P am
i ? Vm are the parents of Vi .
6

Proof: Refer Appendix-5.6
Definition 2 (Ordered factorization). An ordered factorization over a set O of ordered V variables Y1 < Y2 < . . . < Yk , denoted by f (O), is a product of conditional probabilities f (O) =
Q
?({Yi+1 , . . . , Yn }\Xi )|Xi .
i P (Yi |Xi ) where Xi ? {Yi+1 , . . . , Yn } is a minimal set such that Yi ?
Theorem 4. A sufficient condition for recoverability of a relation Q is that Q be decomposable into
an ordered factorization, or a sum of such factorizations, such that every factor Qi = P (Yi |Xi )
satisfies Yi ?
?(Ryi , Rxi )|Xi . A factorization that satisfies this condition will be called admissible.
X

RX

RY

Rx

(a)

4

X3

X2

X1

Y

Rx

X4

Rx

3

2

Z

RY

X

RZ
(c)

(b)

Y

RX

X

RX

Y

RY

Z

RZ
(d)

Figure 2: Graph in which (a) only P (X|Y ) is recoverable (b) P (X4 ) is recoverable only when
conditioned on X1 as shown in Example 6 (c) P (X, Y, Z) is recoverable (d) P (X, Z) is recoverable.
Proof. follows from Theorem-1 noting that ordered factorization is one specific form of decomposition.
Theorem 4 will allow us to confirm recoverability of certain queries Q in models such as those in
Fig. 2(a), (b) and (d), which do not satisfy the requirement in Theorem 2. For example, by applying
Theorem 4 we can conclude that, (1) in Figure 2 (a), P (X|Y ) = P (X|Rx = 0, Ry = 0, Y ) is
recoverable, (2) in Figure 2 (c), P (X, Y, Z) = P (Z|X, Y, Rz = 0, Rx = 0, Ry = 0)P (X|Y, Rx =
0, Ry = 0)P (Y |Ry = 0) is recoverable and (3) in Figure 2 (d), P (X, Z) = P (X, Z|Rx = 0, Rz =
0) is recoverable.
Note that the condition of Theorem 4 differs from that of Theorem 1 in two ways. Firstly, the
decomposition is limited to ordered factorizations i.e. Yi is a singleton and Xi a set. Secondly, both
Yi and Xi are taken from Vo ? Vm , thus excluding R variables.
Example 6. Consider the query Q = P (X4 ) in Fig. 2(b). Q can be decomposed in a variety of
ways, among them
P being the factorizations:
(a) P (X4 ) = Px3 P (X4 |X3 )P (X3 ) for the order X4 , X3
(b) P (X4 ) = x2 P (X4 |X2 )P (X2 ) for the order X4 , X2
P
(c) P (X4 ) = x1 P (X4 |X1 )P (X1 ) for the order X4 , X1
Although each of X1 , X2 and X3 d-separate X4 from RP
X4 , only (c) is admissible since each factor
satisfies Theorem 4. Specifically, (c) can be written as x1 P (X4? |X1 , RX4 = 0)P (X1 ) and can
be estimated by the deletion schedule (X1 , X4 ), i.e., in each stratum of X1 , we delete samples for
which RX4 = 1 and compute P (X4? , Rx4 = 0, X1 ). In (a) and (b) however, Theorem-4 is not
satisfied since the graph does not permit us to rewrite P (X3 ) as P (X3 |Rx3 = 0) or P (X2 ) as
P (X2 |Rx2 = 0).
3.2

Heuristics for Finding Admissible Factorization

Consider the task of estimating Q = P (X), where X is a set, by searching for an admissible
factorization of P (X) (one that satisfies Theorem 4), possibly by resorting to additional variables,
Z, residing outside of X that serve as separating sets. Since there are exponentially large number
of ordered factorizations, it would be helpful to rule out classes of non-admissible ordering prior
to their enumeration whenever non-admissibility can be detected in the graph. In this section, we
provide lemmata that would aid in pruning process by harnessing information from the graph.
Lemma 1. An ordered set O will not yield an admissible decomposition if there exists a partially
observed variable Vi in the order O which is not marginally independent of RVi such that all minimal
separators (refer Appendix-5.1, Definition-4) of Vi that d-separate it from Rvi appear before Vi .
Proof: Refer Appendix-5.7
7

A

B

D

C

E

RC

X

Y

RY

RX

F

RD

RA

RE

RB
(b)

(a)

Figure 3: demonstrates (a) pruning in Example-7 (b) P (X, Y ) is recoverable in Example-5
Applying lemma-1 requires a solution to a set of disjunctive constraints which can be represented
by directed constraint graphs [5].
Example 7. Let Q = P (X) be the relation to be recovered from the graph in Fig. 3 (a). Let
X = {A, B, C, D, E} and Z = F . The total number of ordered factorizations is 6! = 720.
The independencies implied by minimal separators (as required by Lemma-1) are: A??RA |B,
B?
?RB |?, C?
?RC |{D, E}, ( D?
?RD |A or D??RD |C or D??RD |B ) and (E??RE |{B, F } or
E?
?RE |{B, D} or E?
?RE |C). To test whether (B,A,D,E,C,F) is potentially admissible we need
not explicate all 6 variables; this order can be ruled out as soon as we note that A appears after B.
Since B is the only minimal separator that d-separates A from RA and B precedes A, Lemma-1 is
violated. Orders such as (C, D, E, A, B, F ), (C, D, A, E, B, F ) and (C, E, D, A, F, B) satisfy the
condition stated in Lemma 1 and are potential candidates for admissibility.
The following lemma presents a simple test to determine non-admissibility by specifying the condition under which a given order can be summarily removed from the set of candidate orders that are
likely to yield admissible factorizations.
Lemma 2. An ordered set O will not yield an admissible decomposition if it contains a partially
observed variable Vi for which there exists no set S ? V that d-separates Vi from RVi .
Proof: The factor P (Vi |Vi+1 , . . . , Vn ) corresponding to Vi can never satisfy the condition required
by Theorem 4.
An interesting consequence of Lemma 2 is the following corollary that gives a sufficient condition
under which no ordered factorization can be labeled admissible.
Corollary 2. For any disjoint sets X and Y , there exists no admissible factorization for recovering
the relation P (Y |X) by Theorem 4 if Y contains a partially observed variable Vi for which there
exists no set S ? V that d-separates Vi from RVi .

4

Conclusions

We have demonstrated that causal graphical models depicting the data generating process can serve
as a powerful tool for analyzing missing data problems and determining (1) if theoretical impediments exist to eliminating bias due to data missingness, (2) whether a given procedure produces
consistent estimates, and (3) whether such a procedure can be found algorithmically. We formalized
the notion of recoverability and showed that relations are always recoverable when data are missing
at random (MCAR or MAR) and, more importantly, that in many commonly occurring problems,
recoverability can be achieved even when data are missing not at random (MNAR). We further
presented a sufficient condition to ensure recoverability of a given relation Q (Theorem 1) and operationalized Theorem 1 using graphical criteria (Theorems 2, 3 and 4). In summary, we demonstrated
some of the insights and capabilities that can be gained by exploiting causal knowledge in missing
data problems.

Acknowledgment
This research was supported in parts by grants from NSF #IIS-1249822 and #IIS-1302448 and ONR
#N00014-13-1-0153 and #N00014-10-1-0933

References
[1] P.D. Allison. Missing data series: Quantitative applications in the social sciences, 2002.

8

[2] T. Bu, N. Duffield, F.L. Presti, and D. Towsley. Network tomography on general topologies. In ACM
SIGMETRICS Performance Evaluation Review, volume 30, pages 21?30. ACM, 2002.
[3] E.R. Buhi, P. Goodson, and T.B. Neilands. Out of sight, not out of mind: strategies for handling missing
data. American journal of health behavior, 32:83?92, 2008.
[4] R.M. Daniel, M.G. Kenward, S.N. Cousens, and B.L. De Stavola. Using causal diagrams to guide analysis
in missing data problems. Statistical Methods in Medical Research, 21(3):243?256, 2012.
[5] R. Dechter, I. Meiri, and J. Pearl. Temporal constraint networks. Artificial intelligence, 1991.
[6] C.K. Enders. Applied Missing Data Analysis. Guilford Press, 2010.
[7] U.M. Fayyad. Data mining and knowledge discovery: Making sense out of data. IEEE expert, 11(5):20?
25, 1996.
[8] F. M. Garcia. Definition and diagnosis of problematic attrition in randomized controlled experiments.
Working paper, April 2013. Available at SSRN: http://ssrn.com/abstract=2267120.
[9] R.D. Gill and J.M. Robins. Sequential models for coarsening and missingness. In Proceedings of the
First Seattle Symposium in Biostatistics, pages 295?305. Springer, 1997.
[10] R.D. Gill, M.J. Van Der Laan, and J.M. Robins. Coarsening at random: Characterizations, conjectures, counter-examples. In Proceedings of the First Seattle Symposium in Biostatistics, pages 255?294.
Springer, 1997.
[11] J.W Graham. Missing Data: Analysis and Design (Statistics for Social and Behavioral Sciences).
Springer, 2012.
[12] D.F. Heitjan and D.B. Rubin. Ignorability and coarse data. The Annals of Statistics, pages 2244?2253,
1991.
[13] R.J.A. Little and D.B. Rubin. Statistical analysis with missing data. Wiley, 2002.
[14] B.M. Marlin and R.S. Zemel. Collaborative prediction and ranking with non-random missing data. In
Proceedings of the third ACM conference on Recommender systems, pages 5?12. ACM, 2009.
[15] B.M. Marlin, R.S. Zemel, S. Roweis, and M. Slaney. Collaborative filtering and the missing at random
assumption. In UAI, 2007.
[16] B.M. Marlin, R.S. Zemel, S.T. Roweis, and M. Slaney. Recommender systems: missing data and statistical model estimation. In IJCAI, 2011.
[17] P.E. McKnight, K.M. McKnight, S. Sidani, and A.J. Figueredo. Missing data: A gentle introduction.
Guilford Press, 2007.
[18] Harvey J Miller and Jiawei Han. Geographic data mining and knowledge discovery. CRC, 2009.
[19] K. Mohan and J. Pearl. On the testability of models with missing data. To appear in the Proceedings of
AISTAT-2014; Available at http://ftp.cs.ucla.edu/pub/stat ser/r415.pdf.
[20] J. Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann, 1988.
[21] J. Pearl. Causality: models, reasoning and inference. Cambridge Univ Press, New York, 2009.
[22] J. Pearl and K. Mohan. Recoverability and testability of missing data: Introduction and summary of
results. Technical Report R-417, UCLA, 2013. Available at http://ftp.cs.ucla.edu/pub/stat ser/r417.pdf.
[23] C.Y.J. Peng, M. Harwell, S.M. Liou, and L.H. Ehman. Advances in missing data methods and implications
for educational research. Real data analysis, pages 31?78, 2006.
[24] J.L. Peugh and C.K. Enders. Missing data in educational research: A review of reporting practices and
suggestions for improvement. Review of educational research, 74(4):525?556, 2004.
[25] D.B. Rubin. Inference and missing data. Biometrika, 63:581?592, 1976.
[26] D.B. Rubin. Multiple Imputation for Nonresponse in Surveys. Wiley Online Library, New York, NY,
1987.
[27] D.B. Rubin. Multiple imputation after 18+ years. Journal of the American Statistical Association,
91(434):473?489, 1996.
[28] J.L. Schafer and J.W. Graham. Missing data: our view of the state of the art. Psychological Methods,
7(2):147?177, 2002.
[29] F. Thoemmes and N. Rose. Selection of auxiliary variables in missing data problems: Not all auxiliary
variables are created equal. Technical Report Technical Report R-002, Cornell University, 2013.
[30] M.J. Van der Laan and J.M. Robins. Unified methods for censored longitudinal data and causality.
Springer Verlag, 2003.
[31] W. Wothke. Longitudinal and multigroup modeling with missing data. Lawrence Erlbaum Associates
Publishers, 2000.

9

"
1994,Single Transistor Learning Synapses,,994-single-transistor-learning-synapses.pdf,Abstract Missing,"Single Transistor Learning Synapses

Paul Hasler, Chris Diorio, Bradley A. Minch, Carver Mead
California Institute of Technology
Pasadena, CA 91125
(818) 395 - 2812
paul@hobiecat.pcmp.caltech.edu

Abstract
We describe single-transistor silicon synapses that compute, learn,
and provide non-volatile memory retention. The single transistor
synapses simultaneously perform long term weight storage, compute the product of the input and the weight value, and update the
weight value according to a Hebbian or a backpropagation learning
rule. Memory is accomplished via charge storage on polysilicon
floating gates, providing long-term retention without refresh. The
synapses efficiently use the physics of silicon to perform weight updates; the weight value is increased using tunneling and the weight
value decreases using hot electron injection. The small size and
low power operation of single transistor synapses allows the development of dense synaptic arrays. We describe the design, fabrication, characterization, and modeling of an array of single transistor synapses. When the steady state source current is used as
the representation of the weight value, both the incrementing and
decrementing functions are proportional to a power of the source
current. The synaptic array was fabricated in the standard 21'm
double - poly, analog process available from MOSIS.

1

INTRODUCTION

The past few years have produced a number of efforts to design VLSI chips which
""learn from experience."" The first step toward this goal is developing a silicon
analog for a synapse. We have successfully developed such a synapse using only

Paul Hasler, Chris Diorio, Bradley A. Minch, Carver Mead

818

Gate

Aoating Gate

!
Drain

Source

...I._High

,oltage

--~~--'-~------r-~--~~--~

Figure 1: Cross section of the single transistor synapse. Our single transistor
synapse uses a separate tunneling voltage terminal The pbase implant results in
a larger threshold voltage, which results in all the electrons reaching the top of the
Si02 barrier to be swept into the floating gate.

a single transistor. A synapse has two functional requirements. First, it must
compute the product of the input multiplied by the strength (the weight) of the
synapse. Second, the synapse must compute the weight update rule. For a Hebbian
synapse, the change in the weight is the time average of the product of the input and
output activity. In many supervised algorithms like backpropagation, this weight
change is the time average of the product of the input and some fed back error
signal. Both of these computations are similar in function. We have developed
single transistor synapses which simultaneously perform long term weight storage,
compute the product of the input and the weight value, and update the weight
value according to a Hebbian or a backpropagation learning rule. The combination
of functions has not previously been achieved with floating gate synapses.
There are five requirements for a learning synapse. First, the weight should be
stored permanently in the absence of learning. Second, the synapse must compute
as an output the product of the input signal with the synaptic weight. Third, each
synapse should require minimal area, resulting in the maximum array size for a given
area. Fourth, each synapse should operate with low power dissipation so that the
synaptic array is not power constrained. And finally, the array should be capable
of implementing either Hebbian or Backpropagation learning rule for modifying
the weight on the floating gate. We have designed, fabricated, characterized, and
modeled an array of single transistor synapses which satisfy these five criteria. We
believe this is the first instance of a single transistor learning synapse fabricated in
a standard process.

2

OVERVIEW

Figure 1 shows the cross section for the single transistor synapse. Since the floating gate is surrounded by Si0 2 , an excellent insulator, charge leakage is negligible
resulting in nearly permanent storage of the weight value. An advantage of using
floating gate devices for learning rules is the timescales required to add and remove
charge from the floating gate are well matched to the learning rates of visual and
auditory signals. In addition, these learning rates can be electronically controlled.
Typical resolution of charge on the floating gate after ten years is four bits (Holler
89). The FETs are in a moderately doped (1 x 10 17 em- 3 ) substrate, to achieve a

Single Transistor Learning Synapses

Vg1

t

819

Vg2

Vtun1

Vd1
(1,1)

t

(1,2)

---+----~--------~----~----VS1~
---+-..-------------+_e_--------- Vtun2

_

Vd2

(2,1)

(2,2)

---+--~~---1---~----VS2~
Figure 2: Circuit diagram of the single - transistor synapse array. Each transistor has a floating gate capacitively coupled to an input column line. A tunneling
connection (arrow) allows weight increase. Weight decreased is achieved by hot
electron injection in the transistor itself. Each synapse is capable of simultaneous
feedforward computations and weight updates. A 2 x 2 section of the array allows
us to characterize how modifying a single floating gate (such as synapse (1,1)) effects the neighboring floating gate values. The synapse currents are a measure of
the synaptic weights, and are summed along each row by the source (Vs) or drain
(Vd) lines into some soma circuit.

high threshold voltage. The moderately doped substrate is formed in the 2pm MOSIS process by the pbase implant. npn transistor. The implant has the additional
benefit of increasing the efficiency of the hot electron injection process by increasing
the electric field in the channel. Each synapse has an additional tunneling junction
for modifying the charge on the floating gate. The tunneling junction is formed
with high quality gate oxide separating a well region from the floating gate.
Each synapse in our synaptic array is a single transistor with its weight stored as
a charge on a floating silicon gate. Figure 2 shows the circuit diagram of a 2 x
2 array of synapses. The column 'gate' inputs (Vg) are connected to second level
polysilicon which capacitively couples to the floating gate. The inputs are shared
along a column. The source (Vs), drain (lid), and tunneling (Viun) terminals are
shared along a row. These terminals are involved with computing the output current
and feeding back 'error' signal voltages. Many other synapses use floating gates to
store the weight value, as in (Holler 89), but none of the earlier approaches update
the charge on the floating gate during the multiplication of the input and floating
gate value. In these previous approaches one must drive the floating gate over large
a voltage range to tunnel electrons onto the floating gate. Synaptic computation
must stop for this type of weight update.
The synapse computes as an output current a product of weight and input signal,

820

Paul Hasler. Chris Diorio. Bradley A. Minch. Carver Mead

10.7

Tunneling Opel1llions
synapse (1.2)
... . . . '.

Iojec:tion Opel1lliolll
10""

synapse (2,1), synapse (2.2)

10""

"". """" synapse (1.1)

... ..-

15

S

j

10. 10

10.11

10- 12

row 2 background alln:nl

row I IJeckarouod alrlent
10-13 ""-_--'-_ _-'-_ _-'--_ _'""--_----'_ _--'-_ _...J
o
SO
I 00
I SO
200
2S0
300
3SO

Step iteration Number

Figure 3: Output currents from a 2 x 2 section of the synapse array, showing
180 injection operations followed by 160 tunneling operations. For the injection
operations, the drain (V dl) is pulsed from 2.0 V upto 3.3 V for 0.5s with Vg1 at 8V
and Vg2 at OV. For the tunneling operations, the tunneling line (Vtunl) is pulsed
from 20 V up to 33.5 V with Vg2 at OV and ""91 at 8V. Because our measurements
from the 2 x 2 section come from a larger array, we also display the 'background'
current from all other synapses on the row. This background current is several orders
of magnitude smaller than the selected synapse current, and therefore negligible.

and can simultaneously increment or decrement the weight as a function of its input
and error voltages. The particular learning algorithm depends on the circuitry at
the boundaries of the array; in particular the circuitry connected to each of the
source, drain, and tunneling lines in a row. With charge Qlg on the floating gate
and Vs equal to 0 the subthreshold source current is described by

(1)

k;.

where Qo is a device dependent parameter, and UT is the thermal voltage
The coupling coefficient, 6, of the gate input to the transistor surface potential is
typically less than 0.1. From ( 1) We can consider the weight as a current I, defined
by
ISllnapse

=

(

~~)

Ioe---qo- e

T

e

6gVp
T

= Ie

6gVp
T

(2)

where Vgo is the input voltage bias, and ~ Vg is Vg - ""90. The synaptic current is
thus the product of the weight, I, and a weak exponential function of the input
voltage.
The single transistor learning synapses use a combination of electron tunneling and
hot electron injection to adapt the charge on the floating gate, and thereby the

821

Single Transistor Learning Synapses

weight of the synapse. Hot electron injection adds electrons to the floating gate,
thereby decreasing the weight. Injection occurs for large drain voltages; therefore
the floating gate charge can be reduced during normal feedforward operation by raising the drain voltage. Electron tunneling removes electrons from the floating gate,
thereby increasing the weight. The tunneling line controls the tunneling current;
thus the floating gate charge can be increased during normal feedforward operation
by raising the tunneling line voltage. The tunneling rate is modulated by both the
input voltage and the charge on the floating gate.
Figure 3 shows an example the nature of the weight update process. The source
current is used as a measure of the synapse weight. The experiment starts with all
four synapses set to the same weight current. Then, synapse (1,1) is injected for 180
cycles to preferentially decrease its weight. Finally, synapse (1,1) is tunneled for 160
cycles to preferentially increase its weight. This experiment shows that a synapse
can be incremented by applying a high voltage on tunneling terminals and a low
voltage on the input, and can be decremented by applying a high voltage on drain
terminals and a high voltage on the input. In the next two sections, we consider
the nature of these update functions. In section three we examine the dependence
of hot electron injection on the source current of our synapses. In section four we
examine the dependence of electron tunneling on the source current of our synapses.

3

Hot Electron Injection

Hot electron injection gives us a method to add electrons to the floating gate. The
underlying physics of the injection process is to give some electrons enough energy
and direction in the channel to drain depletion region to surmount the Si0 2 energy
barrier. A device must satisfy two requirements to inject an electron on a floating
gate. First, we need a region where the potential drops more than 3.1 volts in a
distance of less than 0.2pm to allow electrons to gain enough energy to surmount
the oxide barrier. Second, we need a field in the oxide in the proper direction
to collect electrons after they cross the barrier. The moderate substrate doping
level allows us to easily achieve both effects in subthreshold operation. First, the
higher substrate doping results in a much higher threshold voltage (6.1 V), which
guarantees that the field in the oxide at the drain edge of the channel will be in
the proper direction for collecting electrons over the useful range of drain voltages.
Second, the higher substrate doping results in higher electric fields which yield
higher injection efficiencies. The higher injection efficiencies allow the device to have
a wide range of drain voltages substantially below the threshold voltage. Figure 4
shows measured data on the change in source current during injection vs. source
current for several values of drain voltage.
Because the source current, I, is related to the floating gate charge, Q,g as shown
in ( 1) and the charge on the floating gate is related to the tunneling or injection
current (I, g) by
dQ,g _ I
(3)
dt - Ig
an approximate model for the change of the weight current value is
I
(4)
-dl
=
-1
dt
Qo

'9

Paul Hasler, Chris Diorio, Bradley A. Minch, Carver Mead

822

!

10""

i

10.10

~

10'""

synapse (1,1)

i!,

J.
.S

3.3

10.12

g

synapse (1,2)

10'""
3.6

Sour.-e Curren! (Weiaht Value)

Figure 4: Source Current Decrement during injection vs. Source Current for several
values of drain voltage. The injection operation decreases the synaptic weight. V 92
was held at OV, and V 91 was at 8V during the 0.5s injecting pulses. The change in
source current is approximately proportional to the source current to the f3 power,
where of f3 is between 1.7 and 1.85 for the range of drain voltages shown. The change
in source current in synapse (1,2) is much less than the corresponding change in
synapse (1,1) and is nearly independent of drain voltage. The effect of this injection
on synapses (2,1) and (2,2) is negligible.

The injection current can be approximated over the range of drain voltages shown
in Fig. 4 by (Hasler 95)

(5)
where Vd-c is the voltage from the drain to the drain edge of the channel, Vd is the
drain voltage, /0 is a slowly varying function defined in (Hasler 95), and Vini is in
the range of 60m V to 100mV. A is device dependent parameter, Since hot electron
injection adds electrons to the floating gate, the current into the floating gate (If 9)
is negative, which results in
IfJ ~
-dI
=
-A-e .nJ
(6)
dt
Qo
The model agrees well with the data in Fig. 4, with f3 in the range of 1. 7 - 1.9.
Injection is very selective along a row with a selectivity coefficient between 102 and
107 depending upon drain voltage and weight. The injection operations resulted in
negligible changes in source current for synapses (2,1) and (2,2).

4

ELECTRON TUNNELING

Electron tunneling gives us a method for removing electrons from the floating gate.
Tunneling arises from the fact that an electron wavefunction has finite extent. For a

Single Transistor Learning Synapses

i

10""

I
'Ii
i!

10.9

~

VlUn

I
a

)10.

823

=36.0 ?'
3S.0 ..--

10

34.0

3S.S
34.S

.5

110.

11

33.S

33.0

Source Cumnt (Weig/ll Value)

Figure 5: Synapse (1,1) source current increment vs. Source Current for several
values of tunneling voltage. The tunneling operation increases the synaptic weight.
V 91 was held at OV and V 92 was 8V while the tunneling line was pulsed for 0.5s
from 20V to the voltage shown. The change in source current is approximately
proportional to the a power of the svurce current where a is between 0.7 and 0.9
for the range of tunneling voltages shown. The effect of this tunneling procedure
on synapse (2,1) and (2,2) are negligible. The selectivity ratio of synapses on the
same row is typically between 3-7 for our devices.

thin enough barrier, this extent is sufficient for an electron to penetrate the barrier.
An electric field across the oxide will result in a thinner barrier to the electrons on
the floating gate. For a high enough electric field, the electrons can tunnel through
the oxide.
When traveling through the oxide, some electrons get trapped in the oxide, which
changes the barrier profile. To reduce this trapping effect we tunnel through high
quality gate oxide, which has far less trapping than interpoly oxide. Both injection
and tunneling have very stable and repeatable characteristics. When tunneling at
a fixed oxide voltage, the tunneling current decreases only 50 percent after lOnG of
charge has passed through the oxide. This quantity of charge is orders of magnitude
more than we would expect a synapse to experience over a lifetime of operation.
Figure 5 shows measured data on the change in source current during tunneling as
a function of source current for several values of tunneling voltage. The functional
form of tunneling current is of the form (Lenzlinger 69)

(7)
where Yo, f Olun are model parameters which roughly correspond with theory. Tunneling removes electrons from the floating gate; therefore the floating gate current
is positive. By expanding Vfg for fixed Viun as VfgO + ~ Vfg and inserting ( 1), the

824

Paul Hasler, Chris Diorio, Bradley A. Minch, Carver Mead

II Parameter I Typical Values II Parameter I Typical Values II
{3
Q'
1.7 - 1.9
0.7 - 0.9
.2 pC
6
0.02 - 0.1
Qo
8.6 x 10 ?~u
A
78mV
Vinj

Table 1: Typical measured values of the parameters in the modeling of the single
transistor synapse array.

resulting current change is

dI
I
dt =

Q:n e- Viu

Vg

v/ gO Iso

(

I
Iso

)Q

(8)

where IsO is the bias current corresponding to VfgO. The model qualitatively agrees
with the data in Fig. 5, with Q' in the range of 0.7- 0.9. The tunneling selectivity
between synapses on different rows is very good, but tunneling selectivity along
along a row is poor. We typically measure tunneling selectivity ratios along a row
between 3 - 7 for our devices.

5

Model of the Array of Single Transistor Synapses

Finally, we present an approximate model of our array of these single transistor
synapses. The learning increment of the synapse at position (i,i) can be modeled
as
61l.V
ISllnapse'j = Iije ~ == Iso WijXj
dW. .
1
_
Vq
_
;d;
_
(9)
=..:.:...!.L =.:.?t.II..A.e VI""n v/ gO W~x~ 1 _....d.... e inj WP.xl? 1
dt
Qo
I) )
Qo
I) )
for the synapse at position (i,i), where Wi,j can be considered the weight value,
and x j are the effective inputs network. Typical values for the parameters in ( 9 )
are given in Table 1.

Acknowledgments
The work was supported by the office of Naval Research, the Advanced Research
Projects Agency, and the Beckman Foundation.

References
P. Hasler, C. Diorio, B. Minch, and C. Mead (1995) ""An Analytic model of Hot
Electron Injection from Boltzman Transport"", Tech. Report 123456
M. Holler, S. Tam, H. Castro, and R. Benson (1989), ""An electrically trainable
artificial neural network with 10240 'floating gate' synapses"", International Joint
Conference on Neural Networks, Wasllington, D.C., June 1989, pp. 11-191 - 11-196.
M. Lenzlinger and E. H. Snow (1969), ""Fowler-Nordheim tunneling into thermally
grown Si0 2 ,"" J. Appl. Phys., vol. 40, pp. 278-283, 1969.

PARTVll
SPEECH AND SIGNAL PROCESSING

"
2010,Generalized roof duality and bisubmodular functions,,4021-generalized-roof-duality-and-bisubmodular-functions.pdf,"Consider a convex relaxation $\hat f$ of a pseudo-boolean function $f$. We say that the relaxation is {\em totally half-integral} if $\hat f(\bx)$ is a polyhedral function with half-integral extreme points $\bx$, and this property is preserved after adding an arbitrary combination of constraints of the form $x_i=x_j$, $x_i=1-x_j$, and $x_i=\gamma$ where $\gamma\in\{0,1,\frac{1}{2}\}$ is a constant. A well-known example is the {\em roof duality} relaxation for quadratic pseudo-boolean functions $f$. We argue that total half-integrality is a natural requirement for generalizations of roof duality to arbitrary pseudo-boolean functions.  Our contributions are as follows. First, we provide a complete characterization of totally half-integral relaxations $\hat f$ by establishing a one-to-one correspondence with {\em bisubmodular functions}. Second, we give a new characterization of bisubmodular functions. Finally, we show some relationships between general totally half-integral relaxations and relaxations based on the roof duality.","Generalized roof duality and bisubmodular functions

Vladimir Kolmogorov
Department of Computer Science
University College London, UK
v.kolmogorov@cs.ucl.ac.uk

Abstract
Consider a convex relaxation f? of a pseudo-boolean function f . We say that
the relaxation is totally half-integral if f?(x) is a polyhedral function with halfintegral extreme points x, and this property is preserved after adding an arbitrary
combination of constraints of the form xi = xj , xi = 1 ? xj , and xi = ? where
? ? {0, 1, 21 } is a constant. A well-known example is the roof duality relaxation
for quadratic pseudo-boolean functions f . We argue that total half-integrality is a
natural requirement for generalizations of roof duality to arbitrary pseudo-boolean
functions.
Our contributions are as follows. First, we provide a complete characterization
of totally half-integral relaxations f? by establishing a one-to-one correspondence
with bisubmodular functions. Second, we give a new characterization of bisubmodular functions. Finally, we show some relationships between general totally
half-integral relaxations and relaxations based on the roof duality.

1

Introduction

Let V be a set of |V | = n nodes and B ? K1/2 ? K be the following sets:
B = {0, 1}V

K1/2 = {0, 21 , 1}V

K = [0, 1]V

A function f : B ? R is called pseudo-boolean. In this paper we consider convex relaxations
f? : K ? R of f which we call totally half-integral:
Definition 1. (a) Function f? : P ? R where P ? K is called half-integral if it is a convex
polyhedral function such that all extreme points of the epigraph {(x, z) | x ? P, z ? f?(x)} have
the form (x, f?(x)) where x ? K1/2 . (b) Function f? : K ? R is called totally half-integral if
restrictions f? : P ? R are half-integral for all subsets P ? K obtained from K by adding an
arbitrary combination of constraints of the form xi = xj , xi = xj , and xi = ? for points x ? K.
Here i, j denote nodes in V , ? denotes a constant in {0, 1, 12 }, and z ? 1 ? z.
A well-known example of a totally half-integral
relaxation
is the roof duality relaxation for quadratic
P
P
pseudo-boolean functions f (x) =
i ci x i +
(i,j) cij xi xj studied by Hammer, Hansen and
Simeone [13]. It is known to possess the persistency property: for any half-integral minimizer
x
? ? arg min f?(?
x) there exists minimizer x ? arg min f (x) such that xi = x
?i for all nodes i with
integral component x
?i . This property is quite important in practice as it allows to reduce the size
of the minimization problem when x
? 6= 21 . The set of nodes with guaranteed optimal solution can
sometimes be increased further using the PROBE technique [6], which also relies on persistency.
The goal of this paper is to generalize the roof duality approach to arbitrary pseudo-boolean functions. The total half-integrality is a very natural requirement of such generalizations, as discussed
later in this section. As we prove, total half-integrality implies persistency.
1

We provide a complete characterization of totally half-integral relaxations. Namely, we prove in section 2 that if f? : K ? R is totally half-integral then its restriction to K1/2 is a bisubmodular function,
and conversely any bisubmodular function can be extended to a totally half-integral relaxation.
Definition 2. Function f : K1/2 ? R is called bisubmodular if
? x, y ? K1/2

f (x u y) + f (x t y) ? f (x) + f (y)

(1)

where binary operators u, t : K1/2 ? K1/2 ? K1/2 are defined component-wise as follows:
u

0

0

0

1
2

1
2
1
2

1

1
2
1
2
1
2
1
2

1

t

0

1
2

1

1
2
1
2

0

0

0

1
2

1
2

0

1
2

1

1

1

1
2

1

1

(2)

As our second contribution, we give a new characterization of bisubmodular functions (section 3).
Using this characterization, we then prove several results showing links with the roof duality relaxation (section 4).
1.1

Applications

This work has been motivated by computer vision applications. A fundamental task in vision is
to infer pixel properties from observed data. These properties can be the type of object to which
the pixel belongs, distance to the camera, pixel intensity before being corrupted by noise, etc. The
popular MAP-MRF approach casts the P
inference task as an energy minimization problem with the
objective function of the form f (x) = C fC (x) where C ? V are subsets of neighboring pixels
of small cardinality (|C| = 1, 2, 3, . . .) and terms fC (x) depend only on labels of pixels in C.
For some vision applications the roof duality approach [13] has shown a good performance [30,
32, 23, 24, 33, 1, 16, 17].1 Functions with higher-order terms are steadily gaining popularity in
computer vision [31, 33, 1, 16, 17]; it is generally accepted that they correspond to better image
models. Therefore, studying generalizations of roof duality to arbitrary pseudo-boolean functions
is an important task. In such generalizations the total half-integrality property is essential. Indeed,
in practice, the relaxation f? is obtained as the sum of relaxations f?C constructed for each term
independently. Some of these terms can be c|xi ? xj | and c|xi + xj ? 1|. If c is sufficiently
large, then applying the roof duality relaxation to these terms would yield constraints xi = xj and
x = xj present in the definition of total half-integrality. Constraints xi = ? ? {0, 1, 21 } can also
be simulated via the roof duality, e.g. xi = xj , xi = xj for the same pair of nodes i, j implies
xi = xj = 12 .
1.2

Related work

Half-integrality There is a vast literature on using half-integral relaxations for various combinatorial optimization problems. In many cases these relaxations lead to 2-approximation algorithms.
Below we list a few representative papers.
The earliest work recognizing half-integrality of polytopes with certain pairwise constraints was
perhaps by Balinksi [3], while the persistency property goes back to Nemhauser and Trotter [28]
who considered the vertex cover problem. Hammer, Hansen and Simeone [13] established that these
properties hold for the roof duality relaxation for quadratic pseudo-boolean functions. Their work
was generalized to arbitrary pseudo-boolean functions by Lu and Williams [25]. (The relaxation
in [25] relied on converting function f to a multinomial representation; see section 4 for more
details.) Hochbaum [14, 15] gave a class of integer problems with half-integral relaxations. Very
recently, Iwata and Nagano [18] formulated a half-integral relaxation for the problem of minimizing
submodular function f (x) under constraints of the form xi + xj ? 1.
1
In many vision problems variables xi are not binary. However, such problems are often reduced to
a sequence of binary minimization problems using iterative move-making algorithms, e.g. using expansion
moves [9] or fusion moves [23, 24, 33, 17].

2

In computer vision, several researchers considered the following scheme: given a function f (x) =
P
fC (x), convert terms fC (x) to quadratic pseudo-boolean functions by introducing auxiliary
binary variables, and then apply the roof duality relaxation to the latter. Woodford et al. [33] used
this technique for the stereo reconstruction problem, while Ali et al. [1] and Ishikawa [16] explored
different conversions to quadratic functions.
To the best of our knowledge, all examples of totally half-integral relaxations proposed so far belong
to the class of submodular relaxations, which is defined in section 4. They form a subclass of more
general bisubmodular relaxations.
Bisubmodularity Bisubmodular functions were introduced by Chandrasekaran and Kabadi as rank
functions of (poly-)pseudomatroids [10, 19]. Independently, Bouchet [7] introduced the concept of
?-matroids which is equivalent to pseudomatroids. Bisubmodular functions and their generalizations have also been considered by Qi [29], Nakamura [27], Bouchet and Cunningham [8] and Fujishige [11]. The notion of the Lov?asz extension of a bisubmodular function introduced by Qi [29]
will be of particular importance for our work (see next section).
It has been shown that some submodular minimization algorithms can be generalized to bisubmodular functions. Qi [29] showed the applicability of the ellipsoid method. A weakly polynomial combinatorial algorithm for minimizing bisubmodular functions was given by Fujishige and Iwata [12],
and a strongly polynomial version was given by McCormick and Fujishige [26].
Recently, we introduced strongly and weakly tree-submodular functions [22] that generalize bisubmodular functions.

2

Total half-integrality and bisubmodularity

The first result of this paper is following theorem.
Theorem 3. If f? : K ? R is a totally half-integral relaxation then its restriction to K1/2 is bisubmodular. Conversely, if function f : K1/2 ? R is bisubmodular then it has a unique totally halfintegral extension f? : K ? R.
This section is devoted to the proof of theorem 3. Denote L = [?1, 1]V , L1/2 = {?1, 0, 1}V . It
? : L ? R and h : L1/2 ? R obtained from f? and f via
will be convenient to work with functions h
a linear change of coordinates xi 7? 2xi ? 1. Under this change totally half-integral relaxations are
transformed to totally integral relaxations:
? : L ? R be a function of n variables. (a) h
? is called integral if it is a convex
Definition 4. Let h
?
polyhedral function such that all extreme points of the epigraph {(x, z) | x ? L, z ? h(x)}
have the
1/2
?
? is called totally integral if it is integral and for an arbitrary
form (x, h(x))
where x ? L . (b) h
ordering of nodes the following functions of n ? 1 variables (if n > 1) are totally integral:
? 0 (x1 , . . . , xn?1 )
h
? 0 (x1 , . . . , xn?1 )
h

? 1 , . . . , xn?1 , xn?1 )
= h(x
? 1 , . . . , xn?1 , ?xn?1 )
= h(x

? 0 (x1 , . . . , xn?1 )
h

? 1 , . . . , xn?1 , ?)
= h(x

for any constant ? ? {?1, 0, 1}

The definition of a bisubmodular function is adapted as follows: function h : L1/2 ? R is bisubmodular if inequality (1) holds for all x, y ? L1/2 where operations u, t are defined by tables (2)
after replacements 0 7? ?1, 21 7? 0, 1 7? 1. To prove theorem 3, it suffices to establish a link
? : L ? R and bisubmodular functions h : L1/2 ? R. We can
between totally integral relaxations h
?
assume without loss of generality that h(0)
= h(0) = 0, since adding a constant to the functions
does not affect the theorem.
A pair ? = (?, ?) where ? : V ? {1, . . . , n} is a permutation of V and ? ? {?1, 1}V will be
called a signed ordering. Let us rename nodes in V so that ?(i) = i. To each signed ordering ? we
associate labelings x0 , x1 , . . . , xn ? L1/2 as follows:
x0 = (0, 0, . . . , 0)

x1 = (?1 , 0, . . . , 0)
3

...

xn = (?1 , ?2 , . . . , ?n )

(3)

where nodes are ordered according to ?.
? : RV ? R is defined in
Consider function h : L1/2 ? R with h(0) = 0. Its Lov?asz extension h
V
the following way [29]. Given a vector x ? R , select a signed ordering ? = (?, ?) as follows:
(i) choose ? so that values |xi |, i ? V are non-increasing, and rename nodes accordingly so that
|x1 | ? . . . ? |xn |; (ii) if xi 6= 0 set ?i = sign(xi ), otherwise choose ?i ? {?1, 1} arbitrarily. It
is not difficult to check that
n
X
x=
?i xi
(4a)
i=1
i

where labelings x are defined in (3) (with respect to the selected signed ordering) and ?i = |xi | ?
|xi+1 | for i = 1, . . . , n ? 1, ?n = |xn |. The value of the Lov?asz extension is now defined as
?
h(x)
=

n
X

?i h(xi )

(4b)

i=1

? is convex on
Theorem 5 ([29]). Function h is bisubmodular if and only if its Lov?asz extension h
L. 2
Let L? be the set of vectors in L for which signed ordering ? = (?, ?) can be selected. Clearly,
L? = {x ? L | |x1 | ? . . . ? |xn |, xi ?i ? 0 ?i ? V }. It is easy to check that L? is the convex hull
? is linear on L? and coincides with h in each corner
of n + 1 points (3). Equations (4) imply that h
x0 , . . . , x n .
? : L ? R is totally integral. Then h
? is linear on simplex L? for each
Lemma 6. Suppose function h
signed ordering ? = (?, ?).
Proof. We use induction on n = |V |. For n = 1 the claim is straightforward; suppose that n ? 2.
? is linear on the boundary ?L? ; this
Consider signed ordering ? = (?, ?). We need to prove that h
?
will imply that g? is linear on L? since otherwise h would have an extreme point in the the interior
L? \?L? which cannot be integral.
Let X = {x0 , . . . , xn } be the set of extreme points of L? defined by (3). The boundary ?L? is the
union of n + 1 facets L0? , . . . , Ln? where Li? is the convex hull of points in X\{xi }. Let us prove
? is linear on L0 . All points x ? X\{x0 } satisfy x1 = ?1 , therefore L0 = {x ? L? | x1 =
that h
?
?
? 0 (x2 , . . . , xn ) = h(?
? 1 , x2 , . . . , xn ), and let L0 0 be the
?1 }. Consider function of n ? 1 variables h
?
? 0 is linear on L0 0 , and thus h
? is linear on
projection of L0? to RV \{1} . By the induction hypothesis h
?
L0? .
? is linear on other facets can be proved in a similar way. Note that for i = 2, . . . , n ? 1
The fact that h
there holds Li? = {x ? L? | xi = ?i?1 ?i xi?1 }, and for i = n we have Ln? = {x ? L? | xn = 0}.
? : L ? R with h(0)
?
Corollary 7. Suppose function h
= 0 is totally integral. Let h be the restriction
1/2
?
?
? and h
? coincide on L.
of h to L and h be the Lov?asz extension of h. Then h
Theorem 5 and corollary 7 imply the first part of theorem 3. The second part will follow from
? : L ? R is
Lemma 8. If h : L1/2 ? R with h(0) = 0 is bisubmodular then its Lov?asz extension h
totally integral.
? is assumed to be convex on RV rather than on L.
Note, Qi formulates this result slightly differently: h
?
? on RV . Indeed, it can be checked
However, it is easy to see that convexity of h on L implies convexity of h
?
?
?
that h is positively homogeneous, i.e. h(?x) = ? h(x) for any ? ? 0, x ? RV . Therefore, for any x, y ? RV
and ?, ? ? 0 with ? + ? = 1 there holds
2

1?
??
??
?
?
?
h(?x
+ ?y) = h(??x
+ ??y) ? h(?x)
+ h(?y)
= ?h(x)
+ ? h(y)
?
?
?
? on L, assuming that ? is a sufficiently small
where the inequality in the middle follows from convexity of h
constant.

4

Proof. We use induction on n = |V |. For n = 1 the claim is straightforward; suppose that n ? 2.
? is convex on L. Function h
? is integral since it is linear on each simplex L? and
By theorem 5, h
1/2
? 0 considered in definition 4 are
vertices of L? belong to L . It remains to show that functions h
0
V \{n}
totally integral. Consider the following functions h : {?1, 0, 1}
? R:
h0 (x1 , . . . , xn?1 ) =
h0 (x1 , . . . , xn?1 ) =
h0 (x1 , . . . , xn?1 ) =

h(x1 , . . . , xn?1 , xn?1 )
h(x1 , . . . , xn?1 , ?xn?1 )
h(x1 , . . . , xn?1 , ?) , ? ? {?1, 0, 1}

It can be checked that these functions are bisubmodular, and their Lov?asz extensions coincide with
? 0 used in definition 4. The claim now follows from the induction hypothesis.
respective functions h

3

A new characterization of bisubmodularity

In this section we give an alternative definition of bisubmodularity; it will be helpful later for describing a relationship to the roof duality. As is often done for bisubmodular functions, we will
encode each half-integral value xi ? {0, 1, 12 } via two binary variables (ui , ui0 ) according to the
following rules:
1
0 ? (0, 1)
1 ? (1, 0)
2 ? (0, 0)
Thus, labelings in K1/2 will be represented via labelings in the set
X ? = {u ? {0, 1}V | (ui , ui0 ) 6= (1, 1) ? i ? V }
where V = {i, i0 | i ? V } is a set with 2n nodes. The node i0 for i ? V is called the ?mate? of
i; intuitively, variable ui0 corresponds to the complement of ui . We define (i0 )0 = i for i ? V .
Labelings in X ? will be denoted either by a single letter, e.g. u or v, or by a pair of letters, e.g.
(x, y). In the latter case we assume that the two components correspond to labelings of V and
V \V , respectively, and the order of variables in both components match. Using this convention, the
one-to-one mapping X ? ? K1/2 can be written as (x, y) 7? 21 (x + y). Accordingly, instead of
function f : K1/2 ? R we will work with the function g : X ? ? R defined by


x+y
g(x, y) = f
(5)
2
Note that the set of integer labelings B ? K1/2 corresponds to the set X ? = {u ? X ? | (ui , ui0 ) 6=
(0, 0)}, so function g : X ? ? R can be viewed as a discrete relaxation of function g : X ? ? R.
Definition 9. Function f : X ? ? R is called bisubmodular if
f (u u v) + f (u t v) ? f (u) + f (v)

? u, v ? X ?

(6)

where u u v = u ? v, u t v = REDUCE(u ? v) and REDUCE(w) is the labeling obtained from
w by changing labels (wi , wi0 ) from (1, 1) to (0, 0) for all i ? V .
To describe a new characterization, we need to introduce some additional notation. We denote
X = {0, 1}V to be the set of all binary labelings of V . For a labeling u ? X , define labeling u0 by
(u0 )i = ui0 . Labels (ui , ui0 ) are transformed according to the rules
(0, 1) ? (0, 1)

(1, 0) ? (1, 0)

(0, 0) ? (1, 1)
0

(1, 1) ? (0, 0)
00

0

(7)
0

Equivalently, this mapping can be written as (x, y) = (y, x). Note that u = u, (u ? v) = u ? v 0
and (u ? v)0 = u0 ? v 0 for u, v ? X . Next, we define sets
X ? = {u ? X | u ? u0 } = {u ? X | (ui , u0i ) 6= (1, 1) ?i ? V }
X + = {u ? X | u ? u0 } = {u ? X | (ui , u0i ) 6= (0, 0) ?i ? V }
X?
X?

= {u ? X | u = u0 } = {u ? X | (ui , u0i ) ? {(0, 1), (1, 0)}
= X? ? X+

?i ? V } = X ? ? X +

Clearly, u ? X ? if and only if u0 ? X + . Also, any function g : X ? ? R can be uniquely extended
to a function g : X ? ? R so that the following condition holds:
g(u0 ) = g(u)
5

?u ? X?

(8)

Proposition 10. Let g : X ? ? R be a function satisfying (8). The following conditions are equivalent:
(a) g is bisubmodular, i.e. it satisfies (6).
(b) g satisfies the following inequalities:
g(u ? v) + g(u ? v) ? g(u) + g(v)

if u, v, u ? v, u ? v ? X ?

(9)

(c) g satisfies those inequalities in (6) for which u = w ? ei , v = w ? ej where w = u ? v
and i, j are distinct nodes in V with wi = wj = 0. Here ek for node k ? V denotes the
labeling in X with ekk = 1 and ekk0 = 0 for k 0 ? V \{k}.
(d) g satisfies those inequalities in (9) for which u = w ? ei , v = w ? ej where w = u ? v
and i, j are distinct nodes in V with zi = zj = 0.
A proof is given [20]. Note, an equivalent of characterization (c) was given by Ando et al. [2]; we
state it here for completeness.
Remark 1 In order to compare characterizations (b,d) to existing characterizations (a,c), we need
to analyze the sets of inequalities in (b,d) modulo eq. (8), i.e. after replacing terms g(w), w ? X +
with g(w0 ). In can be seen that the inequalities in (a) are neither subset nor superset of those in (b)3 ,
so (b) is a new characterization. It is also possible to show that from this point of view (c) and (d)
are equivalent.

4

Submodular relaxations and roof duality

Consider a submodular function g : X ? R satisfying the following ?symmetry? condition:
g(u0 ) = g(u)

?u ? X

(10)

We call such function g a submodular relaxation of function f (x) = g(x, x). Clearly, it satisfies
conditions of proposition 10, so g is also a bisubmodular relaxation of f . Furthermore, minimizing
g is equivalent to minimizing its restriction g : X ? ? R; indeed, if u ? X is a minimizer of g then
so are u0 and u ? u0 ? X ? .
In this section we will do the following: (i) prove that any pseudo-boolean function f : B ? R has
a submodular relaxation g : X ? R; (ii) show that the roof duality relaxation for quadratic pseudoboolean functions is a submodular relaxation, and it dominates all other bisubmodular relaxations;
(iii) show that for non-quadratic pseudo-boolean functions bisubmodular relaxations can be tighter
than submodular ones; (iv) prove that similar to the roof duality relaxation, bisubmodular relaxations
possess the persistency property.
Review of roof duality Consider a quadratic pseudo-boolean function f : B ? R:
X
X
f (x) =
fi (xi ) +
fij (xi , xj )
i?V

(11)

(i,j)?E

where (V, E) is an undirected graph and xi ? {0, 1} for i ? V are binary variables. Hammer,
Hansen and Simeone [13] formulated several linear programming relaxations of this function and
3

Denote u =



1 0 1 0
0 0 0 0



and v =



0 1 0 0
0 0 1 0



where the top and bottom rows correspond to the labelings

of V and V \V respectively, with |V | = 4. Plugging pair (u, v) into (6) gives the following inequality:








g 00 00 00 00 + g 10 10 00 00 ? g 10 00 10 00 + g 00 10 01 00
This inequality is a part of (a), but it is not present in (b): pairs (u, v) and (u0 , v 0 ) do not satisfy the RHS
of (9), while pairs (u, v 0 ) and (u0 , v) give a different inequality:








g 10 00 00 00 + g 00 10 00 00 ? g 10 00 10 00 + g 00 10 01 00
where we used condition (8). Conversely, the second inequality is a part of (b) but it is not present in (a).

6

showed their equivalence. One of these formulations was called a roof dual. An efficient maxflowbased method for solving the roof duality relaxation was given by Hammer, Boros and Sun [5, 4].
We will rely on this algorithmic description of the roof duality approach [4]. The method?s idea
can be summarized as follows. Each variable xi is replaced with two binary variables ui and ui0
corresponding to xi and 1 ? xi respectively. The new set of nodes is V = {i, i0 | i ? V }. Next,
function f is transformed to a function g : X ? R by replacing each term according to the following
rules:
fi (xi ) 7?
fij (xi , xj ) 7?
fij (xi , xj ) 7?

1
[fi (ui ) + fi (ui0 )]
2
1
[fij (ui , uj ) + fij (ui0 , uj 0 )]
2
1
[fij (ui , uj 0 ) + fij (ui0 , uj )]
2

(12a)
if fij (?, ?) is submodular

(12b)

if fij (?, ?) is not submodular

(12c)

g is a submodular quadratic pseudo-boolean function, so it can be minimized via a maxflow algorithm. If u ? X is a minimizer of g then the roof duality relaxation has a minimizer x
? with
x
?i = 12 (ui + ui0 ) [4].
It is easy to check that g(u) = g(u0 ) for all u ? X , therefore g is a submodular relaxation. Also, f
and g are equivalent when ui0 = ui for all i ? V , i.e.
?x ? B

g(x, x) = f (x)

(13)

Invariance to variable flipping Suppose that g is a (bi-)submodular relaxation of function f :
B ? R. Let i be a fixed node in V , and consider function f 0 (x) obtained from f (x) by a change of
coordinates xi 7? xi and function g 0 (u) obtained from g(u) by swapping variables ui and ui0 . It is
easy to check that g 0 is a (bi-)submodular relaxation of f 0 . Furthermore, if f is a quadratic pseudoboolean function and g is its submodular relaxation constructed by the roof duality approach, then
applying the roof duality approach to f 0 yields function g 0 . We will sometimes use such ?flipping?
operation for reducing the number of considered cases.
Conversion to roof duality Let us now consider a non-quadratic pseudo-boolean function f : B ?
R. Several papers [33, 1, 16] proposed the following scheme: (1) Convert f to a quadratic pseudoboolean function f? by introducing k auxiliary binary variables so that f (x) = min??{0,1}k f?(x, ?)
for all labelings x ? B. (2) Construct submodular relaxation g?(x, ?, y, ?) of f? by applying the roof
duality relaxation to f?; then
g?(x, ?, y, ?) = g?(y, ?, x, ?) , g?(x, ?, x, ?) = f?(x, ?)
(3) Obtain function g by
min?,? ?{0,1}k g?(x, ?, y, ?).

minimizing

out

auxiliary

?x, y ? B, ?, ? ? {0, 1}k
variables:

g(x, y)

=

One can check that g(x, y) = g(y, x), so g is a submodular relaxation4 . In general, however,
it may not be a relaxation of function f , i.e. (13) may not hold; we are only guaranteed to have
g(x, x) ? f (x) for all labelings x ? B.
Existence of submodular relaxations It is easy to check that if f : B ? R is submodular
then function g(x, y) = 21 [f (x) + f (y)] is a submodular relaxation of f .5 Thus, monomials of
the form c?i?A xi where c ? 0 and A ? V have submodular relaxations. Using the ?flipping?
operation xi 7? xi , we conclude that submodular relaxations also exist for monomials of the form
4

It is well-known that minimizing variables out preserves submodularity. Indeed, suppose that h(x) =
?
? is a submodular function. Then h is also submodular since
min? h(x,
?) where h
?
?
? ? y, ? ? ?) + h(x
? ? y, ? ? ?) ? h(x ? y) + h(x ? y)
h(x) + h(y) = h(x,
?) + h(y,
?) ? h(x
5
In fact, it dominates all other bisubmodular relaxations g? : X ? ? R of f . Indeed, consider labeling
(x, y) ? X ? . It can be checked that (x, y) = u u v = u t v where u = (x, x) and v = (y, y), therefore
g?(x, y) ? 21 [?
g (u) + g?(v)] = 21 [f (x) + f (y)] = g(x, y).

7

c?i?A xi ?i?B xi where c ? 0 and A, B are disjoint subsets of U . It is known that any pseudoboolean function f can be represented as a sum of such monomials (see e.g. [4]; we need to represent
?f as a posiform and take its negative). This implies that any pseudo-boolean function f has a
submodular relaxation.
Note that this argument is due to Lu and Williams [25] who converted function f to a sum of
monomials of the form c?i?A xi and cxk ?i?A xi , c ? 0, k ?
/ A. It is possible to show that the
relaxation proposed in [25] is equivalent to the submodular relaxation constructed by the scheme
above (we omit the derivation).
Submodular vs. bisubmodular relaxations An important question is whether bisubmodular
relaxations are more ?powerful? compared to submodular ones. The next theorem gives a class of
functions for which the answer is negative; its proof is given in [20].
Theorem 11. Let g be the submodular relaxation of a quadratic pseudo-boolean function f defined
by (12), and assume that the set E does not have parallel edges. Then g dominates any other
bisubmodular relaxation g? of f , i.e. g(u) ? g?(u) for all u ? X ? .
For non-quadratic pseudo-boolean functions, however, the situation can be different. In [20]. we
give an example of a function f of n = 4 variables which has a tight bisubmodular relaxation g (i.e.
g has a minimizer in X ? ), but all submodular relaxations are not tight.
Persistency Finally, we show that bisubmodular functions possess the autarky property, which
implies persistency.
Proposition 12. Let f : K1/2 ? R be a bisubmodular function and x ? K1/2 be its minimizer.
[Autarky] Let y be a labeling in B. Consider labeling z = (y t x) t x. Then z ? B and
f (z) ? f (y).
[Persistency] Function f : B ? R has a minimizer x? ? B such that x?i = xi for nodes i ? V
with integral xi .
Proof. It can be checked that zi = yi if xi = 21 and zi = xi if xi ? {0, 1}. Thus, z ? B. For
any w ? K1/2 there holds f (w t x) ? f (w) + [f (x) ? f (w u x)] ? f (w). This implies that
f ((y t x) t x) ? f (y). Applying the autarky property to a labeling y ? arg min{f (x) | x ? B }
yields persistency.

5

Conclusions and future work

We showed that bisubmodular functions can be viewed as a natural generalization of the roof duality
approach to higher-order cliques. As mentioned in the introduction, thisP
work has been motivated
by computer vision applications that use functions of the form f (x) = C fC (x). An important
open question is how to construct bisubmodular relaxations f?C for individual terms. For terms of
low order, e.g. with |C| = 3, this potentially could be done by solving a small linear program.
Another important question is how to minimize such functions. Algorithms in [12, 26] are unlikely
to be practical for most vision problems, which typically have tens of thousands of variables. However, in our case we need to minimize a bisubmodular function which has a special structure: it
is represented as a sum of low-order bisubmodular terms. We recently showed [21] that a sum of
low-order submodular terms can be optimized more efficiently using maxflow-like techniques. We
conjecture that similar techniques can be developed for bisubmodular functions as well.

References
[1] Asem M. Ali, Aly A. Farag, and Georgy L. Gimel?Farb. Optimizing binary MRFs with higher order
cliques. In ECCV, 2008.
[2] Kazutoshi Ando, Satoru Fujishige, and Takeshi Naitoh. A characterization of bisubmodular functions.
Discrete Mathematics, 148:299?303, 1996.
[3] M. L. Balinski. Integer programming: Methods, uses, computation. Management Science, 12(3):253?
313, 1965.

8

[4] E. Boros and P. L. Hammer. Pseudo-boolean optimization. Discrete Applied Mathematics, 123(1-3):155
? 225, November 2002.
[5] E. Boros, P. L. Hammer, and X. Sun. Network flows and minimization of quadratic pseudo-Boolean
functions. Technical Report RRR 17-1991, RUTCOR, May 1991.
[6] E. Boros, P. L. Hammer, and G. Tavares. Preprocessing of unconstrained quadratic binary optimization.
Technical Report RRR 10-2006, RUTCOR, 2006.
[7] A. Bouchet. Greedy algorithm and symmetric matroids. Math. Programming, 38:147?159, 1987.
[8] A. Bouchet and W. H. Cunningham. Delta-matroids, jump systems and bisubmodular polyhedra. SIAM
J. Discrete Math., 8:17?32, 1995.
[9] Y. Boykov, O. Veksler, and R. Zabih. Fast approximate energy minimization via graph cuts. PAMI, 23(11),
November 2001.
[10] R. Chandrasekaran and Santosh N. Kabadi. Pseudomatroids. Discrete Math., 71:205?217, 1988.
[11] S Fujishige. Submodular Functions and Optimization. North-Holland, 1991.
[12] Satoru Fujishige and Satoru Iwata. Bisubmodular function minimization. SIAM J. Discrete Math.,
19(4):1065?1073, 2006.
[13] P. L. Hammer, P. Hansen, and B. Simeone. Roof duality, complementation and persistency in quadratic
0-1 optimization. Mathematical Programming, 28:121?155, 1984.
[14] D. Hochbaum. Instant recognition of half integrality and 2-approximations. In 3rd International Workshop
on Approximation Algorithms for Combinatorial Optimization, 1998.
[15] D. Hochbaum. Solving integer programs over monotone inequalities in three variables: A framework for
half integrality and good approximations. European Journal of Operational Research, 140(2):291?321,
2002.
[16] H. Ishikawa. Higher-order clique reduction in binary graph cut. In CVPR, 2009.
[17] H. Ishikawa. Higher-order gradient descent by fusion-move graph cut. In ICCV, 2009.
[18] Satoru Iwata and Kiyohito Nagano. Submodular function minimization under covering constraints. In
FOCS, October 2009.
[19] Santosh N. Kabadi and R. Chandrasekaran. On totally dual integral systems. Discrete Appl. Math.,
26:87?104, 1990.
[20] V. Kolmogorov.
Generalized roof duality and bisubmodular functions.
Technical Report
arXiv:1005.2305v2, September 2010.
[21] V. Kolmogorov. Minimizing a sum of submodular functions. Technical Report arXiv:1006.1990v1, June
2010.
[22] V. Kolmogorov. Submodularity on a tree: Unifying L\ -convex and bisubmodular functions. Technical
Report arXiv:1007.1229v2, July 2010.
[23] Victor Lempitsky, Carsten Rother, and Andrew Blake. LogCut - efficient graph cut optimization for
Markov random fields. In ICCV, 2007.
[24] Victor Lempitsky, Carsten Rother, Stefan Roth, and Andrew Blake. Fusion moves for Markov random
field optimization. PAMI, July 2009.
[25] S. H. Lu and A. C. Williams. Roof duality for polynomial 0-1 optimization. Math. Programming,
37(3):357?360, 1987.
[26] S. Thomas McCormick and Satoru Fujishige. Strongly polynomial and fully combinatorial algorithms for
bisubmodular function minimization. Math. Program., Ser. A, 122:87?120, 2010.
[27] M. Nakamura. A characterization of greedy sets: universal polymatroids (I). In Scientific Papers of the
College of Arts and Sciences, volume 38(2), pages 155?167. The University of Tokyo, 1998.
[28] G. L. Nemhauser and L. E. Trotter. Vertex packings: Structural properties and algorithms. Mathematical
Programming, 8:232?248, 1975.
[29] Liqun Qi. Directed submodularity, ditroids and directed submodular flows. Mathematical Programming,
42:579?599, 1988.
[30] A. Raj, G. Singh, and R. Zabih. MRF?s for MRI?s: Bayesian reconstruction of MR images via graph cuts.
In CVPR, 2006.
[31] Stefan Roth and Michael J. Black. Fields of experts. IJCV, 82(2):205?229, 2009.
[32] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary MRFs via extended roof
duality. In CVPR, June 2007.
[33] O. Woodford, P. Torr, I. Reid, and A. Fitzgibbon. Global stereo reconstruction under second order smoothness priors. In CVPR, 2008.

9

"
2007,Progressive mixture rules are deviation suboptimal,,3303-progressive-mixture-rules-are-deviation-suboptimal.pdf,"We consider the learning task consisting in predicting as well as the best function in a finite reference set G up to the smallest possible additive term. If R(g) denotes the generalization error of a prediction function g, under reasonable assumptions on the loss function (typically satisfied by the least square loss when the output is bounded), it is known that the progressive mixture rule g_n satisfies E R(g_n) < min_{g in G} R(g) + Cst (log|G|)/n where n denotes the size of the training set, E denotes the expectation wrt the training set distribution. This work shows that, surprisingly, for appropriate reference sets G, the deviation convergence rate of the progressive mixture rule is only no better than Cst / sqrt{n}, and not the expected Cst / n. It also provides an algorithm which does not suffer from this drawback.","Progressive mixture rules are deviation suboptimal

Jean-Yves Audibert
Willow Project - Certis Lab
ParisTech, Ecole des Ponts
77455 Marne-la-Vall?ee, France
audibert@certis.enpc.fr

Abstract
We consider the learning task consisting in predicting as well as the best function
in a finite reference set G up to the smallest possible additive term. If R(g) denotes
the generalization error of a prediction function g, under reasonable assumptions
on the loss function (typically satisfied by the least square loss when the output is
bounded), it is known that the progressive mixture rule g? satisfies
(1)
ER(?
g ) ? ming?G R(g) + Cst log |G| ,
n

where n denotes the size of the training set, and E denotes the expectation w.r.t.
the training set distribution.This work shows that, surprisingly, for appropriate
reference sets G, the?deviation convergence rate of the progressive mixture rule is
no better than Cst / n: it fails to achieve the expected Cst /n. We also provide
an algorithm which does not suffer from this drawback, and which is optimal in
both deviation and expectation convergence rates.

1

Introduction

Why are we concerned by deviations? The efficiency of an algorithm can be summarized by its
expected risk, but this does not precise the fluctuations of its risk. In several application fields of
learning algorithms, these fluctuations play a key role: in finance for instance, the bigger the losses
can be, the more money the bank needs to freeze in order to alleviate these possible losses. In this
case, a ?good? algorithm is an algorithm having not only low expected risk but also small deviations.
Why are we interested in the learning task of doing as well as the best prediction function of a given
finite set? First, one way of doing model selection among a finite family of submodels is to cut the
training set into two parts, use the first part to learn the best prediction function of each submodel
and use the second part to learn a prediction function which performs as well as the best of the
prediction functions learned on the first part of the training set. This scheme is very powerful since
it leads to theoretical results, which, in most situations, would be very hard to prove without it. Our
work here is related to the second step of this scheme.
Secondly, assume we want to predict the value of a continuous variable, and that we have many
candidates for explaining it. An input point can then be seen as the vector containing the prediction
of each candidate. The problem is what to do when the dimensionality d of the input data (equivalently the number of prediction functions) is much higher than the number of training points n. In
this setting, one cannot use linear regression and its variants in order to predict as well as the best
candidate up to a small additive term. Besides, (penalized) empirical risk minimization is doomed
to be suboptimal (see the second part of Theorem 2 and also [1]).
As far as the expected risk is concerned, the only known correct way of predicting as well as the
best prediction function is to use the progressive mixture rule or its variants. These algorithms are
introduced in Section 2 and their main good property is given in Theorem 1. In this work we prove
that they do not work well as far as risk deviations are concerned (see the second part of Theorem
1

3). We also provide a new algorithm for this ?predict as well as the best? problem (see the end of
Section 4).

2

The progressive mixture rule and its variants

We assume that we observe n pairs of input-output denoted Z1 = (X1 , Y1 ), . . . , Zn = (Xn , Yn )
and that each pair has been independently drawn from the same unknown distribution denoted P .
The input and output spaces are denoted respectively X and Y, so that P is a probability distribution
on the product space Z , X ? Y. The quality of a (prediction) function g : X ? Y is measured by
the risk (or generalization error):
R(g) = E(X,Y )?P `[Y, g(X)],
where `[Y, g(X)] denotes the loss (possibly infinite) incurred by predicting g(X) when the true
output is Y . We work under the following assumptions for the data space and the loss function
` : Y ? Y ? R ? {+?}.
Main assumptions. The input space is assumed to be infinite: |X | = +?. The output space is
a non-trivial (i.e. infinite) interval of R symmetrical w.r.t. some a ? R: for any y ? Y, we have
2a ? y ? Y. The loss function is

? uniformly exp-concave: there exists ? > 0 such that for any y ? Y, the set y 0 ? R :
	
0
`(y, y 0 ) < +? is an interval containing a on which the function y 0 7? e??`(y,y ) is
concave.
? symmetrical: for any y1 , y2 ? Y, `(y1 , y2 ) = `(2a ? y1 , 2a ? y2 ),
? admissible: for any y, y 0 ? Y?]a; +?[, `(y, 2a ? y 0 ) > `(y, y 0 ),
? well behaved at center: for any y ? Y?]a; +?[, the function `y : y 0 7? `(y, y 0 ) is twice
continuously differentiable on a neighborhood of a and `0y (a) < 0.
These assumptions imply that
? Y has necessarily one of the following form: ] ? ?; +?[, [a ? ?; a + ?] or ]a ? ?; a + ?[
for some ? > 0.
? for any y ? Y, from the exp-concavity assumption, the function `y : y 0 7? `(y, y 0 ) is
convex on the interval on which it is finite1 . As a consequence, the risk R is also a convex
function (on the convex set of prediction functions for which it is finite).
The assumptions were motivated by the fact that they are satisfied in the following settings:
? least square loss with bounded outputs: Y = [ymin ; ymax ] and `(y1 , y2 ) = (y1 ?y2 )2 . Then
we have a = (ymin + ymax )/2 and may take ? = 1/[2(ymax ? ymin )2 ].


1
? entropy loss: Y = [0; 1] and `(y1 , y2 ) = y1 log yy21 + (1 ? y1 ) log 1?y
1?y2 . Note that
`(0, 1) = `(1, 0) = +?. Then we have a = 1/2 and may take ? = 1.
? exponential (or AdaBoost) loss: Y = [?ymax ; ymax ] and `(y1 , y2 ) = e?y1 y2 . Then we
2
have a = 0 and may take ? = e?ymax .
? logit loss: Y = [?ymax ; ymax ] and `(y1 , y2 ) = log(1 + e?y1 y2 ). Then we have a = 0 and
2
may take ? = e?ymax .
Progressive indirect mixture rule. Let G be a finite reference set of prediction functions. Under the
previous assumptions, the only known algorithms satisfying (1) are the progressive indirect mixture
rules defined below.
For any i ? {0, . . . , n}, the cumulative loss suffered by the prediction function g on the first i pairs
of input-output is
Pi
?i (g) , j=1 `[Yj , g(Xj )],
1
??`y
Indeed, if ? denotes the
, from Jensen?s inequality, for any probability distribution,
 function e
E`y (Y ) = E ? ?1 log ?(Y ) ? ? ?1 log E?(Y ) ? ? ?1 log ?(EY ) = `y (EY ).

2

where by convention we take ?0 ? 0. Let ? denote the uniform distribution on G. We define the
probability distribution ?
?i on G as
?
?i ? e???i ? ?
P
0
equivalently for any g ? G, ?
?i (g) = e???i (g) /( g0 ?G e???i (g ) ). This distribution concentrates
? i be a prediction
on functions having low cumulative loss up to time i. For any i ? {0, . . . , n}, let h
function such that
? (x, y) ? Z

? i (x)] ? ? 1 log Eg??? e??`[y,g(x)] .
`[y, h
i
?

(2)

The progressive indirect mixture rule produces the prediction function
Pn ?
1
g?pim = n+1
i=0 hi .
? i does exist since one may
From the uniform exp-concavity assumption and Jensen?s inequality, h
?
take hi = Eg???i g. This particular choice leads to the progressive mixture rule, for which the
predicted output for any x ? X is


P
Pn
???i (g)
1
P e
g?pm (x) = g?G n+1
g(x).
0
???
(g
)
i=0
i
e
g 0 ?G

Consequently, any result that holds for any progressive indirect mixture rule in particular holds for
the progressive mixture rule.
The idea of a progressive mean of estimators has been introduced by Barron ([2]) in the context
of density estimation with Kullback-Leibler loss. The form g?pm is due to Catoni ([3]). It was also
independently proposed in [4]. The study of this procedure was made in density estimation and least
square regression in [5, 6, 7, 8]. Results for general losses can be found in [9, 10]. Finally, the
progressive indirect mixture rule is inspired by the work of Vovk, Haussler, Kivinen and Warmuth
[11, 12, 13] on sequential prediction and was studied in the ?batch? setting in [10]. Finally, in the
upper bounds we state, e.g. Inequality (1), one should notice that there is no constant larger than 1
in front of ming?G R(g), as opposed to some existing upper bounds (e.g. [14]). This work really
studies the behaviour of the excess risk, that is the random variable R(?
g ) ? ming?G R(g).
The largest integer smaller or equal to the logarithm in base 2 of x is denoted by blog2 xc .

3

Expectation convergence rate

The following theorem, whose proof is omitted, shows that the expectation convergence rate of any
progressive indirect mixture rule is (i) at least (log |G|)/n and (ii) cannot be uniformly improved,
even when we consider only probability distributions on Z for which the output has almost surely
two symmetrical values (e.g. {-1;+1} classication with exponential or logit losses).
Theorem 1 Any progressive indirect mixture rule satisfies
ER(?
gpim ) ? min R(g) +
g?G

log |G|
?(n+1) .

Let y1 ? Y ? {a} and d be a positive integer. There exists a set G of d prediction functions such that:
for any learning algorithm, there exists a probability distribution generating the data for which
? the output marginal is supported by 2a ? y1 and y1 : P (Y ? {2a ? y1 ; y1 }) = 1,

2 |G|c
? ER(?
g ) ? min R(g) + e?1 ? 1 ? blogn+1
, with ? , sup [`(y1 , a) ? `(y1 , y)] > 0.
g?G

y?Y

The second part of Theorem 1 has the same (log |G|)/n rate as the lower bounds obtained in sequential prediction ([12]). From the link between sequential predictions and our ?batch? setting with i.i.d.
data (see e.g. [10, Lemma 3]), upper bounds for sequential prediction lead to upper bounds for i.i.d.
data, and lower bounds for i.i.d. data leads to lower bounds for sequential prediction. The converse
of this last assertion is not true, so that the second part of Theorem 1 is not a consequence of the
lower bounds of [12].
3

The following theorem, whose
p proof is also omitted, shows that for appropriate set G: (i) the empirical risk minimizer has a (log |G|)/n expectation convergence rate, and (ii) any empirical risk
minimizer and any of its penalized variants are really
ppoor algorithms in our learning task since their
expectation convergence rate cannot be faster than (log |G|)/n (see [5, p.14] and [1] for results of
the same spirit). This last point explains the interest we have in progressive mixture rules.
Theorem 2 If B , supy,y0 ,y00 ?Y [`(y, y 0 ) ? `(y, y 00 )] < +?, then any empirical risk minimizer,
which produces a prediction function g?erm in argming?G ?n , satisfies:
q
ER(?
germ ) ? min R(g) + B 2 logn |G| .
g?G

Let y1 , y?1 ? Y?]a; +?[ and d be a positive integer. There exists a set G of d prediction functions
such that: for any learning algorithm producing a prediction function in G (e.g. g?erm ) there exists a
probability distribution generating the data for which
? the output marginal is supported by 2a ? y1 and y1 : P (Y ? {2a ? y1 ; y1 }) = 1,
q

blog2 |G|c
? ER(?
g ) ? min R(g) + 8?
?
2
, with ? , `(y1 , 2a ? y?1 ) ? `(y1 , y?1 ) > 0.
n
g?G

The lower bound of Theorem 2 also says that one should not use cross-validation. This holds for the
loss functions considered in this work, and not for, e.g., the classification loss: `(y, y 0 ) = 1y6=y0 .

4

Deviation convergence rate

The following theorem shows
? that the deviation convergence rate of any progressive indirect mixture rule is (i) at least 1/ n and (ii) cannot be uniformly improved, even when we consider only
probability distributions on Z for which the output has almost surely two symmetrical values (e.g.
{-1;+1} classication with exponential or logit losses).
Theorem 3 If B , supy,y0 ,y00 ?Y [`(y, y 0 ) ? `(y, y 00 )] < +?, then any progressive indirect mixture
rule satisfies: for any  > 0, with probability at least 1 ?  w.r.t. the training set distribution, we
have
q
?1 )
log |G|
R(?
gpim ) ? min R(g) + B 2 log(2
+ ?(n+1)
n+1
g?G

Let y1 and y?1 in Y?]a; +?[ such that `y1 is twice continuously differentiable on [a; y?1 ] and
`0y1 (y?1 ) ? 0 and `00y1 (y?1 ) > 0. Consider the prediction functions g1 ? y?1 and g2 ? 2a ? y?1 .
For any training set size n large enough, there exist  > 0 and a distribution generating the data
such that
? the output marginal is supported by y1 and 2a ? y1
? with probability larger than , we have
R(?
gpim ) ?

min

g?{g1 ,g2 }

q
?1 )
R(g) ? c log(e
n

where c is a positive constant depending only on the loss function, the symmetry parameter
a and the output values y1 and y?1 .
Proof 1 See Section 5.
This result is quite surprising since it gives an example of an algorithm which is optimal in terms of
expectation convergence rate and for which the deviation convergence rate is (significantly) worse
than the expectation convergence rate.
In fact, despite their popularity based on their unique expectation convergence rate, the progressive
mixture rules are not good algorithms since a long argument essentially based on convexity shows
that the following algorithm has both expectation and deviation convergence rate of order 1/n. Let
4

g?erm be the minimizer of the empirical risk among functions in G. Let g? be the minimizer of the
empirical risk in the star G? = ?g?G [g; g?erm ]. The algorithm producing g? satisfies for some C > 0,
for any  > 0, with probability at least 1 ?  w.r.t. the training set distribution, we have
?1

R(?
g ) ? min R(g) + C log( n
g?G

|G|)

.

This algorithm has also the benefit of being parameter-free. On the contrary, in practice, one will
have recourse to cross-validation to tune the parameter ? of the progressive mixture rule.
To summarize, to predict as well as the best prediction function in a given set G, one should not
restrain the algorithm to produce its prediction function among the set G. The progressive mixture rules satisfy this principle since they produce a prediction function in the convex hull of G.
This allows to achieve (log |G|)/n convergence rates in expectation. The proof of the lower bound
of Theorem 3 shows that the progressive mixtures overfit the data: the deviations of their excess
risk are not PAC bounded by C log(?1 |G|)/n while an appropriate algorithm producing prediction
functions on the edges of the convex hull achieves the log(?1 |G|)/n deviation convergence rate.
Future work might look at whether one can transpose this algorithm to the sequential prediction
setting, in which, up to now, the algorithms to predict as well as the best expert were dominated by
algorithms producing a mixture expert inside the convex hull of the set of experts.

5
5.1

Proof of Theorem 3
Proof of the upper bound

Let Zn+1 = (Xn+1 , Yn+1 ) be an input-output pair independent from the training set Z1 , . . . , Zn
and with the same distribution P . From the convexity of y 0 7? `(y, y 0 ), we have
Pn
1
?
(3)
R(?
gpim ) ? n+1
i=0 R(hi ).
Now from [15, Theorem 1] (see also [16, Proposition 1]), for any  > 0, with probability at least
1 ? , we have
q

Pn
1
? i ) ? 1 Pn ` Yi+1 , h(X
? i+1 ) + B log(?1 )
(4)
R(h
n+1

i=0

n+1

i=0

2(n+1)

Using [12, Theorem 3.8] and the exp-concavity assumption, we have


Pn
? i+1 ) ? min Pn ` Yi+1 , g(Xi+1 ) +
` Yi+1 , h(X
i=0

i=0

g?G

log |G|
?

Let g? ? argminG R. By Hoeffding?s inequality, with probability at least 1 ? , we have
q

Pn
log(?1 )
1
`
Y
,
g
?
(X
)
?
R(?
g
)
+
B
i+1
i+1
i=0
n+1
2(n+1)

(5)

(6)

Merging (3), (4), (5) and (6), with probability at least 1 ? 2, we get
q

Pn
?1 )
log |G|
1
R(?
gpim ) ? n+1
?(Xi+1 ) + ?(n+1)
+ B log(
i=0 ` Yi+1 , g
2(n+1)
q
?1 )
log |G|
? R(?
g ) + B 2 log(
+ ?(n+1)
.
n+1
5.2

Sketch of the proof of the lower bound

We cannot use standard tools like Assouad?s argument (see e.g. [17, Theorem 14.6]) because if it
were possible, it would mean that the lower bound would hold for any algorithm and in particular
for g?, and this is false. To prove that any progressive indirect mixture rule have no fast exponential
deviation inequalities, we will show that on some event with not too small probability, for most of
the i in {0, . . . , n}, ????i concentrates on the wrong function.
The proof is organized as follows. First we define the probability distribution for which we will
prove that the progressive indirect mixture rules cannot have fast deviation convergence rates. Then
we define the event on which the progressive indirect mixture rules do not perform well. We lower
bound the probability of this excursion event. Finally we conclude by lower bounding R(?
gpim ) on
the excursion event.
Before starting the proof, note that from the ?well behaved at center? and exp-concavity assumptions, for any y ? Y?]a; +?[, on a neighborhood of a, we have: `00y ? ?(`0y )2 and since `0y (a) < 0,
y1 and y?1 exist. Due to limited space, some technical computations have been removed.
5

5.2.1

Probability distribution generating the data and first consequences.

Let ? ?]0; 1] be a parameter to be tuned later. We consider a distribution generating the data such
that the output distribution satisfies for any x ? X
P (Y = y1 |X = x) = (1 + ?)/2 = 1 ? P (Y = y2 |X = x),

where y2 = 2a ? y1 . Let y?2 = 2a ? y?1 . From the symmetry and admissibility assumptions, we have
`(y2 , y?2 ) = `(y1 , y?1 ) < `(y1 , y?2 ) = `(y2 , y?1 ). Introduce
? , `(y1 , y?2 ) ? `(y1 , y?1 ) > 0.

We have
R(g2 ) ? R(g1 ) =

1+?
2 [`(y1 , y?2 )

? `(y1 , y?1 )] +

1??
2 [`(y2 , y?2 )

(7)
? `(y2 , y?1 )] = ??.

(8)

Therefore g1 is the best prediction function in {g1 , g2 } for the distribution we have chosen. Introduce
Pi
Wj , 1Yj =y1 ? 1Yj =y2 and Si , j=1 Wj . For any i ? {1, . . . , n}, we have
Pi
Pi
?i (g2 ) ? ?i (g1 ) = j=1 [`(Yj , y?2 ) ? `(Yj , y?1 )] = j=1 Wj ? = ? Si
The weight given by the Gibbs distribution ????i to the function g1 is
????i (g1 ) =
5.2.2

e???i (g1 )
e???i (g1 ) +e???i (g2 )

=

1
1+e?[?i (g1 )??i (g2 )]

=

1
1+e???Si

.

(9)

An excursion event on which the progressive indirect mixture rules will not perform
well.

Equality (9) leads us to consider the event:

	
E? = ?i ? {?, . . . , n}, Si ? ?? ,
with ? the smallest integer larger than (log n)/(??) such that n ? ? is even (for convenience). We
have
log n
log n
(10)
?? ? ? ? ?? + 2.
The event E? can be seen as an excursion event of the random walk defined through the random
variables Wj = 1Yj =y1 ? 1Yj =y2 , j ? {1, . . . , n}, which are equal to +1 with probability (1 + ?)/2
and ?1 with probability (1 ? ?)/2.
From (9), on the event E? , for any i ? {?, . . . , n}, we have
????i (g1 ) ?

1
n+1 .

(11)

This means that ????i concentrates on the wrong function, i.e. the function g2 having larger risk
(see (8)).
5.2.3

Lower bound of the probability of the excursion event.

This requires to look at the probability that a slightly shifted random walk in the integer space has a
very long excursion above a certain threshold. To lower bound this probability, we will first look at
the non-shifted random walk. Then we will see that for small enough shift parameter, probabilities
of shifted random walk events are close to the ones associated to the non-shifted random walk.
Let N be a positive integer. Let ?1 , . . . , ?N be N independent Rademacher variables: P(?i =
Pi
+1) = P(?i = ?1) = 1/2. Let si , j=1 ?i be the sum of the first i Rademacher variables. We
start with the following lemma for sums of Rademacher variables (proof omitted).
Lemma 1 Let m and t be positive integers. We have




P max sk ? t; sN 6= t; sN ? t ? m = 2P t < sN ? t + m
1?k?N

(12)

0
Let ?10 , . . . , ?N
be N independent shifted Rademacher variables to the extent that P(?i0 = +1) =
(1 + ?)/2 = 1 ? P(?i0 = ?1). These random variables satisfy the following key lemma (proof
omitted)

6

 PN


	
Lemma 2 For any set A ? (1 , . . . , N ) ? {?1, 1}n :  i=1 i  ? M where M is a positive
integer, we have
M/2

	 
N/2 
	
0
(13)
P (?10 , . . . , ?N
) ? A ? 1??
1 ? ?2
P (?1 , . . . , ?N ) ? A
1+?
We may now lower bound the probability of the excursion event E? . Let M be an integer larger than
? . We still use Wj , 1Yj =y1 ? 1Yj =y2 for j ? {1, . . . , n}. By using Lemma 2 with N = n ? 2? ,
we obtain

Pi
P(E? ) ? P W1 = ?1, . . . , W2? = ?1; ? 2? < i ? n,
j=2? +1 Wj ? ?


Pi
1?? 2?
0
=
P ? i ? {1, . . . , N }
j=1 ?j ? ?
2


N

1?? 2? 1?? M/2
1 ? ? 2 2 P |sN | ? M ; ? i ? {1, . . . , N } si ? ?
?
2
1+?
By using Lemma 1, since ? ? M , the r.h.s. probability can be lower bounded, and after some
computations, we obtain
2? 1?? M/2
N
(14)
P(E? ) ? ? 1??
1 ? ? 2 2 [P(sN = ? ) ? P(sN = M )]
2
1+?
where we recall that ? have the order of log n, N = n ? 2? has the order of n and that ? > 0 and
M ? ? have to be appropriately chosen.
To control the probabilities of the r.h.s., we use Stirling?s formula
?
?
nn e?n 2?n e1/(12n+1) < n! < nn e?n 2?n e1/(12n) ,
and get for any s ? [0; N ] such that N ? s even,
q 
? N2  s  2s
1
1
1? N
2
s2
P(sN = s) ?
1
?
e? 6(N +s) ? 6(N ?s)
?N
N2
1+ s

(15)

(16)

N

and similarly
P(sN = s) ?

q

2
?N



1?

s2
N2

? N2 

s
1? N
s
1+ N

 2s

1

e 12N +1 .

(17)

?
These computations and (14) leads us to take M as the smallest integer
?larger than n such that
n ? M is even. Indeed,
p from (10), (16) and (17), we obtain limn?+? n[P(sN = ? ) ? P(sN =
M )] = c, where c = 2/? 1 ? e?1/2 > 0. Therefore for n large enough we have


N
1?? 2? 1?? M/2
2 2
?
(18)
P(E? ) ? 2c?
1
?
?
2
1+?
n
?
The last two terms of the r.h.s. of (18) leads us to take ? of order 1/ n up to possibly a logarithmic
term. We obtain the following lower bound on the excursion probability
p
Lemma 3 If ? = C0 (log n)/n with C0 a positive constant, then for any large enough n,
P(E? ) ?

1
nC0

.

5.2.4

Behavior of the progressive indirect mixture rule on the excursion event.
Pn ?
From now on, we work on the event E? . We have g?pim = ( i=0 h
i )/(n + 1). We still use ? ,
`(y1 , y?2 )?`(y1 , y?1 ) = `(y2 , y?1 )?`(y2 , y?2 ). On the event E? , for any x ? X and any i ? {?, . . . , n},
? i , we have
by definition of h
? i (x)] ? `(y2 , y?2 )
`[y2 , h

)}
? ? ?1 log E
e??{`[y2 ,g(x)]?`(y2 ,y?2	
???i
 g??
1
???
???
= ? ? log e
+ (1 ? e
)?	???i (g2 )
1
? ? ?1 log 1 ? (1 ? e??? ) n+1

? i (x)] ? `(y2 , y?2 ) ? Cn?1 , with C > 0
In particular, for any n large enough, we have `[y2 , h
independent from ?. From the convexity of the function y 7? `(y2 , y) and by Jensen?s inequality,
we obtain
Pn
1
??
?1
?
`[y2 , g?pim (x)] ? `(y2 , y?2 ) ? n+1
< C1 logn n
i=0 `[y2 , hi (x)] ? `(y2 , y?2 ) ? n+1 + Cn
7

for some constant C1 > 0 independent from ?. Let us now prove that for n large enough, we have
q
(19)
y?2 ? g?pim (x) ? y?2 + C logn n ? y?1 ,
with C > 0 independent from ?.
From (19), we obtain
R(?
gpim ) ? R(g1 )





= 1+?
?pim ) ? `(y1 , y?1 ) + 1??
`(y2 , g?pim ) ? `(y2 , y?1 ) 
2 `(y1 , g
2

= 1+?
gpim ) ? `y1 (y?1 ) + 1??
`y1 (2a
2 `y1 (?
2
 ? g?pim ) ? `y1 (y?2 )

1+?
1??
=
? + `y1 (?
gpim ) ? `y1 (y?2 ) + 2 ? ? + `y1 (2a ? g?pim ) ? `y1 (y?1 )
2
0
? ?? ? (?
gpim
q ? y?2 )|`y1 (y?2 )|
? ?? ? C2

log n
n ,

(20)
p
with C2 independent from ?. We may take ? = 2C? 2 (log n)/n and obtain: for n large enough,
p
on the event E? , we have R(?
gpim ) ? R(g1 ) ? C log n/n. From Lemma 3, this inequality holds
with probability at least 1/nC4 for some C4 > 0. To conclude,
q for any n large enough, there exists
?1

)
. where c is a positive constant
 > 0 s.t. with probability at least , R(?
gpim ) ? R(g1 ) ? c log(e
n
depending only on the loss function, the symmetry parameter a and the output values y1 and y?1 .

References
[1] G. Lecu?e. Suboptimality of penalized empirical risk minimization in classification. In Proceedings of the
20th annual conference on Computational Learning Theory, 2007.
[2] A. Barron. Are bayes rules consistent in information? In T.M. Cover and B. Gopinath, editors, Open
Problems in Communication and Computation, pages 85?91. Springer, 1987.
[3] O. Catoni. A mixture approach to universal model selection. preprint LMENS 97-30, Available from
http://www.dma.ens.fr/edition/preprints/Index.97.html, 1997.
[4] A. Barron and Y. Yang. Information-theoretic determination of minimax rates of convergence. Ann. Stat.,
27(5):1564?1599, 1999.
[5] O. Catoni. Universal aggregation rules with exact bias bound. Preprint n.510, http://www.proba.
jussieu.fr/mathdoc/preprints/index.html\#1999, 1999.
[6] G. Blanchard. The progressive mixture estimator for regression trees. Ann. Inst. Henri Poincar?e, Probab.
Stat., 35(6):793?820, 1999.
[7] Y. Yang. Combining different procedures for adaptive regression. Journal of multivariate analysis,
74:135?161, 2000.
[8] F. Bunea and A. Nobel. Sequential procedures for aggregating arbitrary estimators of a conditional mean,
2005. Technical report.
[9] A. Juditsky, P. Rigollet, and A.B. Tsybakov. Learning by mirror averaging. Preprint n.1034, Laboratoire
de Probabilit?es et Mod`eles Al?eatoires, Universit?es Paris 6 and Paris 7, 2005.
[10] J.-Y. Audibert. A randomized online learning algorithm for better variance control. In Proceedings of the
19th annual conference on Computational Learning Theory, pages 392?407, 2006.
[11] V.G. Vovk. Aggregating strategies. In Proceedings of the 3rd annual workshop on Computational Learning Theory, pages 371?386, 1990.
[12] D. Haussler, J. Kivinen, and M. K. Warmuth. Sequential prediction of individual sequences under general
loss functions. IEEE Trans. on Information Theory, 44(5):1906?1925, 1998.
[13] V.G. Vovk. A game of prediction with expert advice. Journal of Computer and System Sciences, pages
153?173, 1998.
[14] M. Wegkamp. Model selection in nonparametric regression. Ann. Stat., 31(1):252?273, 2003.
[15] T. Zhang. Data dependent concentration bounds for sequential prediction algorithms. In Proceedings of
the 18th annual conference on Computational Learning Theory, pages 173?187, 2005.
[16] N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning algorithms.
IEEE Transactions on Information Theory, 50(9):2050?2057, 2004.
[17] L. Devroye, L. Gy?orfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer-Verlag,
1996.

8

"
2016,Improved Dropout for Shallow and Deep Learning,Poster,6561-improved-dropout-for-shallow-and-deep-learning.pdf,"Dropout has been witnessed with great success in training deep neural networks by independently   zeroing  out the outputs of neurons at random. It has also received a surge of interest for shallow learning, e.g., logistic regression.  However, the independent  sampling for dropout could be suboptimal for the sake of convergence. In this paper, we propose to use multinomial  sampling for dropout, i.e., sampling features or neurons according to  a multinomial distribution with different probabilities for different features/neurons. To exhibit the optimal dropout probabilities, we analyze the shallow learning with multinomial  dropout and establish the risk bound for stochastic optimization. By minimizing a sampling dependent factor in the risk bound, we obtain a distribution-dependent dropout with sampling probabilities dependent on the second order statistics of the data distribution. To tackle the issue of evolving  distribution of neurons in deep learning, we propose an efficient adaptive  dropout (named \textbf{evolutional dropout}) that computes the sampling probabilities on-the-fly from a mini-batch of examples. Empirical studies on several benchmark datasets demonstrate that the proposed dropouts achieve  not only much faster convergence and  but also a smaller testing error than the standard dropout.  For example, on the CIFAR-100 data, the evolutional  dropout achieves relative improvements  over 10\% on the prediction performance and over 50\% on the convergence speed compared to the standard dropout.","Improved Dropout for Shallow and Deep Learning

Zhe Li1 , Boqing Gong2 , Tianbao Yang1
The University of Iowa, Iowa city, IA 52245
2
University of Central Florida, Orlando, FL 32816
{zhe-li-1,tianbao-yang}@uiowa.edu
bgong@crcv.ucf.edu
1

Abstract
Dropout has been witnessed with great success in training deep neural networks by
independently zeroing out the outputs of neurons at random. It has also received
a surge of interest for shallow learning, e.g., logistic regression. However, the
independent sampling for dropout could be suboptimal for the sake of convergence. In this paper, we propose to use multinomial sampling for dropout, i.e.,
sampling features or neurons according to a multinomial distribution with different
probabilities for different features/neurons. To exhibit the optimal dropout probabilities, we analyze the shallow learning with multinomial dropout and establish
the risk bound for stochastic optimization. By minimizing a sampling dependent
factor in the risk bound, we obtain a distribution-dependent dropout with sampling
probabilities dependent on the second order statistics of the data distribution. To
tackle the issue of evolving distribution of neurons in deep learning, we propose
an efficient adaptive dropout (named evolutional dropout) that computes the sampling probabilities on-the-fly from a mini-batch of examples. Empirical studies on
several benchmark datasets demonstrate that the proposed dropouts achieve not
only much faster convergence and but also a smaller testing error than the standard
dropout. For example, on the CIFAR-100 data, the evolutional dropout achieves
relative improvements over 10% on the prediction performance and over 50% on
the convergence speed compared to the standard dropout.

1

Introduction

Dropout has been widely used to avoid overfitting of deep neural networks with a large number of
parameters [9, 16], which usually identically and independently at random samples neurons and sets
their outputs to be zeros. Extensive experiments [4] have shown that dropout can help obtain the
state-of-the-art performance on a range of benchmark data sets. Recently, dropout has also been
found to improve the performance of logistic regression and other single-layer models for natural
language tasks such as document classification and named entity recognition [21].
In this paper, instead of identically and independently at random zeroing out features or neurons, we
propose to use multinomial sampling for dropout, i.e., sampling features or neurons according to
a multinomial distribution with different probabilities for different features/neurons. Intuitively, it
makes more sense to use non-uniform multinomial sampling than identical and independent sampling
for different features/neurons. For example, in shallow learning if input features are centered, we
can drop out features with small variance more frequently or completely allowing the training to
focus on more important features and consequentially enabling faster convergence. To justify the
multinomial sampling for dropout and reveal the optimal sampling probabilities, we conduct a
rigorous analysis on the risk bound of shallow learning by stochastic optimization with multinomial
dropout, and demonstrate that a distribution-dependent dropout leads to a smaller expected risk (i.e.,
faster convergence and smaller generalization error).
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

Inspired by the distribution-dependent dropout, we propose a data-dependent dropout for shallow
learning, and an evolutional dropout for deep learning. For shallow learning, the sampling probabilities are computed from the second order statistics of features of the training data. For deep learning,
the sampling probabilities of dropout for a layer are computed on-the-fly from the second-order
statistics of the layer?s outputs based on a mini-batch of examples. This is particularly suited for deep
learning because (i) the distribution of each layer?s outputs is evolving over time, which is known
as internal covariate shift [5]; (ii) passing through all the training data in deep neural networks (in
particular deep convolutional neural networks) is much more expensive than through a mini-batch
of examples. For a mini-batch of examples, we can leverage parallel computing architectures to
accelerate the computation of sampling probabilities.
We note that the proposed evolutional dropout achieves similar effect to the batch normalization
technique (Z-normalization based on a mini-batch of examples) [5] but with different flavors. Both
approaches can be considered to tackle the issue of internal covariate shift for accelerating the
convergence. Batch normalization tackles the issue by normalizing the output of neurons to zero
mean and unit variance and then performing dropout independently 1 . In contrast, our proposed
evolutional dropout tackles this issue from another perspective by exploiting a distribution-dependent
dropout, which adapts the sampling probabilities to the evolving distribution of a layer?s outputs. In
other words, it uses normalized sampling probabilities based on the second order statistics of internal
distributions. Indeed, we notice that for shallow learning with Z-normalization (normalizing each
feature to zero mean and unit variance) the proposed data-dependent dropout reduces to uniform
dropout that acts similarly to the standard dropout. Because of this connection, the presented
theoretical analysis also sheds some lights on the power of batch normalization from the angle
of theory. Compared to batch normalization, the proposed distribution-dependent dropout is still
attractive because (i) it is rooted in theoretical analysis of the risk bound; (ii) it introduces no
additional parameters and layers without complicating the back-propagation and the inference; (iii) it
facilitates further research because its shares the same mathematical foundation as standard dropout
(e.g., equivalent to a form of data-dependent regularizer) [18].
We summarize the main contributions of the paper below.
? We propose a multinomial dropout and demonstrate that a distribution-dependent dropout
leads to a faster convergence and a smaller generalization error through the risk bound
analysis for shallow learning.
? We propose an efficient evolutional dropout for deep learning based on the distributiondependent dropout.
? We justify the proposed dropouts for both shallow learning and deep learning by experimental results on several benchmark datasets.
In the remainder, we first review some related work and preliminaries. We present the main results in
Section 4 and experimental results in Section 5.

2

Related Work

In this section, we review some related work on dropout and optimization algorithms for deep
learning.
Dropout is a simple yet effective technique to prevent overfitting in training deep neural networks [16].
It has received much attention recently from researchers to study its practical and theoretical properties.
Notably, Wager et al. [18], Baldi and Sadowski [2] have analyzed the dropout from a theoretical
viewpoint and found that dropout is equivalent to a data-dependent regularizer. The most simple
form of dropout is to multiply hidden units by i.i.d Bernoulli noise. Several recent works also found
that using other types of noise works as well as Bernoulli noise (e.g., Gaussian noise), which could
lead to a better approximation of the marginalized loss [20, 7]. Some works tried to optimize the
hyper-parameters that define the noise level in a Bayesian framework [23, 7]. Graham et al. [3] used
the same noise across a batch of examples in order to speed up the computation. The adaptive dropout
proposed in[1] overlays a binary belief network over a neural netowrk, incurring more computational
overhead to dropout because one has to train the additional binary belief network. In constrast,
1

The author also reported that in some cases dropout is even not necessary

2

the present work proposes a new dropout with noise sampled according to distribution-dependent
sampling probabilities. To the best of our knowledge, this is the first work that rigorously studies this
type of dropout with theoretical analysis of the risk bound. It is demonstrated that the new dropout
can improve the speed of convergence.
Stochastic gradient descent with back-propagation has been used a lot in optimizing deep neural
networks. However, it is notorious for its slow convergence especially for deep learning. Recently,
there emerge a battery of studies trying to accelearte the optimization of deep learning [17, 12, 22, 5, 6],
which tackle the problem from different perspectives. Among them, we notice that the developed
evolutional dropout for deep learning achieves similar effect as batch normalization [5] addressing
the internal covariate shift issue (i.e., evolving distributions of internal hidden units).

3

Preliminaries

In this section, we present some preliminaries, including the framework of risk minimization in
machine learning and learning with dropout noise. We also introduce the multinomial dropout, which
allows us to construct a distribution-dependent dropout as revealed in the next section.
Let (x, y) denote a feature vector and a label, where x ? Rd and y ? Y. Denote by P the joint
distribution of (x, y) and denote by D the marginal distribution of x. The goal of risk minimization
is to learn a prediction function f (x) that minimizes the expected loss, i.e., minf ?H EP [`(f (x), y)],
where `(z, y) is a loss function (e.g., the logistic loss) that measures the inconsistency between z
and y and H is a class of prediction functions. In deep learning, the prediction function f (x) is
determined by a deep neural network. In shallow learning, one might be interested in learning a linear
model f (x) = w> x. In the following presentation, the analysis will focus on the risk minimization
of a linear model, i.e.,
min L(w) , EP [`(w> x, y)]

(1)

w?Rd

In this paper, we are interested in learning with dropout, i.e., the feature vector x is corrupted by
a dropout noise. In particular, let  ? M denote a dropout noise vector of dimension d, and the
b = x ? , where the operator ? represents the element-wise
corrupted feature vector is given by x
b denote the joint distribution of the new data (b
b denote the marginal
multiplication. Let P
x, y) and D
b. With the corrupted data, the risk minimization becomes
distribution of x
b
, EPb [`(w> (x ? ), y)]
min L(w)

(2)

w?Rd

In standard dropout [18, 4], the entries of the noise vector  are sampled independently according
1
to Pr(j = 0) = ? and Pr(j = 1??
) = 1 ? ?, i.e., features are dropped with a probability ? and
b

j
1
scaled by 1??
with a probability 1 ? ?. We can also write j = 1??
, where bj ? {0, 1}, j ? [d]
1
are i.i.d Bernoulli random variables with Pr(bj = 1) = 1 ? ?. The scaling factor 1??
is added to
ensure that E [b
x] = x. It is obvious that using the standard dropout different features will have equal
probabilities to be dropped out or to be selected independently. However, in practice some features
could be more informative than the others for learning purpose. Therefore, it makes more sense to
assign different sampling probabilities for different features and make the features compete with each
other.

To this end, we introduce the following multinomial dropout.
b = x ? , where
Definition 1. (Multinomial Dropout) A multinomial dropout is defined as x
mi
i = kp
,
i
?
[d]
and
{m
,
.
.
.
,
m
}
follow
a
multinomial
distribution
M
ult(p
1
d
1 , . . . , pd ; k) with
i
Pd
i=1 pi = 1 and pi ? 0.
Remark: The multinomial dropout allows us to use non-uniform sampling probabilities p1 , . . . , pd
for different features. The value of mi is the number of times that the i-th feature is selected in k
independent trials of selection. In each trial, the probability that the i-th feature is selected is given by
pi . As in the standard dropout, the normalization by kpi is to ensure that E [b
x] = x. The parameter k
plays the same role as the parameter 1 ? ? in standard dropout, which controls the number of features
to be dropped. In particular, the expected total number of the kept features using multinomial dropout
is k and that using standard dropout is d(1 ? ?). In the sequel, to make fair comparison between
3

the two dropouts, we let k = d(1 ? ?). In this case, when a uniform distribution pi = 1/d is used
mi
in multinomial dropout to which we refer as uniform dropout, then i = 1??
, which acts similarly
to the standard dropout using i.i.d Bernoulli random variables. Note that another choice to make
the sampling probabilities different is still using i.i.d Bernoulli random variables but with different
probabilities for different features. However, multinomial dropout is more suitable because (i) it is
easy to control the level of dropout by varying
P the value of k; (ii) it gives rise to natural competition
among features because of the constraint i pi = 1; (iii) it allows us to minimize the sampling
dependent risk bound for obtaining a better distribution than uniform sampling.
Dropout is a data-dependent regularizer Dropout as a regularizer has been studied in [18, 2] for
logistic regression, which is stated in the following proposition for ease of discussion later.
Proposition 1. If `(z, y) = log(1 + exp(?yz)), then
b, y)] = EP [`(w> x, y)] + RD,M (w)
EPb [`(w> x
(3)
h
i
> x?
exp(w> x?
2 )+exp(?w
2 )
where M denotes the distribution of  and RD,M (w) = ED,M log exp(w> x/2)+exp(?w
> x/2) .
Remark: It is notable that RD,M ? 0 due to the Jensen inequality. Using the second order Taylor
expansion, [18] showed that the following approximation of RD,M (w) is easy to manipulate and
understand:
>
>
>
bD,M (w) = ED [q(w x)(1 ? q(w x))w CM (x ? )w]
R
2

(4)

1
where q(w> x) = 1+exp(?w
> x/2) , and CM denotes the covariance matrix in terms of . In particular,
if  is the standard dropout noise, then CM [x ? ] = diag(x21 ?/(1 ? ?), . . . , x2d ?/(1 ? ?)), where
diag(s1 , . . . , sn ) denotes a d?d diagonal matrix with the i-th entry equal to si . If  is the multinomial
dropout noise in Definition 1, we have

CM [x ? ] =

4

1
1
diag(x2i /pi ) ? xx>
k
k

(5)

Learning with Multinomial Dropout

In this section, we analyze a stochastic optimization approach for minimizing the dropout loss
in (2). Assume the sampling probabilities are known. We first obtain a risk bound of learning with
multinomial dropout for stochastic optimization. Then we try to minimize the factors in the risk
bound that depend on the sampling probabilities. We would like to emphasize that our goal here is
not to show that using dropout would render a smaller risk than without using dropout, but rather
focus on the impact of different sampling probabilities on the risk. Let the initial solution be w1 . At
the iteration t, we sample (xt , yt ) ? P and t ? M as in Definition 1 and then update the model by
wt+1 = wt ? ?t ?`(wt> (xt ? t ), yt )

(6)

where ?` denotes the (sub)gradient in terms of wt and ?t is a step size. Suppose we run the stochastic
P
b n = n1 nt=1 wt .
optimization by n steps (i.e., using n examples) and compute the final solution as w
We note that another approach of learning with dropout is to minimize the empirical risk by marginalizing out the dropout noise, i.e., replacing the true expectations EP and ED in (3) with empirical
expectations over a set of samples (x1 , y1 ), . . . , (xn , yn ) denoted by EPn and EDn . Since the
data dependent regularizer RDn ,M (w) is difficult to compute, one usually uses an approximation
bD ,M (w) (e.g., as in (4)) in place of RD ,M (w). However, the resulting problem is a non-convex
R
n
n
optimization, which together with the approximation error would make the risk analysis much more
involved. In contrast, the update in (6) can be considered as a stochastic gradient descent update
for solving the convex optimization problem in (2), allowing us to establish the risk bound based
on previous results of stochastic gradient descent for risk minimization [14, 15]. Nonetheless, this
restriction does not lose the generality. Indeed, stochastic optimization is usually employed for
solving empirical loss minimization in big data and deep learning.
b n in expectation.
The following theorem establishes a risk bound of w
4

Theorem 1. Let L(w) be the expected risk of w defined in (1). Assume EDb [kx ? k22 ] ? B 2 and
`(z, y) is G-Lipschitz continuous. For any kw? k2 ? r, by appropriately choosing ?, we can have
GBr
b n ) + RD,M (w
b n )] ? L(w? ) + RD,M (w? ) + ?
E[L(w
n
where E[?] is taking expectation over the randomness in (xt , yt , t ), t = 1, . . . , n.
Remark: In the above theorem, we can choose w? to be the best model that minimizes the expected
risk in (1). Since RD,M (w) ? 0, the upper bound in the theorem above is also the upper bound of
b n ), in expectation. The proof of the above theorem follows the standard
b n , i.e., L(w
the risk of w
analysis of stochastic gradient descent. The detailed proof of theorem is included in the appendix.
4.1

Distribution Dependent Dropout

Next, we consider the sampling dependent factors in the risk bounds. From Theorem 1, we can
see that there are two terms that depend on the sampling probabilities, i.e., B 2 - the upper bound
b n ) ? RD,M (w? ). We note that the second term also
of EDb [kx ? k22 ], and RD,M (w? ) ? RD,M (w
b n , which is more difficult to optimize. We first try to minimize EDb [kx?k22 ] and
depends on w? and w
present the discussion on minimizing RD,M (w? ) later. From Theorem 1, we can see that minimizing
EDb [kx ? k22 ] would lead to not only a smaller risk (given the same number of total examples, smaller
EDb [kx ? k22 ] gives a smaller risk bound) but also a faster convergence (with the same number of
iterations, smaller EDb [kx ? k22 ] gives a smaller optimization error).
Due to the limited space, the proofs of Proposition 2, 3, 4 are included in supplement. The following
proposition simplifies the expectation EDb [kx ? k22 ].
Proposition 2. Let  follow the distribution M defined in Definition 1. Then
EDb [kx ? k22 ] =

d
d
1X 1
k?1X
ED [x2i ]
ED [x2i ] +
k i=1 pi
k i=1

(7)

Given the expression of EDb [kx ? k22 ] in Proposition 2, we can minimize it over p, leading to the
following result.
Proposition 3. The solution to p? = arg minp?0,p> 1=1 EDb [kx ? k22 ] is given by
p
ED [x2i ]
?
q
pi = P
, i = 1, . . . , d
(8)
d
ED [x2j ]
j=1
Next, we examine RD,M (w? ). Since direct manipulation on RD,M (w? ) is difficult, we try to
bD,M (w? ) for logistic loss. The following theorem
minimize the second order Taylor expansion R
b
establishes an upper bound of RD,M (w? ).
bD,M (w? ) ?
Proposition
the distribution
M defined in Definition 1. We have R
P4. Let  follow

2
E
[x
]
d
D
1
2
i
? ED [kxk22 ]
i=1
8k kw? k2
pi
Remark: By minimizing the relaxed upper bound in Proposition 4, we obtain the same sampling
probabilities as in (8). We note that a tighter upper bound can be established, however, which will
yield sampling probabilities dependent on the unknown w? .
In summary, using the probabilities in (8), we can reduce both EDb [kx ? k22 ] and RD,M (w? ) in the
risk bound, leading to a faster convergence and a smaller generalization error. In practice, we can use
empirical second-order statistics to compute the probabilities, i.e.,
q P
n
1
2
j=1 [[xj ]i ]
n
q P
pi = P
(9)
d
n
1
2]
[[x
]
0
j
i
i0 =1
j=1
n
where [xj ]i denotes the i-th feature of the j-th example, which gives us a data-dependent dropout.
We state it formally in the following definition.
5

Evolutional Dropout for Deep Learning
Input: a batch of outputs of a layer: X l = (xl1 , . . . , xlm )
and dropout level parameter k ? [0, d]
b l = X l ? ?l
Output: X
Compute sampling probabilities by (10)
For j = 1, . . . , m
Sample mlj ? M ult(pl1 , . . . , pld ; k)
mlj
Construct lj =
? Rd , where pl = (pl1 , . . . , pld )>
kpl
b l = X l ? ?l
Let ?l = (l1 , . . . , lm ) and compute X
Figure 1: Evolutional Dropout applied to a layer over a mini-batch
Definition 2. (Data-dependent Dropout) Given a set of training examples (x1 , y1 ), . . . , (xn , yn ). A
mi
b = x ? , where i = kp
, i ? [d] and {m1 , . . . , md } follow a
data-dependent dropout is defined as x
i
multinomial distribution M ult(p1 , . . . , pd ; k) with pi given by (9).
Remark: Note that if the data is normalized such that each feature has zero mean and unit variance
(i.e., according to Z-normliazation), the data-dependent dropout reduces to uniform dropout. It
implies that the data-dependent dropout achieves similar effect as Z-normalization plus uniform
dropout. In this sense, our theoretical analysis also explains why Z-normalization usually speeds up
the training [13].
4.2

Evolutional Dropout for Deep Learning

Next, we discuss how to implement the distribution-dependent dropout for deep learning. In training
deep neural networks, the dropout is usually added to the intermediate layers (e.g., fully connected
layers and convolutional layers). Let xl = (xl1 , . . . , xld ) denote the outputs of the l-th layer (with the
index of data omitted). Adding dropout to this layer is equivalent to multiplying xl by a dropout
bl = xl ? l as the input to the next layer. Inspired by the datanoise vector l , i.e., feeding x
dependent dropout, we can generate l according to a distribution given in Definition 1 with sampling
probabilities pli computed from {xl1 , . . . , xln } similar to that (9). However, deep learning is usually
trained with big data and a deep neural network is optimized by mini-batch stochastic gradient
descent. Therefore, at each iteration it would be too expensive to afford the computation to pass
through all examples. To address this issue, we propose to use a mini-batch of examples to calculate
the second-order statistics similar to what was done in batch normalization. Let X l = (xl1 , . . . , xlm )
denote the outputs of the l-th layer for a mini-batch of m examples. Then we can calculate the
q P
probabilities for dropout by
m
1
l 2
j=1 [[xj ]i ]
m
l
q P
pi = P
, i = 1, . . . , d
(10)
m
d
1
l 2
i0 =1
j=1 [[xj ]i0 ]
m
which define the evolutional dropout named as such because the probabilities pli will also evolve as
the the distribution of the layer?s outputs evolve. We describe the evolutional dropout as applied to a
layer of a deep neural network in Figure 1.
Finally, we would like to compare the evolutional dropout with batch normalization. Similar to batch
normalization, evolutional dropout can also address the internal covariate shift issue by adapting
the sampling probabilities to the evolving distribution of layers? outputs. However, different from
batch normalization, evolutional dropout is a randomized technique, which enjoys many benefits
as standard dropout including (i) the back-propagation is simple to implement (just multiplying the
b l by the dropout mask to get the gradient of X l ); (ii) the inference (i.e., testing) remains
gradient of X
the same 2 ; (iii) it is equivalent to a data-dependent regularizer with a clear mathematical explanation;
2

Different from some implementations for standard dropout which doest no scale by 1/(1 ? ?) in training
but scale by 1 ? ? in testing, here we do scale in training and thus do not need any scaling in testing.

6

(iv) it prevents units from co-adapting of neurons, which facilitate generalization. Moreover, the
evolutional dropout has its root in distribution-dependent dropout, which has theoretical guarantee to
accelerate the convergence and improve the generalization for shallow learning.

5

Experimental Results

In the section, we present some experimental results to justify the proposed dropouts. In all experiments, we set ? = 0.5 in the standard dropout and k = 0.5d in the proposed dropouts for fair
comparison, where d represents the number of features or neurons of the layer that dropout is applied
to. For the sake of clarity, we divided the experiments into three parts. In the first part, we compare
the performance of the data-dependent dropout (d-dropout) to the standard dropout (s-dropout)
for logistic regression. In the second part, we compare the performance of evolutional dropout
(e-dropout) to the standard dropout for training deep convolutional neural networks. Finally, we
compare e-dropout with batch normalization.
s-dropout(tr)
s-dropout(te)
d-dropout(tr)
d-dropout(te)

0.45
0.4
0.35

0.15

0.3
0.25
0.2

0.1
0.05

0

1

2

3

# of iters

4

5

6

?10 4

0.12
0.1

0.06

0.05
0

0

0.14

0.08

0.15

0.1

0.9

s-dropout(tr)
s-dropout(te)
d-dropout(tr)
d-dropout(te)

0.16

error

error

0.2

error

0.18

0.5

s-dropout(tr)
s-dropout(te)
d-dropout(tr)
d-dropout(te)

1

2

# of iters

3

4
?10 4

0.7
0.6
0.5
0.4

no BN and no Dropout
BN
BN+Dropout
Evolutional Dropout

0.3
0.2
0.1

0.04
0

0.8

test accuracy

0.3
0.25

0

2

4

# of iters

6

8
?10 5

0

0

1

2

3

4

# of iters

5

6

7
?10 4

Figure 2: Left three: data-dependent dropout vs. standard dropout on three data sets (real-sim,
news20, RCV1) for logistic regression; Right: Evolutional dropout vs BN on CIFAR-10. (best seen
in color).
5.1

Shallow Learning

We implement the presented stochastic optimization algorithm. To evaluate the performance
of data-dependent dropout for shallow learning, we use the three data sets: real-sim, news20
and RCV13 . In this experiment, we use a fixed step size and tune the step size in
[0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001] and report the best results in terms of convergence
speed on the training data for both standard dropout and data-dependent dropout. The left three
panels in Figure 2 show the obtained results on these three data sets. In each figure, we plot both
the training error and the testing error. We can see that both the training and testing errors using the
proposed data-dependent dropout decrease much faster than using the standard dropout and also a
smaller testing error is achieved by using the data-dependent dropout.
5.2

Evolutional Dropout for Deep Learning

We would like to emphasize that we are not aiming to obtain better prediction performance by trying
different network structures and different engineering tricks such as data augmentation, whitening,
etc., but rather focus on the comparison of the proposed dropout to the standard dropout using
Bernoulli noise on the same network structure. In our experiments, we use the default splitting of
training and testing data in all data sets. We directly optimize the neural networks using all training
images without further splitting it into a validation data to be added into the training in later stages,
which explains some marginal gaps from the literature results that we observed (e.g., on CIFAR-10
compared with [19]).
We conduct experiments on four benchmark data sets for comparing e-dropout and s-dropout: MNIST
[10], SVHN [11], CIFAR-10 and CIFAR-100 [8]. We use the same or similar network structure as in
the literatures for the four data sets. In general, the networks consist of convolution layers, pooling
layers, locally connected layers, fully connected layers, softmax layers and a cost layer. For the
detailed neural network structures and their parameters, please refer to the supplementary materials.
The dropout is added to some fully connected layers or locally connected layers. The rectified linear
activation function is used for all neurons. All the experiments are conducted using the cuda-convnet
library 4 . The training procedure is similar to [9] using mini-batch SGD with momentum (0.9). The
3
4

https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/
https://code.google.com/archive/p/cuda-convnet/

7

0.09

s-dropout(tr)
s-dropout(te)
e-dropout(tr)
e-dropout(te)

0.8
0.7

0.4
0.3
0.2

0.05

1

s-dropout(tr)
s-dropout(te)
e-dropout(tr)
e-dropout(te)

0.5
0.4

error

0.06

0.5

0.8
0.7

0.3

0.04

0.2

0.03

0.1

2000

4000

6000

# of iters

(a) MNIST

8000

0.01

0.6
0.5
0.4
0.3
0.2

0
0

s-dropout(tr)
s-dropout(te)
e-dropout(tr)
e-dropout(te)

0.9

0.02

0.1
0

0.07

error

error

0.6

0.6

s-dropout(tr)
s-dropout(te)
e-dropout(tr)
e-dropout(te)

0.08

error

0.9

0

2000

4000

6000

8000

10000

0

1

2

3

4

# of Iters

12000

# of iters

(b) SVHN

(c) CIFAR-10

5

6

?10 5

0.1

0

2

4

6

8

10

# of iters

12
?10 4

(d) CIFAR-100

Figure 3: Evolutional dropout vs. standard dropout on four benchmark datasets for deep learning
(best seen in color).
size of mini-batch is fixed to 128. The weights are initialized based on the Gaussian distribution
with mean zero and standard deviation 0.01. The learning rate (i.e., step size) is decreased after a
number of epochs similar to what was done in previous works [9]. We tune the initial learning rates
for s-dropout and e-dropout separately from 0.001, 0.005, 0.01, 0.1 and report the best result on each
data set that yields the fastest convergence.
Figure 3 shows the training and testing error curves in the optimization process on the four data sets
using the standard dropout and the evolutional dropout. For SVHN data, we only report the first
12000 iterations, after which the error curves of the two methods almost overlap. We can see that
using the evolutional dropout generally converges faster than using the standard dropout. On CIFAR100 data, we have observed significant speed-up. In particular, the evolutional dropout achieves
relative improvements over 10% on the testing performance and over 50% on the convergence speed
compared to the standard dropout.
5.3

Comparison with the Batch Normalization (BN)

Finally, we make a comparison between the evolutional dropout and the batch normalization. For
batch normalization, we use the implementation in Caffe 5 . We compare the evolutional dropout with
the batch normalization on CIFAR-10 data set. The network structure is from the Caffe package and
can be found in the supplement, which is different from the one used in the previous experiment.
It contains three convolutional layers and one fully connected layer. Each convolutional layer is
followed by a pooling layer. We compare four methods: (1) No BN and No dropout - without using
batch normalization and dropout; (2) BN; (3) BN with standard dropout; (4) Evolutional Dropout.
The rectified linear activation is used in all methods. We also tried BN with the sigmoid activation
function, which gives worse results. For the methods with BN, three batch normalization layers are
inserted before or after each pooling layer following the architecture given in Caffe package (see
supplement). For the evolutional dropout training, only one layer of dropout is added to the the last
convolutional layer. The mini-batch size is set to 100, the default value in Caffe. The initial learning
rates for the four methods are set to the same value (0.001), and they are decreased once by ten times.
The testing accuracy versus the number of iterations is plotted in the right panel of Figure 2, from
which we can see that the evolutional dropout training achieves comparable performance with BN
+ standard dropout, which justifies our claim that evolutional dropout also addresses the internal
covariate shift issue.

6

Conclusion

In this paper, we have proposed a distribution-dependent dropout for both shallow learning and
deep learning. Theoretically, we proved that the new dropout achieves a smaller risk and faster
convergence. Based on the distribution-dependent dropout, we developed an efficient evolutional
dropout for training deep neural networks that adapts the sampling probabilities to the evolving
distributions of layers? outputs. Experimental results on various data sets verified that the proposed
dropouts can dramatically improve the convergence and also reduce the testing error.
Acknowledgments
We thank anonymous reviewers for their comments. Z. Li and T. Yang are partially supported by
National Science Foundation (IIS-1463988, IIS-1545995). B. Gong is supported in part by NSF
(IIS-1566511) and a gift from Adobe.
5

https://github.com/BVLC/caffe/

8

References
[1] Jimmy Ba and Brendan Frey. Adaptive dropout for training deep neural networks. In Advances in Neural
Information Processing Systems, pages 3084?3092, 2013.
[2] Pierre Baldi and Peter J Sadowski. Understanding dropout. In Advances in Neural Information Processing
Systems, pages 2814?2822, 2013.
[3] Benjamin Graham, Jeremy Reizenstein, and Leigh Robinson. Efficient batchwise dropout training using
submatrices. CoRR, abs/1502.02478, 2015.
[4] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580,
2012.
[5] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
[6] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980,
2014.
[7] Diederik P. Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization
trick. CoRR, abs/1506.02557, 2015.
[8] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images, 2009.
[9] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional
neural networks. In Advances in neural information processing systems, pages 1097?1105, 2012.
[10] Yann LeCun, L?on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278?2324, 1998.
[11] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in
natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised
feature learning, volume 2011, page 4. Granada, Spain, 2011.
[12] Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized optimization in
deep neural networks. In Advances in Neural Information Processing Systems, pages 2413?2421, 2015.
[13] Marc?Aurelio Ranzato, Alex Krizhevsky, and Geoffrey E. Hinton. Factored 3-way restricted boltzmann
machines for modeling natural images. In AISTATS, pages 621?628, 2010.
[14] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic convex optimization.
In The 22nd Conference on Learning Theory (COLT), 2009.
[15] Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Smoothness, low noise and fast rates. In Advances
in Neural Information Processing Systems 23 (NIPS), pages 2199?2207, 2010.
[16] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout:
A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15
(1):1929?1958, 2014.
[17] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and
momentum in deep learning. In Proceedings of the 30th international conference on machine learning
(ICML-13), pages 1139?1147, 2013.
[18] Stefan Wager, Sida Wang, and Percy S Liang. Dropout training as adaptive regularization. In Advances in
Neural Information Processing Systems, pages 351?359, 2013.
[19] Li Wan, Matthew Zeiler, Sixin Zhang, Yann L Cun, and Rob Fergus. Regularization of neural networks
using dropconnect. In Proceedings of the 30th International Conference on Machine Learning (ICML-13),
pages 1058?1066, 2013.
[20] Sida Wang and Christopher Manning. Fast dropout training. In Proceedings of the 30th International
Conference on Machine Learning (ICML-13), pages 118?126, 2013.
[21] Sida I Wang, Mengqiu Wang, Stefan Wager, Percy Liang, and Christopher D Manning. Feature noising for
log-linear structured prediction. In EMNLP, pages 1170?1179, 2013.
[22] Sixin Zhang, Anna Choromanska, and Yann LeCun. Deep learning with elastic averaging sgd. arXiv
preprint arXiv:1412.6651, 2014.
[23] Jingwei Zhuo, Jun Zhu, and Bo Zhang. Adaptive dropout rates for learning with corrupted features. In
IJCAI, pages 4126?4133, 2015.

9

"
2017,Joint distribution optimal transportation for domain adaptation,Poster,6963-joint-distribution-optimal-transportation-for-domain-adaptation.pdf,"This paper deals with the unsupervised domain adaptation problem, where one wants to estimate a prediction function $f$ in a given target domain without any labeled sample by exploiting the knowledge available from a source domain where labels are known. Our work makes the following assumption: there exists a  non-linear transformation between the joint feature/label space distributions of the two domain $\ps$ and $\pt$. We propose a solution of this problem with optimal transport, that allows to recover an estimated target $\pt^f=(X,f(X))$ by optimizing simultaneously the optimal coupling and $f$. We show that our method corresponds to the minimization of a bound on the target error, and provide an efficient algorithmic solution, for which convergence is proved. The versatility of our approach, both in terms of class of hypothesis or loss functions is demonstrated with real world classification and regression problems, for which we reach or surpass state-of-the-art results.","Joint distribution optimal transportation for domain
adaptation

Nicolas Courty?
Universit? de Bretagne Sud,
IRISA, UMR 6074, CNRS,
courty@univ-ubs.fr

R?mi Flamary?
Universit? C?te d?Azur,
Lagrange, UMR 7293 , CNRS, OCA
remi.flamary@unice.fr

Amaury Habrard
Univ Lyon, UJM-Saint-Etienne, CNRS,
Lab. Hubert Curien UMR 5516, F-42023
amaury.habrard@univ-st-etienne.fr

Alain Rakotomamonjy
Normandie Universite
Universit? de Rouen, LITIS EA 4108
alain.rakoto@insa-rouen.fr

Abstract
This paper deals with the unsupervised domain adaptation problem, where one
wants to estimate a prediction function f in a given target domain without any
labeled sample by exploiting the knowledge available from a source domain where
labels are known. Our work makes the following assumption: there exists a nonlinear transformation between the joint feature/label space distributions of the two
domain Ps and Pt that can be estimated with optimal transport. We propose a
solution of this problem that allows to recover an estimated target Ptf = (X, f (X))
by optimizing simultaneously the optimal coupling and f . We show that our method
corresponds to the minimization of a bound on the target error, and provide an
efficient algorithmic solution, for which convergence is proved. The versatility of
our approach, both in terms of class of hypothesis or loss functions is demonstrated
with real world classification and regression problems, for which we reach or
surpass state-of-the-art results.

1

Introduction

In the context of supervised learning, one generally assumes that the test data is a realization of the
same process that generated the learning set. Yet, in many practical applications it is often not the
case, since several factors can slightly alter this process. The particular case of visual adaptation [1]
in computer vision is a good example: given a new dataset of images without any label, one may want
to exploit a different annotated dataset, provided that they share sufficient common information and
labels. However, the generating process can be different in several aspects, such as the conditions and
devices used for acquisition, different pre-processing, different compressions, etc. Domain adaptation
techniques aim at alleviating this issue by transferring knowledge between domains [2]. We propose
in this paper a principled and theoretically founded way of tackling this problem.
The domain adaptation (DA) problem is not new and has received a lot of attention during the past ten
years. State-of-the-art methods are mainly differing by the assumptions made over the change in data
distributions. In the covariate shift assumption, the differences between the domains are characterized
by a change in the feature distributions P(X), while the conditional distributions P(Y |X) remain
unchanged (X and Y being respectively the instance and label spaces). Importance re-weighting can
be used to learn a new classifier (e.g. [3]), provided that the overlapping of the distributions is large
?

Both authors contributed equally.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

enough. Kernel alignment [4] has also been considered for the same purpose. Other types of method,
denoted as Invariant Components by Gong and co-authors [5], are looking for a transformation
T such that the new representations of input data are matching, i.e. Ps (T (X)) = Pt (T (X)).
Methods are then differing by: i) The considered class of transformation, that are generally defined
as projections (e.g. [6, 7, 8, 9, 5]), affine transform [4] or non-linear transformation as expressed by
neural networks [10, 11] ii) The types of divergences used to compare Ps (T (X)) and Pt (T (X)),
such as Kullback Leibler [12] or Maximum Mean Discrepancy [9, 5]. Those divergences usually
require that the distributions share a common support to be defined. A particular case is found in
the use of optimal transport, introduced for domain adaptation by [13, 14]. T is then defined to be
a push-forward operator such that Ps (X) = Pt (T (X)) and that minimizes a global transportation
effort or cost between distributions. The associated divergence is the so-called Wasserstein metric,
that has a natural Lagrangian formulation and avoids the estimation of continuous distribution by
means of kernel. As such, it also alleviates the need for a shared support.
The methods discussed above implicitly assume that the conditional distributions are unchanged by
T , i.e. Ps (Y |T (X)) ? Pt (Y |T (X)) but there is no clear reason for this assumption to hold. A
more general approach is to adapt both marginal feature and conditional distributions by minimizing
a global divergence between them. However, this task is usually hard since no label is available in
the target domain and therefore no empirical version Pt (Y |X) can be used. This was achieved by
restricting to specific class of transformation such as projection [9, 5].
Contributions and outline. In this work we propose a novel framework for unsupervised domain
adaptation between joint distributions. We propose to find a function f that predicts an output
value given an input x ? X , and that minimizes the optimal transport loss between the joint source
distribution Ps and an estimated target joint distribution Ptf = (X, f (X)) depending on f (detailed
in Section 2). The method is denoted as JDOT for ?Joint Distribution Optimal Transport"" in the
remainder. We show that the resulting optimization problem stands for a minimization of a bound
on the target error of f (Section 3) and propose an efficient algorithm to solve it (Section 4). Our
approach is very general and does not require to learn explicitly a transformation, as it directly solves
for the best function. We show that it can handle both regression and classification problems with a
large class of functions f including kernel machines and neural networks. We finally provide several
numerical experiments on real regression and classification problems that show the performances of
JDOT over the state-of-the-art (Section 5).

2

Joint distribution Optimal Transport

Let ? ? Rd be a compact input measurable space of dimension d and C the set of labels. P(?)
denotes the set of all the probability measures over ?. The standard learning paradigm assumes
s
classically the existence of a set of data Xs = {xsi }N
i=1 associated with a set of class label information
s Ns
s
t
Ys = {yi }i=1 , yi ? C (the learning set), and a data set with unknown labels Xt = {xti }N
i=1 (the
testing set). In order to determine the set of labels Yt associated with Xt , one usually relies on an
empirical estimate of the joint probability distribution P(X, Y ) ? P(? ? C) from (Xs , Ys ), and
the assumption that Xs and Xt are drawn from the same distribution ? ? P(?). In the considered
adaptation problem, one assumes the existence of two distinct joint probability distributions Ps (X, Y )
and Pt (X, Y ) which correspond respectively to two different source and target domains. We will
write ?s and ?t their respective marginal distributions over X.
2.1

Optimal transport in domain adaptation

The Monge problem is seeking for a map T0 : ? ? ? that pushes ?s toward ?t defined as:
Z
T0 = argmin
d(x, T (x))d?s (x),
s.t. T #?s = ?t ,
T

?

where T #?s the image measure of ?s by T , verifying:
T #?s (A) = ?t (T ?1 (A)), ? Borel subset A ? ?,
+

(1)

and d : ? ? ? ? R is a metric. In the remainder, we will always consider without further
notification the case where d is the squared Euclidean metric. When T0 exists, it is called an optimal
transport map, but it is not always the case (e.g. assume that ?s is defined by one Dirac measure and
2

?t by two). A relaxed version of this problem has been proposed by Kantorovitch [15], who rather
seeks for a transport plan (or equivalently a joint probability distribution) ? ? P(? ? ?) such that:
Z
? 0 = argmin
d(x1 , x2 )d?(x1 , x2 ),
(2)
???(?s ,?t )

???

where ?(?s , ?t ) = {? ? P(? ? ?)|p+ #? = ?s , p? #? = ?t } and p+ and p? denotes the two
marginal projections of ? ? ? to ?. Minimizers of this problem are called optimal transport plans.
Should ? 0 be of the form (id ? T )#?s , then the solution to Kantorovich and Monge problems
coincide. As such the Kantorovich relaxation can be seen as a generalization of the Monge problem,
with less constraints on the existence and uniqueness of solutions [16].
Optimal transport has been used in DA as a principled way to bring the source and target distribution
closer [13, 14, 17], by seeking for a transport plan between the empirical distributions of Xs and
Xt and interpolating Xs thanks to a barycentric mapping [14], or by estimating a mapping which is
not the solution of Monge problem but allows to map unseen samples [17]. Moreover, they show
that better constraining the structure of ? through entropic or classwise regularization terms helps in
achieving better empirical results.
2.2

Joint distribution optimal transport loss

The main idea of this work is is to handle a change in both marginal and conditional distributions.
As such, we are looking for a transformation T that will align directly the joint distributions Ps and
Pt . Following the Kantovorich formulation of (2), T will be implicitly expressed through a coupling
between both joint distributions as:
Z
? 0 = argmin
D(x1 , y1 ; x2 , y2 )d?(x1 , y1 ; x2 , y2 ),
(3)
???(Ps ,Pt )

(??C)2

where D(x1 , y1 ; x2 , y2 ) = ?d(x1 , x2 ) + L(y1 , y2 ) is a joint cost measure combining both the
distances between the samples and a loss function L measuring the discrepancy between y1 and y2 .
While this joint cost is specific (separable), we leave for future work the analysis of generic joint cost
function. Putting it in words, matching close source and target samples with similar labels costs few.
? is a positive parameter which balances the metric in the feature space and the loss. As such, when
? ? +?, this cost is dominated by the metric in the input feature space, and the solution of the
coupling problem is the same as in [14]. It can be shown that a minimizer to (3) always exists and is
unique provided that D(?) is lower semi-continuous (see [18], Theorem 4.1), which is the case when
d(?) is a norm and for every usual loss functions [19].
In the unsupervised DA problem, one does not have access to labels in the target domain, and as such
it is not possible to find the optimal coupling. Since our goal is to find a function on the target domain
f : ? ? C, we suggest to replace y2 by a proxy f (x2 ). This leads to the definition of the following
joint distribution that uses a given function f as a proxy for y:
Ptf = (x, f (x))x??t

(4)

PNs
?
In practice we consider empirical versions of Ps and Ptf , i.e. P?s = N1s i=1
?xsi ,yis and Ptf =
PNt
1
i=1 ?xti ,f (xti ) . ? is then a matrix which belongs to ? , i.e.the transportation polytope of nonNt
negative matrices between uniform distributions. Since our goal is to estimate a prediction f on
the target domain, we propose to find the one that produces predictions that match optimally source
labels to the aligned target instances in the transport plan. For this purpose, we propose to solve the
following problem for JDOT:
X
?
min
D(xsi , yis ; xtj , f (xtj ))? ij ? min W1 (P?s , Ptf )
(5)
f,???

f

ij

where W1 is the 1-Wasserstein distance for the loss D(x1 , y1 ; x2 , y2 ) = ?d(x1 , x2 ) + L(y1 , y2 ).
We will make clear in the next section that the function f we retrieve is theoretically sound with
respect to the target error. Note that in practice we add a regularization term for function f in order
to avoid overfitting as discussed in Section 4. An illustration of JDOT for a regression problem is
given in Figure 1. In this figure, we have very different joint and marginal distributions but we want
3

y

1.5
1.0
0.5
0.0
0.5
1.0
1.5
2.0

Toy regression distributions

Toy regression models

0
x

5

Model estimated with JDOT

1.0

1.0

0.5

0.5

0.5

0.0

0.0

0.0

0.5

0.5

Source model
Target model
Source samples
Target samples

1.0
5

Joint OT matrices

1.0

2.5

0.0

x

2.5

5.0

0.5
JDOT matrix link
OT matrix link

1.0
2.5

0.0

x

2.5

5.0

Source model
Target model
JDOT model

1.0
2.5

0.0

x

2.5

5.0

Figure 1: Illustration of JDOT on a 1D regression problem. (left) Source and target empirical
distributions and marginals (middle left) Source and target models (middle right) OT matrix on
empirical joint distributions and with JDOT proxy joint distribution (right) estimated prediction
function f .
to illustrate that the OT matrix ? obtained using the true empirical distribution Pt is very similar to
the one obtained with the proxy Ptf which leads to a very good model for JDOT.
Choice of ?. This is an important parameter balancing the alignment of feature space and labels. A
natural choice of the ? parameter is obtained by normalizing the range of values of d(xsi , xtj ) with
? = 1/ maxi,j d(xsi , xtj ). In the numerical experiment section, we show that this setting is very good
in two out of three experiments. However, in some cases, better performances are obtained with a
cross-validation of this parameter. Also note that ? is strongly linked to the smoothness of the loss
L and of the optimal labelling functions and can be seen as a Lipschitz constant in the bound of
Theorem 3.1.
Relation to other optimal transport based DA methods. Previous DA methods based on optimal
transport [14, 17] do not not only differ by the nature of the considered distributions, but also in the
way the optimal plan is used to find f . They learn a complex mapping between the source and target
distributions when the objective is only to estimate a prediction function f on target. To do so, they
rely on a barycentric mapping that minimizes only approximately the Wasserstein distance between
the distributions. As discussed in Section 4, JDOT uses the optimal plan to propagate and fuse the
labels from the source to target. Not only are the performances enhanced, but we also show how this
approach is more theoretically well grounded in next section 3.
Relation to Transport Lp distances. Recently, Thorpe and co-authors introduced the Transportation
Lp distance [20]. Their objective is to compute a meaningful distance between multi-dimensional
signals. Interestingly their distance can be seen as optimal transport between two distributions of
the form (4) where the functions are known and the label loss L is chosen as a Lp distance. While
their approach is inspirational, JDOT is different both in its formulation, where we introduce a more
general class of loss L, and in its objective, as our goal is to estimate the target function f which is
not known a priori. Finally we show theoretically and empirically that our formulation addresses
successfully the problem of domain adaptation.

3

A Bound on the Target Error

Let f be an hypothesis function from a given class of hypothesis H. We define the expected loss in
def
the target domain errT (f ) as errT (f ) = E(x,y)?Pt L(y, f (x)). We define similarly errS (f ) for the
source domain. We assume the loss function L to be bounded, symmetric, k-lipschitz and satisfying
the triangle inequality.
To provide some guarantees on our method, we consider an adaptation of the notion probabilistic
Lipschitzness introduced in [21, 22] which assumes that two close instances must have the same
labels with high probability. It corresponds to a relaxation of the classic Lipschitzness allowing one to
model the marginal-label relatedness such as in Nearest-Neighbor classification, linear classification
or cluster assumption. We propose an extension of this notion in a domain adaptation context by
assuming that a labeling function must comply with two close instances of each domain w.r.t. a
coupling ?.
4

Definition (Probabilistic Transfer Lipschitzness) Let ?s and ?t be respectively the source and
target distributions. Let ? : R ? [0, 1]. A labeling function f : ? ? R and a joint distribution
?(?s , ?t ) over ?s and ?t are ?-Lipschitz transferable if for all ? > 0:
P r(x1 ,x2 )??(?s ,?t ) [|f (x1 ) ? f (x2 )| > ?d(x1 , x2 )] ? ?(?).
Intuitively, given a deterministic labeling functions f and a coupling ?, it bounds the probability of
finding pairs of source-target instances labelled differently in a (1/?)-ball with respect to ?.
We can now give our main result (simplified version):
Let ??
=
?f
?
argmin???(Ps ,P f ) (??C)2 ?d(xs , xt ) + L(ys , yt )d?(xs , ys ; xt , yt ) and W1 (Ps , Pt ) the ast
sociated 1-Wasserstein distance. Let f ? ? H be a Lipschitz labeling function that verifies the
?-probabilistic transfer Lipschitzness (PTL) assumption w.r.t. ?? and that minimizes the joint error
errS (f ? ) + errT (f ? ) w.r.t all PTL functions compatible with ?? . We assume the input instances
are bounded s.t. |f ? (x1 ) ? f ? (x2 )| ? M for all x1 , x2 . Let L be any symmetric loss function,
k-Lipschitz and satisfying the triangle inequality. Consider a sample of Ns labeled source instances
drawn from Ps and Nt unlabeled instances drawn from ?t , and then for all ? > 0, with ? = k?, we
have with probability at least 1 ? ? that:
r


2
1
2
1
?f
?
log( ) ?
errT (f ) ? W1 (Ps , Pt ) +
+?
+ errS (f ? ) + errT (f ? ) + kM ?(?).
c0
?
NS
NT
f

Theorem 3.1 Let

be

any

labeling

function

of

?

H.

R

The detailed proof of Theorem 3.1 is given in the supplementary material. The previous bound on the
target error above is interesting to interpret. The first two terms correspond to the objective function
(5) we propose to minimize accompanied with a sampling bound. The last term ?(?) assesses the
probability under which the probabilistic Lipschitzness does not hold. The remaining two terms
involving f ? correspond to the joint error minimizer illustrating that domain adaptation can work
only if we can predict well in both domains, similarly to existing results in the literature [23, 24].
If the last terms are small enough, adaptation is possible if we are able to align well Ps and Ptf ,
provided that f ? and ?? verify the PTL. Finally, note that ? = k? and tuning this parameter is thus
actually related to finding the Lipschitz constants of the problem.

4

Learning with Joint Distribution OT

In this section, we provide some details about the JDOT?s optimization problem given in Equation
(5) and discuss algorithms for its resolution. We will assume that the function space H to which
f belongs is either a RKHS or a function space parametrized by some parameters w ? Rp . This
framework encompasses linear models, neural networks, and kernel methods. Accordingly, we
are going to define a regularization term ?(f ) on f . Depending on how H is defined, ?(f ) is
either a non-decreasing function of the squared-norm induced by the RKHS (so that the representer
theorem is applicable) or a squared-norm on the vector parameter. We will further assume that ?(f )
is continuously differentiable. As discussed above, f is to be learned according to the following
optimization problem
min

f ?H,???

X


? i,j ?d(xsi , xtj ) + L(yis , f (xtj )) + ??(f )

(6)

i,j

where the loss function L is continuous and differentiable with respects to its second variable. Note
that while the above problem does not involve any regularization term on the coupling matrix ?, it is
essentially for the sake of simplicity and readability. Regularizers like entropic regularization [25],
which is relevant when the number of samples is very large, can still be used without significant
change to the algorithmic framework.
Optimization procedure. According to the above hypotheses on f and L, Problem (6) is smooth
and the constraints are separable according to f and ?. Hence, a natural way to solve the problem (6)
is to rely on alternate optimization w.r.t. both parameters ? and f . This algorithm well-known as
Block Coordinate Descent (BCD) or Gauss-Seidel method (the pseudo code of the algorithm is given
in appendix). Block optimization steps are discussed with further details in the following.
5

Solving with fixed f boils down to a classical OT problem with a loss matrix C such that Ci,j =
?d(xsi , xtj ) + L(yis , f (xtj )). We can use classical OT solvers such as the network simplex algorithm,
but other strategies can be considered, such as regularized OT [25] or stochastic versions [26].
The optimization problem with fixed ? leads to a new learning problem expressed as
X
min
? i,j L(yis , f (xtj )) + ??(f )
f ?H

(7)

i,j

Note how the data fitting term elegantly and naturally encodes the transfer of source labels yis through
estimated labels of test samples with a weighting depending on the optimal transport matrix. However,
this comes at the price of having a quadratic number Ns Nt of terms, which can be considered as
computationally expensive. We will see in the sequel that we can benefit from the structure of the
chosen loss to greatly reduce its complexity. In addition, we emphasize that when H is a RKHS,
owing to kernel trick and the representer theorem, problem (7) can be re-expressed as an optimization
problem with Nt number of parameters all belonging to R.
Let us now discuss briefly the convergence of the proposed algorithm. Owing to the 2-block coordinate
descent structure, to the differentiability of the objective function in Problem (6) and constraints on f
(or its kernel trick parameters) and ? are closed, non-empty and convex, convergence result of Grippo
et al. [27] on 2-block Gauss-Seidel methods directly applies. It states that if the sequence {? k , f k }
produced by the algorithm has limit points then every limit point of the sequence is a critical point of
Problem (6).
Estimating f for least square regression problems. We detail the use of JDOT for transfer leastsquare regression problem i.e when L is the squared-loss. In this context, when the optimal transport
matrix ? is fixed the learning problem boils down to
X 1
min
k?
yj ? f (xtj )k2 + ?kf k2
(8)
f ?H
n
t
j
P
where the y?j = nt j ? i,j yis is a weighted average of the source target values. Note that this
simplification results from the properties of the quadratic loss and that it may not occur for more
complex regression loss.
Estimating f for hinge loss classification problems. We now aim at estimating a multiclass
classifier with a one-against-all strategy. We suppose that the data fitting is the binary squared hinge
loss of the form L(y, f (x)) = max(0, 1 ? yf (x))2 . In a One-Against-All strategy we often use the
s
s
binary matrices P such that Pi,k
= 1 if sample i is of class k else Pi,k
= 0. Denote as fk ? H the
decision function related to the k-vs-all problem. The learning problem (7) can now be expressed as
X
X
min
P?j,k L(1, fk (xtj )) + (1 ? P?j,k )L(?1, fk (xtj )) + ?
kfk k2
(9)
fk ?H

j,k

k

? is the transported class proportion matrix P
? = 1 ? > Ps . Interestingly this formulation
where P
Nt
illustrates that for each target sample, the data fitting term is a convex sum of hinge loss for a negative
and positive label with weights in ?.

5

Numerical experiments

In this section we evaluate the performance of our method (JDOT) on two different transfer tasks of
classification and regression on real datasets 2 .
Caltech-Office classification dataset. This dataset [28] is dedicated to visual adaptation. It contains
images from four different domains: Amazon, the Caltech-256 image collection, Webcam and DSLR.
Several features, such as presence/absence of background, lightning conditions, image quality, etc.)
induce a distribution shift between the domains, and it is therefore relevant to consider a domain
adaptation task to perform the classification. Following [14], we choose deep learning features
to represent the images, extracted as the weights of the fully connected 6th layer of the DECAF
convolutional neural network [29], pre-trained on ImageNet. The final feature vector is a sparse 4096
dimensional vector.
2

Open Source Python implementation of JDOT: https://github.com/rflamary/JDOT

6

Table 1: Accuracy on the Caltech-Office Dataset. Best value in bold.
Domains

Base

SurK

SA

ARTL

OT-IT

OT-MM

JDOT

caltech?amazon
caltech?webcam
caltech?dslr
amazon?caltech
amazon?webcam
amazon?dslr
webcam?caltech
webcam?amazon
webcam?dslr
dslr?caltech
dslr?amazon
dslr?webcam
Mean
Mean rank
p-value

92.07
76.27
84.08
84.77
79.32
86.62
71.77
79.44
96.18
77.03
83.19
96.27
83.92
5.33
< 0.01

91.65
77.97
82.80
84.95
81.36
87.26
71.86
78.18
95.54
76.94
82.15
92.88
83.63
5.58
< 0.01

90.50
81.02
85.99
85.13
85.42
89.17
75.78
81.42
94.90
81.75
83.19
88.47
85.23
4.00
0.01

92.17
80.00
88.54
85.04
79.32
85.99
72.75
79.85
100.00
78.45
83.82
98.98
85.41
3.75
0.04

89.98
80.34
78.34
85.93
74.24
77.71
84.06
89.56
99.36
85.57
90.50
96.61
86.02
3.50
0.25

92.59
78.98
76.43
87.36
85.08
79.62
82.99
90.50
99.36
83.35
90.50
96.61
86.95
2.83
0.86

91.54
88.81
89.81
85.22
84.75
87.90
82.64
90.71
98.09
84.33
88.10
96.61
89.04
2.50
?

Table 2: Accuracy on the Amazon review experiment. Maximum value in bold font.
Domains

NN

DANN

JDOT (mse)

JDOT (Hinge)

books?dvd
books?kitchen
books?electronics
dvd?books
dvd?kitchen
dvd?electronics
kitchen?books
kitchen?dvd
kitchen?electronics
electronics?books
electronics?dvd
electronics?kitchen

0.805
0.768
0.746
0.725
0.760
0.732
0.704
0.723
0.847
0.713
0.726
0.855

0.806
0.767
0.747
0.747
0.765
0.738
0.718
0.730
0.846
0.718
0.726
0.850

0.794
0.791
0.778
0.761
0.811
0.778
0.732
0.764
0.844
0.740
0.738
0.868

0.795
0.794
0.781
0.763
0.821
0.788
0.728
0.765
0.845
0.749
0.737
0.872

Mean
p-value

0.759
0.004

0.763
0.006

0.783
0.025

0.787
?

We compare our method with four other methods: the surrogate kernel approach ([4], denoted
SurK), subspace adaptation for its simplicity and good performances on visual adaptation ([8], SA),
Adaptation Regularization based Transfer Learning ([30], ARTL), and the two variants of regularized
optimal transport [14]: entropy-regularized OT-IT and classwise regularization implemented with the
Majoration-Minimization algorithm OT-MM, that showed to give better results in practice than its
group-lasso counterpart. The classification is conducted with a SVM together with a linear kernel for
every method. Its results when learned on the source domain and tested on the target domain are also
reported to serve as baseline (Base). All the methods have hyper-parameters, that are selected using
the reverse cross-validation of Zhong and colleagues [31].The dimension d for SA is chosen from
{1, 4, 7, . . . , 31}. The entropy regularization for OT-IT and OT-MM is taken from {102 , . . . , 105 },
102 being the minimum value for the Sinkhorn algorithm to prevent numerical errors. Finally the ?
parameter of OT-MM is selected from {1, . . . , 105 } and the ? in JDOT from {10?5 , 10?4 , . . . , 1}.
The classification accuracy for all the methods is reported in Table 1. We can see that JDOT
is consistently outperforming the baseline (5 points in average), indicating that the adaptation is
successful in every cases. Its mean accuracy is the best as well as its average ranking. We conducted
a Wilcoxon signed-rank test to test if JDOT was statistically better than the other methods, and
report the p-value in the tables. This test shows that JDOT is statistically better than the considered
methods, except for OT based ones that where state of the art on this dataset [14].
Amazon review classification dataset We now consider the Amazon review dataset [32] which
contains online reviews of different products collected on the Amazon website. Reviews are encoded
with bag-of-word unigram and bigram features as input. The problem is to predict positive (higher
than 3 stars) or negative (3 stars or less) notation of reviews (binary classification). Since different
7

Table 3: Comparison of different methods on the Wifi localization dataset. Maximum value in bold.
Domains

KRR

SurK

DIP

DIP-CC

GeTarS

CTC

CTC-TIP

JDOT

t1 ? t2
t1 ? t3
t2 ? t3

80.84?1.14
76.44?2.66
67.12?1.28

90.36?1.22
94.97?1.29
85.83 ? 1.31

87.98?2.33
84.20?4.29
80.58 ? 2.10

91.30?3.24
84.32?4.57
81.22 ? 4.31

86.76 ? 1.91
90.62?2.25
82.68 ? 3.71

89.36?1.78
94.80?0.87
87.92 ? 1.87

89.22?1.66
92.60 ? 4.50
89.52 ? 1.14

93.03 ? 1.24
90.06 ? 2.01
86.76 ? 1.72

hallway1
hallway2
hallway3

60.02 ?2.60
49.38 ? 2.30
48.42 ?1.32

76.36 ? 2.44
64.69 ?0.77
65.73 ? 1.57

77.48 ? 2.68
78.54 ? 1.66
75.10? 3.39

76.24? 5.14
77.8? 2.70
73.40? 4.06

84.38 ? 1.98
77.38 ? 2.09
80.64 ? 1.76

86.98 ? 2.02
87.74 ? 1.89
82.02? 2.34

86.78 ? 2.31
87.94 ? 2.07
81.72 ? 2.25

98.83?0.58
98.45?0.67
99.27?0.41

words are employed to qualify the different categories of products, a domain adaptation task can be
formulated if one wants to predict positive reviews of a product from labelled reviews of a different
product. Following [33, 11], we consider only a subset of four different types of product: books,
DVDs, electronics and kitchens. This yields 12 possible adaptation tasks. Each domain contains
2000 labelled samples and approximately 4000 unlabelled ones. We therefore use these unlabelled
samples to perform the transfer, and test on the 2000 labelled data.
The goal of this experiment is to compare to the state-of-the-art method on this subset, namely
Domain adversarial neural network ([11], denoted DANN), and to show the versatility of our method
that can adapt to any type of classifier. The neural network used for all methods in this experiment is
a simple 2-layer model with sigmoid activation function in the hidden layer to promote non-linearity.
50 neurons are used in this hidden layer. For DANN, hyper-parameters are set through the reverse
cross-validation proposed in [11], and following the recommendation of authors the learning rate
is set to 10?3 . In the case of JDOT, we used the heuristic setting of ? = 1/ maxi,j d(xsi , xtj ), and
as such we do not need any cross-validation. The squared Euclidean norm is used for both metric
in feature space and we test as loss functions both mean squared errors (mse) and Hinge losses. 10
iterations of the block coordinate descent are realized. For each method, we stop the learning process
of the network after 5 epochs. Classification accuracies are presented in table 2. The neural network
(NN), trained on source and tested on target, is also presented as a baseline. JDOT surpasses DANN
in 11 out of 12 tasks (except on books?dvd). The Hinge loss is better in than mse in 10 out of 12
cases, which is expected given the superiority of the Hinge loss on classification tasks [19].
Wifi localization regression dataset For the regression task, we use the cross-domain indoor Wifi
localization dataset that was proposed by Zhang and co-authors [4], and recently studied in [5]. From
a multi-dimensional signal (collection of signal strength perceived from several access points), the
goal is to locate the device in a hallway, discretized into a grid of 119 squares, by learning a mapping
from the signal to the grid element. This translates as a regression problem. As the signals were
acquired at different time periods by different devices, a shift can be encountered and calls for an
adaptation. In the remaining, we follow the exact same experimental protocol as in [4, 5] for ease of
comparison. Two cases of adaptation are considered: transfer across periods, for which three time
periods t1, t2 and t3 are considered, and transfer across devices, where three different devices are
used to collect the signals in the same straight-line hallways (hallway1-3), leading to three different
adaptation tasks in both cases.
We compare the result of our method with several state-of-the-art methods: kernel ridge regression
with RBF kernel (KRR), surrogate kernel ([4], denoted SurK), domain-invariant projection and its
cluster regularized version ([7], denoted respectively DIP and DIP-CC), generalized target shift ([34],
denoted GeTarS), and conditional transferable components, with its target information preservation
regularization ([5], denoted respectively CTC and CTC-TIP). As in [4, 5], the hyper-parameters of
the competing methods are cross-validated on a small subset of the target domain. In the case of
JDOT, we simply set the ? to the heuristic value of ? = 1/ maxi,j d(xsi , xtj ) as discussed previously,
and f is estimated with kernel ridge regression.
Following [4], the accuracy is measured in the following way: the prediction is said to be correct if it
falls within a range of three meters in the transfer across periods, and six meters in the transfer across
devices. For each experiment, we randomly sample sixty percent of the source and target domain, and
report the mean and standard deviation of ten repetitions accuracies in Table 3. For transfer across
periods, JDOT performs best in one out of three tasks. For transfer across devices, the superiority of
JDOT is clearly assessed, for it reaches an average score > 98%, which is at least ten points ahead
of the best competing method for every task. Those extremely good results could be explained by the
fact that using optimal transport allows to consider large shifts of distribution, for which divergences
(such as maximum mean discrepancy used in CTC) or reweighting strategies can not cope with.
8

6

Discussion and conclusion

We have presented in this paper the Joint Distribution Optimal Transport for domain adaptation,
which is a principled way of performing domain adaptation with optimal transport. JDOT assumes
the existence of a transfer map that transforms a source domain joint distribution Ps (X, Y ) into
a target domain equivalent version Pt (X, Y ). Through this transformation, the alignment of both
feature space and conditional distributions is operated, allowing to devise an efficient algorithm that
simultaneously optimizes for a coupling between Ps and Pt and a prediction function that solves the
transfer problem. We also proved that learning with JDOT is equivalent to minimizing a bound on
the target distribution. We have demonstrated through experiments on classical real-world benchmark
datasets the superiority of our approach w.r.t. several state-of-the-art methods, including previous
work on optimal transport based domain adaptation, domain adversarial neural networks or transfer
components, on a variety of task including classification and regression. We have also showed the
versatility of our method, that can accommodate with several types of loss functions (mse, hinge) or
class of hypothesis (including kernel machines or neural networks). Potential follow-ups of this work
include a semi-supervised extension (using unlabelled examples in source domain) and investigating
stochastic techniques for solving efficiently the adaptation. From a theoretical standpoint, future
works include a deeper study of probabilistic transfer lipschitzness and the development of guarantees
able to take into the complexity of the hypothesis class and the space of possible transport plans.

Acknowledgements
This work benefited from the support of the project OATMIL ANR-17-CE23-0012 of the French
National Research Agency (ANR), the Normandie Projet GRR-DAISI, European funding FEDER
DAISI and CNRS funding from the D?fi Imag?In. The authors also wish to thank Kai Zhang and
Qiaojun Wang for providing the Wifi localization dataset.

References
[1] V. M. Patel, R. Gopalan, R. Li, and R. Chellappa. Visual domain adaptation: an overview of recent
advances. IEEE Signal Processing Magazine, 32(3), 2015.
[2] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on Knowledge and Data
Engineering, 22(10):1345?1359, 2010.
[3] M. Sugiyama, S. Nakajima, H. Kashima, P.V. Buenau, and M. Kawanabe. Direct importance estimation
with model selection and its application to covariate shift adaptation. In NIPS, 2008.
[4] K. Zhang, V. W. Zheng, Q. Wang, J. T. Kwok, Q. Yang, and I. Marsic. Covariate shift in Hilbert space: A
solution via surrogate kernels. In ICML, 2013.
[5] M. Gong, K. Zhang, T. Liu, D. Tao, C. Glymour, and B. Sch?lkopf. Domain adaptation with conditional
transferable components. In ICML, volume 48, pages 2839?2848, 2016.
[6] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic flow kernel for unsupervised domain adaptation. In
CVPR, 2012.
[7] M. Baktashmotlagh, M. Harandi, B. Lovell, and M. Salzmann. Unsupervised domain adaptation by domain
invariant projection. In ICCV, pages 769?776, 2013.
[8] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars. Unsupervised visual domain adaptation using
subspace alignment. In ICCV, 2013.
[9] M. Long, J. Wang, G. Ding, J. Sun, and P. Yu. Transfer joint matching for unsupervised domain adaptation.
In CVPR, pages 1410?1417, 2014.
[10] Y. Ganin and V. Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML, pages
1180?1189, 2015.
[11] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky.
Domain-adversarial training of neural networks. Journal of Machine Learning Research, 17(59):1?35,
2016.

9

[12] S. Si, D. Tao, and B. Geng. Bregman divergence-based regularization for transfer subspace learning. IEEE
Transactions on Knowledge and Data Engineering, 22(7):929?942, July 2010.
[13] N. Courty, R. Flamary, and D. Tuia. Domain adaptation with regularized optimal transport. In ECML/PKDD,
2014.
[14] N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy. Optimal transport for domain adaptation. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 2016.
[15] L. Kantorovich. On the translocation of masses. C.R. (Doklady) Acad. Sci. URSS (N.S.), 37:199?201,
1942.
[16] F. Santambrogio. Optimal transport for applied mathematicians. Birk?user, NY, 2015.
[17] M. Perrot, N. Courty, R. Flamary, and A. Habrard. Mapping estimation for discrete optimal transport. In
NIPS, pages 4197?4205, 2016.
[18] C. Villani. Optimal transport: old and new. Grund. der mathematischen Wissenschaften. Springer, 2009.
[19] Lorenzo Rosasco, Ernesto De Vito, Andrea Caponnetto, Michele Piana, and Alessandro Verri. Are loss
functions all the same? Neural Computation, 16(5):1063?1076, 2004.
[20] M. Thorpe, S. Park, S. Kolouri, G. Rohde, and D. Slepcev. A transportation lp distance for signal analysis.
CoRR, abs/1609.08669, 2016.
[21] R. Urner, S. Shalev-Shwartz, and S. Ben-David. Access to unlabeled data can speed up prediction time. In
Proceedings of ICML, pages 641?648, 2011.
[22] S. Ben-David, S. Shalev-Shwartz, and R. Urner. Domain adaptation?can quantity compensate for quality?
In Proc of ISAIM, 2012.
[23] Y. Mansour, M. Mohri, and A. Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In
Proc. of COLT, 2009.
[24] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. Wortman Vaughan. A theory of
learning from different domains. Machine Learning, 79(1-2):151?175, 2010.
[25] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In NIPS, 2013.
[26] A. Genevay, M. Cuturi, G. Peyr?, and F. Bach. Stochastic optimization for large-scale optimal transport. In
NIPS, pages 3432?3440, 2016.
[27] Luigi Grippo and Marco Sciandrone. On the convergence of the block nonlinear gauss?seidel method
under convex constraints. Operations research letters, 26(3):127?136, 2000.
[28] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. In ECCV,
LNCS, pages 213?226, 2010.
[29] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional
activation feature for generic visual recognition. In ICML, 2014.
[30] M. Long, J. Wang, G. Ding, S. Jialin Pan, and P.S. Yu. Adaptation regularization: A general framework for
transfer learning. IEEE TKDE, 26(7):1076?1089, 2014.
[31] E. Zhong, W. Fan, Q. Yang, O. Verscheure, and J. Ren. Cross validation framework to choose amongst
models and datasets for transfer learning. In ECML/PKDD, 2010.
[32] J. Blitzer, R. McDonald, and F. Pereira. Domain adaptation with structural correspondence learning. In
Proc. of the 2006 conference on empirical methods in natural language processing, pages 120?128, 2006.
[33] M. Chen, Z. Xu, K. Weinberger, and F. Sha. Marginalized denoising autoencoders for domain adaptation.
In ICML, 2012.
[34] K. Zhang, M. Gong, and B. Sch?lkopf. Multi-source domain adaptation: A causal view. In AAAI
Conference on Artificial Intelligence, pages 3150?3157, 2015.

10

"
2011,Hierarchical Multitask Structured Output Learning for Large-scale Sequence Segmentation,,4194-hierarchical-multitask-structured-output-learning-for-large-scale-sequence-segmentation.pdf,"We present a novel regularization-based Multitask Learning (MTL) formulation  for Structured Output (SO) prediction for the case of hierarchical task relations.  Structured output learning often results in dif?cult inference problems and   requires large amounts of training data to obtain accurate models. We propose to  use MTL to exploit information available for related structured output learning  tasks by means of hierarchical regularization. Due to the combination of example   sets, the cost of training models for structured output prediction can easily  become infeasible for real world applications. We thus propose an ef?cient   algorithm based on bundle methods to solve the optimization problems resulting from  MTL structured output learning. We demonstrate the performance of our approach  on gene ?nding problems from the application domain of computational biology.  We show that 1) our proposed solver achieves much faster convergence than previous   methods and 2) that the Hierarchical SO-MTL approach clearly outperforms  considered non-MTL methods.","Hierarchical Multitask Structured Output Learning
for Large-Scale Sequence Segmentation

Nico G?ornitz1
Technical University Berlin,
Franklinstr. 28/29, 10587 Berlin, Germany
Nico.Goernitz@tu-berlin.de

Christian Widmer1
FML of the Max Planck Society
Spemannstr. 39, 72070 T?ubingen, Germany
Christian.Widmer@tue.mpg.de

Georg Zeller
European Molecular Biology Laboratory
Meyerhofstr. 1, 69117 Heidelberg, Germany
Georg.Zeller@gmail.com

Andr?e Kahles
FML of the Max Planck Society
Spemannstr. 39, 72070 T?ubingen, Germany
Andre.Kahles@tue.mpg.de

S?oren Sonnenburg2
TomTom
An den Treptowers 1, 12435 Berlin, Germany
Soeren.Sonnenburg@tomtom.com

Gunnar R?atsch
FML of the Max Planck Society
Spemannstr. 39, 72070 T?ubingen, Germany
Gunnar.Raetsch@tue.mpg.de

Abstract
We present a novel regularization-based Multitask Learning (MTL) formulation
for Structured Output (SO) prediction for the case of hierarchical task relations.
Structured output prediction often leads to difficult inference problems and hence
requires large amounts of training data to obtain accurate models. We propose to
use MTL to exploit additional information from related learning tasks by means of
hierarchical regularization. Training SO models on the combined set of examples
from multiple tasks can easily become infeasible for real world applications. To
be able to solve the optimization problems underlying multitask structured output learning, we propose an efficient algorithm based on bundle-methods. We
demonstrate the performance of our approach in applications from the domain of
computational biology addressing the key problem of gene finding. We show that
1) our proposed solver achieves much faster convergence than previous methods
and 2) that the Hierarchical SO-MTL approach outperforms considered non-MTL
methods.

1

Introduction

In Machine Learning, model quality is most often limited by the lack of sufficient training data.
When data from different, but related tasks, is available, it is possible to exploit it to boost the performance of each task by transferring relevant information. Multitask learning (MTL) considers
the problem of inferring models for several tasks simultaneously, while imposing regularity criteria
or shared representations in order to allow learning across tasks. This has been an active research
focus and various methods (e.g., [5, 8]) have been explored, providing empirical findings [16] and
theoretical foundations [3, 4]. Recently, also the relationships between tasks have been studied (e.g.,
[1]) assuming a cluster relationship [11] or a hierarchy [6, 23, 13] between tasks. Our proposed
method follows this line of research in that it exploits externally provided hierarchical task relations. The generality of regularization-based MTL approaches makes it possible to extend them
beyond the simple cases of classification or regression to Structured Output (SO) learning problems
1
2

These authors contributed equally.
This work was done while SS was at Technical University Berlin

1

[14, 2, 21, 10]. Here, the output is not in the form of a discrete class label or a real valued number,
but a structured entity such as a label sequence, a tree, or a graph. One of the main contributions
of this paper is to explicitly extend a regularization-based MTL formulation to the SVM-struct formulation for SO prediction [2, 21]. SO learning methods can be computationally demanding, and
combining information from several tasks leads to even larger problems, which renders many interesting applications infeasible. Hence, our second main contribution is to provide an efficient solver
for SO problems which is based on bundle methods [18, 19, 7]. It achieves much faster convergence
and is therefore an essential tool to cope with the demands of the MTL setting.
SO learning has been successfully applied in the analysis of images, natural language, and sequences. The latter is of particular interest in computational biology for the analysis of DNA, RNA
or protein sequences. This field moreover constitutes an excellent application area for MTL [12, 22].
In computational biology, one often uses supervised learning methods to model biological processes
in order to predict their outcomes and ultimately understand them better. Due to the complexity
of many biological mechanisms, rich computational models have to be developed, which in turn
require a reasonable amount of training data. However, especially in the biomedical domain, obtaining labeled training examples through experiments can be costly. Thus, combining information
from several related tasks can be a cost-effective approach to best exploit the available label data.
When transferring label information across tasks, it often makes sense to assume hierarchical task
relations. In particular, in computational biology, where evolutionary processes often impose a task
hierarchy [22]. For instance, we might be interested in modeling a common biological mechanism
in several organisms such that each task corresponds to one organism. In this setting, we expect
that the longer the common evolutionary history between two organisms, the more beneficial it is
to share information between the corresponding tasks. In this work, we chose a challenging problem from genome biology to demonstrate that our approach is practically feasible in terms of speed
and accuracy. In ab initio gene finding [17], the task is to build an accurate model of a gene and
subsequently use it to predict the gene content of newly sequenced genomes or to refine existing
annotations. Despite many commonalities between sequence features of genes across organisms,
sequence differences have made it very difficult to build universal gene finders that achieve high
accuracy in cross-organism prediction. This problem is hence ideally suited for the application of
the proposed SO-MTL approach.

2

Methods

Regularization based supervised learning methods, such as the SVM or Logistic Regression play
a central role in many applications. In its most general form, such a method consists of a loss
function L that captures the error with respect to the training data S = {(x1 , y1 ), . . . , (xn , yn )} and
a regularizer R that penalizes model complexity
n
X
J(w) =
L(w, xi , yi ) + R(w).
i=1

In the case of Multitask Learning (MTL), one is interested in obtaining several models w1 , ..., wT
based on T associated sets of examples St = {(x1 , y1 ), . . . , (xnt , ynt )}, t = 1, . . . , T . To couple
individual tasks, an additional regularization term RM T L is introduced that penalizes the disagreement between the individual models (e.g., [1, 8]):
!
nt
T
X
X
J(w1 , ..., wT ) =
L(w, xi , yi ) + R(wt ) + RM T L (w1 , ..., wT ).
t=1

i=1

Special cases include T = 2 and RM T L (w1 , w2 ) = ? ||w1 ? w2 || (e.g., [8, 16]), where ? is a
hyper-parameter controlling the strength of coupling of the solutions for both tasks. For more than
two tasks, the number of coupling terms and hyper-parameters can rise quadratically leading to a
difficult model-selection problem.
2.1 Hierarchical Multitask Learning (HMTL)
We consider the case where tasks correspond to leaves of a tree and are related by its inner nodes. In
[22], the case of taxonomically organized two-class classification tasks was investigated, where each
task corresponds to a species (taxon). The idea was to mimic biological evolution that is assumed to
2

generate more specialized molecular processes with each speciation event from root to leaf. This is
implemented by training on examples available for nodes in the current subtree (i.e., the tasks below
the current node), while similarity to the parent classifier is induced through regularization. Thus,
for each node n, one solves the following optimization problem,
?
?
?
?1

X




2
2
(w?n , b?n ) = argmin
(1 ? ?) ||w|| + ? w ? w?p  + C
` (hx, wi + b, y) , (1)
?
?2
w,b
(x,y)?S

where p is the parent node of n (with the special case of w?p

= 0 for the root node), ` is an appropriate
loss function (e.g., the hinge-loss). The hyper-parameter ? ? [0, 1] determines the contribution of
regularization from the origin vs. the parent node?s parameters (i.e., the strength of coupling between
the node and its parent). The above problem can be equivalently rewritten as:
?
?
?1
?
X



2
(w?n , b?n ) = argmin
||w|| ? ? w, w?p + C
` (hx, wi + b, y) .
(2)
?2
?
w,b
(x,y)?S

For ? = 0, the tasks completely decouple and can be learnt independently. The parameters for
the root node correspond to the globally best model. We will refer to these two cases as base-line
methods for comparisons in the experimental section.
2.2 Structured Output Learning and Extensions for HMTL
In contrast to binary classification, elements from the output space ? (e.g., sequences, trees, or
graphs) of structured output problems have an inherent structure which makes more sophisticated,
problem-specific loss functions desirable. The loss between the true label y ? ? and the predicted
? ? ? is measured by a loss function ? : ? ? ? ? <+ . A widely used approach to predict
label y
? ? ? is the use of a linearly parametrized model given an input vector x ? X and a joint feature
y
map ? : X ? ? ? H that captures the dependencies between input and output (e.g., [21]):
? w (x) = argmax hw, ?(x, y
? )i.
y
? ??
y

The most common approaches to estimate the model parameters w are based on structured output
SVMs (e.g., [2, 21]) and conditional random fields (e.g., [14]; see also [10]). Here we follow
the approach taken in [21, 15], where estimating the parameter vector w amounts to solving the
following optimization problem
(
)
n
X
? )i + ?(y i , y
? ) ? hw, ?(xi , y i )i) ,
min
R(w) + C
`(maxhw, ?(xi , y
(3)
w?H

i=1

? ??
y

where R(w) is a regularizer and ` is a loss function. For `(a) = max(0, a) and R(w) = k w k22 we
obtain the structured output support vector machine [21, 2] with margin rescaling and hinge-loss.
It turns out that we can combine the structured output formulation with hierarchical multitask learning in a straight-forward way. We extend the regularizer R(w) in (3) with a ?-parametrized convex
2
combination of a multitask regularizer 21 ||w ? wp ||2 with the original term. When R(w) = 12 k w k22
and omitting constant terms, we arrive at Rp,? (w) = 21 k w k22 ? ?hw, wp i. Thus we can apply the
described hierarchical multitask learning approach and solve for every node the following optimization problem: (
)
n
X
? )i + ?(y i , y
? ) ? hw, ?(xi , y i )i)
min
Rp,? (w) + C
`(maxhw, ?(xi , y
(4)
w?H

i=1

? ??
y

A major difficulty remains: solving the resulting optimization problems which now can become
considerably larger than for the single-task case.
2.3 A Bundle Method for Efficient Optimization
A common approach to obtain a solution to (3) is to use so-called cutting-plane or column-generation
methods. Here one considers growing subsets of all possible structures and solves restricted optimization problems. An algorithm implementing a variant of this strategy based on primal optimization is given in the appendix (similar in [21]). Cutting-plane and column generation techniques
3

often converge slowly. Moreover, the size of the restricted optimization problems grows steadily
and solving them becomes more expensive in each iteration. Simple gradient descent or second
order methods can not be directly applied as alternatives, because (4) is continuous but non-smooth.
Our approach is instead based on bundle methods for regularized risk minimization as proposed in
[18, 19] and [7]. In case of SVMs, this further relates to the OCAS method introduced in [9]. In
order to achieve fast convergence, we use a variant of these methods adapted to structured output
learning that is suitable for hierarchical multitask learning.
We consider the objective function J(w) = Rp,? (w) + L(w), where
L(w) := C

n
X
i=1

? )i + ?(y i , y
? )} ? hw, ?(xi , y i )i)
`(max {hw, ?(xi , y
? ??
y

and Rp,? (w) is as defined in Section 2.2. Direct optimization of J is very expensive as computing L
involves computing the maximum over the output space. Hence, we propose to optimize an estimate
? (w), which can be computed efficiently. We define the estimated empirical
of the empirical loss L
? (w) as
loss L


N
X
?
L(w)
:= C
`
max {hw, ?i + ?} ? hw, ?(xi , y i )i .
(?,?)??i

i=1

?
?
Accordingly, we define the estimated objective function as J(w)
= Rp,? (w) + L(w).
It is easy to
?
verify that J(w) ? J(w). ?i is a set of pairs (?(xi , y), ?(y i , y)) defined by a suitably chosen,
?
growing subset of ?, such that L(w)
? L(w) (cf. Algorithm 1).
In general, bundle methods are extensions of cutting plane methods that use a prox-function to stabilize the solution of the approximated function. In the framework of regularized risk minimization,
?
a natural prox-function is given by the regularizer. We apply this approach to the objective J(w)
and solve
min Rp,? (w) + max{hai , wi + bi }
(5)
w

i?I

? As proposed in [7, 19], we use a set I
where the set of cutting planes ai , bi lower bound L.
of limited size. Moreover, we calculate an aggregation cutting plane a
?, ?b that lower bounds the
?
estimated empirical loss L. To be able to solve the primal optimization problem in (5) in the dual
space as proposed by [7, 19], we adopt an elegant strategy described in [7] to obtain the aggregated
cutting plane (?
a0 , ?b0 ) using the dual solution ? of (5):
X
X
?b0 =
a
?0 =
?j ai
and
? i bi .
(6)
i?I

i?I

The following two formulations reach the same minimum when optimized with respect to w:


a0 , wi + ?b0 }.
min Rp (w) + maxhai , wi + bi = min {Rp (w) + h?
w?H

i?I

w?H

This new aggregated plane can be used as an additional cutting plane in the next iteration step.
We therefore have a monotonically increasing lower bound on the estimated empirical loss and can
remove previously generated cutting planes without compromising convergence (see [7] for details).
The algorithm is able to handle any (non-)smooth convex loss function `, since only the subgradient
needs to be computed. This can be done efficiently for the hinge-loss, squared hinge-loss, Huberloss, and logistic-loss.
The resulting optimization algorithm is outlined in Algorithm 1. There are several improvements
possible: For instance, one can bypass updating the empirical risk estimates in line 6, when
? (k) ) ? . Finally, while Algorithm 1 was formulated in primal space, it is easy
L(w(k) ) ? L(w
to reformulate in dual variables making it independent of the dimensionality of w ? H.
2.4 Taxonomically Constrained Model Selection
Model selection for multitask learning is particularly difficult, as it requires hyper-parameter selection for several different, but related tasks in a dependent manner. For the described approach, each
4

Algorithm 1 Bundle Methods for Structured Output Algorithm
S ? 1: maximal size of the bundle set
? > 0: linesearch trade-off (cf. [9] for details)
w(1) = wp
k = 1 and a
? = 0, ?b = 0, ?i = ? ?i
repeat
for i = 1, .., n do
y ? = argmaxy?? {hw(k) , ?(xi , y)i + ?(y i , y)}




8:
if ` max {hw, ?(xi , y)i + ?(y i , y)} > `
max hw, ?i + ? then

1:
2:
3:
4:
5:
6:
7:

y??

9:
10:
11:
12:
13:

w? = argmin
w?H

14:
15:
16:
17:
18:
19:

(?,?)??i

?i = ?i ? (?(xi , y ? ), ?(y i , y ? ))
end if
? (k) )
Compute ak ? ?w L(w
?
Compute bk = 
L(w(k) ) ? hw(k) , a
k i
Rp,? (w) + max

max
(k?S)+ <i?k


{hai , wi + bi }, h?
a, wi + ?b

Update a
?, ?b according to (6)
? ? +?(w? ? w(k) ))
? ? = argmin??< J(w
(k+1)
w
= (1 ? ?) w? +?? ? (w? ? w(k) )
k =k+1
end for
? (k) ) ?  and J(w(k) ) ? Jk (w(k) ) ? 
until L(w(k) ) ? L(w

node n in the given taxonomy corresponds to solving an optimization problem that is subject to
hyper-parameters ?n and Cn (except for the root node, where only Cn is relevant). Hence, the direct
optimization of all combinations of dependent hyper-parameters in model selection is not feasible
in many cases. Therefore, we propose to perform a local model selection and optimize the current
Cn and ?n at each node n from top to bottom independently. This corresponds to using the taxonomy for reducing the parameter search space. To clarify this point, assume a perfect binary tree
for n tasks. The length of the path from root to leaf is log2 (n). The parameters along one path are
dependent, e.g. the values chosen at the root will influence the optimal choice further down the tree.
Given k candidate values for parameter ?n , jointly optimizing all interdependent parameters along
one path corresponds to optimizing over a grid of k log2 (n) in contrast to k ? log2 (n) when using our
proposed local strategy.

3

Results

3.1 Background
To demonstrate the validity of our approach, we applied it to the computational biology problem of
gene finding. Here, the task is to identify genomic regions encoding genes (from which RNAs and/or
proteins are produced). Genomic sequence can be represented by long strings of the four letters A, C,
G, and T (genome sizes range from a few megabases to several gigabases). In prokaryotes (mostly
bacteria and archaea) gene structures are comparably simple (cf. Figure 1A): the protein coding
region starts by a start codon (one out of three specific 3-mers in many prokaryotes) followed by a
number of codon triplets (of three nucleotides each) and is terminated by a stop codon (one out of
five specific 3-mers in many prokaryotes). Genic regions are first transcribed to RNA, subsequently
the contained coding region is translated into a protein. Parts of the RNA that are not translated are
called untranslated region (UTR). Genes are separated from one another by intergenic regions. The
protein coding segment is depleted of stop codons making the computational problem of identifying
coding regions relatively straight forward.
In higher eukaryotes (animals, plants, etc.) however, the coding region can be interrupted by introns, which are removed from the RNA before it is translated into protein. Introns are flanked by
specific sequence signals, so-called splice sites (cf. Figure 1B). The presence of introns substantially
complicates the identification of the transcribed and coding regions. In particular, it is usually insufficient to identify regions depleted of stop codons to determine the encoded protein sequence. To
5

accurately detect the transcribed regions in eukaryotic genomes, it is therefore often necessary to
use additional experimental data (e.g., sequencing of RNA fragments). Here, we consider two key
problems in computational gene finding of (i) predicting (only) the coding regions for prokaryotes
and (ii) predicting the exon-intron structure (but not the coding region) for eukaryotes.
A) Prokaryotic Gene
Intergenic

UTR

Start
Codon

Coding region
N x 3 x {A,C,G,T}

ATG

Stop
Codon

UTR

Intergenic

TAA

B) Eukaryotic Gene
Intergenic

Exon

Intron

Exon

Intron

Exon

Intergenic

N x 3 x {A,C,G,T}
UTR

Coding region

UTR

Figure 1: Panel A shows the structure of
a prokaryotic gene. The protein coding region is flanked by a start and a stop codon
and contains a multiple of three nucleotides.
UTR denotes the untranslated region. Panel
B shows the structure of an eukaryotic gene.
The transcribed region contains introns and
exons. Introns are flanked by splice sites and
are removed from the RNA. The remaining
sequence contains the UTRs and coding region.

The problem of identifying genes can be posed as a label sequence learning task, were one assigns
a label (out of intergenic, transcript start, untranslated region, coding start, coding exon, intron,
coding stop, transcript stop) to each position in the genome. The labels have to follow a grammar
dictated by the biological processes of transcription and translation (see Figure 1) making it suitable
to apply structured output learning techniques to identify genes. Because the biological processes
and cellular machineries which recognize genes have slowly evolved over time, genes of closely
related species tend to exhibit similar sequence characteristics. Therefore these problems are very
well suited for the application of multitask learning: sharing information among species is expected
to lead to more accurate gene predictions compared to approaching the problem for each species in
isolation. Currently, the genomes of many prokaryotic and eukaryotic species are being sequenced,
but often very little is known about the genes encoded, and standard methods are typically used to
infer them without systematically exploiting reliable information on related species.
In the following we will consider two different aspects of the described problem. First, focusing on
eukaryotic gene finding for a single species, we show that the proposed optimization algorithm very
quickly converges to the optimal solution. Second, for the problem of prokaryotic gene finding in
several species, we demonstrate that hierarchical multitask structured output learning significantly
improves gene prediction accuracy. The supplement, data and code can be found on the project
website3 .
3.2 Eukaryotic Gene Finding Based on RNA-Seq
We first consider the problem of detecting exonic, intronic and intergenic regions in a single eukaryotic genome. We use experimental data from RNA sequencing (RNA-seq) which provides evidence
for exonic and intronic regions . For simplicity, we assume that for each position in the genome
we are given numbers on how often this position was experimentally determined to be exonic and
intronic, respectively. Ideally, exons and introns belonging to the same gene should have a constant
number of confirmations, whereas these values may vary greatly between different genes. But in
reality, these measurements are typically incomplete and noisy, so that inference techniques greatly
help to reconstruct complete gene structures.
As any HMM or HMSVM, our method employs a state model defining allowed transitions between
states. It consists of five basic states: intergenic, exonic, intron start (donor splice site), intronic,
and intron end (acceptor splice site). These states are duplicated Q = 5 times to model different
levels of confirmation and the whole model is mirrored for simultaneous predictions of genes from
both strands of the genome (see supplement for details). In total, we have 41 states, each of which is
associated with several parameters scoring features derived from the exon and intron confirmation
and computational splice site predictions (see supplement for details). Overall the model has almost
1000 parameters.
We trained the model using 700 training regions with known exon/intron structures and a total length
of ca. 5.2 million nucleotides (data from the nematode C. elegans). We used the column generationbased algorithm (see Appendix) and the Bundle method-based algorithm (Algorithm 1) and recorded
upper and lower bounds of the objective during run time (cf. Figure 2). Whereas both algorithms
3

http://bioweb.me/so-mtl

6

need a similar amount of computation per iteration (mostly decoding steps), the Bundle-method
showed much faster convergence.
We assessed prediction accuracy in a three-fold cross-validation procedure where individual test
sequences consisted of large genomic regions (of several Mbp) each containing many genes. This
evaluation procedure is expected to yield unbiased estimates that are very similar to whole-genome
predictions. Prediction accuracy was compared to another recently proposed, widely used method
called Cufflinks [20]. We observed that our method detects introns and transcripts more accurately
than Cufflinks in the data set analyzed here (cf. Figure 2).
8

10

7

10

Cufflinks

0.9

Our method

0.8

6

10

0.7

5

10

F?Score

objective value

1.0

4

10

3

10

Bundle Method Upper Bound

1

10

5

10

15

20

25

0.4

Original OP Upper Bound

0.2

Original OP Lower Bound

0.1

Target
0

10

0.5

0.3

Bundle Method Lower Bound

2

10

0.6

iteration

30

35

40

0.0

45

Intron

Transcript

Figure 2: Left panel: Convergence for bundle method-based solver versus column generation (log-scale).
Right panel: Prediction accuracy of our eukaryotic gene finding method in comparison to a state-of-the-art
method, Cufflinks [20]. The F-score (harmonic mean of precision and recall) was assessed based on two
metrics: correctly predicted introns as well as transcripts for which all introns were correct (see label).

3.3

Gene Finding in Multiple Prokaryotic Genomes

In a second series of experiments we evaluated the benefit of applying SO-MTL to prokaryotic gene
prediction.
SO prediction method We modeled prokaryotic genes as a Markov chain on the nucleotide level.
To nonetheless account for the biological fact that genetic information is encoded in triplets, the
model contains a 3-cycle of exon states; details are given in Figure 3.
Start

Intergenic

Stop

Exonic3

Start
Codon

Stop
Codon

Exonic2

Exonic1

Figure 3: Simple state model for prokaryotic gene finding.
A suitable model for prokaryotic gene prediction needs to
consider 1) that a gene starts with a start codon (i.e. a certain
triplet of nucleotides) 2) ends with a stop codon and 3) has
a length divisible by 3. Properties 1) and 2) are enforced by
allowing only transitions into and out of the exonic states on
start and stop codons, respectively. Property 3) is enforced
by only allowing transitions from exon state Exonic3 to the
stop codon state.

Data generation We selected a subset of organisms with publicly available genomes to broadly
cover the spectrum of prokaryotic organisms. In order to show that MTL is beneficial even for
relatively distant species, we selected representatives from two different domains: bacteria and archaea. The relationship between these organisms is captured by the taxonomy shown in Figure 4,
which was created based on the information available on the NCBI website4 . For each organism,
we generated one training example per annotated gene. The genomic sequences were cut between
neighboring genes (splitting intergenic regions equally), such that a minimum distance of 6 nucleotides between genes was maintained. Features for SO learning were derived from the nucleotide
sequence by transcoding it to a numerical representation of triplets. This resulted in binary vectors
of size 43 = 64 with exactly one non-zero entry. We sub-sampled from the complete dataset of Ni
examples for each organism i and created new datasets with 20 training examples, 40 evaluation
examples and 200 test examples.
4

ftp://ftp.ncbi.nlm.nih.gov/genomes/Bacteria/

7

Figure 4: Species and their taxonomic hierarchy used for prokaryotic gene finding.

Experimental setup For model selection we used a grid over the following two parameter ranges
C = [100, 250], ? = [0, 0.025, 0.1, 0.25, 0.5, 0.75, 0.9, 1.0] for each node in the taxonomy (cf. Figure 4). Sub-sampling of the dataset was performed 3 times and results were subsequently averaged.
We compared our MTL algorithm to two baseline methods, one where predictors for all tasks where
trained without information transfer (independent) and the other extreme case, where one global
model was fitted for all tasks based on the union of all data sets (union). Performance was measured by the F-score, the harmonic mean of precision and recall, where precision and recall were
determined on nucleotide level (e.g. whether or not an exonic nucleotide was correctly predicted) in
single-gene regions. (Note that due to its per-nucleotide Markov restriction, however, our method is
not able to exploit that there is only one gene per examples sequence.)
Results Figure 5 shows the results for our proposed MTL method and the two baseline methods
described above (see Appendix for table). We observe that it generally pays off to combine information from different organisms, as union always performs better than independent. Indeed MTL
improves over the naive combination method union with F-score increases of up to 4.05 percentage
points in A. tumefaciens. On average, we observe an improvement of 13.99 percentage points for
MTL over independent and 1.13 percentage points for MTL over union, confirming the value of
MTL in transferring information across tasks. In addition, the new bundle method converges at least
twice as fast as the originally proposed cutting plane method.
1.0

F?Score

0.9

0.8

Independent

0.7

Union
MTL

0.6

E.

c

i
ol

u

g
er

f
E.

i
ni
so
A.

ie

ac

ef

m
tu

ri
ylo
.p

ns
H

B.

cis
ra
th

an

B.

s

s
tili
ub

.s

M

it
m

i
hi

ic

us

i
S.

nd
sla

n
ea

m

Figure 5: Evaluation of MTL and baseline methods independent and union.

4

Discussion

We have introduced a regularization-based approach to SO learning in the setting of hierarchical
task relations and have empirically shown its validity on an application from computational biology. To cope with the increased problem size usually encountered in the MTL setting, we have
developed an efficient solver based on bundle-methods and demonstrated its improved convergence
behavior compared to column generation techniques. Applying our SO-MTL algorithm to the problem of prokaryotic gene finding, we could show that sharing information across tasks indeed results
in improved accuracy over learning tasks in isolation. Additionally, the taxonomy, which relates
individual tasks to each other, proved useful in that it led to more accurate predictions than were
obtained when simply training on all examples together. We have previously shown that MTL algorithms excel in a scenarios where there is limited training data relative to the complexity of the
problem and model [23]. As this experiment was carried out on a relatively small data set, more
work is required to turn our approach into a state-of-the-art prokaryotic gene finder.
8

Acknowledgments
We would like to thank the anonymous reviewers for insightful comments. Moreover, we are grateful
to Jonas Behr, Jose Leiva, Yasemin Altun and Klaus-Robert M?uller. This work was supported by
the German Research Foundation (DFG) under the grant RA 1894/1-1.

References
[1] A. Agarwal, S. Gerber, and H. Daum?e III. Learning multiple tasks using manifold regularization. In
Advances in Neural Information Processing Systems 23, 2010.
[2] Y. Altun, I. Tsochantaridis, and T. Hofmann. Hidden markov support vector machines. In Proc. ICML,
2003.
[3] S. Ben-David and R. Schuller. Exploiting task relatedness for multiple task learning. Lecture notes in
computer science, pages 567?580, 2003.
[4] J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. Wortman. Learning bounds for domain adaptation.
Advances in Neural Information Processing Systems, 20, 2007.
[5] R. Caruana. Multitask learning. Machine Learning, 28(1):41?75, 1997.
[6] H. Daum?e III. Bayesian multitask learning with latent hierarchies. In Proceedings of the Twenty-Fifth
Conference on Uncertainty in Artificial Intelligence, 2009.
[7] T.-M.-T. Do. Regularized Bundle Methods for Large-scale Learning Problems with an Application to
Large Margin Training of Hidden Markov Models. PhD thesis, l?Universit?e Pierre & Marie Curie, 2010.
[8] T. Evgeniou, C. Micchelli, and M. Pontil. Learning multiple tasks with kernel methods. Journal of
Machine Learning Research, 6:615?637, 2005.
[9] V. Franc and S. Sonnenburg. OCAS optimized cutting plane algorithm for support vector machines. In
Proc. ICML, 2008.
[10] T. Hazan and R. Urtasun. A primal-dual message-passing algorithm for approximated large scale structured prediction. In Advances in Neural Information Processing Systems 23, 2010.
[11] L. Jacob, F. Bach, and J. Vert. Clustered multi-task learning: A convex formulation. Arxiv preprint
arXiv:0809.2085, 2008.
[12] L. Jacob and J. Vert. Efficient peptide-MHC-I binding prediction for alleles with few known binders.
Bioinformatics, 24(3):358?66, 2008.
[13] S. Kim and E. P. Xing. Tree-guided group lasso for multi-task regression with structured sparsity. Proc.
ICML, 2010.
[14] J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: Probabilistic models for segmenting
and labeling sequence data. In Proc. ICML, 2001.
[15] G. R?atsch and S. Sonnenburg. Large scale hidden semi-markov SVMs. In Advances in Neural Information
Processing Systems 18, 2006.
[16] G. Schweikert, C. Widmer, B. Sch?olkopf, and G. R?atsch. An Empirical Analysis of Domain Adaptation
Algorithms for Genomic Sequence Analysis. In Advances in Neural Information Processing Systems 21,
2009.
[17] G. Schweikert, A. Zien, G. Zeller, J. Behr, C. Dieterich, C. Ong, P. Philips, F. De Bona, L. Hartmann,
A. Bohlen, N. Kr?uger, S. Sonnenburg, and G. R?atsch. mGene: accurate SVM-based gene finding with an
application to nematode genomes. Genome Research, 19(11):2133?43, 2009.
[18] A. Smola, S. Vishwanathan, and Q. Le. Bundle methods for machine learning. In Advances in Neural
Information Processing Systems 20, 2008.
[19] C. Teo, S. Vishwanathan, A.Smola, and Q. Le. Bundle methods for regularized risk minimization. Journal
of Machine Learning Research, 11:311?365, 2010.
[20] C. Trapnell, B. A. Williams, G. Pertea, A. Mortazavi, G. Kwan, M. J. van Baren, S. L. Salzberg, B. J.
Wold, and L. Pachter. Transcript assembly and quantification by RNA-seq reveals unannotated transcripts
and isoform switching during cell differentiation. Nature Biotechnology, 28:511?515, 2010.
[21] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and
interdependent output variables. Journal of Machine Learning Research, 6:1453?1484, 2005.
[22] C. Widmer, J. Leiva, Y. Altun, and G. R?atsch. Leveraging Sequence Classification by Taxonomy-based
Multitask Learning. In Research in Computational Molecular Biology, 2010.
[23] C. Widmer, N. Toussaint, Y. Altun, and G. R?atsch. Inferring latent task structure for Multitask Learning
by Multiple Kernel Learning. BMC Bioinformatics, 11(Suppl 8):S5, 2010.

9

"
2007,Bayesian Agglomerative Clustering with Coalescents,,3266-bayesian-agglomerative-clustering-with-coalescents.pdf,Abstract Missing,"Bayesian Agglomerative Clustering with Coalescents

Yee Whye Teh
Gatsby Unit
University College London

Hal Daum?e III
School of Computing
University of Utah

Daniel Roy
CSAIL
MIT

ywteh@gatsby.ucl.ac.uk

me@hal3.name

droy@mit.edu

Abstract
We introduce a new Bayesian model for hierarchical clustering based on a prior
over trees called Kingman?s coalescent. We develop novel greedy and sequential
Monte Carlo inferences which operate in a bottom-up agglomerative fashion. We
show experimentally the superiority of our algorithms over the state-of-the-art,
and demonstrate our approach in document clustering and phylolinguistics.

1

Introduction

Hierarchically structured data abound across a wide variety of domains. It is thus not surprising that
hierarchical clustering is a traditional mainstay of machine learning [1]. The dominant approach to
hierarchical clustering is agglomerative: start with one cluster per datum, and greedily merge pairs
until a single cluster remains. Such algorithms are efficient and easy to implement. Their primary
limitations?a lack of predictive semantics and a coherent mechanism to deal with missing data?
can be addressed by probabilistic models that handle partially observed data, quantify goodness-offit, predict on new data, and integrate within more complex models, all in a principled fashion.
Currently there are two main approaches to probabilistic models for hierarchical clustering. The
first takes a direct Bayesian approach by defining a prior over trees followed by a distribution over
data points conditioned on a tree [2, 3, 4, 5]. MCMC sampling is then used to obtain trees from
their posterior distribution given observations. This approach has the advantages and disadvantages
of most Bayesian models: averaging over sampled trees can improve predictive capabilities, give
confidence estimates for conclusions drawn from the hierarchy, and share statistical strength across
the model; but it is also computationally demanding and complex to implement. As a result such
models have not found widespread use. [2] has the additional advantage that the distribution induced
on the data points is exchangeable, so the model can be coherently extended to new data. The
second approach uses a flat mixture model as the underlying probabilistic model and structures the
posterior hierarchically [6, 7]. This approach uses an agglomerative procedure to find the tree giving
the best posterior approximation, mirroring traditional agglomerative clustering techniques closely
and giving efficient and easy to implement algorithms. However because the underlying model has
no hierarchical structure, there is no sharing of information across the tree.
We propose a novel class of Bayesian hierarchical clustering models and associated inference algorithms combining the advantages of both probabilistic approaches above. 1) We define a prior and
compute the posterior over trees, thus reaping the benefits of a fully Bayesian approach; 2) the distribution over data is hierarchically structured allowing for sharing of statistical strength; 3) we have
efficient and easy to implement inference algorithms that construct trees agglomeratively; and 4) the
induced distribution over data points is exchangeable. Our model is based on an exchangeable distribution over trees called Kingman?s coalescent [8, 9]. Kingman?s coalescent is a standard model from
population genetics for the genealogy of a set of individuals. It is obtained by tracing the genealogy
backwards in time, noting when lineages coalesce together. We review Kingman?s coalescent in
Section 2. Our own contribution is in using it as a prior over trees in a hierarchical clustering model
(Section 3) and in developing novel inference procedures for this model (Section 4).

(a)

x1

y{1,2}

x2

y{1,2,3,4}

z

x3

y{3,4}
?3
??

t3

?(t) = {{1, 2, 3, 4}}

?2
t2

{{1, 2}, {3, 4}}

x4
?1
t0 = 0

t1
{{1}, {2}, {3, 4}}

{{1}, {2}, {3}, {4}}

(b)

(c)

!"")

#&'

!

#

&"")

$&'

&

$

!&"")

%&'

!!

%

!!"")

!%&'

!%

!$

!%"")

!$&'

!(
!!""#

!!""$

!!""%

!!

!&""'

!&""#

!&""$

!&""%

&

t

!#
!!

!""

!#

!$

%

$

Figure 1: (a) Variables describing the n-coalescent. (b) Sample path from a Brownian diffusion
coalescent process in 1D, circles are coalescent points. (c) Sample observed points from same in
2D, notice the hierarchically clustered nature of the points.

2

Kingman?s coalescent

Kingman?s coalescent is a standard model in population genetics describing the common genealogy
(ancestral tree) of a set of individuals [8, 9]. In its full form it is a distribution over the genealogy of
a countably infinite set of individuals. Like other nonparametric models (e.g. Gaussian and Dirichlet processes), Kingman?s coalescent is most easily described and understood in terms of its finite
dimensional marginal distributions over the genealogies of n individuals, called n-coalescents. We
obtain Kingman?s coalescent as n ? ?.
Consider the genealogy of n individuals alive at the present time t = 0. We can trace their ancestry
backwards in time to the distant past t = ??. Assume each individual has one parent (in genetics,
haploid organisms), and therefore genealogies of [n] = {1, ..., n} form a directed forest. In general,
at time t ? 0, there are m (1 ? m ? n) ancestors alive. Identify these ancestors with their corresponding sets ?1 , ..., ?m of descendants (we will make this identification throughout the paper). Note that
?(t) = {?1 , ..., ?m } form a partition of [n], and interpret t 7? ?(t) as a function from (??, 0] to the
set of partitions of [n]. This function is piecewise constant, left-continuous, monotonic (s ? t implies
that ?(t) is a refinement of ?(s)), and ?(0) = {{1}, ..., {n}} (see Figure 1a). Further, ? completely
and succinctly characterizes the genealogy; we shall henceforth refer to ? as the genealogy of [n].
Kingman?s n-coalescent is simply a distribution over genealogies of [n], or equivalently, over the
space of partition-valued functions like ?. More specifically, the n-coalescent is a continuous-time,
partition-valued, Markov process, which starts at {{1}, ..., {n}} at present time t = 0, and evolves
backwards in time, merging (coalescing) lineages until only one is left. To describe the Markov
process in its entirety, it is sufficient to describe the jump process (i.e. the embedded, discrete-time,
Markov chain over partitions) and the distribution over coalescent times. Both are straightforward
and their simplicity is part of the appeal of Kingman?s coalescent. Let ?li , ?ri be the ith pair of
lineages to coalesce, tn?1 < ? ? ?< t1 < t0 = 0 be the coalescent times and ?i = ti?1 ?ti > 0 be the
duration between adjacent events (see Figure 1a). Under the n-coalescent, every pair of lineages
merges independently with exponential rate 1. Thus the first pair amongst m lineages merge with
 m(m?1)

. Therefore ?i ? Exp n?i+1
independently, the pair ?li , ?ri is chosen from
rate m
2 =
2
2
among those right after time ti , and with probability one a random draw from the n-coalescent is a
binary tree with a single root at t = ?? and the n individuals at time t = 0. The genealogy is:
?
if t = 0;
?{{1}, ..., {n}}
?(t) = ?ti?1 ? ?li ? ?ri + (?li ? ?ri ) if t = ti ;
(1)
?
? ti
if ti+1 < t < ti .
Combining the probabilities of the durations and choices of lineages, the probability of ? is simply:

  n?i+1 Qn?1
 
Qn?1
p(?) = i=1 n?i+1
exp ? n?i+1
?i / 2
= i=1 exp ? n?i+1
?i
(2)
2
2
2
The n-coalescent has some interesting statistical properties [8, 9]. The marginal distribution over
tree topologies is uniform and independent of the coalescent times. Secondly, it is infinitely exchangeable: given a genealogy drawn from an n-coalescent, the genealogy of any m contemporary
individuals alive at time t ? 0 embedded within the genealogy is a draw from the m-coalescent.
Thus, taking n ? ?, there is a distribution over genealogies of a countably infinite population
for which the marginal distribution of the genealogy of any n individuals gives the n-coalescent.
Kingman called this the coalescent.

3

Hierarchical clustering with coalescents

We take a Bayesian approach to hierarchical clustering, placing a coalescent prior on the latent
tree and modeling the observed data with a tree structured Markov process evolving forward in
time. We will alter our terminology from genealogy to tree, from n individuals at present time to n
observed data points, and from individuals on the genealogy to latent variables on the tree-structured
distribution. Let x = {x1 , ..., xn } be n observed data points at the leaves of a tree ? drawn from
the n-coalescent. ? has n ? 1 coalescent points, the ith occuring when ?li and ?ri merge at time ti
to form ?i = ?li ? ?ri . Let tli and tri be the times at which ?li and ?ri are themselves formed.
We use a continuous-time Markov process to define the distribution over the n data points x given
the tree ?. The Markov process starts in the distant past, evolves forward in time, splits at each
coalescent point, and evolves independently down both branches until we reach time 0, when n data
points are observations of the process at the n leaves of the tree. The joint distribution described by
this process respects the conditional independences implied by the structure of the directed tree ?.
Let y?i be a latent variable that takes on the value of the Markov process at ?i just before it splitsLet
y{i} = xi at leaf i. See Figure 1a.
To complete the description of the likelihood model, let q(z) be the initial distribution of the Markov
process at time t = ??, and kst (x, y) be the transition probability from state x at time s to state y
at time t. This Markov process need be neither stationary nor ergodic. Marginalizing over paths of
the Markov process, the joint probability over the latent variables and the observations is:
Qn?1
p(x, y, z|?) = q(z)k?? tn?1 (z, y?n?1 ) i=1 kti tli (y?i , y?li )kti tri (y?i , y?ri )
(3)
Notice that the marginal distributions for each observation p(xi |?) are identical and given by the
Markov process at time 0. However the observations are not independent as they share the same
sample path down the Markov process until it splits. In fact the amount of dependence between two
observations is a function of the time at which the observations coalesce. A more recent coalescent
time implies larger dependence. The overall distribution induced on the observations p(x) inherits
the infinite exchangeability of the n-coalescent. We consider in Section 4.3 a brownian diffusion
(Figures 1(b,c)) and a simple independent sites mutation process on multinomial vectors.

4

Agglomerative sequential Monte Carlo and greedy inference

We develop two classes of efficient and easily implementable inference algorithms for our hierarchical clustering model based on sequential Monte Carlo (SMC) and greedy schemes respectively.
In both classes, the latent variables are integrated out, and the trees are constructed in a bottom-up
fashion. The full tree ? can be expressed as a series of n ? 1 coalescent events, ordered backwards
in time. The ith coalescent event involves the merging of the two subtrees with leaves ?li and ?ri
and occurs at a time ?i before the previous coalescent event. Let ?i = {?j , ?lj , ?rj for j ? i} denote
the first i coalescent events. ?n?1 is equivalent to ? and we shall use them interchangeably.
We assume that the form of the Markov process is such that the latent variables {y?i }n?1
i=1 and z can
be efficiently integrated out using an upward pass of belief propagation on the tree. Let M?i (y) be
the message passed from y?i to its parent; M{i} (y) = ?xi (y) is point mass at xi for leaf i. M?i (y)
is proportional to the likelihood of the observations at the leaves below coalescent event i, given that
y?i = y. Belief propagation computes the messages recursively up the tree; for i = 1, ..., n ? 1:
R
Q
M?i (y) = Z??1
(x, ?i ) b=l,r kti tbi (y, yb )M?bi (yb ) dyb
(4)
i
where Z?i (x, ?i ) is a normalization constant.
The choice of Z does not affect the computed
probability of x, but
RR does impact the accuracy and efficiency of our inference algorithms. We found
that Z?i (x, ?i ) =
q(z)k??ti (z, y)M?i (y) dy dz worked well. At the root, we have:
RR
Z?? (x, ?n?1 ) =
q(z)k?? tn?1 (z, y)M?n?1 (y) dy dz
(5)
The marginal probability p(x|?) is now given by the product of normalization constants:
Qn?1
p(x|?) = Z?? (x, ?n?1 ) i=1 Z?i (x, ?i )

(6)

Multiplying in the prior (2) over ?, we get the joint probability for the tree ? and observations x:
 
Qn?1
p(x, ?) = Z?? (x, ?n?1 ) i=1 exp ? n?i+1
?i Z?i (x, ?i )
(7)
2

Our inference algorithms are based upon (7). The sequential Monte Carlo (SMC) algorithms approximate the posterior over the tree ?n?1 using a weighted sum of samples, while the greedy algorithms
construct ?n?1 by maximizing local terms in (7). Both proceeds by iterating over i = 1, ..., n ? 1,
choosing a duration ?i and a pair of subtrees ?li , ?ri to coalesce at each iteration. This choice
is

?i and a
based upon the ith term in (7), interpreted as the product of a local prior exp ? n?i+1
2
local likelihood Z?i (x, ?i ) for choosing ?i , ?li and ?ri given ?i?1 .
4.1

Sequential Monte Carlo algorithms

SMC algorithms approximate the posterior by iteratively constructing a weighted sum of point
s
masses. At iteration i ? 1, particle s consists of ?i?1
= {?js , ?slj , ?srj for j < i}, and has weight
s
s
wi?1 . At iteration i, s is extended by sampling ?i , ?sli and ?sri from a proposal distribution
s
fi (?is , ?sli , ?sri |?i?1
), and the weight is updated by:
 s
s
s
?i Z?i (x, ?is )/fi (?is , ?sli , ?sri |?i?1
)
(8)
wis = wi?1
exp ? n?i+1
2
s
s
After n ? 1 iterations, we obtain
Pa sets of trees ?n?1 and weights wn?1 . The joint distribution
s
is approximated by: p(?, x) ?
s wn?1 ??n?1 (?), while the posterior is approximated with the
weights normalized. An important aspect of SMC is resampling, which places more particles in
high probability regions and prunes particles stuck in low probability regions. We resample as in
Algorithm 5.1 of [10] when the effective sample size ratio as estimated in [11] falls below one half.

SMC-PriorPrior. The simplest proposal distribution is to sample ?is , ?sli and ?sri from the local
s
s
s
n?i+1

prior. ?i is drawn from an exponential with rate
and ?li , ?ri are drawn uniformly from
2
all available pairs. The weight updates (8) reduce to multiplying by Z?i (x, ?is ). This approach is
computationally very efficient, but performs badly with many objects due to the uniform draws over
pairs. SMC-PriorPost. The second approach addresses the suboptimal choice of pairs to coalesce.
We first draw ?is from its local prior, then draw ?sli , ?sri from the local posterior:
P
s
s
s
s
s 0 0
fi (?sli , ?sri |?is , ?i?1
) ? Z?i (x, ?i?1
, ?is , ?sli , ?sri ); wis = wi?1
?0 ,?0 Z?i (x, ?i?1 , ?i , ?l , ?r ) (9)
l

r

This approach is more computationally demanding since we need to evaluate the local likelihood of
every pair. It also performs significantly better than SMC-PriorPrior. We have found that it works
reasonably well for small data sets but fails in larger ones for which the local posterior for ?i is highly
peaked. SMC-PostPost. The third approach is to draw all of ?is , ?sli and ?sri from their posterior:
 s
s
s
?i Z?i (x, ?i?1
, ?is , ?sli , ?sri )
fi (?is , ?sli , ?sri |?i?1
) ? exp ? n?i+1
2


R
P
s
s
wis = wi?1
exp ? n?i+1
? 0 Z?i (x, ?i?1
, ? 0 , ?0l , ?0r ) d? 0
(10)
?0 ,?0r
2
l

This approach requires the fewest particles, but is the most computationally expensive due to the
integral for each pair. Fortunately, for the case of Brownian diffusion process described below, these
integrals are tractable and related to generalized inverse Gaussian distributions.
4.2

Greedy algorithms

SMC algorithms are attractive because they can produce an arbitrarily accurate approximation to the
full posterior as the number of samples grow. However in many applications a single good tree is
often sufficient. We describe a few greedy algorithms to construct a good tree.
Greedy-MaxProb: the obvious greedy algorithm is to pick ?i , ?li and ?ri maximizing the ith term
in (7). We do so by computing the optimal ?i for each pair of ?li , ?ri , and then picking the pair
maximizing the ith term at its optimal ?i . Greedy-MinDuration: pick the pair to coalesce whose
optimal duration is minimum. Both algorithmsrequire recomputing the optimal duration for each
pair at each iteration, since the prior rate n?i+1
on the duration varies with the iteration i. The total
2
3
computational cost is thus O(n ). We can avoid this by using the alternative view of the n-coalesent
as a Markov process where each pair of lineages coalesces at rate
 1. Greedy-Rate1: for each pair
?li and ?ri we determine the optimal ?i , replacing the n?i+1
prior rate with 1. We coalesce the
2
pair with most recent time (as in Greedy-MinDuration). This reduces the complexity to O(n2 ). We
found that all three performed similarly, and use Greedy-Rate1 in our experiments as it is faster.

4.3

Examples

Brownian diffusion. Consider the case of continuous data evolving via Brownian diffusion. The
transition kernel kst (y, ?) is a Gaussian centred at y with variance (t ? s)?, where ? is a symmetric
positive definite covariance matrix. Because the joint distribution (3) over x, y and z is Gaussian,
we can express each message M?i (y) as a Gaussian with mean yb?i and covariance ?v?i . The local
likelihood is:
2 
b i |? 21 exp ? 1 ||b
b i = ?(v? +v? +tli +tri ?2ti ) (11)
Z? (x, ?i ) = |2? ?
y? ?b
y? || b ; ?
2

i

li

ri

?i

li

ri

where kxk? = x> ??1 x is the Mahanalobis norm. The optimal duration ?i can also be solved for,

?1 q n?i+1
2
2 ? D ? 1 (v
+D
||b
y
?b
y
||
4
?i = 14 n?i+1
?
?
ri ?
li
2
2
2 ?li +v?ri +tli +tri ?2ti?1 ) (12)
where D is the dimensionality. The message at the newly coalesced point has parameters:
?1

y
b?li
y
b?ri
v?i = (v?li + tli ? ti )?1 + (v?ri + tri ? ti )?1
; yb?i = v? +t
+ v? +t
v?i (13)
ri ?ti
li ?ti
li

ri

Multinomial vectors. Consider a Markov process acting on multinomial vectors with each entry
taking one of K values and evolving independently. Entry d evolves at rate ?d and has equilibrium
distribution vector qd . The transition rate matrix is Qd = ?d (qh>1 K ? Ik ) where 1 K is a vector of
K ones and IK is identity matrix of size K, while the transition probability matrix for entry d in
a time interval of length t is eQd t = e??d t IK + (1 ? e??d t )qd>1 K . Representing the message for
entry d from ?i to its parent as a vector M?di = [M?d1
, ..., M?dK
]> , normalized so that qd ? M?di = 1,
i
i
the local likelihood terms and messages are computed as,

PK
Z?di (x, ?i ) = 1 ? e?h (2ti ?tli ?tri ) 1 ? k=1 qdk M?dk
M?dk
(14)
ri
li
M?di = (1 ? e?d (ti ?tli ) (1 ? M?dli ))(1 ? e?d (ti ?tri ) (1 ? M?dri ))/Z?di (x, ?i )

(15)

Unfortunately the optimal ?i cannot be solved analytically and we use Newton steps to compute it.
4.4

Hyperparameter estimation

We perform hyperparameter estimation by iterating between estimating a tree, and estimating the
hyperparameters. In the Brownian case, we place an inverse Wishart prior on ? and the MAP
? is available in a standard closed form. In the multinomial case, the updates are not
posterior ?
available analytically and are solved iteratively. Further information on hyperparameter estimation,
as well predictive densities and more experiments are available in a longer technical report.

5

Experiments

Synthetic Data Sets. In Figure 2 we compare the various SMC algorithms and Greedy-Rate1 on a
range of synthetic data sets drawn from the Brownian diffusion coalescent process itself (? = ID )
to investigate the effects of various parameters on the efficacy of the algorithms1 . Generally SMCPostPost performed best, followed by SMC-PriorPost, SMC-PriorPrior and Greedy-Rate1. With
increasing D the amount of data given to the algorithms increases and all algorithms do better,
especially Greedy-Rate1. This is because the posterior becomes concentrated and the Greedy-Rate1
approximation corresponds well with the posterior. As n increases, the amount of data increases
as well and all algorithms perform better. However, the posterior space also increases and SMCPriorPrior which simply samples from the prior over genealogies does not improve as much. We
see this effect as well when S is small. As S increases all SMC algorithms improve. Finally, the
algorithms were surprisingly robust when there is mismatch between the generated data sets? ? and
the ? used by the model. We expected all models to perform worse with SMC-PostPost best able to
maintain its performance (though this is possibly due to our experimental setup).
MNIST and SPAMBASE. We compare the performance of Greedy-Rate1 to two other hierarchical
clustering algorithms: average-linkage and Bayesian hierarchical clustering (BHC) [6]. In MNIST,
1
Each panel was generated from independent runs. Data set variance affected all algorithms, varying overall
performance across panels. However, trends in each panel are still valid, as they are based on the same data.

av e rage l og p re d i c ti v e

(a) ?0.6

(b ) ?0.6

(c ) ?0.6

(d ) ?0.6

?0.8

?0.8

?0.8

?0.8

?1

?1

?1

?1

?1.2

?1.2

?1.2

?1.2

?1.4

?1.4

?1.4

?1.4

?1.6

4

6
8
D : d i m e n si on s

?1.6

4

6
n : ob se rvati on s

8

?1.6

0.5

1
?: mu tati on rate

2

?1.6

SMC?PostPost
SMC?PriorPost
SMC?PriorPrior
Greedy?Rate1
10

30
50
S : p arti c l e s

70

Figure 2: Predictive performance of algorithms as we vary (a) the numbers of dimensions D, (b)
observations n, (c) the mutation rate ? (? = ?ID ), and (d) number of samples S. In each panel
other parameters are fixed to their middle values (we used S = 50) in other panels, and we report
log predictive probabilities on one unobserved entry, averaged over 100 runs.

Purity
Subtree
LOO-acc

MNIST
Avg-link
BHC
Coalescent
.363?.004 .392?.006 .412?.006
.581?.005 .579?.005 .610?.005
.755?.005 .763?.005 .773?.005

SPAMBASE
Avg-link
BHC
Coalescent
.616?.007 .711?.010 .689?.008
.607?.011 .549?.015 .661?.012
.846?.010 .832?.010 .861?.008

Table 1: Comparative results. Numbers are averages and standard errors over 50 and 20 repeats.

we use 20 exemplars from each of 10 digits from the MNIST data set, reduced via PCA to 20
dimensions, repeating the experiment 50 times. In SPAMBASE, we use 100 examples of 57 binary
attributes from each of 2 classes, repeating 20 times. We present purity scores [6], subtree scores
(#{interior nodes with all leaves of same class}/(n ? #classes)) and leave-one-out accuracies (all
scores between 0 and 1, higher better). The results are in Table 1; except for purity on SPAMBASE,
ours gives the best performance. Experiments not presented here show that all greedy algorithms
perform about the same and that performance improves with hyperparameter updates.
Phylolinguistics. We apply Greedy-Rate1 to a phylolinguistic problem: language evolution. Unlike previous research [12] which studies only phonological data, we use a full typological database
of 139 binary features over 2150 languages: the World Atlas of Language Structures (WALS) [13].
The data is sparse: about 84% of the entries are unknown. We use the same version of the database
as extracted by [14]. Based on the Indo-European subset of this data for which at most 30 features
are unknown (48 languages total), we recover the coalescent tree shown in Figure 3(a). Each language is shown with its genus, allowing us to observe that it teases apart Germanic and Romance
languages, but makes a few errors with respect to Iranian and Greek.
Next we compare predictive abilities to other algoIndo-European Data
rithms. We take a subset of WALS and tested on
Avg-link BHC Coalescent
5% of withheld entries, restoring these with varPurity
0.510 0.491
0.813
ious techniques: Greedy-Rate1; nearest neighbors
Subtree
0.414 0.414
0.690
LOO-acc
0.538 0.590
0.769
(use value from nearest observed neighbor); averageWhole World Data
linkage (nearest neighbor in the tree); and probabilistic
Avg-link BHC Coalescent
PCA (latent dimensions in 5, 10, 20, 40, chosen optiPurity
0.162 0.160
0.269
mistically). We use five subsets of the WALS database,
Subtree
0.227 0.099
0.177
obtained by sorting both the languages and features of
LOO-acc
0.080 0.248
0.369
the database according to sparsity and using a varying
percentage (10% ? 50%) of the densest portion. The Table 2: Comparative performance of varresults are in Figure 3(b). Our approach performed ious algorithms on phylolinguistics data.
reasonably well.
Finally, we compare the trees generated by Greedy-Rate1 with trees generated by either averagelinkage or BHC, using the same evaluation criteria as for MNIST and SPAMBASE, using language
genus as classes. The results are in Table 5, where we can see that the coalescent significantly
outperforms the other methods.

[Celtic] Irish
[Celtic] Gaelic (Scots)
[Celtic] Welsh
[Celtic] Cornish
[Celtic] Breton
[Iranian] Tajik
[Iranian] Persian
[Iranian] Kurdish (Central)
[Romance] French
[Germanic] German
[Germanic] Dutch
[Germanic] English
[Germanic] Icelandic
[Germanic] Swedish
[Germanic] Norwegian
[Germanic] Danish
[Romance] Spanish
[Greek] Greek (Modern)
[Slavic] Bulgarian
[Romance] Romanian
[Romance] Portuguese
[Romance] Italian
[Romance] Catalan
[Albanian] Albanian
[Slavic] Polish
[Slavic] Slovene
[Slavic] Serbian?Croatian
[Slavic] Ukrainian
[Slavic] Russian
[Baltic] Lithuanian
[Baltic] Latvian
[Slavic] Czech
[Iranian] Pashto
[Indic] Panjabi
[Indic] Hindi
[Indic] Kashmiri
[Indic] Sinhala
[Indic] Nepali
[Iranian] Ossetic
[Indic] Maithili
[Indic] Marathi
[Indic] Bengali
[Armenian] Armenian (Western)
[Armenian] Armenian (Eastern)
0.2

0.1

0

(a) Coalescent for a subset of Indo-European languages from WALS.

82

Coalescent
Neighbor
Agglomerative
PPCA

80
78
76
74
72
0.1

0.2

0.3

0.4

0.5

(b) Data restoration on WALS. Y-axis is accuracy;
X-axis is percentage of data set used in experiments.
At 10%, there are N = 215 languages, H = 14
features and p = 94% observed data; at 20%, N =
430, H = 28 and p = 80%; at 30%: N = 645,
H = 42 and p = 66%; at 40%: N = 860, H =
56 and p = 53%; at 50%: N = 1075, H = 70
and p = 43%. Results are averaged over five folds
with a different 5% hidden each time. (We also tried
a ?mode? prediction, but its performance is in the
60% range in all cases, and is not depicted.)

Figure 3: Results of the phylolinguistics experiments.
LLR (t) Top Words
Top Authors (# papers)
32.7 (-2.71) bifurcation attractors hopfield network saddle Mjolsness (9) Saad (9) Ruppin (8) Coolen (7)
0.106 (-3.77) voltage model cells neurons neuron
Koch (30) Sejnowski (22) Bower (11) Dayan (10)
83.8 (-2.02) chip circuit voltage vlsi transistor
Koch (12) Alspector (6) Lazzaro (6) Murray (6)
140.0 (-2.43) spike ocular cells firing stimulus
Sejnowski (22) Koch (18) Bower (11) Dayan (10)
2.48 (-3.66) data model learning algorithm training
Jordan (17) Hinton (16) Williams (14) Tresp (13)
31.3 (-2.76) infomax image ica images kurtosis
Hinton (12) Sejnowski (10) Amari (7) Zemel (7)
31.6 (-2.83) data training regression learning model
Jordan (16) Tresp (13) Smola (11) Moody (10)
39.5 (-2.46) critic policy reinforcement agent controller Singh (15) Barto (10) Sutton (8) Sanger (7)
23.0 (-3.03) network training units hidden input
Mozer (14) Lippmann (11) Giles (10) Bengio (9)

Table 3: Nine clusters discovered in NIPS abstracts data.
NIPS. We applied Greedy-Rate1 to all NIPS abstracts through NIPS12 (1740, total). The data was
preprocessed so that only words occuring in at least 100 abstracts were retained. The word counts
were then converted to binary. We performed one iteration of hyperparameter re-estimation. In
the supplemental material, we depict the top levels of the coalescent tree. Here, we use the tree to
generate a flat clustering. To do so, we use the log likelihood ratio at each branch in the coalescent
to determine if a split should occur. If the log likelihood ratio is greater than zero, we break the
branch; otherwise, we recurse down. On the NIPS abstracts, this leads to nine clusters, depicted
in Table 3. Note that clusters two and three are quite similar?had we used a slighly higher log
likelihood ratio, they would have been merged (the LLR for cluster 2 was only 0.105). Note that
the clustering is able to tease apart Bayesian learning (cluster 5) and non-bayesian learning (cluster
7)?both of which have Mike Jordan as their top author!

6

Discussion

We described a new model for Bayesian agglomerative clustering. We used Kingman?s coalescent
as our prior over trees, and derived efficient and easily implementable greedy and SMC inference
algorithms for the model. We showed empirically that our model gives better performance than other

agglomerative clustering algorithms, and gives good results on applications to document modeling
and phylolinguistics.
Our model is most similar in spirit to the Dirichlet diffusion tree of [2]. Both use infinitely exchangeable priors over trees. While [2] uses a fragmentation process for trees, our prior uses the
reverse?a coalescent process instead. This allows us to develop simpler inference algorithms than
those in [2] (we have not compared our model against the Dirichlet diffusion tree due to the complexity of implementing it). It will be interesting to consider the possibility of developing similar
agglomerative style algorithms for [2]. [3] also describes a hierarchical clustering model involving
a prior over trees, but his prior is not infinitely exchangeable. [5] uses tree-consistent partitions to
model relational data; it would be interesting to apply our approach to their setting. Another related
work is the Bayesian hierarchical clustering of [6], which uses an agglomerative procedure returning
a tree structured approximate posterior for a Dirichlet process mixture model. As opposed to our
work [6] uses a flat mixture model and does not have a notion of distributions over trees.
There are a number of unresolved issues with our work. Firstly, our algorithms take O(n3 ) computation time, except for Greedy-Rate1 which takes O(n2 ) time. Among the greedy algorithms we see
that there are no discernible differences in quality of approximation thus we recommend GreedyRate1. It would be interesting to develop SMC algorithms with O(n2 ) runtime, and compare these
against Greedy-Rate1 on real world problems. Secondly, there are unanswered statistical questions.
For example, since our prior is infinitely exchangeable, by de Finetti?s theorem there is an underlying random distribution for which our observations are i.i.d. draws. What is this underlying random
distribution, and how do samples from this distribution look like? We know the answer for at least a
simple case: if the Markov process is a mutation process with mutation rate ?/2 and new states are
drawn i.i.d. from a base distribution H, then the induced distribution is a Dirichlet process DP(?, H)
[8]. Another issue is that of consistency?does the posterior over random distributions converge to
the true distribution as the number of observations grows? Finally, it would be interesting to generalize our approach to varying mutation rates, and to non-binary trees by using generalizations to
Kingman?s coalescent called ?-coalescents [15].
References
[1] R. O. Duda and P. E. Hart. Pattern Classification And Scene Analysis. Wiley and Sons, New York, 1973.
[2] R. M. Neal. Defining priors for distributions using Dirichlet diffusion trees. Technical Report 0104,
Department of Statistics, University of Toronto, 2001.
[3] C. K. I. Williams. A MCMC approach to hierarchical mixture modelling. In Advances in Neural Information Processing Systems, volume 12, 2000.
[4] C. Kemp, T. L. Griffiths, S. Stromsten, and J. B. Tenenbaum. Semi-supervised learning with trees. In
Advances in Neural Information Processing Systems, volume 16, 2004.
[5] D. M. Roy, C. Kemp, V. Mansinghka, and J. B. Tenenbaum. Learning annotated hierarchies from relational data. In Advances in Neural Information Processing Systems, volume 19, 2007.
[6] K. A. Heller and Z. Ghahramani. Bayesian hierarchical clustering. In Proceedings of the International
Conference on Machine Learning, volume 22, 2005.
[7] N. Friedman. Pcluster: Probabilistic agglomerative clustering of gene expression profiles. Technical
Report Technical Report 2003-80, Hebrew University, 2003.
[8] J. F. C. Kingman. On the genealogy of large populations. Journal of Applied Probability, 19:27?43, 1982.
Essays in Statistical Science.
[9] J. F. C. Kingman. The coalescent. Stochastic Processes and their Applications, 13:235?248, 1982.
[10] P. Fearnhead. Sequential Monte Carlo Method in Filter Theory. PhD thesis, Merton College, University
of Oxford, 1998.
[11] R. M. Neal. Annealed importance sampling. Technical Report 9805, Department of Statistics, University
of Toronto, 1998.
[12] A. McMahon and R. McMahon. Language Classification by Numbers. Oxford University Press, 2005.
[13] M. Haspelmath, M. Dryer, D. Gil, and B. Comrie, editors. The World Atlas of Language Structures.
Oxford University Press, 2005.
[14] H. Daum?e III and L. Campbell. A Bayesian model for discovering typological implications. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2007.
[15] J. Pitman. Coalescents with multiple collisions. Annals of Probability, 27:1870?1902, 1999.

"
1989,Adjoint Operator Algorithms for Faster Learning in Dynamical Neural Networks,,262-adjoint-operator-algorithms-for-faster-learning-in-dynamical-neural-networks.pdf,Abstract Missing,"498

Barben, Toomarian and Gulati

Adjoint Operator Algorithms for Faster
Learning in Dynamical Neural Networks

Nikzad Toomarian

Jacob Barhen

Sandeep Gulati

Center for Space Microelectronics Technology
Jet Propulsion Laboratory
California Institute of Technology
Pasadena, CA 91109

ABSTRACT
A methodology for faster supervised learning in dynamical nonlinear neural networks is presented. It exploits the concept of adjoint
operntors to enable computation of changes in the network's response due to perturbations in all system parameters, using the solution of a single set of appropriately constructed linear equations.
The lower bound on speedup per learning iteration over conventional methods for calculating the neuromorphic energy gradient is
O(N2), where N is the number of neurons in the network.

1

INTRODUCTION

The biggest promise of artifcial neural networks as computational tools lies in the
hope that they will enable fast processing and synthesis of complex information
patterns. In particular, considerable efforts have recently been devoted to the formulation of efficent methodologies for learning (e.g., Rumelhart et al., 1986; Pineda,
1988; Pearlmutter, 1989; Williams and Zipser, 1989; Barhen, Gulati and Zak, 1989).
The development of learning algorithms is generally based upon the minimization
of a neuromorphic energy function. The fundamental requirement of such an approach is the computation of the gradient of this objective function with respect
to the various parameters of the neural architecture, e.g., synaptic weights, neural

Adjoint Operator Algorithms

gains, etc. The paramount contribution to the often excessive cost of learning using dynamical neural networks arises from the necessity to solve, at each learning
iteration, one set of equations for each parameter of the neural system, since those
parameters affect both directly and indirectly the network's energy.
In this paper we show that the concept of adjoint operators, when applied to dynamical neural networks, not only yields a considerable algorithmic speedup, but also
puts on a firm mathematical basis prior results for ""recurrent"" networks, the derivations of which sometimes involved much heuristic reasoning. We have already used
adjoint operators in some of our earlier work in the fields of energy-economy modeling (Alsmiller and Barhen, 1984) and nuclear reactor thermal hydraulics (Barhen
et al., 1982; Toomarian et al., 1987) at the Oak Ridge National Laboratory, where
the concept flourished during the past decade (Oblow, 1977; Cacuci et al., 1980).
In the sequel we first motivate and construct, in the most elementary fashion, a
computational framework based on adjoint operators. We then apply our results
to the Cohen-Grossberg-Hopfield (CGH) additive model, enhanced with terminal
attractor (Barhen, Gulati and Zak, 1989) capabilities. We conclude by presenting
the results of a few typical simulations.

2

ADJOINT OPERATORS

Consider, for the sake of simplicity, that a problem of interest is represented by the
following system of N coupled nonlinear equations

rp( u, p)

o

(2.1)

where rp denotes a nonlinear operator 1 . Let u and p represent the N-vector of
dependent state variables and the M-vector of system parameters, respectively. We
will assume that generally M ? N and that elements of p are, in principle, independent. Furthermore, we will also assume that, for a specific choice of parameters,
a unique solution of Eq. (2.1) exists. Hence, u is an implicit function of p. A
system ""response"", R, represents any result of the calculations that is of interest.
Specifically
(2.2)
R = R(u,p)
i.e., R is a known nonlinear function of p and u and may be calculated from Eq. (2.2)
when the solution u in Eq. (2.1) has been obtained for a given p. The problem of
interest is to compute the ""sensitivities"" of R, i.e., the derivatives of R with respect
to parameters PI"" 1L = 1"""", M. By definition

oR
OPI'

oR au
au OPI'

-+-.-

(2.3)

1 If differential operators appear in Eq. (2.1), then a corresponding set of boundary and/or
initial conditions to specify the domain of cp must also be provided. In general an inhomogeneous
""source"" term can also be present. The learning model discussed in this paper focuses on the
adiabatic approximation only. Nonadiabatic learning algorithms, wherein the response is defined
as a functional, will be discussed in a forthcoming article.

499

500

Barhen, Toomarian and Gulati

Since the response R is known analytically, the computation of oR/oPIS and oR/au
is straightforward. The quantity that needs to be determined is the vector ou/ oPw
Differentiating the state equations (2.1), we obtain a set of equations to be referred
to as ""forward"" sensitivity equations

(2.4)
To simplify the notations, we are omitting the ""transposed"" sign and denoting the
N by N forward sensitivity matrix ocp/ou by A, the N-vector oU/OPIS by I-'ij and
the ""source"" N-vector -ocp/ OPIS by ISS. Thus

(2.5)
Since the source term in Eq. (2.5) explicitly depends on ft, computing dR/dPI-""
requires solving the above system of N algebraic equations for each parameter Pw
This difficulty is circumvented by introd ucing adjoint operators. Let A? denote the
formal adjoint2 of the operator A. The adjoint sensitivity equations can then be
expressed as
IS S-. .
A. I-' ij.
(2.6)
By definition, for algebraic operators

Since Eq. (2.3), can be rewritten as

dR
dpl-'

oR
OPIS

+

oR 1'au q,

(2.8)

s-*

(2.9)

if we identify

oR
au

-

I-'

s.

we observe that the source term for the adjoint equations is independent of the
specific parameter PI-"" Hence, the solution of a single set of adjoint equations will
provide all the information required to compute the gradient of R with respect to all
parameters. To underscore that fact we shall denote I-'ij* as ii. Thus

(2.10)
We will now apply this computational framework to a CGH network enha.nced with
terminal attractor dynamics. The model developed in the sequel differs from our
2 Adjoint operators can only be considered for densely defined linear operators on Banach spaces
(see e.g., Cacuci, 1980). For the neural application under consideration we will limit ourselves to
real Hilbert spaces. Such spaces are self-dual. Furthermore, the domain of an adjoint operator is
detennined by selecting appropriate adjoint boundary conditions l . The associated bilinear form
evaluated on the domain boundary must thus be also generally included.

Adjoint Operator Algorithms

earlier formulations (Barhen, Gulati and Zak, 1989; Barhen, Zak and Gulati, 1989)
in avoiding the use of constraints in the neuromorphic energy function, thereby
eliminating the need for differential equations to evolve the concomitant Lagrange
multipliers. Also, the usual activation dynamics is transformed into a set of equivalent equations which exhibit more ""congenial"" numerical properties, such as ""contraction"" .

3

APPLICATIONS TO NEURAL LEARNING

We formalize a neural network as an adaptive dynamical system whose temporal
evolution is governed by the following set of coupled nonlinear differential equations

2:= Wnm Tnm g-y(zm)

+

(3.1)

kIn

m

where Zn represents the mean soma potential of the nth neuron and Tnm denotes the
synaptic coupling from the m-th to the n-th neuron. The weighting factor Wnm
enforces topological considerations. The constant Kn chara.cterizes the decay of neuron activity. The sigmoidal function g-y(.) modulates the neural response, with gain
tanh(fz). The ""source"" term k In, which includes
given by 1m; typically, g-y(z)
dimensional considerations, encodes contribution in terms of attractor coordinates
of the k-th training sample via the following expression

=

if n E Sx
if n E SH U Sy

(3.2)

The topographic input, output and hidden network partitions Sx, Sy and SH are
architectural requirements related to the encoding of ma.pping-type problems for
which a number of possibilities exist (Barhen, Gulati and Zak, 1989; Barhen, Zak
and Gulati, 1989). In previous articles (ibid; Zak, 1989) we have demonstrated that
in general, for f3 = (2i + 1)-1 and i a strictly positive integer, such attractors have
infinite local stability and provide opportunity for learning in real-time. Typically,
f3 can be set to 1/3. Assuming an adiabatic framework, the fixed point equations
at equilibrium, i.e., as
--+ 0, yield

zn

=

-Kn g-l(k-)
Un
In

~
~

Wnm T.nrn

k -

Urn

+

kI-n

(3.3)

m

=

where Un
g-y(zn) represents the neura.l response. The superscript"""" denotes
quantities evaluated at steady state. Operational network dynamics is then given
by
Un

+

Un

= g-y [ In
Kn

2:= Wnm T,lm
m

Urn

+

In kIn
Kn

1

(3.4)

To proceed formally with the development of a supervised learning algorithm, we
consider an approach based upon the minimization of a constrained ""neuromorphic""
energy function E given by the following expression

E(u,p)

= ~

2:= 2:=
k

n

[ku n

-

kan

]2

V n E Sx U Sy

(3.5)

501

502

Barben, Toomarian and Gulati
We relate adjoint theory to neural learning by identifying the neuromorphic energy
function, E in Eq. (3.5), with the system response R. Also, let p denote the following
system parameters:

The proposed objective function enforces convergence of every neuron in Sx and
Sy to attractor coordinates corresponding to the components in the input-output
training patterns, thereby prompting the network to learn the embedded invariances. Lyapunov stability requires an energy-like function to be monotonically decreasing in time. Since in our model the internal dynamical parameters of interest
are the synaptic strengths Tnm of the interconnection topology, the characteristic
decay constants Kn and the gain parameters In this implies that

E

=

'""""""' '""""""' dE
~ ~ ~
n

m

nm

r..nm + '~
""""""'
n

dE.
dK Kn
n

'""""""' dE.
~ d In
n
In

+

< 0

(3.6)

For each adaptive system parameter, PIA' Lyapunov stability will be satisfied by the
following choice of equations of motion

PIA =

-Tp

dE
dpIA

(3.7)

Examples include

.
dE
Tnm = -TT dTnm

dE
'Y din

,n

dE

-r. -

where the time-scale parameters TT, T,. and T""y > O. Since E depends on PIA
both directly and indirectly, previous methods required solution of a system of N
equations for each parameter PIA to obtain dE/dPIA from du/dPIA. Our methodology
(based on adjoint operators), yields all deri vati ves dE / dplA' V J1. , by solving a
single set of N linear equations.
The nonlinear neural operator for each training pattern k, k
librium is given by

"" l(Jn ("" U,
- P-) = 9 [ - 1 '""""""'
~ Wnm'
Kn m ,

r.""
nm' U-m , + -1
Kn

""1-n

=

1,??? J(, at equi-

1

,n

(3.8)

=

to unity. So, in principle"" Un
where, without loss of generality we have set
""un [T, K, r, ""an,??-j. Using Eqs. (3.8), the forward sensitivity matrix can be
computed and compactly expressed as

{) ""l(Jn
{)

,,-

Um

""A

1
gn Kn

-1
Kn

""Agn

[

Wnm Tnm

Wnm T.nm

-

""- 1

{) In

+ {)""_U m

,,~
fJn unm?

(3.9)

Adjoint Operator Algorithms

where
if n E Sx
ifn E SHUSy
Above,
then

'g. =

k

gn
1-

represents the derivative of 9 with respect to

['g.J 2

'g.

where

Recall that the formal adjoint equation is given as A? v

1 k~

Km

T.

if 9

= tanh,

= s? ; here

k,

mn -

gm Wmn

n, i.e.,

~w.m T. m 'um + 'I. ) 1 (3.11)

g[ :. (

=

ku

(3.10)

TJm Umn

(3.12)

Using Eqs. (2.9) and (3.5), we can compute the formal adjoint source

BE
.ll

v

ifn E Sx USy
if n E SH

k-

Un

(3.13)

The system of adjoint fixed-point equations can then be constructed using Eqs.
(3.12) and (3.13), to yield:
""'""
~
m

1 k~gm

Km

Wmn

T.mn

k-

Vm -

""'"" k
~

,

fJm Umn

k-

Vm

(3.14)

m

Notice that the above coupled system, (3.14), is linear in kv. Furthermore, it
has the same mathematical characteristics as the operational dynamics (3.4). Its
components can be obtained as the equilibrium points, (i.e., Vi --+ 0) of the adjoint
neural dynalnics

1

Km

m

k

~

gm Wmn

T.

mn Vm

(3.15)

As an implementation example, let us conclude by deriving the learning equations
for the synaptic strengths, Tw Recall that

dE

BE
-

dTIJ

BTIJ

+ ""'""
L k-v, IJk S-

p. = (i, j)

(3.16)

k

We differentiate the steady state equations (3.8) with respect to Tij , to obtain the
forward source term,

a k<pn
aIij

-

k~gn
-

[1"""",
;:
n

1 k~,

Kn

""kUI

~ Wnl uin Ujl

I

gn Din Wnj

kUj

(3.17)

503

504

Barben, Toomarian and Gulati

=

Since by definition, fJE / 8Tnm
0 , the explicit energy gradient contribution is
obtained as
~ 1.; II: ~ II: ]
T..nm -- -1""T [Wnm
(3.18)
- - - L.-, Vn 9n Urn

""'n

k

It is straightforward to obtain learning equations for In and ""'n in a similar fashion.

4

ADAPTIVE TIME-SCALES

So far the adaptive learning rates, i.e., Tp in Eq.(3.7), have not been specified. Now
we will show that, by an appropriate selection of these parameters the convergence
of the corresponding dynamical systems can be considerably improved. Without
loss of generality, we shall assume TT
T,.
T-y
T, and we shall seek T in the
form (Barhen et aI, 1989; Zak 1989)

=

=

=

(4.1)
where \7 E denotes the vector with components \7TE, \7 -yE and \7 ,.E. It is straightforward to show that
(4.2)
as \7 E tends to zero, where X is an arbitrary positive constant. If we evaluate the
relaxation time of the energy gradient, we find that

tE =

l

d! \7 E

IVE'-O

!

if f3
if f3

!\7E!I-.6

IVElo

<
>

0
0

(4. 3)

Thus, for f3 ~ 0 the relaxation time is infinite, while for f3 > 0 it is finite. The
dynamical system (3.19) suffers a qualitative change for f3 > 0: it loses uniqueness
of solution. The equilibrium point 1 \7 E 1
0 becomes a singular solution being
intersected by all the transients, and the Lipschitz condition is violated, as one can
see from

=

! !)

( d \7 E

d

d 1 \7 E

1

dt

=

-X

1\7 E 1-.6 _

-00

(4.4)

where 1 \7 E 1 tends to zero, while f3 is strictly positive. Such infinitely stable points
are"" terminal attractors"". By analogy with our previous results we choose f3
2/3,
which yields

=

-1/3
T

(

~~

[\7 T E

]~rn + ~ [\7-yE]~ + ~ [\7 ,.E]~

)

(4.5)

The introduction of these adaptive time-scales dramatically improves the convergence of the corresponding learning dynamical systems.

Adjoint Operator Algorithms

5

SIMULATIONS

The computational framework developed in the preceding section has been applied to a number of problems that involve learning nonlinear mappings, including
Exclusive-OR, the hyperbolic tangent and trignometric functions, e.g., sin. Some of
these mappings (e.g., XOR) have been extensively benchmarked in the literature,
and provide an adequate basis for illustrating the computational efficacy of our proposed formulation. Figures l(a)-I(d) demonstrate the temporal profile of various
network elements during learning of the XOR function. A six neuron feedforward
network was used, that included self-feedback on the output unit and bias. Fig.
l(a) shows the LMS error during the training phase. The worst-case convergence of
the output state neuron to the presented attractor is displayed in Fig. l(b) . Notice
the rapid convergence of the input state due to the terminal attractor effect. The
behavior of the adaptive time-scale parameter T is depicted in Fig. 1(c). Finally,
Fig. l(d) shows the evolution of the energy gradient components.
The test setup for signal processing applications, i.e., learning the sin function and
the tanh sigmoidal nonlinearlity, included a 8-neUl'on fully connected network with
no bias. In each case the network was trained using as little as 4 randomly sampled
training points. Efficacy of recall was determined by presenting 100 random samples. Fig. (2) and (3b) illustrate that we were able to approximate the sin and the
hyperbolic tangent functions using 16 and 4 pairs respectively. Fig. 3(a) demonstrates the network performance when 4 pairs were used to learn the hyperbolic
tangent.
We would like to mention that since our learning methodology involves terminal
at tractors, extreme caution must be exercised when simulating the algorithms in
a digital computing environment. Our discussion on sensitivity of results to the
integration schemes (Barhen, Zak and Gulati, 1989) emphasizes that explicit methods such as Euler or Runge-Kutta shall not be used, since the presence of terminal
at tractors induces extreme stiffness. Practically, this would require an integration
time-step of infinitesimal size, resulting in numerical round-off errors of unacceptable magnitude. Implicit integration techniques such as the Kaps- Rentrop scheme
should therefore be used.

6

CONCLUSIONS

In this paper we have presented a theoretical framework for faster learning in dynamical neural networks. Central to our approach is the concept of adjoint operators
which enables computation of network neuromorphic energy gradients with respect
to all system parameters using the solution of a single set of lineal' equations. If
CF and CA denote the computational costs associated with solving the forward and
adjoint sensitivity equations (Eqs. 2.5 and 2.6), and if M denotes the number of
parameters of interest in the network, the speedup achieved is

505

506

Barhen, Toomarian and Gulati

=

If we assume that C F ~ CA and that M
N 2 + 2N + ... , we see that the lower
bound on speedup per learning iteration is O(N2). Finally, particular care must be
execrcised when integrating the dynamical systems of interest, due to the extreme
stiffness introduced by the terminal attractor constructs.
Acknowledgements

The research described in this paper was performed by the Center for Space Microelectronics Technology, Jet Propulsion Laboratory, California Institute of Technology, and was sponsored by agencies of the U.S. Department of Defense, and
by the Office of Basic Energy Sciences of the U.S. Department of Energy, through
interagency agreements with NASA.
References

R.G. Alsmiller, J. Barhen and J. Horwedel. (1984) ""The Application of Adjoint
Sensitivity Theory to a Liquid Fuels Supply Model"" , Energy, 9(3), 239-253.

J. Barhen, D.G. Cacuci and J.J. Wagschal. (1982) ""Uncertainty Analysis of TimeDependent Nonlinear Systems"", Nucl. Sci. Eng., 81, 23-44.

J. Barhen, S. Gulati and M. Zak. (1989) ""Neural Learning of Constrained Nonlinear
Transformations"", IEEE Computer, 22(6), 67-76.
J. Barhen, M. Zak and S. Gulati. (1989) "" Fast Neural Learning Algorithms Using
Networks with Non-Lipschitzian Dynamics"", in Proc. Neuro-Nimes '89,55-68, EC2,
Nanterre, France.
D.G. Cacuci, C.F. Weber, E.M. Oblow and J.H. Marable. (1980) ""Sensitivity Theory for General Systems of Nonlinear Equations"", Nucl. Sci. Eng., 75, 88-110.
E.M. Oblow. (1977) ""Sensitivity Theory for General Non-Linear Algebraic Equations with Constraints"", ORNL/TM-5815, Oak Ridge National Laboratory.
B.A. Pearlmutter. (1989) ""Learning State Space Trajectories in Recurrent Neural
Networks"", Neural Computation, 1(3), 263-269.
F.J. Pineda. (1988) ""Dynamics and Architecture in Neural Computation"", Journal
of Complexity, 4, 216-245.
D.E. Rumelhart and J .L. Mclelland. (1986) Parallel and Distributed Procesing, MIT
Press, Cambridge, MA.
N. Toomarian, E. Wacholder and S. Kaizerman. (1987) ""Sensitivity Analysis of
Two-Phase Flow Problems"", Nucl. Sci. Eng., 99(1), 53-8l.
R.J. Williams and D. Zipser. (1989) ""A Learning Algorithm for Continually Running Fully Recurrent Neural Networks"", Neural Computation, 1(3), 270-280.
M. Zak. (1989) ""Terminal Attractors"", Neural Networks, 2(4),259-274.

Adjoint Operator Algorithms

(a)

(b)
1.5

4
til

~

:2!

t:r4

P

~

0

~

Q)

a

~ 1'--

Q)

bJI

""8~

~

~

l

iterations

?

,

150

iterations

150

iterations

150

1

20

iterations

(c)

Figure l(a)-(d).

150

(d)

Learning the Exclusive-OR function using a 6-neumn
(including bias) feedforward dynamical nctwork with
sclf-feedback on the output unit.

507

508

Barben, Toomarian and Gulati
1.000,-------------.,..._--_

0 .500

0.000

-0.500

-1.000 t---..:::....~~--t__---t__--__.J
-1.000
-0.500
0 .000
0.500
1.000

Figure 2.

3 (a)

Learning the Sin function using a fully connccted, 8-neunm
network with no bias. The truining set comprised of
4 points that were randomly selected.
1.000 r----------.---:::=;~----.

0.500

0000

-0.500

-1000~~~~~---t__---t__--~

- 1.000

3(b)

-0.500

0.000

0 .500

1.000

1000

0.500

0.000

-0.500

-I.OOG .--""-.-.!~---t__---t__--__.J
- I.oeo
-0 .500
0.000
0.500
1.000

It'igure 3.

Learning the Hyperbolic Tangent function using a fully connected,
8-neunm network with no bias. (a> using 4 randomly selected
training samples; (b> using 16 randomly selected training samples.

"
2016,Cooperative Inverse Reinforcement Learning,Poster,6420-cooperative-inverse-reinforcement-learning.pdf,"For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial- information game with two agents, human and robot; both are rewarded according to the human?s reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.","Cooperative Inverse Reinforcement Learning

Dylan Hadfield-Menell?

Anca Dragan

Pieter Abbeel

Stuart Russell

Electrical Engineering and Computer Science
University of California at Berkeley
Berkeley, CA 94709

Abstract
For an autonomous system to be helpful to humans and to pose no unwarranted
risks, it needs to align its values with those of the humans in its environment in
such a way that its actions contribute to the maximization of value for the humans.
We propose a formal definition of the value alignment problem as cooperative
inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partialinformation game with two agents, human and robot; both are rewarded according
to the human?s reward function, but the robot does not initially know what this
is. In contrast to classical IRL, where the human is assumed to act optimally in
isolation, optimal CIRL solutions produce behaviors such as active teaching, active
learning, and communicative actions that are more effective in achieving value
alignment. We show that computing optimal joint policies in CIRL games can be
reduced to solving a POMDP, prove that optimality in isolation is suboptimal in
CIRL, and derive an approximate CIRL algorithm.

1

Introduction

?If we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere
effectively . . . we had better be quite sure that the purpose put into the machine is the purpose which
we really desire.? So wrote Norbert Wiener (1960) in one of the earliest explanations of the problems
that arise when a powerful autonomous system operates with an incorrect objective. This value
alignment problem is far from trivial. Humans are prone to mis-stating their objectives, which can
lead to unexpected implementations. In the myth of King Midas, the main character learns that
wishing for ?everything he touches to turn to gold? leads to disaster. In a reinforcement learning
context, Russell & Norvig (2010) describe a seemingly reasonable, but incorrect, reward function for
a vacuum robot: if we reward the action of cleaning up dirt, the optimal policy causes the robot to
repeatedly dump and clean up the same dirt.
A solution to the value alignment problem has long-term implications for the future of AI and its
relationship to humanity (Bostrom, 2014) and short-term utility for the design of usable AI systems.
Giving robots the right objectives and enabling them to make the right trade-offs is crucial for
self-driving cars, personal assistants, and human?robot interaction more broadly.
The field of inverse reinforcement learning or IRL (Russell, 1998; Ng & Russell, 2000; Abbeel
& Ng, 2004) is certainly relevant to the value alignment problem. An IRL algorithm infers the
reward function of an agent from observations of the agent?s behavior, which is assumed to be
optimal (or approximately so). One might imagine that IRL provides a simple solution to the value
alignment problem: the robot observes human behavior, learns the human reward function, and
behaves according to that function. This simple idea has two flaws. The first flaw is obvious: we
don?t want the robot to adopt the human reward function as its own. For example, human behavior
(especially in the morning) often conveys a desire for coffee, and the robot can learn this with IRL,
but we don?t want the robot to want coffee! This flaw is easily fixed: we need to formulate the value
?

{dhm, anca, pabbeel, russell}@cs.berkeley.edu

30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

alignment problem so that the robot always has the fixed objective of optimizing reward for the
human, and becomes better able to do so as it learns what the human reward function is.
The second flaw is less obvious, and less easy to fix. IRL assumes that observed behavior is optimal
in the sense that it accomplishes a given task efficiently. This precludes a variety of useful teaching
behaviors. For example, efficiently making a cup of coffee, while the robot is a passive observer, is a
inefficient way to teach a robot to get coffee. Instead, the human should perhaps explain the steps in
coffee preparation and show the robot where the backup coffee supplies are kept and what do if the
coffee pot is left on the heating plate too long, while the robot might ask what the button with the
puffy steam symbol is for and try its hand at coffee making with guidance from the human, even if
the first results are undrinkable. None of these things fit in with the standard IRL framework.
Cooperative inverse reinforcement learning. We propose, therefore, that value alignment should
be formulated as a cooperative and interactive reward maximization process. More precisely, we
define a cooperative inverse reinforcement learning (CIRL) game as a two-player game of partial
information, in which the ?human?, H, knows the reward function (represented by a generalized
parameter ?), while the ?robot?, R, does not; the robot?s payoff is exactly the human?s actual reward.
Optimal solutions to this game maximize human reward; we show that solutions may involve active
instruction by the human and active learning by the robot.
Reduction to POMDP and Sufficient Statistics. As one might expect, the structure of CIRL games
is such that they admit more efficient solution algorithms than are possible for general partialinformation games. Let (? H , ? R ) be a pair of policies for human and robot, each depending, in
general, on the complete history of observations and actions. A policy pair yields an expected sum of
rewards for each player. CIRL games are cooperative, so there is a well-defined optimal policy pair
that maximizes value.2 In Section 3 we reduce the problem of computing an optimal policy pair to the
solution of a (single-agent) POMDP. This shows that the robot?s posterior over ? is a sufficient statistic,
in the sense that there are optimal policy pairs in which the robot?s behavior depends only on this
statistic. Moreover, the complexity of solving the POMDP is exponentially lower than the NEXP-hard
bound that (Bernstein et al., 2000) obtained by reducing a CIRL game to a general Dec-POMDP.
Apprenticeship Learning and Suboptimality of IRL-Like Solutions. In Section 3.3 we model
apprenticeship learning (Abbeel & Ng, 2004) as a two-phase CIRL game. In the first phase, the
learning phase, both H and R can take actions and this lets R learn about ?. In the second phase,
the deployment phase, R uses what it learned to maximize reward (without supervision from H).
We show that classic IRL falls out as the best-response policy for R under the assumption that the
human?s policy is ?demonstration by expert? (DBE), i.e., acting optimally in isolation as if no robot
exists. But we show also that this DBE/IRL policy pair is not, in general, optimal: even if the robot
expects expert behavior, demonstrating expert behavior is not the best way to teach that algorithm.
We give an algorithm that approximately computes H?s best response when R is running IRL under
the assumption that rewards are linear in ? and state features. Section 4 compares this best-response
policy with the DBE policy in an example game and provides empirical confirmation that the bestresponse policy, which turns out to ?teach? R about the value landscape of the problem, is better than
DBE. Thus, designers of apprenticeship learning systems should expect that users will violate the
assumption of expert demonstrations in order to better communicate information about the objective.

2

Related Work

Our proposed model shares aspects with a variety of existing models. We divide the related work into
three categories: inverse reinforcement learning, optimal teaching, and principal?agent models.
Inverse Reinforcement Learning. Ng & Russell (2000) define inverse reinforcement learning
(IRL) as follows: ?Given measurements of an [actor]?s behavior over time. . . . Determine the
reward function being optimized.? The key assumption IRL makes is that the observed behavior is
optimal in the sense that the observed trajectory maximizes the sum of rewards. We call this the
demonstration-by-expert (DBE) assumption. One of our contributions is to prove that this may be
suboptimal behavior in a CIRL game, as H may choose to accept less reward on a particular action
in order to convey more information to R. In CIRL the DBE assumption prescribes a fixed policy
2

A coordination problem of the type described in Boutilier (1999) arises if there are multiple optimal policy
pairs; we defer this issue to future work.

2

Ground Truth

Expert Demonstration

Instructive Demonstration

Figure 1: The difference between demonstration-by-expert and instructive demonstration in the
mobile robot navigation problem from Section 4. Left: The ground truth reward function. Lighter
grid cells indicates areas of higher reward. Middle: The demonstration trajectory generated by the
expert policy, superimposed on the maximum a-posteriori reward function the robot infers. The robot
successfully learns where the maximum reward is, but little else. Right: An instructive demonstration
generated by the algorithm in Section 3.4 superimposed on the maximum a-posteriori reward function
that the robot infers. This demonstration highlights both points of high reward and so the robot learns
a better estimate of the reward.
for H. As a result, many IRL algorithms can be derived as state estimation for a best response to
different ? H , where the state includes the unobserved reward parametrization ?.
Ng & Russell (2000), Abbeel & Ng (2004), and Ratliff et al. (2006) compute constraints that
characterize the set of reward functions so that the observed behavior maximizes reward. In general,
there will be many reward functions consistent with this constraint. They use a max-margin heuristic
to select a single reward function from this set as their estimate. In CIRL, the constraints they compute
characterize R?s belief about ? under the DBE assumption.
Ramachandran & Amir (2007) and Ziebart et al. (2008) consider the case where ? H is ?noisily
expert,? i.e., ? H is a Boltzmann distribution where actions or trajectories are selected in proportion
to the exponent of their value. Ramachandran & Amir (2007) adopt a Bayesian approach and place
an explicit prior on rewards. Ziebart et al. (2008) places a prior on reward functions indirectly by
assuming a uniform prior over trajectories. In our model, these assumptions are variations of DBE
and both implement state estimation for a best response to the appropriate fixed H.
Natarajan et al. (2010) introduce an extension to IRL where R observes multiple actors that cooperate
to maximize a common reward function. This is a different type of cooperation than we consider,
as the reward function is common knowledge and R is a passive observer. Waugh et al. (2011) and
Kuleshov & Schrijvers (2015) consider the problem of inferring payoffs from observed behavior in a
general (i.e., non-cooperative) game given observed behavior. It would be interesting to consider an
analogous extension to CIRL, akin to mechanism design, in which R tries to maximize collective
utility for a group of Hs that may have competing objectives.
Fern et al. (2014) consider a hidden-goal MDP, a special case of a POMDP where the goal is an
unobserved part of the state. This can be considered a special case of CIRL, where ? encodes a
particular goal state. The frameworks share the idea that R helps H. The key difference between the
models lies in the treatment of the human (the agent in their terminology). Fern et al. (2014) model
the human as part of the environment. In contrast, we treat H as an actor in a decision problem that
both actors collectively solve. This is crucial to modeling the human?s incentive to teach.
Optimal Teaching. Because CIRL incentivizes the human to teach, as opposed to maximizing
reward in isolation, our work is related to optimal teaching: finding examples that optimally train
a learner (Balbach & Zeugmann, 2009; Goldman et al., 1993; Goldman & Kearns, 1995). The key
difference is that efficient learning is the objective of optimal teaching, while it emerges as a property
of optimal equilibrium behavior in CIRL.
Cakmak & Lopes (2012) consider an application of optimal teaching where the goal is to teach the
learner the reward function for an MDP. The teacher gets to pick initial states from which an expert
executes the reward-maximizing trajectory. The learner uses IRL to infer the reward function, and
the teacher picks initial states to minimize the learner?s uncertainty. In CIRL, this approach can be
characterized as an approximate algorithm for H that greedily minimizes the entropy of R?s belief.
Beyond teaching, several models focus on taking actions that convey some underlying state, not
necessarily a reward function. Examples include finding a motion that best communicates an agent?s
intention (Dragan & Srinivasa, 2013), or finding a natural language utterance that best communicates
3

a particular grounding (Golland et al., 2010). All of these approaches model the observer?s inference
process and compute actions (motion or speech) that maximize the probability an observer infers the
correct hypothesis or goal. Our approximate solution to CIRL is analogous to these approaches, in
that we compute actions that are informative of the correct reward function.
Principal?agent models. Value alignment problems are not intrinsic to artificial agents. Kerr
(1975) describes a wide variety of misaligned incentives in the aptly titled ?On the folly of rewarding
A, while hoping for B.? In economics, this is known as the principal?agent problem: the principal
(e.g., the employer) specifies incentives so that an agent (e.g., the employee) maximizes the principal?s
profit (Jensen & Meckling, 1976).
Principal?agent models study the problem of generating appropriate incentives in a non-cooperative
setting with asymmetric information. In this setting, misalignment arises because the agents that
economists model are people and intrinsically have their own desires. In AI, misalignment arises
entirely from the information asymmetry between the principal and the agent; if we could characterize
the correct reward function, we could program it into an artificial agent. Gibbons (1998) provides a
useful survey of principal?agent models and their applications.

3

Cooperative Inverse Reinforcement Learning

This section formulates CIRL as a two-player Markov game with identical payoffs, reduces the
problem of computing an optimal policy pair for a CIRL game to solving a POMDP, and characterizes
apprenticeship learning as a subclass of CIRL games.
3.1

CIRL Formulation

Definition 1. A cooperative inverse reinforcement learning (CIRL) game M is a two-player Markov
game with identical payoffs between a human or principal, H, and a robot or agent, R. The
game is described by a tuple, M = hS, {AH , AR }, T (?|?, ?, ?), {?, R(?, ?, ?; ?)}, P0 (?, ?), ?i, with the
following definitions:
S a set of world states: s ? S.
AH a set of actions for H: aH ? AH .
AR a set of actions for R: aR ? AR .
T (?|?, ?, ?) a conditional distribution on the next world state, given previous state and action for
both agents: T (s0 |s, aH , aR ).
? a set of possible static reward parameters, only observed by H: ? ? ?.
R(?, ?, ?; ?) a parameterized reward function that maps world states, joint actions, and reward
parameters to real numbers. R : S ? AH ? AR ? ? ? R.
P0 (?, ?) a distribution over the initial state, represented as tuples: P0 (s0 , ?)
? a discount factor: ? ? [0, 1].
We write the reward for a state?parameter pair as R(s, aH , aR ; ?) to distinguish the static reward
parameters ? from the changing world state s. The game proceeds as follows. First, the initial
state, a tuple (s, ?), is sampled from P0 . H observes ?, but R does not. This observation model
captures the notion that only the human knows the reward function, while both actors know a prior
distribution over possible reward functions. At each timestep t, H and R observe the current state st
R
H
R
and select their actions aH
t , at . Both actors receive reward rt = R(st , at , at ; ?) and observe
each other?s action selection. A state for the next timestep is sampled from the transition distribution,
R
st+1 ? PT (s0 |st , aH
t , at ), and the process repeats.
Behavior in a CIRL game is defined by a pair of policies, (? H , ? R ), that determine action selection
for H and R respectively.
In general,
these policies can
of their observation

?
 be arbitrary functions
?
histories; ? H : AH ? AR ? S ? ? ? AH , ? R : AH ? AR ? S ? AR . The optimal joint
policy is the policy that maximizes value. The value of a state is the expected sum of discounted
rewards under the initial distribution of reward parameters and world states.
Remark 1. A key property of CIRL is that the human and the robot get rewards determined by the
same reward function. This incentivizes the human to teach and the robot to learn without explicitly
encoding these as objectives of the actors.
4

3.2

Structural Results for Computing Optimal Policy Pairs

The analogue in CIRL to computing an optimal policy for an MDP is the problem of computing an
optimal policy pair. This is a pair of policies that maximizes the expected sum of discounted rewards.
This is not the same as ?solving? a CIRL game, as a real world implementation of a CIRL agent must
account for coordination problems and strategic uncertainty (Boutilier, 1999). The optimal policy pair
represents the best H and R can do if they can coordinate perfectly before H observes ?. Computing
an optimal joint policy for a cooperative game is the solution to a decentralized-partially observed
Markov decision process (Dec-POMDP). Unfortunately, Dec-POMDPs are NEXP-complete (Bernstein
et al., 2000) so general Dec-POMDP algorithms have a computational complexity that is doubly
exponential. Fortunately, CIRL games have special structure that reduces this complexity.
Nayyar et al. (2013) shows that a Dec-POMDP can be reduced to a coordination-POMDP. The actor in
this POMDP is a coordinator that observes all common observations and specifies a policy for each
actor. These policies map each actor?s private information to an action. The structure of a CIRL game
implies that the private information is limited to H?s initial observation of ?. This allows the reduction
to a coordination-POMDP to preserve the size of the (hidden) state space, making the problem easier.
Theorem 1. Let M be an arbitrary CIRL game with state space S and reward space ?. There exists
a (single-actor) POMDP MC with (hidden) state space SC such that |SC | = |S| ? |?| and, for any
policy pair in M , there is a policy in MC that achieves the same sum of discounted rewards.
Theorem proofs can be found in the supplementary material. An immediate consequence of this
result is that R?s belief about ? is a sufficient statistic for optimal behavior.
?

?

Corollary 1. Let M be a CIRL game. There exists an optimal policy pair (? H , ? R ) that only
depends on the current state and R?s belief.
Remark 2. In a general Dec-POMDP, the hidden state for the coordinator-POMDP includes each
actor?s history of observations. In CIRL, ? is the only private information so we get an exponential
decrease in the complexity of the reduced problem. This allows one to apply general POMDP
algorithms to compute optimal joint policies in CIRL.
It is important to note that the reduced problem may still be very challenging. POMDPs are difficult
in their own right and the reduced problem still has a much larger action space. That being said,
this reduction is still useful in that it characterizes optimal joint policy computation for CIRL as
significantly easier than Dec-POMDPs. Furthermore, this theorem can be used to justify approximate
methods (e.g., iterated best response) that only depend on R?s belief state.
3.3

Apprenticeship Learning as a Subclass of CIRL Games

A common paradigm for robot learning from humans is apprenticeship learning. In this paradigm,
a human gives demonstrations to a robot of a sample task and the robot is asked to imitate it in a
subsequent task. In what follows, we formulate apprenticeship learning as turn-based CIRL with a
learning phase and a deployment phase. We characterize IRL as the best response (i.e., the policy
that maximizes reward given a fixed policy for the other player) to a demonstration-by-expert policy
for H. We also show that this policy is, in general, not part of an optimal joint policy and so IRL is
generally a suboptimal approach to apprenticeship learning.
Definition 2. (ACIRL) An apprenticeship cooperative inverse reinforcement learning (ACIRL) game
is a turn-based CIRL game with two phases: a learning phase where the human and the robot take
turns acting, and a deployment phase, where the robot acts independently.
Example. Consider an example apprenticeship task where R needs to help H make office supplies.
H and R can make paperclips and staples and the unobserved ? describe H?s preference for paperclips
vs staples. We model the problem as an ACIRL game in which the learning and deployment phase
each consist of an individual action. The world state in this problem is a tuple (ps , qs , t) where ps
and qs respectively represent the number of paperclips and staples H owns. t is the round number.
An action is a tuple (pa , qa ) that produces pa paperclips and qa staples. The human can make 2
items total: AH = {(0, 2), (1, 1), (2, 0)}. The robot has different capabilities. It can make 50
units of each item or it can choose to make 90 of a single item: AR = {(0, 90), (50, 50), (90, 0)}.
We let ? = [0, 1] and define R so that ? indicates the relative preference between paperclips and
staples:R(s, (pa , qa ); ?) = ?pa + (1 ? ?)qa . R?s action is ignored when t = 0 and H?s is ignored
when t = 1. At t = 2, the game is over, so the game transitions to a sink state, (0, 0, 2).
5

Deployment phase ? maximize mean reward estimate. It is simplest to analyze the deployment
phase first. R is the only actor in this phase so it get no more observations of its reward. We have
shown that R?s belief about ? is a sufficient statistic for the optimal policy. This belief about ? induces
a distribution over MDPs. A straightforward extension of a result due to Ramachandran & Amir
(2007) shows that R?s optimal deployment policy maximizes reward for the mean reward function.
Theorem 2. Let M be an ACIRL game. In the deployment phase, the optimal policy for R maximizes
reward in the MDP induced by the mean ? from R?s belief.
In our example, suppose that ? H selects (0, 2) if ? ? [0, 13 ), (1, 1) if ? ? [ 31 , 23 ] and (2, 0) otherwise.
R begins with a uniform prior on ? so observing, e.g., aH = (0, 2) leads to a posterior distribution
that is uniform on [0, 31 ). Theorem 2 shows that the optimal action maximizes reward for the mean ?
so an optimal R behaves as though ? = 61 during the deployment phase.
Learning phase ? expert demonstrations are not optimal. A wide variety of apprenticeship
learning approaches assume that demonstrations are given by an expert. We say that H satisfies the
demonstration-by-expert (DBE) assumption in ACIRL if she greedily maximizes immediate reward
on her turn. This is an ?expert? demonstration because it demonstrates a reward maximizing action
but does not account for that action?s impact on R?s belief. We let ? E represent the DBE policy.
Theorem 2 enables us to characterize the best response for R when ? H = ? E : use IRL to compute
the posterior over ? during the learning phase and then act to maximize reward under the mean ? in
the deployment phase. We can also analyze the DBE assumption itself. In particular, we show that
? E is not H?s best response when ? R is a best response to ? E .
Theorem 3. There exist ACIRL games where the best-response for H to ? R violates the expert
demonstrator assumption. In other words, if br(?) is the best response to ?, then br(br(? E )) 6= ? E .
The supplementary material proves this theorem by computing the optimal equilibrium for our
51
E
example. In that equilibrium, H selects (1, 1) if ? ? [ 41
92 , 92 ]. In contrast, ? only chooses (1, 1) if
? = 0.5. The change arises because there are situations (e.g., ? = 0.49) where the immediate loss of
reward to H is worth the improvement in R?s estimate of ?.
Remark 3. We should expect experienced users of apprenticeship learning systems to present
demonstrations optimized for fast learning rather than demonstrations that maximize reward.
Crucially, the demonstrator is incentivized to deviate from R?s assumptions. This has implications
for the design and analysis of apprenticeship systems in robotics. Inaccurate assumptions about user
behavior are notorious for exposing bugs in software systems (see, e.g., Leveson & Turner (1993)).
3.4

Generating Instructive Demonstrations

Now, we consider the problem of computing H?s best response when R uses IRL as a state estimator.
For our toy example, we computed solutions exhaustively, for realistic problems we need a more
efficient approach. Section 3.2 shows that this can be reduced to an POMDP where the state is a
tuple of world state, reward parameters, and R?s belief. While this is easier than solving a general
D ec- POMDP , it is a computational challenge. If we restrict our attention to the case of linear reward
functions we can develop an efficient algorithm to compute an approximate best response.
Specifically, we consider the case where the reward for a state (s, ?) is defined as a linear combination
of state features for some feature function ? : R(s, aH , aR ; ?) = ?(s)> ?. Standard results from the
IRL literature show that policies with the same expected feature counts have the same value (Abbeel
& Ng, 2004). Combined with Theorem 2, this implies that the optimal ? R under the DBE assumption
computes a policy that matches the observed feature counts from the learning phase.
This suggests a simple approximation scheme. To compute a demonstration trajectory ? H , first
compute the feature counts R would observe in expectation from the true ? and then select actions
that maximize similarity to these target features. If ?? are the expected feature counts induced by ?
then this scheme amounts to the following decision rule:
? H ? argmax ?(? )> ? ? ?||?? ? ?(? )||2 .
(1)
?

This rule selects a trajectory that trades off between the sum of rewards ?(? )> ? and the feature
dissimilarity ||?? ? ?(? )||2 . Note that this is generally distinct from the action selected by the
demonstration-by-expert policy. The goal is to match the expected sum of features under a distribution
of trajectories with the sum of features from a single trajectory. The correct measure of feature
6

num-features = 3
br
?E

9

16

8

3

4
Regret

br
?E

12

6

0

num-features = 10

KL

? 2
||?GT ? ?||

0

Regret for br

1
0.75
Regret

12

0.5

0.25
Regret

KL

? 2
||?GT ? ?||

0

10?3

10?1
?

101

Figure 2: Left, Middle: Comparison of ?expert? demonstration (? E ) with ?instructive? demonstration (br).
Lower numbers are better. Using the best response causes R to infer a better distribution over ? so it does a
better job of maximizing reward. Right: The regret of the instructive demonstration policy as a function of how
optimal R expects H to be. ? = 0 corresponds to a robot that expects purely random behavior and ? = ?
corresponds to a robot that expects optimal behavior. Regret is minimized for an intermediate value of ?: if ? is
too small, then R learns nothing from its observations; if ? is too large, then R expects many values of ? to lead
to the same trajectory so H has no way to differentiate those reward functions.

similarity is regret: the difference between the reward R would collect if it knew the true ? and the
reward R actually collects using the inferred ?. Computing this similarity is expensive, so we use an
`2 norm as a proxy measure of similarity.

4
4.1

Experiments
Cooperative Learning for Mobile Robot Navigation

Our experimental domain is a 2D navigation problem on a discrete grid. In the learning phase of
the game, H teleoperates a trajectory while R observes. In the deployment phase, R is placed in a
random state and given control of the robot. We use a finite horizon H, and let the first H
2 timesteps
be the learning phase. There are N? state features defined as radial basis functions where the centers
are common knowledge. Rewards are linear in these features and ?. The initial world state is in the
middle of the map. We use a uniform distribution on [?1, 1]N? for the prior on ?. Actions move in
one of the four cardinal directions {N, S, E, W } and there is an additional no-op ? that each actor
executes deterministically on the other agent?s turn.
Figure 1 shows an example comparison between demonstration-by-expert and the approximate best
response policy in Section 3.4. The leftmost image is the ground truth reward function. Next to
it are demonstration trajectories produce by these two policies. Each path is superimposed on the
maximum a-posteriori reward function the robot infers from the demonstration. We can see that the
demonstration-by-expert policy immediately goes to the highest reward and stays there. In contrast,
the best response policy moves to both areas of high reward. The robot reward function the robot
infers from the best response demonstration is much more representative of the true reward function,
when compared with the reward function it infers from demonstration-by-expert.
4.2

Demonstration-by-Expert vs Best Responder

Hypothesis. When R plays an IRL algorithm that matches features, H prefers the best response
policy from Section 3.4 to ? E : the best response policy will significantly outperform the DBE policy.
Manipulated Variables. Our experiment consists of 2 factors: H-policy and num-features. We
make the assumption that R uses an IRL algorithm to compute its estimate of ? during learning and maximizes reward under this estimate during deployment. We use Maximum-Entropy
IRL (Ziebart et al., 2008) to implement R?s policy. H-policy varies H?s strategy ? H and has two
levels: demonstration-by-expert (? E ) and best-responder (br). In the ? E level H maximizes reward
during the demonstration. In the br level H uses the approximate algorithm from Section 3.4 to
compute an approximate best response to ? R . The trade-off between reward and communication ? is
set by cross-validation before the game begins. The num-features factor varies the dimensionality of
?across two levels: 3 features and 10 features. We do this to test whether and how the difference
between experts and best-responders is affected by dimensionality. We use a factorial design that
leads to 4 distinct conditions. We test each condition against a random sample of N = 500 different
reward parameters. We use a within-subjects design with respect to the the H-policy factor so the
same reward parameters are tested for ? E and br.
7

Dependent Measures. We use the regret with respect to a fully-observed setting where the robot
knows the ground truth ? as a measure of performance. We let ?? be the robot?s estimate of the reward
parameters and let ?GT be the ground truth reward parameters. The primary measure is the regret of
R?s policy: the difference between the value of the policy that maximizes the inferred reward ?? and
the value of the policy that maximizes the true reward ?GT . We also use two secondary measures.
The first is the KL-divergence between the maximum-entropy trajectory distribution induced by ??
and the maximum-entropy trajectory distribution induced by ?. Finally, we use the `2 -norm between
the vector or rewards defined by ?? and the vector induced by ?GT .
Results. There was relatively little correlation between the measures (Cronbach?s ? of .47), so
we ran a factorial repeated measures ANOVA for each measure. Across all measures, we found a
significant effect for H-policy, with br outperforming ? E on all measures as we hypothesized (all
with F > 962, p < .0001). We did find an interaction effect with num-features for KL-divergence
and the `2 -norm of the reward vector but post-hoc Tukey HSD showed br to always outperform ? E .
The interaction effect arises because the gap between the two levels of H-policy is larger with fewer
reward parameters; we interpret this as evidence that num-features = 3 is an easier teaching problem
for H. Figure 2 (Left, Middle) shows the dependent measures from our experiment.
4.3

Varying R?s Expectations

Maximum-Entropy IRL includes a free parameter ? that controls how optimal R expects H to behave.
If ? = 0, R will update its belief as if H?s observed behavior is independent of her preferences ?. If
? = ?, R will update its belief as if H?s behavior is exactly optimal. We ran a followup experiment
to determine how varying ? changes the regret of the br policy.
Changing ? changes the forward model in R?s belief update: the mapping R hypothesizes between
a given reward parameter ? and the observed feature counts ?? . This mapping is many-to-one for
extreme values of ?. ? ? 0 means that all values of ? lead to the same expected feature counts
because trajectories are chosen uniformly at random. Alternatively, ? >> 0 means that almost all
probability mass falls on the optimal trajectory and many values of ? will lead to the same optimal
trajectory. This suggests that it is easier for H to differentiate different values of ? if R assumes she
is noisily optimal, but only up until a maximum noise level. Figure 2 plots regret as a function of ?
and supports this analysis: H has less regret for intermediate values of ?.

5

Conclusion and Future Work

In this work, we presented a game-theoretic model for cooperative learning, CIRL. Key to this model
is that the robot knows that it is in a shared environment and is attempting to maximize the human?s
reward (as opposed to estimating the human?s reward function and adopting it as its own). This leads
to cooperative learning behavior and provides a framework in which to design HRI algorithms and
analyze the incentives of both actors in a reward learning environment.
We reduced the problem of computing an optimal policy pair to solving a POMDP. This is a useful
theoretical tool and can be used to design new algorithms, but it is clear that optimal policy pairs
are only part of the story. In particular, when it performs a centralized computation, the reduction
assumes that we can effectively program both actors to follow a set coordination policy. This is
clearly infeasible in reality, although it may nonetheless be helpful in training humans to be better
teachers. An important avenue for future research will be to consider the coordination problem: the
process by which two independent actors arrive at policies that are mutual best responses. Returning
to Wiener?s warning, we believe that the best solution is not to put a specific purpose into the machine
at all, but instead to design machines that provably converge to the right purpose as they go along.

Acknowledgments
This work was supported by the DARPA Simplifying Complexity in Scientific Discovery (SIMPLEX)
program, the Berkeley Deep Drive Center, the Center for Human Compatible AI, the Future of Life
Institute, and the Defense Sciences Office contract N66001-15-2-4048. Dylan Hadfield-Menell is
also supported by a NSF Graduate Research Fellowship.

8

References
Abbeel, P and Ng, A. Apprenticeship learning via inverse reinforcement learning. In ICML, 2004.
Balbach, F and Zeugmann, T. Recent developments in algorithmic teaching. In Language and
Automata Theory and Applications. Springer, 2009.
Bernstein, D, Zilberstein, S, and Immerman, N. The complexity of decentralized control of Markov
decision processes. In UAI, 2000.
Bostrom, N. Superintelligence: Paths, dangers, strategies. Oxford, 2014.
Boutilier, Craig. Sequential optimality and coordination in multiagent systems. In IJCAI, volume 99,
pp. 478?485, 1999.
Cakmak, M and Lopes, M. Algorithmic and human teaching of sequential decision tasks. In AAAI,
2012.
Dragan, A and Srinivasa, S. Generating legible motion. In Robotics: Science and Systems, 2013.
Fern, A, Natarajan, S, Judah, K, and Tadepalli, P. A decision-theoretic model of assistance. JAIR, 50
(1):71?104, 2014.
Gibbons, R. Incentives in organizations. Technical report, National Bureau of Economic Research,
1998.
Goldman, S and Kearns, M. On the complexity of teaching. Journal of Computer and System
Sciences, 50(1):20?31, 1995.
Goldman, S, Rivest, R, and Schapire, R. Learning binary relations and total orders. SIAM Journal on
Computing, 22(5):1006?1034, 1993.
Golland, D, Liang, P, and Klein, D. A game-theoretic approach to generating spatial descriptions. In
EMNLP, pp. 410?419, 2010.
Jensen, M and Meckling, W. Theory of the firm: Managerial behavior, agency costs and ownership
structure. Journal of Financial Economics, 3(4):305?360, 1976.
Kerr, S. On the folly of rewarding A, while hoping for B. Academy of Management Journal, 18(4):
769?783, 1975.
Kuleshov, V and Schrijvers, O. Inverse game theory. Web and Internet Economics, 2015.
Leveson, N and Turner, C. An investigation of the Therac-25 accidents. IEEE Computer, 26(7):
18?41, 1993.
Natarajan, S, Kunapuli, G, Judah, K, Tadepalli, P, and Kersting, Kand Shavlik, J. Multi-agent inverse
reinforcement learning. In Int?l Conference on Machine Learning and Applications, 2010.
Nayyar, A, Mahajan, A, and Teneketzis, D. Decentralized stochastic control with partial history
sharing: A common information approach. IEEE Transactions on Automatic Control, 58(7):
1644?1658, 2013.
Ng, A and Russell, S. Algorithms for inverse reinforcement learning. In ICML, 2000.
Ramachandran, D and Amir, E. Bayesian inverse reinforcement learning. In IJCAI, 2007.
Ratliff, N, Bagnell, J, and Zinkevich, M. Maximum margin planning. In ICML, 2006.
Russell, S. and Norvig, P. Artificial Intelligence. Pearson, 2010.
Russell, Stuart J. Learning agents for uncertain environments (extended abstract). In COLT, 1998.
Waugh, K, Ziebart, B, and Bagnell, J. Computational rationalization: The inverse equilibrium
problem. In ICML, 2011.
Wiener, N. Some moral and technical consequences of automation. Science, 131, 1960.
Ziebart, B, Maas, A, Bagnell, J, and Dey, A. Maximum entropy inverse reinforcement learning. In
AAAI, 2008.
9

"
2011,Sparse Features for PCA-Like Linear Regression,,4196-sparse-features-for-pca-like-linear-regression.pdf,"Principal Components Analysis~(PCA) is often used as a feature extraction procedure. Given a matrix $X \in \mathbb{R}^{n \times d}$, whose rows represent $n$ data points with respect to $d$ features, the top $k$ right singular vectors of $X$ (the so-called \textit{eigenfeatures}), are arbitrary linear combinations of all available features. The eigenfeatures are very useful in data analysis, including the regularization of linear regression. Enforcing sparsity on the eigenfeatures, i.e., forcing them to be linear combinations of only a \textit{small} number of actual features (as opposed to all available features), can promote better generalization error and improve the interpretability of the eigenfeatures. We present deterministic and randomized algorithms that construct such sparse eigenfeatures while \emph{provably} achieving in-sample performance comparable to regularized linear regression. Our algorithms are relatively simple and practically efficient, and we demonstrate their performance on several data sets.","Sparse Features for PCA-Like Linear Regression

Petros Drineas
Computer Science Department
Rensselaer Polytechnic Institute
Troy, NY 12180
drinep@cs.rpi.edu

Christos Boutsidis
Mathematical Sciences Department
IBM T. J. Watson Research Center
Yorktown Heights, New York
cboutsi@us.ibm.com

Malik Magdon-Ismail
Computer Science Department
Rensselaer Polytechnic Institute
Troy, NY 12180
magdon@cs.rpi.edu

Abstract
Principal Components Analysis (PCA) is often used as a feature extraction procedure. Given a matrix X ? Rn?d , whose rows represent n data points with respect
to d features, the top k right singular vectors of X (the so-called eigenfeatures),
are arbitrary linear combinations of all available features. The eigenfeatures are
very useful in data analysis, including the regularization of linear regression. Enforcing sparsity on the eigenfeatures, i.e., forcing them to be linear combinations
of only a small number of actual features (as opposed to all available features), can
promote better generalization error and improve the interpretability of the eigenfeatures. We present deterministic and randomized algorithms that construct such
sparse eigenfeatures while provably achieving in-sample performance comparable
to regularized linear regression. Our algorithms are relatively simple and practically efficient, and we demonstrate their performance on several data sets.

1 Introduction
Least-squares analysis was introduced by Gauss in 1795 and has since has bloomed into a staple of
the data analyst. Assume the usual setting with n tuples (x1 , y1 ), . . . , (xn , yn ) in Rd , where xi are
points and yi are targets. The vector of regression weights w? ? Rd minimizes (over all w ? Rd )
the RMS in-sample error
v
u n
uX
E(w) = t (xi ? w ? yi )2 = kXw ? yk2 .
i=1

In the above, X ? Rn?d is the data matrix whose rows are the vectors xi (i.e., Xij = xi [j]); and,
y ? Rn is the target vector (i.e., y[i] = yi ). We will use the more convenient matrix formulation1,
namely given X and y, we seek a vector w? that minimizes kXw ? yk2 . The minimal-norm vector
w? can be computed via the Moore-Penrose pseudo-inverse of X: w? = X+ y. Then, the optimal
in-sample error is equal to:
E(w? ) = ky ? XX+ yk2 .
1
For the sake of simplicity, we assume d ? n and rank (X) = d in our exposition; neither assumption is
necessary.

1

When the data is noisy and X is ill-conditioned, X+ becomes unstable to small perturbations and
overfitting can become a serious problem. Practitioners deal with such situations by regularizing
the regression. Popular regularization methods include, for example, the Lasso [28], Tikhonov
regularization [17], and top-k PCA regression or truncated SVD regularization [21]. In general,
such methods are encouraging some form of parsimony, thereby reducing the number of effective
degrees of freedom available to fit the data. Our focus is on top-k PCA regression which can be
viewed as regression onto the top-k principal components, or, equivalently, the top-k eigenfeatures.
The eigenfeatures are the top-k right singular vectors of X and are arbitrary linear combinations
of all available input features. The question we tackle is whether one can efficiently extract sparse
eigenfeatures (i.e., eigenfeatures that are linear combinations of only a small number of the available
features) that have nearly the same performance as the top-k eigenfeatures.
Basic notation. A, B, . . . are matrices; a, b, . . . are vectors; i, j, . . . are integers; In is the n ? n
identity matrix; 0m?n is the m ? n matrix of zeros; ei is the standard basis (whose dimensionality
will be clear from the context). For vectors, we use the Euclidean norm k ? k2 ; for matrices, the
P
2
Frobenius and the spectral norms: kXkF = i,j X2ij and kXk2 = ?1 (X), i.e., the largest singular
value of X.

Top-k PCA Regression. Let X = U?V T be the singular value decomposition of X, where U
(resp. V) is the matrix of left (resp. right) singular vectors of X with singular values in the diagonal
matrix ?. For k ? d, let Uk , ?k , and Vk contain only the top-k singular vectors and associated
singular values. The best rank-k reconstruction of X in the Frobenius norm can be obtained from
this truncated singular value decomposition as Xk = Uk ?k VkT . The k right singular vectors in Vk
are called the top-k eigenfeatures. The projections of the data points onto the top k eigenfeatures are
obtained by projecting the xi ?s onto the columns of Vk to obtain Fk = XVk = U?V T Vk = U k ?k .
Now, each data point (row) in Fk only has k dimensions. Each column of Fk contains a particular
eigenfeature?s value for every data point and is a linear combination of the columns of X.
The top-k PCA regression uses Fk as the data matrix and y as the target vector to produce regression
weights wk? = F+
k y. The in-sample error of this k-dimensional regression is equal to
?1 T
T
ky ? Fk wk? k2 = ky ? Fk F +
k yk2 = ky ? U k ?k ?k U k yk2 = ky ? U k U k yk2 .

The weights wk? are k-dimensional and cannot be applied to X, but the equivalent weights Vk wk?
can be applied to X and they have the same in-sample error with respect to X:
E(V k wk? ) = ky ? XVk wk? k2 = ky ? F k wk? k2 = ky ? Uk UkT yk2 .
Hence, we will refer to both wk? and Vk wk? as the top-k PCA regression weights (the dimension will
make it clear which one we are talking about) and, for simplicity, we will overload wk? to refer to both
these weight vectors (the dimension will make it clear which). In practice, k is chosen to measure
the ?effective dimension? of the data, and, typically, k ? rank(X) = d. One way to choose k is so
that kX ? Xk kF ? ?k (X) (the ?energy? in the k-th principal component is large compared to the
energy in all smaller principal components). We do not argue the merits of top-k PCA regression;
we just note that top-k PCA regression is a common tool for regularizing regression.
Problem Formulation. Given X ? Rn?d , k (the number of target eigenfeatures for top-k PCA
regression), and r > k (the sparsity parameter), we seek to extract a set of at most k sparse eigenfea? k which use at most r of the actual dimensions. Let F
?k = XV
? k ? Rn?k denote the matrix
tures V
whose columns are the k extracted sparse eigenfeatures, which are a linear combination of a set of at
most r actual features. Our goal is to obtain sparse features for which the vector of sparse regression
?+
? ?+
?k = F
weights w
k y results in an in-sample error ky ? F k F k yk2 that is close to the top-k PCA
regression error ky ? F k F+
k yk2 . Just as with top-k PCA regression, we can define the equivalent
? kw
? k ; we will overload w
? k to refer to these weights as well.
d-dimensional weights V
Finally, we conclude by noting that while our discussion above has focused on simple linear regression, the problem can also be defined for multiple regression, where the vector y is replaced by a
matrix Y ? Rn?? , with ? ? 1. The weight vector w becomes a weight matrix, W, where each
column of W contains the weights from the regression of the corresponding column of Y onto the
features. All our results hold in this general setting as well, and we will actually present our main
contributions in the context of multiple regression.
2

2 Our contributions
Recall from our discussion at the end of the introduction that we will present all our results in the
general setting, where the target vector y is replaced by a matrix Y ? Rn?? . Our first theorem
argues that there exists a polynomial-time deterministic algorithm that constructs a feature matrix
? k ? Rn?k , such that each feature (column of F
? k ) is a linear combination of at most r actual
F
features (columns) from X and results in small in-sample error . Again, this should be contrasted
with top-k PCA regression, which constructs a feature matrix Fk , such that each feature (column of
Fk ) is a linear combination of all features (columns) in X. Our theorems argue that the in-sample
error of our features is almost as good as the in-sample error of top-k PCA regression, which uses
dense features.
Theorem 1 (Deterministic Feature Extraction). Let X ? Rn?d and Y ? Rn?? be the input matrices
in a multiple regression problem. Let k > 0 be a target rank for top-k PCA regression on X and Y.
?k = XV
? k ? Rn?k , such
For any r > k, there exists an algorithm that constructs a feature matrix F
?
that every column of Fk is a linear combination of (the same) at most r columns of X, and
r !


+
9k kX ? Xk kF


?
?
?
?
kYk2 .

Y ? X Wk 
 = kY ? F k Fk YkF ? kY ? XW k kF + 1 +
r
?k (X)
F

(?k (X) is the k-th
 singular value of X.) The running time of the proposed algorithm is T (Vk ) +
O ndk + nrk 2 , where T (Vk ) is the time required to compute the matrix Vk , the top-k right singular vectors of X.

Theorem 1 says that one can construct k features with sparsity O(k) and obtain a comparble regression error to that attained by the dense top-k PCA features, up to additive term that is proportional
to ?k = kX ? Xk kF /?k (X).
To construct the features satisfying the guarantees of the above theorem, we first employ the Algorithm DSF-Select (see Table 1 and Section 4.3) to select r columns of X and form the matrix
C ? Rn?r . Now, let ?C,k (Y) denote the best rank-k approximation (with respect to the Frobenius
norm) to Y in the column-span of C. In other words, ?C,k (Y) is a rank-k matrix that minimizes
kY ? ?C,k (Y) kF over all rank-k matrices in the column-span of C. Efficient algorithms are known
for computing ?C,k (X) and have been described in [2]. Given ?C,k (Y), the sparse eigenfeatures
can be computed efficiently as follows: first, set ? = C + ?C,k (Y). Observe that
C? = CC+ ?C,k (Y) = ?C,k (Y).
The last equality follows because CC + projects onto the column span of C and ?C,k (Y) is already
in the column span of C. ? has rank at most k because ?C,k (Y) has rank at most k. Let the
T
? k = CU? ?? ? Rn?k . Clearly, each column of F
? k is a
SVD of ? be ? = U? ?? V?
and set F
linear combination of (the same) at most r columns of X (the columns in C). The sparse features
?k = XV
? k , so V
? k = X+ F
?k.
themselves can also be obtained because F
? k are a good set of sparse features, we first relate the regression error from using F
?k
To prove that F
to how well ?C,k (Y) approximates Y.
T
T
? k V?
?k F
?+
kY ? ?C,k (Y)kF = kY ? C?kF = kY ? CU? ?? V?
kF = kY ? F
kF ? kY ? F
k YkF .

+

? k Y are the optimal regression weights for the features F
? k . The
The last inequality follows because F
reverse inequality also holds because ?C,k (Y) is the best rank-k approximation to Y in the column
span of C. Thus,
?k F
?+
kY ? F
k YkF = kY ? ?C,k (Y)kF .
The upshot of the above discussion is that if we can find a matrix C consisting of columns of X for
which kY ? ?C,k (Y)kF is small, then we immediately have good sparse eigenfeatures. Indeed, all
that remains to complete the proof of Theorem 1 is to bound kY ? ?C,k (Y)kF for the columns C
returned by the Algorithm DSF-Select.
Our second result employs the Algorithm RSF-Select (see Table 2 and Section 4.4) to select r
columns of X and again form the matrix C ? Rn?r . One then proceeds to construct ?C,k (Y) and
? k as described above. The advantage of this approach is simplicity, better efficiency and a slightly
F
better error bound, at the expense of logarithmically worse sparsity.
3

Theorem 2 (Randomized Feature Extraction). Let X ? Rn?d and Y ? Rn?? be the input matrices
in a multiple regression problem. Let k > 0 be a target rank for top-k PCA regression on X and
Y. For any r > 144k ln(20k), there exists a randomized algorithm that constructs a feature matrix
?k = XV
? k ? Rn?k , such that every column of F
? k is a linear combination of at most r columns
F
of X, and, with probability at least .7 (over random choices made in the algorithm),
r


+


?
? k 
 = kY ? F
?k F
? k Yk ? kY ? XWk k + 36k ln(20k) kX ? Xk kF kYk .

Y ? X W
F
F
2
r
?k (X)
F

The running time of the proposed algorithm is T (Vk ) + O(dk + r log r).

3 Connections with prior work
A variant of our problem is the identification of a matrix C consisting of a small number (say r)
columns of X such that the regression of Y onto C (as opposed to k features from C) gives small insample error. This is the sparse approximation problem, where the number of non-zero weights in the
regression vector is restricted to r. This problem is known to be NP-hard [25]. Sparse approximation
has important applications and many approximation algorithms have been presented [29, 9, 30];
proposed algorithms are typically either greedy or are based on convex optimization relaxations of
the objective. An important difference between sparse approximation and sparse PCA regression is
that our goal is not to minimize the error under a sparsity constraint, but to match the top-k PCA
regularized regression under a sparsity constraint. We argue that it is possible to achieve a provably
accurate sparse PCA-regression, i.e., use sparse features instead of dense ones.
If X = Y (approximating X using the columns of X), then this is the column-based matrix reconstruction problem, which has received much attention in existing literature [16, 18, 14, 26, 5, 12, 20].
In this paper, we study the more general problem where X 6= Y, which turns out to be considerably
more difficult.
Input sparseness is closely related to feature selection and automatic relevance determination. Research in this area is vast, and we refer the reader to [19] for a high-level view of the field. Again,
the goal in this area is different than ours, namely they seek to reduce dimensionality and improve
out-of-sample error. Our goal is to provide sparse PCA features that are almost as good as the exact principal components. While it is definitely the case that many methods outperform top-k PCA
regression, especially for d ? n, this discussion is orthogonal to our work.
The closest result to ours in prior literature is the so-called rank-revealing QR (RRQR) factorization [8]. The authors use a QR-like decomposition to select exactly k columns of X and compare
? k with the top-k PCA regularized solution wk? . They show that
their sparse solution vector w
? k k2 ?
kwk? ? w

p
kX ? Xk k2
k(n ? k) + 1
?,
?k (X)

? k k2 + ky ? Xwk? k2 /?k (X). This bound is similar to our bound in Theorem 1,
where ? = 2 kw
p
but
? only applies to r = k and is considerably weaker. For example, k(n ? k) + 1 kX ? Xk k2 ?
k kX ? Xk kF ; note also that the dependence of the above bound on 1/?k (X) is generally worse
than ours.
The importance of the right singular vectors in matrix reconstruction problems (including PCA)
has been heavily studied in prior literature, going back to work by Jolliffe in 1972 [22]. The idea of
sampling columns from a matrix X with probabilities that are derived from VkT (as we do in Theorem
2) was introduced in [15] in order to construct coresets for regression problems by sampling data
points (rows of the matrix X) as opposed to features (columns of the matrix X). Other prior work
including [15, 13, 27, 6, 4] has employed variants of this sampling scheme; indeed, we borrow
proof techniques from the above papers in our work. Finally, we note that our deterministic feature
selection algorithm (Theorem 1) uses a sparsification tool developed in [2] for column based matrix
reconstruction. This tool is a generalization of algorithms originally introduced in [1].
4

4 Our algorithms
Our algorithms emerge from the constructive proofs of Theorems 1 and 2. Both algorithms necessitate access to the right singular vectors of X, namely the matrix Vk ? Rd?k . In our experiments, we
used PROPACK [23] in order to compute Vk iteratively; PROPACK is a fast alternative to the exact
SVD. Our first algorithm (DSF-Select) is deterministic, while the second algorithm (RSF-Select)
is randomized, requiring logarithmically more columns to guarantee the theoretical bounds. Prior
to describing our algorithms in detail, we will introduce useful notation on sampling and rescaling
matrices as well as a matrix factorization lemma (Lemma 3) that will be critical in our proofs.
4.1 Sampling and rescaling matrices
Let C ? Rn?r contain r columns of X ? Rn?d . We can express the matrix C as C = X?, where
the sampling matrix ? ? Rd?r is equal to [ei1 , . . . , eir ] and ei are standard basis vectors in Rd . In
our proofs, we will make use of S ? Rr?r , a diagonal rescaling matrix with positive entries on the
diagonal. Our column selection algorithms return a sampling and a rescaling matrix, so that X?S
contains a subset of rescaled columns from X. The rescaling is benign since it does not affect the
span of the columns of C = X? and thus the quantity of interest, namely ?C,k (Y).
4.2 A structural result using matrix factorizations
We now present a matrix reconstruction lemma that will be the starting point for our algorithms.
Let Y ? Rn?? be a target matrix and let X ? Rn?d be the basis matrix that we will use in order
to reconstruct Y. More specifically, we seek a sparse reconstruction of Y from X, or, in other
words, we would like to choose r ? d columns from X and form a matrix C ? Rn?r such that
kY ? ?C,k (Y)kF is small. Let Z ? Rd?k be an orthogonal matrix (i.e., Z T Z = Ik ), and express the
matrix X as follows:
X = HZT + E,
where H is some matrix in Rn?k and E ? Rn?d is the residual error of the factorization. It is easy
to prove that the Frobenius or spectral norm of E is minimized when H = XZ. Let ? ? Rd?r and
S ? Rr?r be a sampling and a rescaling matrix respectively as defined in the previous section, and
let C = X? ? Rn?r . Then, the following lemma holds (see [3] for a detailed proof).
Lemma 3 (Generalized Column Reconstruction). Using the above notation, if the rank of the matrix
Z T ?S is equal to k, then
kY ? ?C,k (Y)kF ? kY ? HH+ YkF + kE?S(Z T ?S)+ H+ YkF .

(1)

We now parse the above lemma carefully in order to understand its implications in our setting. For
our goals, the matrix C essentially contains a subset of r features from the data matrix X. Recall that
?C,k (Y) is the best rank-k approximation to Y within the column space of C; and, the difference
Y ? ?C,k (Y) measures the error from performing regression using sparse eigenfeatures that are
constructed as linear combinations of the columns of C. Moving to the right-hand side of eqn. (1),
the two terms reflect a tradeoff between the accuracy of the reconstruction of Y using H and the
error E in approximating X by the product HZT . Ideally, we would like to choose H so that Y can
be accurately approximated and, at the same time, the matrix X is approximated by the product HZ T
with small residual error E. In general, these two goals might be competing and a balance must be
struck. Here, we focus on one extreme of this trade off, namely choosing Z so that the (Frobenius)
norm of the matrix E is minimized. More specifically, since Z has rank k, the best choice for HZT in
order to minimize kEkF is Xk ; then, E = X ? Xk . Using the SVD of Xk , namely Xk = Uk ?k VkT ,
we apply Lemma 3 setting H = Uk ?k and Z = Vk . The following corollary is immediate.
Lemma 4 (Generalization of Lemma 7 in [2]). Using the above notation, if the rank of the matrix
VkT ?S is equal to k, then
T
kY ? ?C,k (Y)kF ? kY ? U k UkT YkF + k(X ? Xk )?S(V kT ?S)+ ??1
k U k YkF .

Our main results will follow by carefully choosing ? and S in order to control the right-hand side of
the above inequality.
5

Algorithm: DSF-Select
1: Input: X, k, r.
2: Output: r columns of X in C.
3: Compute Vk and
E = X ? Xk = X ? XVk VkT .

Algorithm: DetSampling
1: Input: V T = [v1 , . . . , vd ], A = [a1 , . . . , ad ], r.
2: Output: Sampling and rescaling matrices [?, S].
3: Initialize B0 = 0k?k , ? = 0d?r , and S = 0r?r .
4: for ? = 1 to r ??
1 do
5:
Set L? = ? ? rk.
6:
Pick index i ? {1, 2, ..., n} and t such that

4: Run DetSampling to construct sampling and rescaling matrices ? and S:
[?, S] = DetSampling(VkT , E, r).
5: Return C = X?.

U (ai ) ?

1
? L(vi , B? ?1 , L? ).
t

7:
Update B? = B? ?1 + tvi v?iT .
8:
Set ?i? = 1 and S ? ? = 1/ t.
9: end for
10: Return ? and S.

Table 1: DSF-Select: Deterministic Sparse Feature Selection
4.3 DSF-Select: Deterministic Sparse Feature Selection
DSF-Select deterministically selects r columns of the matrix X to form the matrix C (see Table 1
and note that the matrix C = X? might contain duplicate columns which can be removed without
any loss in accuracy). The heart of DSF-Select is the subroutine DetSampling, a near-greedy
algorithm which selects columns of VkT iteratively to satisfy two criteria: the selected columns should
form an approximately orthogonal basis for the columns of VkT so that (VkT ?S)+ is well-behaved;
and E?S should also be well-behaved. These two properties will allow us to prove our results via
Lemma 4. The implementation of the proposed algorithm is quite simple since it relies only on
standard linear algebraic operations.
DetSampling takes as input two matrices: V T ? Rk?d (satisfying V T V = Ik ) and A ? Rn?d . In
order to describe the algorithm, it is convenient to view these two matrices as two sets of column
Pd
vectors, V T = [v1 , . . . , vd ] (satisfying i=1 vi viT = Ik ) and A = [a1 , . . . , ad ]. In DSF-Select
T
T
we set V = Vk and A = E = X ? Xk . Given k and r, the algorithm iterates from ? = 0 up to
? = r ? 1 and its main operation is to compute the functions ?(L , B) and L(v, B, L) that are defined
as follows:
? ( L, B) =

k
X
i=1

1
,
?i ? L

?2

L (v, B, L) =

vT (B ? (L + 1) Ik ) v
?1
? vT (B ? ( L + 1) Ik ) v.
? ( L + 1, B) ? ? (L, B)

In the above, B ? Rk?k is a symmetric matrix with eigenvalues ?1 , . . . , ?k and L ? R is a parameter.
We also define the function U (a) for a vector a ? Rn as follows:
r !
k
aT a
U (a) = 1 ?
.
r kAk2F
At every step ? , the algorithm selects a column ai such that U (ai ) ? L(vi , B ? ?1 , L? ); note that
B ? ?1 is a k ? k matrix which is also updated at every step of the algorithm (see Table 1). The
existence of such a column is guaranteed by results in [1, 2].
It is worth noting that in practical implementations of the proposed algorithm, there might exist
multiple columns which satisfy the above requirement. In our implementation we chose to break
such ties arbitrarily. However, more careful and informed choices, such as breaking the ties in a way
that makes maximum progress towards our objective, might result in considerable savings. This is
indeed an interesting open problem.
The running time of our algorithm is dominated by the search for a column which satisfies
U (ai ) ? L(vi , B? ?1 , L? ). To compute the function L, we first need to compute ?(L? , B? ?1 ) (which
necessitates the eigenvalues of B ? ?1 ) and then we need to compute the inverse of B ? ?1 ?(L + 1) Ik .
These computations need O(k 3 ) time per iteration, for a total of O(rk 3 ) time over all r iterations.
Now, in order to compute the function L for each vector vi for all i = 1, . . . , d, we need an additional
6

Algorithm: RSF-Select

1:
2:
3:
4:

Input: X, k, r.
Output: r columns of X in C.
Compute Vk .
Run RandSampling to construct sampling and rescaling matrices ? and S:
[?, S] = RandSampling(VkT , r).

5: Return C = X?.

Algorithm: RandSampling
1: Input: V T = [v1 , . . . , vd ] and r.
2: Output: Sampling and rescaling matrices [?, S].
3: For i = 1, ..., d compute probabilities
pi =

1
kvi k22 .
k

4: Initialize ? = 0d?r and S = 0r?r .
5: for ? = 1 to r do
6:
Select an index i? ? {1, 2, ..., d} where the
probability of selecting index i is equal to pi .
?
7:
Set ?i? ? = 1 and S? ? = 1/ rpi? .
8: end for
9: Return ? and S.

Table 2: RSF-Select: Randomized Sparse Feature Selection
O(dk 2 ) time per iteration; the total time for all r iterations is O(drk 2 ). Next, in order to compute
the function U , we need to compute aiT ai (for all i = 1, . . . , d) which necessitates O(nnz(A)) time,
where nnz(A) is the number of non-zero elements of A. In our setting, A = E ? Rn?d , so the
overall running time is O(drk 2 + nd). In order to get the final running time we also need to account
for the computation of Vk and E.
The theoretical properties of DetSampling were analyzed in detail in [2], building on the original
analysis of [1]. The following lemma from [2] summarizes important properties of ?.
Lemma 5 ([2]). DetSampling with inputs V T and A returns a sampling matrix ? ? Rd?r and a
rescaling matrix S ? Rr?r satisfying
r
k
T
+
k(V ?S) k2 ? 1 ?
;
kA?SkF ? kAkF .
r
We apply Lemma 5 with V = VTk and A = E and we combine it with Lemma 4 to conclude the
proof of Theorem 1; see [3] for details.
4.4 RSF-Select: Randomized Sparse Feature Selection
RSF-Select is a randomized algorithm that selects r columns of the matrix X in order to form the
matrix C (see Table 2). The main differences between RSF-Select and DSF-Select are two: first,
RSF-Select only needs access to V kT and, second, RSF-Select uses a simple sampling procedure in
order to select the columns of X to include in C. This sampling procedure is described in algorithm
RandSampling and essentially selects columns of X with probabilities that depend on the norms of
the columns of VkT . Thus, RandSampling first computes a set of probabilities that are proportional
to the norms of the columns of VkT and then samples r columns of X in r independent identical trials
with replacement, where in each trial a column is sampled according to the computed probabilities.
Note that a column could be selected multiple times. In terms of running time, and assuming that
the matrix Vk that contains the top k right singular vectors of X has already been computed, the
proposed algorithm needs O(dk) time to compute the sampling probabilities and an additional O(d+
r log r) time to sample r columns from X. Similar to Lemma 5, we can prove analogous properties
for the matrices ? and S that are returned by algorithm RandSampling. Again, combining with
Lemma 4 we can prove Theorem 2; see [3] for details.

5 Experiments
The goal of our experiments is to illustrate that our algorithms produce sparse features which perform as well in-sample as the top-k PCA regression. It turns out that the out-of-sample performance
is comparable (if not better in many cases, perhaps due to the sparsity) to top-k PCA-regression.
7

(n; d)

Data

wk?
Arcene

(100;10,000)

I-sphere

(351;34)

LibrasMov

(45;90)

Madelon

(2,000;500)

HillVal

(606;100)

Spambase

(4601;57)

0.93
0.99
0.57
0.58
2.9
3.3
0.98
0.98
0.68
0.68
0.30
0.30

k = 5, r = k + 1
? kDSF w
? kRSF w
? krnd
w
0.88
0.94
0.52
0.53
2.9
3.6
0.98
0.98
0.66
0.67
0.30
0.30

0.91
0.98
0.55
0.57
3.1
3.7
0.98
0.98
0.67
0.68
0.31
0.30

1.0
1.0
0.57
0.57
3.7
3.7
1.0
1.0
0.68
0.68
0.28
0.38

wk?
0.93
1.0
0.57
0.58
2.9
3.3
0.98
0.98
0.68
0.68
0.3
0.3

k = 5, r = 2k
? kDSF w
? kRSF
w
0.89
0.97
0.51
0.54
2.4
3.3
0.97
0.98
0.65
0.67
0.3
0.3

0.86
0.98
0.52
0.55
2.6
3.6
0.97
0.98
0.67
0.69
0.3
0.3

? krnd
w
1.0
1.0
0.56
0.56
3.6
3.6
1.0
1.0
0.69
0.69
0.25
0.35

Table 3: Comparison of DSF-select and RSF-select with top-k PCA. The top entry in each cell
is the in-sample error, and the bottom entry is the out-sample error. In bold is the method achieving
the best out-sample error.
Compared to top-k PCA, our algorithms are efficient and work well in practice, even better than the
theoretical bounds suggest.
We present our findings in Table 3 using data sets from the UCI machine learning repository. We
used a five-fold cross validation design with 1,000 random splits: we computed regression weights
using 80% of the data and estimated out-sample error in the remaining 20% of the data. We set k = 5
in the experiments (no attempt was made to optimize k). Table 3 shows the in- and out-sample error
? kDSF ;
for four methods: top-k PCA regression, wk? ; r-sparse features regression using DSF-select, w
RSF
? k ; r-sparse features regression using r random
r-sparse features regression using RSF-select, w
? krnd .
columns, w

6 Discussion
The top-k PCA regression constructs ?features? without looking at the targets ? it is target-agnostic.
So are all the algorithms we discussed here, as our goal was to compare with top-k PCA. However,
there is unexplored potential in Lemma 3. We only explored one extreme choice for the factorization,
namely the minimization of some norm of the matrix E. Other choices, in particular non-targetagnostic choices, could prove considerably better. Such investigations are left for future work.
As mentioned when we discussed our deterministic algorithm, it will often be the case that in some
steps of the greedy selection process, multiple columns could satisfy the criterion for selection. In
such a situation, we are free to choose any one; we broke ties arbitrarily in our implementation,
and even as is, the algorithm performed as well or better than top-k PCA. However, we expect that
breaking the ties so as to optimize the ultimate objective would yield considerable additional benefit;
this would also be non-target-agnostic.
Acknowledgments
This work has been supported by two NSF CCF and DMS grants to Petros Drineas and Malik
Magdon-Ismail.

References
[1] J. Batson, D. Spielman, and N. Srivastava. Twice-ramanujan sparsifiers. In Proceedings of ACM STOC,
pages 255?262, 2009.
[2] C. Boutsidis, P. Drineas, and M. Magdon-Ismail. Near-optimal column based matrix reconstruction. In
Proceedings of IEEE FOCS, 2011.
[3] C. Boutsidis, P. Drineas, and M. Magdon-Ismail.
manuscript, 2011.
[4] C. Boutsidis and M. Magdon-Ismail.
arXiv:1109.5664v1, 2011.

Sparse features for PCA-like linear regression.

Deterministic feature selection for k-means clustering.

8

[5] C. Boutsidis, M. W. Mahoney, and P. Drineas. An improved approximation algorithm for the column
subset selection problem. In Proceedings of ACM -SIAM SODA, pages 968?977, 2009.
[6] C. Boutsidis, M. W. Mahoney, and P. Drineas. Unsupervised feature selection for the k-means clustering
problem. In Proceedings of NIPS, 2009.
[7] J. Cadima and I. Jolliffe. Loadings and correlations in the interpretation of principal components. Applied
Statistics, 22:203?214, 1995.
[8] T. Chan and P. Hansen. Some applications of the rank revealing QR factorization. SIAM Journal on
Scientific and Statistical Computing, 13:727?741, 1992.
[9] A. Das and D. Kempe. Algorithms for subset selection in linear regression. In Proceedings of ACM
STOC, 2008.
[10] A. Dasgupta, P. Drineas, B. Harb, R. Kumar, and M. W. Mahoney. Sampling algorithms and coresets for
Lp regression. In Proceedings of ACM-SIAM SODA, 2008.
[11] A. d?Aspremont, L. El Ghaoui, M. I. Jordan, and G. R. G. Lanckriet. A direct formulation for sparse PCA
using semidefinite programming. In Proceedings of NIPS, 2004.
[12] A. Deshpande and L. Rademacher. Efficient volume sampling for row/column subset selection. In Proceedings of ACM STOC, 2010.
[13] P. Drineas, R. Kannan, and M. Mahoney. Fast Monte Carlo algorithms for matrices I: Approximating
matrix multiplication. SIAM Journal of Computing, 36(1):132?157, 2006.
[14] P. Drineas, M. Mahoney, and S. Muthukrishnan. Polynomial time algorithm for column-row based
relative-error low-rank matrix approximation. Technical Report 2006-04, DIMACS, March 2006.
[15] P. Drineas, M. Mahoney, and S. Muthukrishnan. Sampling algorithms for ?2 regression and applications.
In Proceedings of ACM-SIAM SODA, pages 1127?1136, 2006.
[16] G. Golub. Numerical methods for solving linear least squares problems. Numerische Mathematik, 7:206?
216, 1965.
[17] G. Golub, P. Hansen, and D. O?Leary. Tikhonov regularization and total least squares. SIAM Journal on
Matrix Analysis and Applications, 21(1):185?194, 2000.
[18] M. Gu and S. Eisenstat. Efficient algorithms for computing a strong rank-revealing QR factorization.
SIAM Journal on Scientific Computing, 17:848?869, 1996.
[19] I. Guyon and A. Elisseeff. Special issue on variable and feature selection. Journal of Machine Learning
Research, 3, 2003.
[20] N. Halko, P. Martinsson, and J. Tropp. Finding structure with randomness: probabilistic algorithms for
constructing approximate matrix decompositions. SIAM Review, 2011.
[21] P. Hansen. The truncated SVD as a method for regularization. BIT Numerical Mathematics, 27(4):534?
553, 1987.
[22] I. Jolliffe. Discarding variables in Principal Component Analysis: asrtificial data. Applied Statistics,
21(2):160?173, 1972.
[23] R. Larsen.
PROPACK: A software package for the symmetric eigenvalue problem and singular value problems on Lanczos and Lanczos bidiagonalization with partial reorthogonalization.
http://soi.stanford.edu/?rmunk/?PROPACK/.
[24] B. Moghaddam, Y. Weiss, and S. Avidan. Spectral bounds for sparse PCA: exact and greedy algorithms.
In Proceedings of NIPS, 2005.
[25] B. Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on Computing, 24(2):227?
234, 1995.
[26] M. Rudelson and R. Vershynin. Sampling from large matrices: An approach through geometric functional
analysis. Journal of the ACM, 54, 2007.
[27] N. Srivastava and D. Spielman. Graph sparsifications by effective resistances. In Proceedings of ACM
STOC, pages 563?568, 2008.
[28] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society,
pages 267?288, 1996.
[29] J. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on Information
Theory, 50(10):2231?2242, 2004.
[30] T. Zhang. Generating a d-dimensional linear subspace efficiently. In Adaptive forward-backward greedy
algorithm for sparse learning with linear models, 2008.

9

"
1987,Phasor Neural Networks,,90-phasor-neural-networks.pdf,Abstract Missing,"584

PHASOR NEURAL NETVORKS
Andr~

J. Noest, N.I.B.R., NL-ll0S AZ Amsterdam, The Netherlands.
ABSTRACT

A novel network type is introduced which uses unit-length 2-vectors
for local variables. As an example of its applications, associative
memory nets are defined and their performance analyzed. Real systems
corresponding to such 'phasor' models can be e.g. (neuro)biological
networks of limit-cycle oscillators or optical resonators that have
a hologram in their feedback path.
INTRODUCTION
Most neural network models use either binary local variables or
scalars combined with sigmoidal nonlinearities. Rather awkward coding
schemes have to be invoked if one wants to maintain linear relations
between the local signals being processed in e.g. associative memory
networks, since the nonlinearities necessary for any nontrivial
computation act directly on the range of values assumed by the local
variables. In addition, there is the problem of representing signals
that take values from a space with a different topology, e.g. that
of the circle, sphere, torus, etc. Practical examples of such a
signal are the orientations of edges or the directions of local optic
flow in images, or

~he

phase of a set of (sound or EM) waves as they

arrive on an array of detectors. Apart from the fact that 'circular'
signals occur in technical as well as biological systems, there are
indications that some parts of the brain (e.g. olfactory bulb, cf.
Dr.B.Baird's contribution to these proceedings) can use limit-cycle
oscillators formed by local feedback circuits as functional building
blocks, even for signals without circular symmetry. Vith respect to
technical implementations, I had speculated before the conference
whether it could be useful to code information in the phase of the
beams of optical neurocomputers, avoiding slow optical switching
elements and using only (saturating) optical amplification and a
? American Institute of Physics 1988

585

hologram encoding the (complex) 'synaptic' weight factors. At the
conference, I learnt that Prof. Dana Anderson had independently
developed an optical device (cf. these proceedings) that basically
works this way, at least in the slow-evolution limit of the dynamic
hologram. Hopefully, some of the theory that I present here can be
applied to his experiment. In turn, such implementations call for
interesting extensions of the present models.
BASIC ELEMENTS OF GENERAL PHASOR NETVORKS
Here I study the perhaps simplest non-scalar network by using unitlength 2-vectors (phasors) as continuous local variables. The signals
processed by the network are represented in the relative phaseangles.
Thus, the nonlinearities (unit-length 'clipping') act orthogonally to
the range of the variables coding the information. The behavior of
the network is invariant under any rigid rotation of the complete set
of phasors, representing an arbitrary choice of a global reference
I

phase. Statistical physicists will recognize the phasor model as a
generalization of 02-spin models to include vector-valued couplings.
All 2-vectors are treated algebraically as complex numbers, writing

x for

Ixl for the length, Ixl for the phase-angle, and

the complex

conjugate of a 2-vector x.
A phasor network then consists of N?l phasors s. , with Is.l=l,
1

interacting via couplings c .. , with
1J

C ..

11

1

= O. The c 1J
.. are allowed

to be complex-valued quantities. For optical implementations this
is clearly a natural choice, but it may seem less so for biological
systems. However, if the coupling between two limitcycle oscillators
with frequency f is mediated via a path having propagationdelay d,
then that coupling in fact acquires a phaseshift of

f.d.2~

radians.

Thus, complex couplings can represent such systems more faithfully
than the usual models which neglect propagationdelays altogether.
Only 2-point couplings are treated here, but multi-point couplings
c.1)'k' etc., can be treated similarly.
The dynamics of each phasor depends only on its local field
h.=
1

!z:4~ c 1J
.. s.
J
J

+ n.

1

where z is the number of inputs

586

c .. ~O per cell and n. is a local noise term (complex and Gaussian).
1J

1

Various dynamics are possible, and yield largely similar results:
Continuous-time, parallel evolution:

(""type A"")

d (/s./) = Ih. l.sin(/h.1 - Is./)

(IT

1

1

1

1

s.(t+dt)= h.1 Ih. I , either serially in

Discrete-time updating:

1 1 1

random i-sequence (""type B""), or in parallel for all i (""type C"").
The natural time scale for type-B dynamics is obtained by scaling
the discrete time-interval eft as ,.., liN ; type-C dynamics has cl't=l.
LYAPUNOV FUNCTION

(alias ""ENERGY"", or ""HAMILTONIAN"" )

If one limits the attention temporarily to purely deterministic
(n.=O) models, then the question suggests itself whether a class of
1

couplings exists for which one can easily find a Lyapunov function
i.e. a function of the network variables that is monotonic under the
dynamics. A well-known example

1

is the 'energy' of the binary and

scalar Hopfield models with symmetric interactions. It turns out that
a very similar function exists for phasor networks with type-A or B
dynamics and a Hermitian matrix of couplings.
-H =

L
?
1

Hermiticity (c ..

1J

5.1

h.

1

=

=c .. ) makes
J 1

(lIz)

L

5.1 c 1J
.. s.
J

? .
1,J

H real-valued and non-increasing in time.

This can be shown as follows, e.g. for the serial dynamics (type B).
Suppose, without loss of generality, that phasor i=l is updated.
Then

-z H

Ls. c ' l sl
1>1
+ sl' 2: c ' 1 5.
i>l

=

+

1

z 51 h1

+

1

1

I.I:
i ,j>l

-s.

1

c .. s.
1J

J

+ constant.

1

Vith Hermitian couplings, H becomes real-valued, and one also has

l:c 1 ?
i>l

I:c' 5.
i>1 1 l 1
Thus, - H - constant

1

s.
1

=

z h1

.

51 h1 + sl h1 = 2 Re(sl h 1 )
Clearly, H is minimized with respect to sl by sl(t+1) = hll Ih11 ?
Type-A dynamics has the same Lyapunovian, but type C is more complex.
=

The existence of Hermitian interactions and the corresponding energy
function simplifies greatly the understanding and design of phasor
networks, although non-Hermitian networks can still have a Lyapunov-

587

function, and even networks for which such a function is not readily
found can be useful, as will be illustrated later.
AN APPLICATION: ASSOCIATIVE MEMORY.
A large class of collective computations, such as optimisations
and content-addressable memory, can be realised with networks having
an energy function. The basic idea is to define the relevant penalty
function over the solution-space in the form of the generic 'energy'
of the net, and simply let the network relax to minima of this energy.
As a simple example, consider an associative memory built within the
framework of Hermitian phasor networks.
In order to store a set of patterns in the network, i.e. to make
a set of special states (at least approximatively) into attractive
fixed points of the dynamics, one needs to choose an appropriate
set of couplings. One particularly simple way of doing this is via
the phasor-analog of ""Hebb's rule"" (note the Hermiticity)
p s(.k). s-(.k), h
c .. =
were s.(k).IS phasor 1. .In I earne d pattern k .
IJ

rk

1

J

1

The rule is understood to apply only to the input-sets

'i

of each i.

Such couplings should be realisable as holograms in optical networks,
but they may seem unrealistic in the context of biological networks
of oscillators since the phase-shift (e.g. corresponding to a delay)
of a connection may not be changeable at will. However, the required
coupling can still be implemented naturally if e.g. a few paths with
different fixed delays exist between pairs of cells. The synaps in
each path then simply becomes the projection of the complex coupling
on the direction given by the phase of its path, i.e. it is just a
classical Hebb-synapse that computes the correlation of its pre- and
post-synaptic (imposed) signals, which now are phase-shifted versions
of the phasors s~~)The required complex c .. are then realised as the
1

IJ

vector sum over at least two signals arriving via distinct paths with
corresponding phase-shift and real-valued synaps. Two paths suffice
if they have orthogonal phase-shifts, but random phases will do as
well if there are a reasonable number of paths.
Ve need to have a concise way of expressing how 'near' any state
of the net is to one or more of the stored patterns. A natural way

588

of doing this is via a set of p order parameters called ""overlaps""
N

-(k)

1
s 1.. s.1
?
N 11:
1

I

; 1

Note the constraint on the p overlaps

P

I

< k -< p

-

?

2

Mk ~ 1 if all the patterns

k

are orthogonal, or merely random in the limit N-.QO. This will be
assumed from now on. Also, one sees at once that the whole behaviour
of the network does not depend on any rigid rotation of all phasors
over some angle since H, Mk , c .. and the dynamics are invariant under
1J
multiplication of all s.
by
a
fixed
phasor : s~
= S.s. with ISI=1.
I
I
I
Let us find the performance at low loading: N,p,z .. oo, with p/z.. O
and zero local noise. Also assume an initial overlap m)O with only
one pattern, say with k=1. Then the local field is
hi
1 s~1~
hP~
Z
1
1

and

h.*
1

f

s~k)
~s .. k s~k~
1
J
J
j' i

1
= -z

I: sP~s.

jl'i

=

J

J

(1)
= m1 . si ? S

~ fs~k). L: s~k~s.
z k=2

1

j(~i J

J

h(1)
i +
+ O(1//Z)

h71

,

with

O( ./( p-l) Iz')

where
S~f(i);ISI=1,

.

Thus, perfect recall (M 1=1) occurs in one 'pass' at loadings p/z ... O.
EXACTLY SOLVABLE CASE:

SPARSE and ASYMMETRIC couplings

Although it would be interesting to develop the full thermodynamics
of Hermitian phasor networks with p and z of order N (analogous to the
analysis of the finite-T Hopfield model by the teams of Amit 2 and van
Hemmen 3 ), I will analyse here instead a model with sparse, asymmetric
connectivity, which has the great advantages of being exactly solvable
with relative ease, and of being arguably more realistic biologically
and more easily scalable technologically. In neurobiological networks
a cell has up to z;10 4 asymmetric connections, whereas N;101~ This
probably has the same reason as applies to most VLSI chips, namely to
alleviate wiring problems. For my present purposes, the theoretical
advantage of getting some exact results is of primary interest 4
Suppose each cell has z incoming connections from randomly selected
other cells. The state of each cell at time t depends on at most zt
.
cells at time t=O. Thus, If
z t ?N 112 and N large, then the respective

589

4
trees of 'ancestors' of any pair cells have no cells in common. In
x

particular, if z_ (logN) , for any finite x, then there are no common
ancestors for any finite time t in the limit N-.OO. For fundamental
information-theoretic reasons, one can hope to be able to store p
patterns with p at most of order z for any sort of 2-point couplings.
Important questions to be settled are: Yhat are the accuracy and
speed of the recall process, and how large are the basins of the
attractors representing recalled patterns?
Take again initial conditions (t=O) with, say, m(t)= Hl > H>l = O.
Allowing again local random Gaussian (complex) noise n., the local
? ld s become, In
. now f amI'1'Iar notatIon,
.
h .= h(l)
1 n .?
f Ie
. + h*. +
1

1

1

1

As in the previous section, the h~l)term consists of the 'signal'
1

m(t).s. (modulo the rigid rotation S) and a random term of variance

*

1

at most liz. For p _ z, the h. term becomes important. Being sums of

*

1

z(p-1) phasors oriented randomly relative to the signal, the h. are
1

independent Gaussian zero-mean 2-vectors with variance (p-1)/z , as
p,z and N.. oo

. Finally, let the local noises n.1 have variance r2.

Then the distribution of the s.(t+l) phasors can be found in terms of
1
* .?
2
the signal met) and the total variance a=(p/z)+r of the random h.+n
1
1
After somewhat tedious algebraic manipulations (to be reported in
detail elsewhere) one obtains the dynamic behaviour of met)
m(t+1) = F(m(t),a)

for discrete parallel (type-C) dynamics,

and
d met) = F(m(t),a) - met)

for type-A or type-B dynamics ,

Tt

where the function
m

F(m,a) =

+""

2
Idx.(1+cos2x).expl-(m.sinx) la].(l+erfl(m.cosx)/~)
-1'C

The attractive fixed points H* (a)= F(H * ,a) represent the retrieval
accuracy when the loading-pIus-noise factor equals a. See figure 1.
For a?l one obtains the expansion 1-H* (a)

= a/4 + 3a 2 132 + O(a 3 ).

The recall solutions vanish continuously as H*_(a -a) 112 at a =tc/4.
c

c

One also obtains (at any t) the distribution of the phase scatter of
the phasors around the ideal values occurring in the stored pattern.

590

P(/u./)
1

= (1/2n).exp(-m 2 /a).(1+I1t.L.exp(L 2 ).(1+erf(L?
-(k)

, and
L = (m/la).cos(/u./)
1

where

u.=
s. s.
111

,

(modulo S).

Useful approximations for the high, respectively low M regimes are:
M ?ra: PUu./)
1

(MIl'a1l).exp[-(M./u./)2 /a ]
1

;

I/u./1 ?""XI2
1

M ? f i : PUu./)
= (1I21t).(1+L ?./;l)
1
Figure ~
RETRIEVAL-ERROR and BASIN OF ATTRACTION versus LOADING + NOISE.

Q
Q

Q

en

Q
Q

.,;
Q

"".,;
Q

I:

UI

..,)

Q

-C0

a.

1:)

CD

x

-

c-

Q

Q

.

Q

Q
Q

'""Q
0

Q

0

Q
0

c:
""'0.00

0.10

0.20

O. 30

0 ? 40

a

0 ? 50

= p/z

0 ? 60

+ r-r

O. 70

0 ? 80

0 ? 90

1. 00

591

DISCUSSION
It has been shown that the usual binary or scalar neural networks
can be generalized to phasor networks, and that the general structure
of the theoretical analysis for their use as associative memories can
be extended accordingly. This suggests that many of the other useful
applications of neural nets (back-prop, etcJ can also be generalized
to a phasor setting. This may be of interest both from the point of
view of solving problems naturally posed in such a setting, as well
as from that of enabling a wider range of physical implementations,
such as networks of limit-cycle oscillators, phase-encoded optics,
or maybe even Josephson-junctions.
The performance of phasor networks turns out to be roughly similar
to that of the scalar systems; the maximum capacity

p/z=~/4

for

phasor nets is slightly larger than its value 2/n for binary nets,
but there is a seemingly faster growth of the recall error 1-M at
small a (linear for phasors, against exp(-1/(2a?

for binary nets).

However, the latter measures cannot be compared directly since they
stem from quite different order parameters. If one reduces recalled
phasor patterns to binary information, performance is again similar.
Finally, the present methods and results suggest several roads to
further generalizations, some of which may be relevant with respect
to natural or technical implementations. The first class of these
involves local variables ranging over the k-sphere with k>l. The
other generalizations involve breaking the O(n) (here n=2) symmetry
of the system, either by forcing the variables to discrete positions
on the circle (k-sphere), and/or by taking the interactions between
two variables to be a more general function of the angular distance
between them. Such models are now under development.
REFERENCES
1. J.J.Hopfield, Proc.Nat.Acad.Sci.USA 79, 2554 (1982) and
idem, Proc.Nat.Acad.Sci.USA 81, 3088 (1984).
2. D.J.Amit, H.Gutfreund and H.Sompolinski, Ann.Phys. 173, 30 (1987).
3. D.Grensing, R.Kuhn and J.L. van Hemmen, J.Phys.A 20, 2935 (1987).
4. B.Derrida, E.Gardner and A.Zippelius, Europhys.Lett. 4, 167 (1987)

"
1992,Connected Letter Recognition with a Multi-State Time Delay Neural Network,,620-connected-letter-recognition-with-a-multi-state-time-delay-neural-network.pdf,Abstract Missing,"Connected Letter Recognition with a
Multi-State Time Delay Neural Network

Hermann Hild and Alex Waibel
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213-3891, USA

Abstract
The Multi-State Time Delay Neural Network (MS-TDNN) integrates a nonlinear time alignment procedure (DTW) and the highaccuracy phoneme spotting capabilities of a TDNN into a connectionist speech recognition system with word-level classification and
error backpropagation. We present an MS-TDNN for recognizing
continuously spelled letters, a task characterized by a small but
highly confusable vocabulary. Our MS-TDNN achieves 98.5/92.0%
word accuracy on speaker dependent/independent tasks, outperforming previously reported results on the same databases. We propose training techniques aimed at improving sentence level performance, including free alignment across word boundaries, word duration modeling and error backpropagation on the sentence rather
than the word level. Architectures integrating submodules specialized on a subset of speakers achieved further improvements.

1

INTRODUCTION

The recognition of spelled strings of letters is essential for all applications involving
proper names, addresses or other large sets of special words which due to their sheer
size can not be in the basic vocabulary of a recognizer. The high confusability of the
English letters makes the seemingly easy task a very challenging one, currently only
addressed by a few systems, e.g. those of R. Cole et. al. [JFC90, FC90, CFGJ91]
for isolated spoken letter recognition. Their connectionist systems first find a broad
phonetic segmentation, from which a letter segmentation is derived, which is then
712

Connected Letter Recognition with a Multi-State Time Delay Neural Network

?

Word Layer

A

27 word units

Sil

(only 4 shown)

DTW Layer
27 word templates
(only 4 shown)

unity

---- .....

Phoneme Layer
59 phoneme units
(only 9 shown)

Hidden Layer
15 hidden units

Input Layer
16 melscale FFT coefficients

Figure 1: The MS-TDNN recognizing the excerpted word 'B'. Only the activations
for the words 'SIL', 'A', 'B', and 'c' are shown.

In this paper, we present the MS-TDNN as a
classified by another network.
connectionist speech recognition system for connected letter recognition. After describing the baseline architecture, training techniques aimed at improving sentence
level performance and architectures with gender-specific sub nets are introduced.
Baseline Architecture. Time Delay Neural Networks (TDNNs) can combine the
robustness and discriminative power of Neural Nets with a time-shift invariant architecture to form high accuracy phoneme classifiers [WHH+S9]. The Multi-State
TDNN (MS-TDNN) [HFW91, Haf92, HW92], an extension of the TDNN, is capable
of classifying words (represen ted as sequences of phonemes) by integrating a nonlinear time alignment procedure (DTW) into the TDNN architecture. Figure 1 shows
an MS-TDNN in the process of recognizing the excerpted word 'B', represented by
16 melscale FFT coefficients at a 10-msec frame rate. The first three layers constitute a standard TDNN, which uses sliding windows with time delayed connections
to compute a score for each phoneme (state) for every frame, these are the activations in the ""Phoneme Layer"". In the ""DTW Layer"" , each word to be recognized
is modeled by a sequence of phonemes. The corresponding activations are simply

713

714

Hild and Waibel

copied from the Phoneme Layer into the word models of the DTW Layer, where an
optimal alignment path is found for each word. The activations along these paths
are then collected in the word output units. All units in the DTW and Word Layer
are linear and have no biases. 15 (25 to 100) hidden units per frame were used
for speaker-dependent (-independent) experiments, the entire 26 letter network has
approximately 5200 (8600 to 34500) parameters.

Training starts with ""bootstrapping"", during which only the front-end TDNN is
used with fixed phoneme boundaries as targets. In a second phase, training is performed with word level targets. Phoneme boundaries are freely aligned within given
word boundaries in the DTW layer. The error derivatives are backpropagated from
the word units through the alignment path and the front-end TDNN.
The choice of sensible objective functions is of great importance. Let Y =
(Yl, ... ,Yn) the output and T = (tl, ... , t n ) the target vector. For training on
the phoneme level (bootstrapping), there is a target vector T for each frame in
time, representing the correct phoneme j in a ""1-out-of-n"" coding, i.e. ti = Dij. To
see why the standard Mean Square Error (MSE = l:?:l (Yi - ti)2) is problematic
for ""1-out-of-n"" codings for large n (n = 59 in our case), consider for example that
for a target (1.0,0.0, ... ,0.0) the output vector (0.0, ... ,0.0) has only half the error
than the more desirable output (1.0,0.2, ... ,0.2). To avoid this problem, we are
usmg
n

i=1

which (like cross entropy) punishes ""outliers"" with an error approaching infinity for
Iti - yd approaching 1.0. For the word level training, we have achieved best results
with an objective function similar to the ""Classification Figure of Merit (CFM)""
[HW90], which tries to maximize the distance d = Yc - Yhi between the correct score
Yc and the highest incorrect score Yhi instead of using absolute target values of 1.0
and 0.0 for correct and incorrect word units:

ECFM(T, Y)

= f(yc -

Yhd

= f(d) = (1 -

d)2

The philosophy here is not to ""touch"" any output unit not directly related to correct
classification. We found it even useful to apply error backpropagation only in the
case of a wrong or too narrow classification, i.e. if Yc - Yhi < DllaJety...margin.

2
2.1

IMPROVING CONTINUOUS RECOGNITION
TRAINING ACROSS WORD BOUNDARIES

A proper treatment of word 1 boundaries is especially important for a short word
vocabulary, since most phones are at word boundaries. While the phoneme boundaries within a word are freely aligned by the DTW during ""word level training"" , the
word boundaries are fixed and might be error prone or suboptimal. By extending
the alignment one phoneme to the left (last phoneme of previous word) and the
right (first phoneme of next word), the word boundaries can be optimally adjusted
1 In our context, a ""word"" consists of one spelled letter, and a ""sentence"" is a continuously spelled string of letters.

Connected Letter Recognition with a Multi-State Time Delay Neural Network

1. 410~
:-1. :

..

b

B

b

.u

p

u:~

b

~

.

... 2

.

..

. .:

..

-

word boundaries -

. . .

~

? u 4~

~

word boundary found
by free alignment

(a) alignment across word boundaries

c
B
~~~+---~----~~--~
probed)

d

(durat1.on)

(b) duration dependent word penalties

b
h""

B

b
all

b
P

':~
P

b
X>. '
A::!,~

. :.

aU'~

.

I-- word bound_ri ? ? ~

word boundary found

by fr_ allgnlngnt

(C) training on the sentence level
Figure 2: Various techniques to improve sentence level recognition performance

715

716

Hild and Waibel

in the same way as the phoneme boundaries within a word. Figure 2(a) shows an
example in which the word to recognize is surrounded by a silence and a 'B', thus
the left and right context (for all words to be recognized) is the phoneme 'sil' and
'b', respectively. The gray shaded area indicates the extension necessary to the
DTW alignment. The diagram shows how a new boundary for the beginning of
the word 'A' is found. As indicated in figure 3, this techniques improves continuous
recognition significantly, but it doesn't help for excerpted words.

2.2

WORD DURATION DEPENDENT PENALIZING OF
INSERTION AND DELETION ERRORS

In ""continuous testing mode"" , instead of looking at word units the well-known ""One
Stage DTW"" algorithm [Ney84) is used to find an optimal path through an unspecified sequence of words. The short and confusable English letters cause many word
insertion and deletion errors, such as ""T E"" vs. ''T'' or ""0"" vs. ""0 0"", therefore
proper duration modeling is essential.
As suggested in [HW92], minimum phoneme duration can be enforced by ""state
duplication"". In addition, we are modeling a duration and word dependent penalty
Penw(d)
log(k + probw(d?, where the pdf probw(d) is approximated from the
training data and k is a small constant to avoid zero probabilities. Pen w(d) is
added to the accumulated score AS of the search path, AS AS + Aw * Pen w(d),
whenever it crosses the boundary of a word w in Ney's ""One Stage DTW"" algorithm, as indicated in figure 2(b). The ratio Aw , which determines the degree of
influence of the duration penalty, is another important degree of freedom. There
is no straightforward mathematically exact way to compute the effect of a change
of the ""weight"" Aw to the insertion and deletion rate. Our approach is a (pseudo)
gradient descent, which changes Aw proportional to E(w) = (#ins w - #delw)/#w,
i.e. we are trying to maximize the relative balance of insertion and deletion errors.

=

=

2.3

ERROR BACKPOPAGATION AT THE SENTENCE LEVEL

Usually the MS-TDNN is trained to classify excerpted words, but evaluated on continuously spoken sentences. We propose a simple but effective method to extend
training on the sentence level. Figure 2( c) shows the alignment path of the sentence
""e A B"", in which a typical error, the insertion of an 'A', occurred. In a forced
alignment mode (i.e. the correct sequence of words is enforced), positive training is
applied along the correct path, while the units along the incorrect path receive negative training . Note that the effect of positive and negative training is neutralized if
the paths are the same, only differing parts receive non-zero error backpropagation.

2.4

LEARNING CURVES

Figure 3 demonstrates the effect of the various training phases. The system is
bootstrapped (a) during iteration 1 to 130. Word level training starts (b) at iteration
110. Word level training with additional ""training across word boundaries"" (c) is
started at iteration 260. Excerpted word performance is not improved after (c), but
continuous recognition becomes significantly better, compare (d) and (e). In (d),
sentence level training is started directly after iteration 260, while in (e) sentence
level training is started after additional ""across boundaries (word level) training"".

Connected Letter Recognition with a Multi-State Time Delay Neural Network

(~)

~( c)

95.0

---~--~---~~-.-- - - - ---.---- -- --~ -- --- - -- --

90 . 0

85.0

~

?

??
?

o

20

40

60

80

.
..
.

..

.. ..
... .
.

..
..

.

.
..

. ~ - - - - ~ - - - -:- - - - i - - - -:- - - - -! - - - - - - - -:- - - - ~ - - - - :- - - - -:- - - ,

.
I

I

I .

.
I

?

~00120140160~80200220 2 402602803003 2 03403603804004 2 0

Figure 3: Learning curves (a = bootstrapping, b,c = word level (excerpted words),
sentence level training (continuous speech)) on the training (0), crossvalidad,e
tion (-) and test set (x) for the speaker-independent RM Spell-Mode data.

=

3

GENDER SPECIFIC SUBNETS

A straightforward approach to building a more specialized system is simply to train
two entirely individual networks for male and female speakers only. During training,
the gender of a speaker is known, during testing it is determined by an additional
""gender identification network"" , which is simply another MS-TDNN with two output units representing male and female speakers. Given a sentence as input, this
network classifies the speaker's gender with approx. 99% correct. The overall modularized network improved the word accuracy from 90 .8% (for the ""pooled"" net,
see table 1) to 91.3%. However, a hybrid approach with specialized gender-specific
connections at the lower, input level and shared connections for the remaining net
worked even better. As depicted in figure 4, in this architecture the gender identification network selects one of the two gender-specific bundles of connections between
the input and hidden layer. This technique improved the word accuracy to 92.0% .
More experiments with speaker-specific subnetworks are reported in [HW93] .

4

EXPERIMENTAL RESULTS

Our MS-TDNN achieved excellent performance on both speaker dependent and
independent tasks. For speaker dependent testing, we used the ""eMU AlphData"", with 1000 sentences (Le. a continuously spelled string of letters) from each
of 3 male and 3 female speakers. 500, 100, and 400 sentences were used as train-

717

718

Hild and Waibel

Phoneme Layer
Hidden Layer

Input Layer

F F T
Figure 4: A network architecture with gender-sp ecific and shared connections. Only
the front-end TDNN is shown.
ing, cross-validation and test set, respectively. The DARPA Resource Management
Spell-Mode Data were used for speaker independent testing. This data base
contains about 1700 sentences, spelled by 85 male and 35 female speakers. The
speech of 7 male and 4 female speakers was set aside for the test set, one sentence
from all 109 and all sentences from 6 training speakers were used for crossvalidation.
Table 1 summarizes our results. With the help of the training techniques described
above we were able to outperform previously reported [HFW91] speaker dependent
results as well as the HMM-based SPHINX System.

5

SUMMARY AND FUTURE WORK

We have presented a connectionist speech recognition system for high accuracy
connected letter recognition. New training techniques aimed at improving sentence
level recognition enabled our MS-TDNN to outperform previous systems of its own
kind as well as a state-of-the art HMM-based system (SPHINX). Beyond the gender
specific subnets, we are experimenting with an MS-TDNN which maintains several
""internal speaker models"" for a more sophisticated speaker-independent system. In
the future we will also experiment with context dependent phoneme models.
Acknowledgements
The authors gratefully acknowledge support by the National Science Foundation
and DARPA. We wish to thank Joe Tebelskis for insightful discussions, Arthur
McNair for keeping our machines running, and especially Patrick Haffner. Many of
the ideas presented have been developed in collaboration with him.

References
[CFGJ91]

R. A. Cole, M. Fanty, Gopalakrishnan, and R. D.T. Janssen. SpeakerIndependent Name Retrival from Spellings Using a Database of 50,000 Names.

Connected Letter Recognition with a Multi-State Time Delay Neural Network

Speaker Dependent (eMU Alph Data)
500/2500 train, 100/500 crossvalidation, 400/2000 test sentences/words
our
speaker
SPHINX[HFW91] MS-TDNN[HFW91]
MC:_'T'nNN
rnjrnt
rndbs
rnaern
fcaw
flgt
fee

96.0
83.9

97.5
89.7

-

-

-

98.5
91.1
94.6
98.8
86.9
91.0

Speaker Independent (Resource Management Spell-Mode)

109 (ca. 11000) train, 11 (ca. 900) test speaker (words).
our MS-TDNN
SPHINX[HH92]
gender specific
+ Senone
88.7

90.4

90.8

92.0

Table 1: Word accuracy (in % on the test sets) on speaker dependent and speaker
independent connected letter tasks.
In Proceedings of the International Conference on Acoustics, Speech and Signal Processing, Toronto, Ontario, Canada, May 1991. IEEE.
M. Fanty and R. Cole. Spoken letter recognition. In Proceedings of the Neural
[FC90]
Information Processing Systems Conference NIPS, Denver, November 1990.
P. Haffner. Connectionist Word-Level Classification in Speech Recognition.
[Haf92]
In Proc. IEEE International Conference on Acoustics, Speech, and Signal
Processing. IEEE, 1992.
P. Haffner, M. Franzini, and A. Waibel. Integrating Time Alignment and
[HFW91]
Neural Networks for High Performance Continuous Speech Recognition. In
Proc. Int. Conf. on Acoustics, Speech, and Signal Processing. IEEE, 1991.
[HH92]
M.-Y. Hwang and X. Huang. Subphonetic Modeling with Markov States Senone. In Proc. IEEE International Conference on Acoustics, Speech, and
Signal Processing, pages 133 - 137. IEEE, 1992.
[HW90]
J. Hampshire and A. Waibel. A Novel Objective Function for Improved
Phoneme Recognition Using Time Delay Neural Networks. IEEE Transactions on Neural Networks, June 1990.
P. Haffner and A. Waibel. Multi-state Time Delay Neural Networks for Con[HW92]
tinuous Speech Recognition. In NIPS(4). Morgan Kaufman, 1992.
H. Hild and A. Waibel. Multi-SpeakerjSpeaker-Independent Architectures for
[HW93]
the Multi-State Time Delay Neural Network. In Proc. IEEE International
Conference on Acoustics, Speech, and Signal Processing. IEEE, 1993.
[JFC90]
R.D.T. Jansen, M. Fanty, and R. A. Cole. Speaker-independent Phonetic
Classification in Continuous English Letters. In Proceedings of the JJCNN
90, Washington D.C., July 1990.
[Ney84]
H. Ney. The Use of a One-Stage Dynamic Programming Algorithm for Connected Word Recognition. In IEEE Transactions on Acoustics, Speech, and
Signal Processing, pages 263-271. IEEE, April 1984.
[WHH+89] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang. Phoneme
Recognition Using Time-Delay Neural Networks. IEEE, Transactions on
Acoustics, Speech and Signal Processing, March 1989.

719

PART IX

ApPLICATIONS

"
2002,A Formulation for Minimax Probability Machine Regression,,2274-a-formulation-for-minimax-probability-machine-regression.pdf,Abstract Missing,"A Formulation for Minimax Probability
Machine Regression

Thomas Strohmann
Department of Computer Science
University of Colorado, Boulder
strohman@cs.colorado.edu

Gregory Z. Grudic
Department of Computer Science
University of Colorado, Boulder
grudic@cs.colorado.edu

Abstract
We formulate the regression problem as one of maximizing the minimum probability, symbolized by ?, that future predicted outputs of the
regression model will be within some ?? bound of the true regression
function. Our formulation is unique in that we obtain a direct estimate
of this lower probability bound ?. The proposed framework, minimax
probability machine regression (MPMR), is based on the recently described minimax probability machine classification algorithm [Lanckriet
et al.] and uses Mercer Kernels to obtain nonlinear regression models.
MPMR is tested on both toy and real world data, verifying the accuracy
of the ? bound, and the efficacy of the regression models.

1

Introduction

The problem of constructing a regression model can be posed as maximizing the minimum
probability of future predictions being within some bound of the true regression function.
We refer to this regression framework as minimax probability machine regression (MPMR).
For MPMR to be useful in practice, it must make minimal assumptions about the distributions underlying the true regression function, since accurate estimation of these distribution
is prohibitive on anything but the most trivial regression problems. As with the minimax
probability machine classification (MPMC) framework proposed in [1], we avoid the use
of detailed distribution knowledge by obtaining a worst case bound on the probability that
the regression model is within some ? > 0 of the true regression function. Our regression formulation closely follows the classification formulation in [1] by making use of the
following theorem due to Isii [2] and extended by Bertsimas and Sethuraman [3]:
1
supE[z]=?z,Cov[z]=?z P r{aT z ? b} =
, ? 2 = inf aT z?b (z ? ?
z)T ??1
z) (1)
z (z ? ?
1 + ?2
where a and b are constants, z is a random vector, and the supremum is taken over all distributions having mean ?
z and covariance matrix ?z . This theorem assumes linear boundaries,
however, as shown in [1], Mercer kernels can be used to obtain nonlinear versions of this
theorem, giving one the ability to estimate upper and lower bounds on probability that
points generated form any distribution having mean ?
z and covariance ?z , will be on one
side of a nonlinear boundary. In [1], this formulation is used to construct nonlinear classifiers (MPMC) that maximize the minimum probability of correct classification on future
data.

In this paper we exploit the above theorem (??) for building nonlinear regression functions
which maximize the minimum probability that the future predictions will be within an ? to
the true regression function. We propose to implement MPMR by using MPMC to construct a classifier that separates two sets of points: the first set is obtained by shifting all of
the regression data +? along the dependent variable axis; and the second set is obtained by
shifting all of the regression data ?? along the dependent variable axis. The the separating
surface (i.e. classification boundary) between these two classes corresponds to a regression
surface, which we term the minimix probability machine regression model. The proposed
MPMR formulation is unique because it directly computes a bound on the probability that
the regression model is within ?? of the true regression function (see Theorem 1 below).
The theoretical foundations of MPMR are formalized in Section 2. Experimental results on synthetic and real data are given in Section 3, verifying the accuracy of
the minimax probability regression bound and the efficacy of the regression models. Proofs of the two theorems presented in this paper are given in the appendix.
Matlab and C source code for generating MPMR models can be downloaded from
http://www.cs.colorado.edu/?grudic/software.

2

Regression Model

We assume that learning data is generated from some unknown regression function f :
<d 7? < that has the form:
y = f (x) + ?
(2)
where x ? <d are generated according to some bounded distribution ?, y ? <,
E[?] = 0, V ar[?] = ? 2 , and ? ? < is finite. We are given N learning examples
? = {(x1 , y1 ), ..., (xN , yN )}, where ?i ? {1, ..., N }, xi = (xi1 , ..., xid ) ? <d is generated from the distribution ?, and yi ? <. The goal of our formulation is two-fold: first
we wish to use ? to construct an approximation f? of f , such that, for any x generated from
the distribution ?, we can approximate y? using
y? = f?(x)
(3)
The second goal of our formulation is, for any ? ? <, ? > 0, estimate the bound on the
minimum probability, symbolized by ?, that f?(x) is within ? of y (define in (2)):
? = inf Pr {|?
y ? y| ? ?}
(4)
Our proposed formulation of the regression problem is unique because we obtain direct
estimates of ?. Therefore we can estimate the predictive power of a regression function by
a bound on the minimum probability that we are within ? of the true regression function. We
refer to a regression function that directly estimates (4) as a mimimax probability machine
regression (MPMR) model.
The proposed MPMR formulation is based on the kernel formulation for mimimax probability machine classification (MPMC) presented in [1]. Therefore, the MPMR model has
the form:
N
X
y? = f? (x) =
?i K (xi , x) + b
(5)
i=1

where, K (xi , x) = ?(xi )?(x) is a kernel satisfying Mercer?s Conditions, xi , ?i ?
{1, ..., N }, are obtained from the learning data ?, and ?i , b ? < are outputs of the MPMR
learning algorithm.
2.1

Kernel Based MPM Classification

Before formalizing the MPMR algorithm for calculating ?i and b from the training data ?,
we first describe the MPMC formulation upon which it is based. In [1], the binary classification problem is posed as one of maximizing the probability of correctly classifying future

data. Specifically, two sets of points are considered, here symbolized by {u 1 , ..., uNu },
where ?i ? {1, ..., Nu }, ui ? <m , belonging to the first class, and {v1 , ..., vNv }, where
?i ? {1, ..., Nv }, vi ? <m , belonging to the second class. The points ui are assumed to
be generated from a distribution that has mean u and a covariance matrix ? u , and correspondingly, the points vi are assumed to be generated from a distribution that has mean v
and a covariance matrix ?v . For the nonlinear kernel formulation, these points are mapped
into a higher dimensional space ? : <m 7? <h as follows: u 7? ?(u) with corresponding
mean and covariance matrix (?(u), ??(u) ), and v 7? ?(v) with corresponding mean and
covariance matrix (?(v), ??(v) ). The binary classifier derived in [1] has the form (c = ?1
for the first class and c = +1 for the""second):
#
Nu
+Nv
X
c
?i K (zi , z) + bc
c = sign
(6)
i=1

where K c (zi , z) = ?(zi )?(z), zi = ui for i = 1, ..., Nu , zi = vi?Nu for i =
Nu + 1, ..., Nu + Nv , and ? = (?1 , ..., ?Nu +Nv ), bc obtained by solving the following
optimization problem:



 )
(


 K


 K



 ?v 

 ?u 
?u ? k
?v = 1
min 
 ? ? 
 + 
 ? ? 
 s.t.? T k
(7)
?

 Nv 

 Nu 
2
2
?u ; where K
?v ; where k
?v , k
?u ? <Nu +Nv defined
? u = K u ? 1 Nu k
? v = K v ? 1 Nv k
where K
P
P
N
N
v
u
1
1
c
c
?
? v ]i =
as: [k
j=1 K (vj , zi ) and [ku ]i = Nu
j=1 K (uj , zi ); where 1k is a k
Nv
dimensional column vector of ones; where Ku contains the first Nu rows of the Gram
matrix K (i.e. a square matrix consisting of the elements Kij = K c (zi , zj )); and finally
Kv contains the last Nv rows of the Gram matrix K. Given that ? solves the minimization
problem in (7), bc can be calculated
using:
r
r
1 T ?T ?
1 T ?T ?
T?
?v + ?
b c = ? ku ? ?
? Ku Ku ? = ? T k
? Kv Kv ?
(8)
Nu
Nv
where,
r
?1
r
1 T ?T ?
1 T ?T ?
? Ku Ku ? +
? Kv Kv ?
(9)
?=
Nu
Nv
One significant advantage of this framework for binary classification is that, given perfect
knowledge of the statistics u, ?u , v, ?v , the maximum probability of incorrect classification is bounded by 1 ? ?, where ? can be directly calculated from ? as follows:
?2
?=
(10)
1 + ?2
This result is used below to formulate a lower bound on the probability that that the approximated regression function is within ? of the true regression function.
2.2

Kernel Based MPM Regression

In order to use the above MPMC formulation for our proposed MPMR framework, we first
take the original learning data ? and create two classes of points ui ? <d+1 and vi ? <d+1 ,
for i = 1, ..., N , as follows:
ui = (yi + ?, xi1 , xi2 , ..., xid )
(11)
vi = (yi ? ?, xi1 , xi2 , ..., xid )
Given these two sets of points, we obtain ? by minimizing equation (7). Then, from (6),
the MPM classification boundary between points ui and vi is given by
2N
X
?i K c (zi , z) + bc = 0
(12)
i=1

We interpret this classification boundary as a regression surface because it acts to separate
points which are ? above the y values in the learning set ?, and ? below the y values

in ?. Furthermore, given any point x = (x1 , ..., xd ) generated from the distribution ?,
calculating y? the regression model output (5), involves finding a y? that solves equation (12),
where z = (?
y , x1 , ..., xd ), and, recalling from above, zi = ui for i = 1, ..., N , zi = vi?N
for i = N + 1, ..., 2N (note that Nu = Nv = N ). If K c (zi , z) is nonlinear, solving (12)
for y? is in general a nonlinear single variable optimization problem, which can be solved
using a root finding algorithm (for example the Newton-Raphson Method outlined in [4]).
However, below we present a specific form of nonlinear K c (zi , z) that allows (12) to be
solved analytically.
It is interesting to note that the above formulation of a regression model can be derived
using any binary classification algorithm, and is not limited to the MPMC algorithm.
Specifically, if a binary classifier is built to separate any two sets of points (11), then
finding a crossing point y? at where the classifier separates these classes for some input
x = (x1 , ..., xd ), is equivalent to finding the output of the regression model for input
x = (x1 , ..., xd ). It would be interesting to explore the efficacy of various classification algorithms for this type of regression model formulation. However, as formalized in Theorem
1 below, using the MPM framework gives us one clear advantage over other techniques. We
now state the main result of this paper:
Theorem 1: For any x = (x1 , ..., xd ) generated according to the distribution ?, assume
that there exists only one y? that solves equation (12). Assume also perfect knowledge of the
statistics u, ?u , v, ?v . Then, the minimum probability that y? is within ? of y (as defined in
(2)) is given by:
?2
? = inf Pr {|?
y ? y| ? ?} =
(13)
1 + ?2
where ? is defined in (9).
Proof: See Appendix.
Therefore, from the above theorem, the MPMC framework directly computes the lower
bound on the probability that the regression model is within ? of the function that generated
the learning data ? (i.e. the true regression function). However, one key requirement of the
theorem is perfect knowledge of the statistics u, ?u , v, ?v . In the actual implementation of
MPMR, these statistics are estimated from ?, and it is an open question (which we address
in Section 3) as to how accurately ? can be estimated from real data.
In order to avoid the use of nonlinear optimizations techniques to solve (12) for y?, we
restrict the form of the kernel K c (zi , z) to the following:
K c (zi , z) = yi0 y? + K (xi , x)
(14)
where K (xi , x) = ?(xi )?(x) is a kernel satisfying Mercer?s Conditions; where z =
0
(?
y , x1 , ..., xd ); where zi = ui , yi0 = yi +  for i = 1, ..., N ; and where zi = vi?N , yi?N
=
c
yi ?  for i = N + 1, ..., 2N . Given this restriction on K (zi , z), we now state our final
theorem which uses the following lemma:
Lemma 1:
Proof: See Appendix.

k?u ? k?v = 2y0

Theorem 2: Assume that (14) is true. Then all of the following are true:
Part 1: Equation (12) has an analytical solution as defined in (5), where
?i = ?2(?i + ?i+N )
?u = K
?v
Part 2: K

b = ?2bc

(15)

Table 1: Results over 100 random trials for sinc data: mean squared errors and the standard deviation; MPTD?: fraction of test points that are within  = 0.2 of y; predicted ?:
predicted probability that the model is within ? = 0.2 of y.
2

? =0
? 2 = 0.5
? 2 = 1.0

mean (std)
mean (std)
mean (std)

mean squared error
0.0 (0.0)
0.0524 (0.0386)
0.2592 (0.3118)

MPTD?
1.0 (0.0)
0.6888 (0.1133)
0.3870 (0.1110)

predicted ?
1.0 (0.0)
0.1610 (0.0229)
0.0463 (0.0071)

Part 3: The problem of finding an optimal ? in (7) is reduced to solving the following
linear least squares problem for t ? <
2N ?1 :


?

min 
K
u (?o + Ft)
t

2 2

 

2N ?(2N ?1)
?u ? k
?v / 
?
?
where ? = ?o + Ft , ?o = k
k
?
k
is an

 u
v 
 , and F ? <
?u ? k
?v .
orthogonal matrix whose columns span the subspace of vectors orthogonal to k
Proof: See Appendix.
Therefore, Theorem 2 establishes that the MPMR formulation proposed in this paper has a
closed form analytical solution, and its computational complexity is equivalent to solving
a linear system of 2N ? 1 equations in 2N ? 1 unknowns.

3

Experimental Results

For complete implementation details of the MPMR algorithm used in the
following experiments, see the Matlab and C source code available at
http://www.cs.colorado.edu/?grudic/software.
Toy Sinc Data: Our toy example uses the noisy sinc function yi = sin(?xi )/(?xi ) +
?i i = 1, ..., N , where ?i is drawn from a Gaussian distribution with mean 0 and variance
? 2 [5]. We use a RBF kernel K(a, b) = exp(?|a ? b|2 ) and N = 100 training examples.
Figure 1 (a), (b), and (c), and Table 1 show the results for different variances ? 2 and a
constant value of ? = 0.2. Figure 1 (d) and (e) illustrate how different tube sizes 0.05 ?
? ? 2 affect the mean squared error (on 100 random test points), the predicted ? and
measured percentage of test data within ? (here called MPTD?) of the regression model.
Each experiment consists of 100 random trials. The average mean squared error in (e)
has a small deviation (0.0453) over all tested ? and always was within the range 0.19 to
0.35. This indicates that the accuracy of the regression model is essentially independent
from the choice of ?. Also note that the mean predicted ? is a lower bound on the mean
MPTD?. The tightness of this lower bound varies for different amounts of noise (Table 1)
and different choices of ? (Figure 1 d).
Boston Housing Data: We test MPMR on the widely used Boston housing regression
data available from the UCI repository. Following the experiments done in [5], we use the
RBF kernel K(a, b) = exp(?ka ? bk/(2? 2 )), where (2? 2 )) = 0.3 ? d and d = 13 for
this data set. No attempt was made to pick optimal values for ? using cross validation.
The Boston housing data contains 506 training examples, which we randomly divided into
N = 481 training examples and 25 testing examples for each test run. 100 such random
tests where run for each of ? = 0.1, 1.0, 2.0, ..., 10.0. Results are reported in Table 2 for 1)
average mean squared errors and the standard deviation; 2) MPTD?: fraction of test points
that are within  of y and the standard deviation; 3) predicted ?: predicted probability that
the model is within ? of y and standard deviation. We first note that the results compare
favorably to those reported for other state of the art regression algorithms [5], even though

2

1.5

4

3

learning examples
true regression function
MPMR function
MPMR function + ?
MPMR function ? ?

learning examples
true regression function
MPMR function
MPMR function + ?
MPMR function ? ?

2.5

2

learning examples
true regression function
MPMR function
MPMR function + ?
MPMR function ? ?

3

2

1.5

1

1

y

1

0

y

y

0.5

0.5

0

?1
?0.5

0
?2

?1

?3

?2

?1

0

1

2

3

x

?1.5
?3
?2
Percentage of Test Data within ? ?

?1

0

1

2

?3
?3

3

?1

0

1

2

3

x

2

a) ? = 0.2, ? 2 = 0

?2

x

c) ? = 0.2, ? 2 = 1.0

b) ? = 0.2, ? = 0.5
1

0.9

1

Average MSE (100 runs)

Probability

0.8

0.8

MPTD? ? std

0.6

estimated ? ? std

0.4

0.7

0.6

0.5

0.4

0.3

0.2

0.2

0.1

0

0

0

0.2

0.4

0.6

0.8

?

1

1.2

1.4

0

0.2

0.4

0.6

1.6

0.8

?

1

1.2

1.4

1.6

d) MPTD? and predicted ? e) mean squared error on test
data w.r.t. ?, ? 2 = 1.0
w.r.t. ?, ? 2 = 1.0
Figure 1: Experimental results on toy sinc data.

Table 2: Results over 100 random trials for the Boston Housing Data for ? =
0.1, 1.0, 2.0, ..., 10.0: mean squared errors and the standard deviation; MPDT?: fraction
of test points that are within  of y and the standard deviation; predicted ?: predicted
probability that the model is within ? of y and standard deviation.
Average MSE (100 runs)

?
MSE
STD
MPDT?
STD
?
STD

0.1
9.9
5.9
0.05
0.04
0.002
0.0005

1.0
10.5
9.5
0.33
0.09
0.19
0.03

2.0
10.9
8.6
0.58
0.09
0.51
0.06

3.0
9.5
5.9
0.76
0.08
0.69
0.05

4.0
10.3
8.1
0.84
0.07
0.80
0.04

4.0
9.9
8.0
0.89
0.06
0.87
0.03

6.0
10.5
8.5
0.93
0.05
0.90
0.01

7.0
10.5
8.1
0.95
0.04
0.92
0.01

8.0
9.0
10.0
9.2
10.1
10.6
5.3
6.9
7.6
0.97
0.97
0.98
0.03
0.03
0.02
0.94
0.95
0.96
0.009 0.009 0.008

no attempt was made to optimize for ?. Second, as with the toy data, the errors are relatively
independent of ?. Finally, we note that the mean predicted ? is lower than the measured
average MPTD?, thus validating the the MPMR algorithm does indeed predict an effective
lower bound on the probability that the regression model is within ? of the true regression
function.

4

Discussion and Conclusion

We formalize the regression problem as one of maximizing the minimum probability, ?,
that the regression model is within ?? of the true regression function. By estimating mean
and covariance matrix statistics of the regression data (and making no other assumptions
on the underlying true regression function distributions), the proposed minimax probability
machine regression (MPMR) algorithm obtains a direct estimate of ?. Two theorems are
presented proving that, given perfect knowledge of the mean and covariance statistics of the
true regression function, the proposed MPMR algorithm directly computes the exact lower
probability bound ?. We are unaware of any other nonlinear regression model formulation
that has this property.

Experimental results are given showing: 1) the regression models produced are competitive with existing state of the art models; 2) the mean squared error on test data is relatively
independent of the choice of ?; and 3) estimating mean and covariance statistics directly
from the learning data gives accurate lower probability bound ? estimates that the regression model is within ?? of the true regression function - thus supporting our theoretical
results.
Future research will focus on a theoretical analysis of the conditions under which the accuracy of the regression model is independent of ?. Also, we are analyzing the rate, as a
function of sample size, at which estimates of the lower probability bound ? converge to
the true value. Finally, the proposed minimax probability machine regression framework
is a new formulation of the regression problem, and therefore its properties can only be
fully understood through extensive experimentation. We are currently applying MPMR to
a wide variety of regression problems and have made Matlab / C source code available
(http://www.cs.colorado.edu/?grudic/software) for others to do the same.

References
[1] G. R. G. Lanckriet, L. E. Ghaoui, C. Bhattacharyya, and M. I. Jordan. Minimax probability machine. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances
in Neural Information Processing Systems 14, Cambridge, MA, 2002. MIT Press.
[2] A. W. Marshall and I. Olkin. Multivariate chebyshev inequalities. Annals of Mathematical Statistics, 31(4):1001?1014, 1960.
[3] I. Popescu and D. Bertsimas. Optimal inequalities in probability theory: A convex optimization approach. Technical Report TM62, INSEAD, Dept. Math. O.R., Cambridge,
Mass, 2001.
[4] W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. Numerical Recipes
in C. Cambridge University Press, New York NY, 1988.
[5] Bernhard Sch?olkopf, Peter L. Bartlett, Alex J. Smola, and Robert Williamson. Shrinking the tube: A new support vector regression algorithm. In D. A. Cohn M. S. Kearns,
S. A. Solla, editor, Advances in Neural Information Processing Systems, volume 11,
Cambridge, MA, 1999. The MIT Press.
Appendix: Proofs of Theorems 1 and 2
Proof of Theorem 1:
Consider any point x = (x1 , ..., xd ) generated according to the distribution ?. This point
will have a corresponding y (defined in (2)), and from (10), the probability that z +? = (y +
?, x1 , ..., xd ) will be classified correctly (as belonging to class u) by (6) is ?. Furthermore,
the classification boundary occurs uniquely at the point where z = (?
y , x1 , ..., xd ), where,
from the assumptions, y? is the unique solution to (12). Similarly, for the same point y,
the probability that z?? = (y ? ?, x1 , ..., xd ) will be classified correctly (as belonging
to class v) by (6) is also ?, and the classifications boundary occurs uniquely at the point
where z = (?
y , x1 , ..., xd ). Therefore, both z+? = (y + ?, x1 , ..., xd ) and z?? = (y ?
?, x1 , ..., xd ) are, with probability ?, on the correct side of the regression surface, defined
by z = (?
y , x1 , ..., xd ). Therefore, z+? differs from z by at most +? in the first dimension,
and z?? differs from z by at most ?? in the first dimension. Thus, the minimum bound on
the probability that |y ? y?| ? ? is ? (defined in (10)), which has the same form as ?. This
completes the proof.

Proof of Lemma 1:
PN
PN
[k?u ]i ? [k?v ]i = N1 ( l=1 K c (ul , zi )) ? N1 ( l=1 K c (vl , zi )) =
PN
1
1
0
0
0
0
l=1 (yl + )yi + K(xl , xi ) ? ((yl ? )yi + K(xl , xi )) = N N 2yi = 2yi
N
Proof of Theorem 2:

Part 1: Plugging (14) into (12), we get:
2N
P
0=
?i [yi0 y? + K (xi , x)] + bc
0=

0=

i=1
N
P
i=1
N
P

i=1

?i [(yi + ?) y? + K (xi , x)] +

N
P

i=1

?i+N [(yi ? ?) y? + K (xi , x)] + bc

{(?i + ?i+N ) [yi y? + K (xi , x)] + (?i ? ?i+N ) ??
y } + bc

When we solve analytically for y?, giving (5), the coefficients ?i and the offset b have a
N
P
denominator that looks like: ?
[(?i + ?i+N ) yi + (?i ? ?i+N ) ?] = ?? T y0
i=1

1
Applying Lemma 1 and (7) we obtain: 1 = ? T (?(ku ) ? k?v ) = ? T 2y0 ? ?? T y0 = ? 2
for the denominator of ?i and b.

Part 2: The values zi are defined as: z1 = u1 , ..., zN = uN , zN +1 = v1 = u1 ?
T
T
(2, 0, ? ? ? , 0) , ..., z2N = vN = uN ? (2, 0, ? ? ? , 0) . Since K?u = Ku ? 1N k?u we have
the following term for a single matrix entry:
PN
[K?u ]i,j = K c (ui , zj ) ? N1 l=1 K c (ul , zj ) i = 1, .., N j = 1, ..., 2N
Similarly the matrix entries for K?v look like:
PN
[K?v ]i,j = K c (vi , zj ) ? N1 l=1 K c (vl , zj ) i = 1, .., N j = 1, ..., 2N

We show that these entries are the same for all i and j:
PN
T
T
[K?u ]i,j = K c (vi + (2 0 ? ? ? 0) , zj ) ? N1 l=1 K c (vl + (2 0 ? ? ? 0) , zj ) =
PN
K c (vi , zj ) + 2[zj ]1 ? N1 ( l=1 K c (vl , zj ) + 2[zj ]1 ) =
PN
PN
K c (vi , zj ) + 2[zj ]1 ? N1 l=1 K c (vl , zj ) ? N1 l=1 2[zj ]1 =
PN
K c (vi , zj ) + 2[zj ]1 ? N1 l=1 K c (vl , zj ) ? N1 N 2[zj ]1 =
PN
K c (vi , zj ) ? N1 l=1 K c (vl , zj ) = [K?v ]i,j
This completes the proof of Part 2.

Part 3: From Part 2 we know that K?u = K?v . Therefore, the minimization problem
(7) collapses to minkK?u ?k22 with respect to ? (the N is constant and can be removed).
Formulating this minimization with the use of the orthogonal matrix F and an initial vector
?o this becomes (see [1]): minkK?u (?o + Ft)k22 with respect to t ? <2N ?1 . We set
h(t) = kK?u (? + Ft)k22 . Therefore in order to find the minimum we must solve 2N ? 1
linear equations: 0 = dtdi h(t) i = 1, ..., 2N ? 1. This completes the proof of Part 3.

"
1990,Design and Implementation of a High Speed CMAC Neural Network Using Programmable CMOS Logic Cell Arrays,,404-design-and-implementation-of-a-high-speed-cmac-neural-network-using-programmable-cmos-logic-cell-arrays.pdf,Abstract Missing,"Design and Implementation of a High Speed
CMAC Neural Network Using Programmable
CMOS Logic Cell Arrays

W. Thomas Miller, III, Brian A. Box, and Erich C. Whitney
Department of Electrical and Computer Engineering
Kingsbury Hall
University of New Hampshire
Durham, New Hampshire 03824
James M. Glynn
Shenandoah Systems Company
1A Newington Park
West Park Drive
Newington, New Hampshire 03801

Abstract
A high speed implementation of the CMAC neural network was designed
using dedicated CMOS logic. This technology was then used to implement
two general purpose CMAC associative memory boards for the VME bus.
Each board implements up to 8 independent CMAC networks with a total
of one million adjustable weights. Each CMAC network can be configured
to have from 1 to 512 integer inputs and from 1 to 8 integer outputs.
Response times for typical CMAC networks are well below 1 millisecond,
making the networks sufficiently fast for most robot control problems, and
many pattern recognition and signal processing problems.

1

INTRODUCTION

We have been investigating learning techniques for the control of robotic manipulators which utilize extensions of the CMAC neural network as developed by Albus

1022

Design and Implementation of a High Speed CMAC Neural Network
(1972; 1975; 1979). The learning control techniques proposed have been studied
in our laboratory in a series of real time experimental studies (Miller, 1986; 1987;
1989; Miller et al., 1987; 1988; 1990). These studies successfully demonstrated the
ability to learn the kinematics of a robot/video camera system interacting with
randomly oriented objects on a moving conveyor, and to learn the dynamics of a
multi-axis industrial robot during high speed motions. We have also investigated
the use of CMAC networks for pattern recognition (Glanz and Miller, 1987; Herold
et al., 1988) and signal processing (Glanz and Miller, 1989) applications, with encouraging results. The primary goal of this project was to implement a compact,
high speed version of the CMAC neural network using CMOS logic cell arrays. Two
prototype CMAC associative memory systems for the industry standard VME bus
were then constructed.

2

THE CMAC NEURAL NETWORK

Figure 1 shows a simple example of a CMAC network with two inputs and one
output. Each variable in the input state vector is fed to a series of input sensors
with overlapping receptive fields. The width of the receptive field of each sensor
produces input generalization, while the offset of the adjacent fields produces input
quantization. The binary outputs of the input sensors are combined in a series of
threshold logic units (called state space detectors) with thresholds adjusted to produce logical AND functions. Each of these units receives one input from the group
of sensors for each input variable, and thus its input receptive field is the interior
of a hypercube in the input hyperspace. The input sensors are interconnected in
a sparse and regular fashion, so that each input vector excites a fixed number of
state space detectors. The outputs of the state space detectors are connected randomly to a smaller set of threshold logic units (called multiple field detectors) with
thresholds adjusted such that the output will be on if any input is on. The receptive
field of each of these units is thus the union of the fields of many of the state space
detectors. Finally, the output of each multiple field detector is connected, through
an adjustable weight, to an output summing unit. The output for a given input is
thus the sum of the weights selected by the excited multiple field detectors.
The nonlinear nature of the CMAC network is embodied in the interconnections of
the input sensors, state space detectors, and multiple field detectors, which perform
a fixed nonlinear associative mapping of the continuous valued input vector to a
many dimensional binary valued vector (which has tens or hundreds of thousands
of dimensions in typical implementations). The adaptation problem is linear in this
many dimensional space, and all of the convergence theorems for linear adaptive
elements apply.

3

THE CMAC HARDWARE DESIGN

The custom implementation of the CMAC associative memory required the development of two devices. The first device performs the input associative mapping,
converting application relevant input vectors into traditional RAM addresses. The
second device performs CMAC response accumulation, summing the weights from
all excited receptive fields. Both devices were implemented using 70 MHz XILINX

1023

1024

Miller, Box, Whitney, and Glynn

Input
Sensors

State Space
Detectors

.?

Weights

t

E

'S
Q.
C

f(?)
C'oI

'S
Q.
C

C>

110 Total
Units

c

Ii

o

c>

Multiple Field
Detectors

Logical AND unit
Logical OR unit

Figure 1: A Simple Example of a CMAC Neural Network

3090 programmable logic cell arrays.
The associative mapping device uses a bit recursive mapping scheme developed at
UNH, which is similar in philosophy to the CMAC mapping proposed by Albus,
but is structured for efficient implementation using discrete logic. The"" address"" of
each excited virtual receptive field is formed recursively by clocking the input vector
components sequentially from a buffer FIFO. The hashing of the virtual receptive
field address to a physical RAM address is performed simultaneously, using pipelined
logic. The resulting associative mapping generates one 18 bit RAM address for a
given input vector. The multiple addresses, corresponding to the multiple receptive
fields excited by a single input vector could be generated simultaneously using
parallel addressing circuits, or sequentially using a single circuit.
The second CMAC device serves basically as an accumulator during CMAC response
generation. As successive addresses are produced by the associative mapping circuit,
the accumulator sums the corresponding values from the data RAM. During memory
training, the response accumulation circuit adds the training adjustment to each
of the addressed memory locations, placing the result back in the RAM. Eight
independent CMAC output channels were placed on a single device.
In the final VME system design (Figure 2), a single CMAC associative mapping
device was used. Overlapping receptive fields were implemented sequentially using
the same device. A single CMAC response accumulation device was used, providing
eight parallel output channels. A weight vector memory containing 1 million 8
bit weights was provided using 85 nanosecond 512 KByte static RAM SIMMs.
A TMS320E15 micro controller was utilized to supervise communications with the
VME bus. The operational firmware for the micro controller chip was designed to

Design and Implementation of a High Speed CMAC Neural Network

CMAC Associative Mapping

CMAC Output Accumulators

VME PI Connector

Figure 2: The Component Side of the VME Based CMAC Associative 1\lemory
Card. The two large XILINX 3090 logic cell arrays implement the CMAC associative
mapping and the response accumulation/weight adjustment circuitry. The weights
are stored in the 1 Mbyte static RAM. The TMS320E15 microcontroller supervises
communications between the CMAC hardware and the VME host.
provide maximum flexibility in the logical organization of the CMAC associative
memory, as viewed by the VME host system. The board can be initialized to act as
from 1 to 8 independent virtual CMAC networks. For each network, the number of
16 bit inputs is selectable from 1 to 512, the number of 16 bit outputs is selectable
from 1 to 8, and the number of overlapping receptive fields is selectable from 2 to
256.
Figure 3 shows typical response times during training and response generation operations for a CMAC network with 1 million adjustable weights. The data shown
represent networks with 32 integer inputs and 8 integer outputs, with the number of overlapping receptive fields varied between 8 and 256. Throughout most of
this range CMAC training and response times are well below 1 millisecond. These
performance specifications should accommodate typical real time control problems
(allowing 1000 cycle per second control rates), as well as many problems in pattern
recognition.
A similar CMAC system for the 16 bit PC-AT bus has been developed by the
Shenandoah Systems Company for commercial applications. This CMAC system
supports both 8 and 16 bit adjustable weights (1 Mbyte total storage), and 8 independent virtual CMAC networks on a single card. Response times for the commercial CMAC-AT card are similar to those shown in Figure 3. A commercial version

1025

1026

Miller, Box, Whitney, and Glynn

???T??T?rTT???????????????c?i1~c?????~?fM?~n?~????p?~?~?~?T??????????????????????r???????????j

????+????j????i???j???j??????? 32 Inputs ?

a (kitputs' ?

1'ltiilian '!Ie'ights .................+............. j

! II ! I 1~lliJc.nd I I IIIII1

I I

-...
I

I
11+81

11+82

Figure 3: CMAC Associative Memory Response and Training Times. Response
times are shown for values of the generalization parameter (the number of overlapping receptive fields) between 8 and 256. In each case the CMAC had 32 integel'
inputs, 8 integer outputs, and one million adjustable weights.
of the VME bus design is currently under development .
Acknow ledgements

This work was sponsored in part by the Office of Naval Research (ONR Grant
Number N00014-89-J-1686) and the National Institute of Standards and Technology.
References

Albus, J. S., Theoretical and Experimental Aspects of a Cerebellar Model. PhD
Thesis, University of Maryland, Dec . 1972.
Albus, J. S., A New Approach to Manipulator Control: The Cerebellar Model Articulation Controller (CMAC). Trans. of the ASME, Journal of Dynamic Systems,
Measurement and Control, vol. 97, pp. 220-227, September, 1975.
Albus, J. S., Mechanisms of Planning and Problem Solving in the Brain. Mathematical Biosciences, vol. 45, pp. 247-293, August, 1979.
Miller, W. T., A Nonlinear Learning Controller for Robotic Manipulators. Proc. of
the SPIE: Intelligent Robots and Computer Vision, vol 726, pp . 416-423, October,
1986.

Design and Implementation of a High Speed CMAC Neural Network
Miller, W. T., Sensor Based Control of Robotic Manipulators Using A General
Learning Algorithm. IEEE J. of Robotics and Automation, vol. RA-3, pp. 157165, April, 1987.
Miller, W. T., Glanz, F. H., and Kraft, 1. G., Application of a General Learning
Algorithm to the Control of Robotic Manipulators. The International Journal of
Robotics Research, vol. 6.2, pp. 84-98, Summer, 1987.
Miller, W.T., and Hewes, R.P., Real Time Experiments in Neural Network Based
Learning Control During High Speed, Nonrepetitive Robot Operations. Proceedings
of the Third IEEE International Symposium on Intelligent Control, Washington,
D.C., August 24-26, 1988.
Miller, W, T., Real Time Application of Neural Networks for Sensor-Based Control
of Robots with Vision. IEEE Transactions on Systems, Man, and Cybernetics.
Special issue on Information Technology for Sensory-Based Robot Manipulators,
vol. 19, pp. 825-831, 1989.
Miller, W. T., Hewes, R. P., Glanz, F. H., and Kraft, 1. G., Real Time Dynamic
Control of an Industrial Manipulator Using a Neural Network Based Learning Controller. IEEE J. of Robotics and Automation vol. 6, pp. 1-9, 1990.
Glanz, F. H., Miller, W. T., Shape Recognition Using a CMAC Based Learning
System. Proceedings SPIE: Intelligent Robots and Computer Vision, Cambridge,
Mass., Nov., 1987.
Herold, D. J., Miller, W. T., Kraft, L. G., and Glanz, F. H., Pattern Recognition
Using a CMAC Based Learning System. Proceedings SPIE: Automated Inspection
and High Speed Vision Architectures II, vol. 1004, pp. 84-90, 1988.
Glanz, F. H., and Miller, W. T., Deconvolution and Nonlinear Inverse Filtering
Using a Neural Network. Proc. ICASSP 89, Glasgow, Scotland, May 23-26, 1989,
vol. 4, pp. 2349-2352.

1027

"
2003,Kernel Dimensionality Reduction for Supervised Learning,,2513-kernel-dimensionality-reduction-for-supervised-learning.pdf,Abstract Missing,"Kernel Dimensionality Reduction for Supervised
Learning

Kenji Fukumizu
Institute of Statistical
Mathematics
Tokyo 106-8569 Japan
fukumizu@ism.ac.jp

Francis R. Bach
CS Division
University of California
Berkeley, CA 94720, USA
fbach@cs.berkeley.edu

Michael I. Jordan
CS Division and Statistics
University of California
Berkeley, CA 94720, USA
jordan@cs.berkeley.edu

Abstract
We propose a novel method of dimensionality reduction for supervised
learning. Given a regression or classification problem in which we wish
to predict a variable Y from an explanatory vector X, we treat the problem of dimensionality reduction as that of finding a low-dimensional ?effective subspace? of X which retains the statistical relationship between
X and Y . We show that this problem can be formulated in terms of
conditional independence. To turn this formulation into an optimization
problem, we characterize the notion of conditional independence using
covariance operators on reproducing kernel Hilbert spaces; this allows us
to derive a contrast function for estimation of the effective subspace. Unlike many conventional methods, the proposed method requires neither
assumptions on the marginal distribution of X, nor a parametric model
of the conditional distribution of Y .

1

Introduction

Many statistical learning problems involve some form of dimensionality reduction. The
goal may be one of feature selection, in which we aim to find linear or nonlinear combinations of the original set of variables, or one of variable selection, in which we wish to select
a subset of variables from the original set. Motivations for such dimensionality reduction
include providing a simplified explanation and visualization for a human, suppressing noise
so as to make a better prediction or decision, or reducing the computational burden.
We study dimensionality reduction for supervised learning, in which the data consists of
(X, Y ) pairs, where X is an m-dimensional explanatory variable and Y is an -dimensional
response. The variable Y may be either continuous or discrete. We refer to these problems
generically as ?regression,? which indicates our focus on the conditional probability density
pY |X (y|x). Thus, our framework includes classification problems, where Y is discrete.
We wish to solve a problem of feature selection in which the features are linear combinations of the components of X. In particular, we assume that there is an r-dimensional
subspace S ? Rm such that the following equality holds for all x and y:
(1)
pY |X (y|x) = pY |?S X (y|?S x),
where ?S is the orthogonal projection of Rm onto S. The subspace S is called the effective subspace for regression. Based on observations of (X, Y ) pairs, we wish to re-

cover a matrix whose columns span S. We approach the problem within a semiparametric statistical framework?we make no assumptions regarding the conditional distribution
pY |?S X (y|?S x) or the distribution pX (x) of X. Having found an effective subspace, we
may then proceed to build a parametric or nonparametric regression model on that subspace. Thus our approach is an explicit dimensionality reduction method for supervised
learning that does not require any particular form of regression model; it can be used as a
preprocessor for any supervised learner.
Most conventional approaches to dimensionality reduction make specific assumptions regarding the conditional distribution pY |?S X (y|?S x), the marginal distribution pX (x), or
both. For example, classical two-layer neural networks can be seen as attempting to estimate an effective subspace in their first layer, using a specific model for the regressor.
Similar comments apply to projection pursuit regression [1] and ACE [2], which assume
T
an additive model E[Y |X] = g1 (?1T X) + ? ? ? + gK (?K
X). While canonical correlation
analysis (CCA) and partial least squares (PLS, [3]) can be used for dimensionality reduction in regression, they make a linearity assumption and place strong restrictions on the
allowed dimensionality. The line of research that is closest to our work is sliced inverse regression (SIR, [4]) and related methods including principal Hessian directions (pHd, [5]).
SIR is a semiparametric method that can find effective subspaces, but only under strong
assumptions of ellipticity for the marginal distribution pX (x). pHd also places strong restrictions on pX (x). If these assumptions do not hold, there is no guarantee of finding the
effective subspace.
In this paper we present a novel semiparametric method for dimensionality reduction that
we refer to as Kernel Dimensionality Reduction (KDR). KDR is based on a particular class
of operators on reproducing kernel Hilbert spaces (RKHS, [6]). In distinction to algorithms
such as the support vector machine and kernel PCA [7, 8], KDR cannot be viewed as a ?kernelization? of an underlying linear algorithm. Rather, we relate dimensionality reduction
to conditional independence of variables, and use RKHSs to provide characterizations of
conditional independence and thereby design objective functions for optimization. This
builds on the earlier work of [9], who used RKHSs to characterize marginal independence
of variables. Our characterization of conditional independence is a significant extension,
requiring rather different mathematical tools?the covariance operators on RKHSs that we
present in Section 2.2.

2
2.1

Kernel method of dimensionality reduction for regression
Dimensionality reduction and conditional independence

The problem discussed in this paper is to find the effective subspace S defined by Eq. (1),
given an i.i.d. sample {(Xi , Yi )}ni=1 , sampled from the conditional probability Eq. (1) and
a marginal probability pX for X. The crux of the problem is that we have no a priori
knowledge of the regressor, and place no assumptions on the conditional probability pY |X
or the marginal probability pX .
We do not address the problem of choosing the dimensionality r in this paper?in practical
applications of KDR any of a variety of model selection methods such as cross-validation
can be reasonably considered. Rather our focus is on the problem of finding the effective
subspace for a given choice of dimensionality.
The notion of effective subspace can be formulated in terms of conditional independence.
Let Q = (B, C) be an m-dimensional orthogonal matrix such that the column vectors of
B span the subspace S (thus B is m ? r and C is m ? (m ? r)), and define U = B T X
and V = C T X. Because Q is an orthogonal matrix, we have pX (x) = pU,V (u, v) and
pX,Y (x, y) = pU,V,Y (u, v, y). Thus, Eq. (1) is equivalent to
pY |U,V (y|u, v) = pY |U (y|u).

(2)

Y

Y

Y
X

V

U

X

V |U
X = (U,V)

Figure 1: Graphical representation of dimensionality reduction for regression.

This shows that the effective subspace S is the one which makes Y and V conditionally
independent given U (see Figure 1).
Mutual information provides another viewpoint on the equivalence between conditional
independence and the effective subspace. It is well known that


(3)
I(Y, X) = I(Y, U ) + EU I(Y |U, V |U ) ,
where I(Z, W ) is the mutual information between Z and W . Because Eq. (1) implies
I(Y, X) = I(Y, U ), the effective subspace S is characterized as the subspace which retains
the entire mutual information between X and Y , or equivalently, such that I(Y |U, V |U ) =
0. This is again the conditional independence of Y and V given U .
2.2

Covariance operators on kernel Hilbert spaces and conditional independence

We use cross-covariance operators [10] on RKHSs to characterize the conditional independence of random variables. Let (H, k) be a (real) reproducing kernel Hilbert space of
functions on a set ? with a positive definite kernel k : ? ? ? ? R and an inner product
?, ?H . The most important aspect of a RKHS is the reproducing property:
f, k(?, x)H = f (x)

for all x ? ? and f ? H.


In this paper we focus on the Gaussian kernel k(x1 , x2 ) = exp ?x1 ? x2 2 /2? 2 .

(4)

Let (H1 , k1 ) and (H2 , k2 ) be RKHSs over measurable spaces (?1 , B1 ) and (?2 , B2 ), respectively, with k1 and k2 measurable. For a random vector (X, Y ) on ?1 ? ?2 , the
cross-covariance operator ?Y X from H1 to H2 is defined by the relation
g, ?Y X f H2 = EXY [f (X)g(Y )] ? EX [f (X)]EY [g(Y )]

(= Cov[f (X), g(Y )]) (5)

for all f ? H1 and g ? H2 . Eq. (5) implies that the covariance of f (X) and g(Y ) is given
by the action of the linear operator ?Y X and the inner product. Under the assumption that
EX [k1 (X, X)] and EY [k2 (Y, Y )] are finite, by using Riesz?s representation theorem, it is
not difficult to see that a bounded operator ?Y X is uniquely defined by Eq. (5). We have
??Y X = ?XY , where A? denotes the adjoint of A. From Eq. (5), we see that ?Y X captures
all of the nonlinear correlations defined by the functions in HX and HY .
Cross-covariance operators provide a useful framework for discussing conditional probability and conditional independence, as shown by the following theorem and its corollary1 :
Theorem 1. Let (H1 , k1 ) and (H2 , k2 ) be RKHSs on measurable spaces ?1 and ?2 , respectively, with k1 and k2 measurable, and (X, Y ) be a random vector on ?1 ??2 . Assume
that EX [k1 (X, X)] and EY [k2 (Y, Y )] are finite, and for all g ? H2 the conditional expectation EY |X [g(Y ) | X = ?] is an element of H1 . Then, for all g ? H2 we have
?XX EY |X [g(Y ) | X = ?] = ?XY g.
1

Full proofs of all theorems can be found in [11].

(6)

? ?1 be the right inverse of ?XX on (Ker?XX )? . Under the same
Corollary 2. Let ?
XX
assumptions as Theorem 1, we have, for all f ? (Ker?XX )? and g ? H2 ,
? ?1 ?XY gH = f, EY |X [g(Y ) | X = ?]H .
f, ?
1
1
XX
1/2

(7)

1/2

Sketch of the proof. ?XY can be decomposed as ?XY = ?XX V ?Y Y for a bounded oper? ?1 ?XY is well-defined, because Range?XY ?
ator V (Theorem 1, [10]). Thus, we see ?
XX
?
Range?XX = (Ker?XX ) . Then, Eq. (7) is a direct consequence of Theorem 1.
Given that ?XX is invertible, Eq. (7) implies
EY |X [g(Y ) | X = ?] = ??1
XX ?XY g

for all g ? H2 .

(8)

This can be understood by analogy to the conditional expectation of Gaussian random
variables. If X and Y are Gaussian random variables, it is well-known that the conditional
expectation is given by EY |X [aT Y | X = x] = xT ??1
XX ?XY a for an arbitrary vector a,
where ?XX and ?XY are the variance-covariance matrices in the ordinary sense.
Using cross-covariance operators, we derive an objective function for characterizing conditional independence. Let (H1 , k1 ) and (H2 , k2 ) be RKHSs on measurable spaces ?1
and ?2 , respectively, with k1 and k2 measurable, and suppose we have random variables
U ? H1 and Y ? H2 . We define the conditional covariance operator ?Y Y |U on H1 by
? ?1 ?U Y .
?Y Y |U := ?Y Y ? ?Y U ?
UU

(9)

Corollary 2 easily yields the following result on the conditional covariance of variables:
Theorem 3. Assume that EX [k1 (X, X)] and EY [k2 (Y, Y )] are finite, and that
EY |X [f (Y )|X] is an element of H1 for all f ? H2 . Then, for all f, g ? H2 , we have


g, ?Y Y |U f H2 = EY [f (Y )g(Y )] ? EU EY |U [f (Y )|U ]EY |U [g(Y )|U ]



(10)
= EU CovY |U f (Y ), g(Y ) | U .
As in the case of Eq. (8), Eqs. (9) and (10) can be viewed as the analogs of the well-known
equality for Gaussian variables: Cov[aT Y, bT Y |U ] = aT (?Y Y ? ?Y U ??1
U U ?U Y )b.
From Theorem 3, it is natural to use minimization of ?Y Y |U as a basis of a method for
finding the most informative U , which gives the least VarY |U [f (Y )|U ]. The following
definition is needed to justify this intuition. Let (?, B) be a measurable space, let (H, k) be
a RKHS over ? with k measurable and bounded, and let M be the set of all the probability
measures on (?, B). The RKHS H is called probability-determining, if the map
MP

?

(f ? EX?P [f (X)]) ? H?

(11)

?

is one-to-one, where H is the dual space of H. The following theorem can be proved
using a argument similar to that used in the proof of Theorem 2 in [9].
Theorem 4. For an arbitrary ? > 0, the RKHS with Gaussian kernel k(x, y) = exp(?x?
y2 /2? 2 ) on Rm is probability-determining.
Recall that for two RKHSs H1 and H2 on ?1 and ?2 , respectively, the direct product
H1 ?H2 is the RKHS on ?1 ??2 with the kernel k1 k2 [6]. The relation between conditional
independence and the conditional covariance operator is given by the following theorem:
Theorem 5. Let (H11 , k11 ), (H12 , k12 ), and (H2 , k2 ) be RKHSs on measurable spaces
?11 , ?12 , and ?2 , respectively, with continuous and bounded kernels. Let (X, Y ) =
(U, V, Y ) be a random vector on ?11 ? ?12 ? ?2 , where X = (U, V ), and let H1 =
H11 ? H12 be the direct product. It is assumed that EY |U [g(Y )|U = ?] ? H11 and
EY |X [g(Y )|X = ?] ? H1 for all g ? H2 . Then, we have
?Y Y |U ? ?Y Y |X ,

(12)

where the inequality refers to the order of self-adjoint operators. If further H2 is
probability-determining, in particular, for Gaussian kernels, we have the equivalence:
?Y Y |X = ?Y Y |U

??

Y?
?V | U.

(13)

Sketch of the proof. Taking the
 of the well-known equality VarY |U [g(Y )|U ] =
 expectation
EV |U Var
EY |U,V [g(Y
[g(Y
)|U,
V
]
+
Var
Y
|U,V
V
|U



 )|U, V ] with respect to U , we
 obtain EU VarY |U [g(Y )|U ] ?EX VarY |X [g(Y )|X] = EU VarV |U [EY |X [g(Y )|X]] ? 0,
which implies Eq. (12). The equality holds iff EY |X [g(Y )|X] = EY |U [g(Y )|U ] for a.e. X.
Since H2 is probability-determining, this means PY |X = PY |U , that is, Y ?
?V | U .
From Theorem 5, for probability-determining kernel spaces, the effective subspace S can
be characterized in terms of the solution to the following minimization problem:
min ?Y Y |U ,
S

2.3

subject to

U = ?S X.

(14)

Kernel generalized variance for dimensionality reduction

To derive a sampled-based objective function from Eq. (14) for a finite sample, we have to
estimate the conditional covariance operator with given data, and choose a specific way to
evaluate the size of self-adjoint operators. Hereafter, we consider only Gaussian kernels,
which are appropriate for both continuous and discrete variables.
For the estimation of the operator, we follow the procedure in [9] (see also [11] for further
details), and use the centralized Gram matrix [9, 8], which is defined as:
 

 



? Y = In ? 1 1n 1T GY In ? 1 1n 1T , K
? U = In ? 1 1n 1T GU In ? 1 1n 1T (15)
K
n
n
n
n
n
n
n
n
where 1n = (1, . . . , 1)T , (GY )ij = k1 (Yi , Yj ) is the Gram matrix of the samples of Y ,
and (GU )ij = k2 (Ui , Uj ) is given by the projection Ui = B T Xi . With a regularization
? Y Y |U is then defined by
constant ? > 0, the empirical conditional covariance matrix ?
2
?2 ? ?
?Y Y ??
?Y U?
?
? ? ?
? Y Y |U := ?
? ?1 ?
?
?
KU KY . (16)
U U U Y = (KY + ?In ) ? KY KU (KU + ?In )

? Y Y |U in the ordered set of positive definite matrices can be evaluated by its
The size of ?
? Y Y |U , such as
determinant. Although there are other choices for measuring the size of ?
the trace and the largest eigenvalue, we focus onthe determinant
in this paper. Using the

A B
?1 T
Schur decomposition, det(A ? BC B ) = det B T C /detC, we have
? [Y U ][Y U ] / det ?
? UU ,
? Y Y |U = det ?
(17)
det ?

 ?


2
? [Y U ][Y U ] = ?? Y Y ?? Y U = (KY +?In ) K? Y K? U 2 .
? [Y U ][Y U ] is defined by ?
where ?
? U +?In )
? UY ?
? UU
?U K
?Y
(K
?
K
? Y Y , which yields
We symmetrize the objective function by dividing by the constant det ?
min
m?r

B?R

? [Y U ][Y U ]
det ?
,
? Y Y det ?
? UU
det ?

where U = B T X.

(18)

We refer to this minimization problem with respect to the choice of subspace S or matrix
B as Kernel Dimensionality Reduction (KDR).
Eq. (18) has been termed the ?kernel generalized variance? (KGV) by Bach and Jordan [9].
They used it as a contrast function for independent component analysis, in which the goal
is to minimize a mutual information. They showed that KGV is in fact an approximation
of the mutual information among the recovered sources around the factorized distributions.
In the current setting, on the other hand, our goal is to maximize the mutual information

R(b1 )
R(b2 )

SIR(10)
0.987
0.421

SIR(15)
0.993
0.705

SIR(20)
0.988
0.480

SIR(25)
0.990
0.526

pHd
0.110
0.859

KDR
0.999
0.984

Table 1: Correlation coefficients. SIR(m) indicates the SIR method with m slices.

I(Y, U ), and with an entirely different argument, we have shown that KGV is an appropriate objective function for the dimensionality reduction problem, and that minimizing
Eq. (18) can be viewed as maximizing the mutual information I(Y, U ).
Given that the numerical task that must be solved in KDR is the same as the one to be
solved in kernel ICA, we can import all of the computational techniques developed in [9]
for minimizing KGV. In particular, the optimization routine that we use is gradient descent
with a line search, where we exploit incomplete Cholesky decomposition to reduce the
n ? n matrices to low-rank approximations. To cope with local optima, we make use of an
annealing technique, in which the scale parameter ? for the Gaussian kernel is decreased
gradually during the iterations of optimization. For a larger ?, the contrast function has
fewer local optima, and the search becomes more accurate as ? is decreased.

3

Experimental results

We illustrate the effectiveness of the proposed KDR method through experiments, comparing it with several conventional methods: SIR, pHd, CCA, and PLS.
The first data set is a synthesized one with 300 samples of 17 dimensional X and one
dimensional Y , which are generated by Y ? 0.9X1 + 0.2/(1 + X17 ) + Z, where Z ?
N (0, 0.012 ) and X follows a uniform distribution on [0, 1]17 . The effective subspace is
given by b1 = (1, 0, . . . , 0) and b2 = (0, . . . , 0, 1). We compare the KDR method with
SIR and pHd only?CCA and PLS cannot find a 2-dimensional subspace, because Y is onedimensional. To evaluate estimation accuracy, we use the multiple correlation coefficient
R(b) = max??S ? T ?XX b/(? T ?XX ? ? b T ?XX b)1/2 , which is used in [4]. As shown
in Table 1, KDR outperforms the others in finding the weak contribution of b2 .
Next, we apply the KDR method to classification problems, for which many conventional
methods of dimensionality reduction are not suitable. In particular, SIR requires the dimensionality of the effective subspace to be less than the number of classes, because SIR uses
the average of X in slices along the variable Y . CCA and PLS have a similar limitation
on the dimensionality of the effective subspace. Thus we compare the result of KDR only
with pHd, which is applicable to general binary classification problems.
We show the visualization capability of the dimensionality reduction methods for the Wine
dataset from the UCI repository to see how the projection onto a low-dimensional space realizes an effective description of data. The Wine data consists of 178 samples with 13 variables and a label with three classes. Figure 2 shows the projection onto the 2-dimensional
subspace estimated by each method. KDR separates the data into three classes most completely. We can see that the data are nonlinearly separable in the two-dimensional space.
In the third experiment, we investigate how much information on the classification is preserved in the estimated subspace. After reducing the dimensionality, we use the support
vector machine (SVM) method to build a classifier in the reduced space, and compare its
accuracy with an SVM trained using the full-dimensional vector X. We use three data sets
from the UCI repository. Figure 3 shows the classification rates for the test set for subspaces of various dimensionality. We can see that KDR yields good classification even in
low-dimensional subspaces, while pHd is much worse in small dimensionality. It is noteworthy that in the Ionosphere data set the classifier in dimensions 5, 10, and 20 outperforms

20

20

KDR

20

CCA

15

15

15

10

10

10

5

5

5

0

0

0

-5

-5

-5

-10

-10

-10

-15

-15

-15

-20

-20

-20

-15

-10

-5

0

5

20

10

15

PLS

-20

20

-20

-15

-10

-5

0

5

10

15

20

20

SIR

15

15

10

10

5

5

0

0

-5

-5

-10

-10

-15

-15

-20

-20

-15

-10

-5

0

5

10

15

20

pHd

-20

-20

-15

-10

-5

0

5

10

15

20

-20

-15

-10

-5

0

5

10

15

20

Figure 2: Projections of Wine data: ?+?, ???, and gray ?? represent the three classes.

the classifier in the full-dimensional space. This is caused by suppressing noise irrelevant
to explain Y . These results show that KDR successfully finds an effective subspace which
preserves the class information even when the dimensionality is reduced significantly.

4

Extension to variable selection

The KDR method can be extended to variable selection, in which a subset of given explanatory variables {X1 , . . . , Xm } is selected. Extension of the KGV objective function
to variable selection is straightforward. We have only to compare the KGV values for all
the subspaces spanned by combinations of a fixed number of selected variables. We of
course do not avoid the combinatorial problem of variable selection; the total number of
combinations may be intractably large for a large number of explanatory variables m, and
greedy or random search procedures are needed.
We first apply this kernel method to the Boston Housing data (506 samples with 13 dimensional X), which has been used as a typical example of variable selection. We select
four variables that attain the smallest KGV value among all the combinations. The selected
variables are exactly the same as the ones selected by ACE [2]. Next, we apply the method
to the leukemia microarray data of 7129 dimensions ([12]). We select 50 effective genes
to classify two types of leukemia using 38 training samples. For optimization of the KGV
value, we use a greedy algorithm, in which new variables are selected one by one, and
subsequently a variant of genetic algorithm is used. Half of the 50 genes accord with 50
genes selected by [12]. With the genes selected by our method, the same classifier as that
used in [12] classifies correctly 32 of the 34 test samples, for which, with their 50 genes,
Golub et al. ([12]) report a result of classifying 29 of the 34 samples correctly.

5

Conclusion

We have presented KDR, a novel method of dimensionality reduction for supervised learning. One of the striking properties of this method is its generality. We do not place any
strong assumptions on either the conditional or the marginal distribution, in distinction to

(a) Heart-disease

(b) Ionosphere

(c) Wisconsin Breast Cancer

100

100

80
75
70
65
60
Kernel
PHD
All variables

55
50

3

5
7
9
11
Dimensionality

13

Kernel
PHD
All variables

98

Classification rate (%)

Classification rate (%)

Classification rate (%)

85

96
94
92
90
88

3 5

10 15 20
Dimensionality

34

95
90
85
80

Kernel
PHD
All variables

75
70
0

10
20
Dimensionality

30

Figure 3: Classification accuracy of the SVM for test data after dimensionality reduction.

essentially all existing methods for dimensionality reduction in regression, including SIR,
pHd, CCA, and PPR. We have demonstrating promising empirical performance of KDR,
showing its practical utility in data visualization and feature selection for prediction. We
have also discussed an extension of KDR method to variable selection.
The theoretical basis of KDR lies in the nonparametric characterization of conditional independence that we have presented in this paper. Extending earlier work on the kernel-based
characterization of marginal independence [9], we have shown that conditional independence can be characterized in terms of covariance operators on a kernel Hilbert space.
While our focus has been on the problem of dimensionality reduction, it is also worth noting that there are many possible other applications of this result. In particular, conditional
independence plays an important role in the structural definition of graphical models, and
our result may have implications for model selection and inference in graphical models.

References
[1] Friedman, J.H. and Stuetzle, W. Projection pursuit regression. J. Amer. Stat. Assoc., 76:817?
823, 1981.
[2] Breiman, L. and Friedman, J.H. Estimating optimal transformations for multiple regression and
correlation. J. Amer. Stat. Assoc., 80:580?598, 1985.
[3] Wold, H. Partial least squares. in S. Kotz and N.L. Johnson (Eds.), Encyclopedia of Statistical
Sciences, Vol. 6, Wiley, New York. pp.581?591. 1985.
[4] Li, K.-C. Sliced inverse regression for dimension reduction (with discussion). J. Amer. Stat.
Assoc., 86:316?342, 1991.
[5] Li, K.-C. On principal Hessian directions for data visualization and dimension reduction: Another application of Stein?s lemma. J. Amer. Stat. Assoc., 87:1025?1039, 1992.
[6] Aronszajn, N. Theory of reproducing kernels. Trans. Amer. Math. Soc., 69(3):337?404, 1950.
[7] Sch?olkopf, B., Burges, C.J.C., and Smola, A. (eds.) Advances in Kernel Methods: Support
Vector Learning. MIT Press. 1999.
[8] Sch?olkopf, B., Smola, A and M?uller, K.-R. Nonlinear component analysis as a kernel eigenvalue
problem. Neural Computation, 10:1299?1319, 1998.
[9] Bach, F.R. and Jordan, M.I. Kernel independent component analysis. JMLR, 3:1?48, 2002.
[10] Baker, C.R. Joint measures and cross-covariance operators. Trans. Amer. Math. Soc., 186:273?
289, 1973.
[11] Fukumizu, K., Bach, F.R. and Jordan, M.I. Dimensionality reduction for supervised learning
with reproducing kernel Hilbert spaces. JMLR, 5:73?99, 2004.
[12] Golub T.R. et al. Molecular classification of cancer: Class discovery and class prediction by
gene expression monitoring. Science, 286:531?537, 1999.

"
2015,Expectation Particle Belief Propagation,Poster,5674-expectation-particle-belief-propagation.pdf,We propose an original particle-based implementation of the Loopy Belief Propagation (LPB) algorithm for pairwise Markov Random Fields (MRF) on a continuous state space. The algorithm constructs adaptively efficient proposal distributions approximating the local beliefs  at each note of the MRF. This is achieved by considering proposal distributions in the exponential family whose parameters are updated iterately in an Expectation Propagation (EP) framework. The proposed particle scheme provides consistent estimation of the LBP marginals as the number of particles increases. We demonstrate that it provides more accurate results than the Particle Belief Propagation (PBP) algorithm of Ihler and McAllester (2009) at a fraction of the computational cost and is additionally more robust empirically. The computational complexity of our algorithm at each iteration is quadratic in the number of particles. We also propose an accelerated implementation with sub-quadratic computational complexity which still provides consistent estimates of the loopy BP marginal distributions and performs almost as well as the original procedure.,"Expectation Particle Belief Propagation

Thibaut Lienart, Yee Whye Teh, Arnaud Doucet
Department of Statistics
University of Oxford
Oxford, UK
{lienart,teh,doucet}@stats.ox.ac.uk

Abstract
We propose an original particle-based implementation of the Loopy Belief Propagation (LPB) algorithm for pairwise Markov Random Fields (MRF) on a continuous state space. The algorithm constructs adaptively efficient proposal distributions approximating the local beliefs at each note of the MRF. This is achieved
by considering proposal distributions in the exponential family whose parameters
are updated iterately in an Expectation Propagation (EP) framework. The proposed particle scheme provides consistent estimation of the LBP marginals as the
number of particles increases. We demonstrate that it provides more accurate results than the Particle Belief Propagation (PBP) algorithm of [1] at a fraction of
the computational cost and is additionally more robust empirically. The computational complexity of our algorithm at each iteration is quadratic in the number
of particles. We also propose an accelerated implementation with sub-quadratic
computational complexity which still provides consistent estimates of the loopy
BP marginal distributions and performs almost as well as the original procedure.

1

Introduction

Undirected Graphical Models (also known as Markov Random Fields) provide a flexible framework
to represent networks of random variables and have been used in a large variety of applications in
machine learning, statistics, signal processing and related fields [2]. For many applications such as
tracking [3, 4], sensor networks [5, 6] or computer vision [7, 8, 9] it can be beneficial to define MRF
on continuous state-spaces.
Given a pairwise MRF, we are here interested in computing the marginal distributions at the nodes
of the graph. A popular approach to do this is to consider the Loopy Belief Propagation (LBP) algorithm [10, 11, 2]. LBP relies on the transmission of messages between nodes. However when dealing with continuous random variables, computing these messages exactly is generally intractable.
In practice, one must select a way to tractably represent these messages and a way to update these
representations following the LBP algorithm. The Nonparametric Belief Propagation (NBP) algorithm [12] represents the messages with mixtures of Gaussians while the Particle Belief Propagation
(PBP) algorithm [1] uses an importance sampling approach. NBP relies on restrictive integrability
conditions and does not offer consistent estimators of the LBP messages. PBP offers a way to circumvent these two issues but the implementation suggested proposes sampling from the estimated
beliefs which need not be integrable. Moreover, even when they are integrable, sampling from
the estimated beliefs is very expensive computationally. Practically the authors of [1] only sample
approximately from those using short MCMC runs, leading to biased estimators.
In our method, we consider a sequence of proposal distributions at each node from which one can
sample particles at a given iteration of the LBP algorithm. The messages are then computed using
importance sampling. The novelty of the approach is to propose a principled and automated way
of designing a sequence of proposals in a tractable exponential family using the Expectation Prop1

agation (EP) framework [13]. The resulting algorithm, which we call Expectation Particle Belief
Propagation (EPBP), does not suffer from restrictive integrability conditions and sampling is done
exactly which implies that we obtain consistent estimators of the LBP messages. The method is empirically shown to yield better approximations to the LBP beliefs than the implementation suggested
in [1], at a much reduced computational cost, and than EP.

2
2.1

Background
Notations

We consider a pairwise MRF, i.e. a distribution over a set of p random variables indexed by a set
V = {1, . . . , p}, which factorizes according to an undirected graph G = (V, E) with
Y
Y
p(xV ) ?
?u (xu )
?uv (xu , xv ).
(1)
u?V

(u,v)?E

The random variables are assumed to take values on a continuous, possibly unbounded, space X .
The positive functions ?u : X 7? R+ and ?uv : X ? X 7? R+ are respectively known as the node
and edge potentials. The aim is to approximate the marginals pu (xu ) for all u ? V . A popular
approach is the LBP algorithm discussed earlier. This algorithm is a fixed point iteration scheme
yielding approximations called the beliefs at each node [10, 2]. When the underlying graph is a tree,
the resulting beliefs can be shown to be proportional to the exact marginals. This is not the case in
the presence of loops in the graph. However, even in these cases, LBP has been shown to provide
good approximations in a wide range of situations [14, 11]. The LBP fixed-point iteration can be
written as follows at iteration t:
?
Z
Y
?
t
?
m
(x
)
=
?
(x
,
x
)?
(x
)
mt?1
?
v
uv
u
v
u
u
uv
wu (xu )dxu
?
w??u \v
,
(2)
Y
?
?
But (xu ) = ?u (xu )
mtwu (xu )
?
?
w??u

where ?u denotes the neighborhood of u i.e., the set of nodes {w | (w, u) ? E}, muv is known as
the message from node u to node v and Bu is the belief at node u.
2.2

Related work

The crux of any generic implementation of LBP for continuous state spaces is to select a way to represent the messages and design an appropriate method to compute/approximate the message update.
In Nonparametric BP (NBP) [12], the messages are represented by mixtures of Gaussians. In theory,
computing the product of such messages can be done analytically but in practice this is impractical
due to the exponential growth in the number of terms to consider. To circumvent this issue, the
authors suggest an importance sampling approach targeting the beliefs and fitting mixtures of Gaussians to the resulting weighted particles. The computation of the update (2) is then always done over
a constant number of terms.
A restriction of ?vanilla? Nonparametric BP is that the messages must be finitely integrable for the
message representation to make sense. This is the case if the following two conditions hold:
Z
Z
sup
?uv (xu , xv )dxu < ?, and
?u (xu )dxu < ?.
(3)
xv

These conditions do however not hold in a number of important cases as acknowledged in [3]. For
instance, the potential ?u (xu ) is usually proportional to a likelihood of the form p(yu |xu ) which
need not be integrable in xu . Similarly, in imaging applications for example, the edge potential can
encode similarity between pixels which also need not verify the integrability condition as in [15].
Further, NBP does not offer consistent estimators of the LBP messages.
Particle BP (PBP) [1] offers a way to overcome the shortcomings of NBP: the authors also consider
importance sampling to tackle the update of the messages but without fitting a mixture of Gaussians.
2

(i)

For a chosen proposal distribution qu on node u and a draw of N particles {xu }N
i=1 ? qu (xu ), the
messages are represented as mixtures:
m
b PBP
uv (xv ) :=

N
X

(i)
?uv
?uv (x(i)
u , xv ),

(i)
with ?uv
:=

i=1

(i)
1 ?u (xu ) Y
(i)
m
b PBP
wu (xu ).
N qu (xu(i) )
w??u \v

(4)

This algorithm has the advantage that it does not require the conditions (3) to hold. The authors
suggest two possible choices of sampling distributions: sampling from the local potential ?u , or
sampling from the current belief estimate. The first case is only valid if ?u is integrable w.r.t. xu
which, as we have mentioned earlier, might not be the case in general and the second case implies
sampling from a distribution of the form
Y
buPBP (xu ) ? ?u (xu )
B
m
b PBP
(5)
wu (xu )
w??u

which is a product of mixtures. As in NBP, na??ve sampling of the proposal has complexity O(N |?u | )
and is thus in general too expensive to consider. Alternatively, as the authors suggest, one can run
a short MCMC simulation targeting it which reduces the complexity to order O(|?u |N 2 ) since the
buPBP point-wise, is of order O(|?u |N ), and we
cost of each iteration, which requires evaluating B
need O(N ) iterations of the MCMC simulation. The issue with this approach is that it is still computationally expensive, and it is unclear how many iterations are necessary to get N good samples.
2.3

Our contribution

In this paper, we consider the general context where the edge and node-potentials might be nonnormalizable and non-Gaussian. Our proposed method is based on PBP, as PBP is theoretically
better suited than NBP since, as discussed earlier, it does not require the conditions (3) to hold, and,
provided that one samples from the proposals exactly, it yields consistent estimators of the LBP
messages while NBP does not. Further, the development of our method also formally shows that
considering proposals close to the beliefs, as suggested by [1], is a good idea. Our core observation
is that since sampling from a proposal of the form (5) using MCMC simulation is very expensive,
we should consider using a more tractable proposal distribution instead. However it is important that
the proposal distribution is constructed adaptively, taking into account evidence collected through
the message passing itself, and we propose to achieve this by using proposal distributions lying in a
tractable exponential family, and adapted using the Expectation Propagation (EP) framework [13].

3

Expectation Particle Belief Propagation

Our aim is to address the issue of selecting the proposals in the PBP algorithm. We suggest using
exponential family distributions as the proposals on a node for computational efficiency reasons,
with parameters chosen adaptively based on current estimates of beliefs and EP. Each step of our
algorithm involves both a projection onto the exponential family as in EP, as well as a particle
approximation of the LBP message, hence we will refer to our method as Expectation Particle
Belief Propagation or EPBP for short.
For each pair of adjacent nodes u and v, we will use muv (xv ) to denote the exact (but unavailable)
LBP message from u to v, m
b uv (xv ) to denote the particle approximation of muv , and ?uv an exponential family projection of m
b uv . In addition, let ??u denote an exponential family projection of the
node potential ?u . We will consider approximations consisting of N particles. In the following, we
will derive the form of our particle approximated message m
b uv (xv ), along with the choice of the
proposal distribution qu (xu ) used to construct m
b uv . Our starting point is the edge-wise belief over
xu and xv , given the incoming particle approximated messages,
Y
Y
buv (xu , xv ) ? ?uv (xu , xv )?u (xu )?v (xv )
B
m
b wu (xu )
m
b ?v (xv ).
(6)
w??u \v

???v \u

buv (xv ),
The exact LBP message muv (xv ) can be derived by computing the marginal distribution B
and constructing muv (xv ) such that
buv (xv ) ? muv (xv )M
cvu (xv ),
B
3

(7)

cvu (xv ) = ?v (xv ) Q
where M
b ?v (xv ) is the (particle approximated) pre-message from v to
???v \u m
u. It is easy to see that the resulting message is as expected,
Z
Y
muv (xv ) ? ?uv (xu , xv )?u (xu )
m
b wu (xu )dxu .
(8)
w??u \v

Since the above exact LBP belief and message are intractable in our scenario of interest, the idea
buv (xu , xv ) instead. Consider a proposal distribution
is to use an importance sampler targeting B
of the form qu (xu )qv (xv ). Since xu and xv are independent under the proposal, we can draw
(i)
(j) N
N independent samples, say {xu }N
i=1 and {xv }j=1 , from qu and qv respectively. We can then
approximate the belief using a N ? N cross product of the particles,
N
(j)
X
buv (x(i)
B
u , xv )
buv (xu , xv ) ? 1
B
?(x(i) ,x(j) ) (xu , xv )
u
v
N 2 i,j=1 qu (xu(i) )qv (x(j)
v )
(i)
(j)
(i) c
(j) Q
(i)
N
b wu (xu )
vu (xv )
1 X ?uv (xu , xv )?u (xu )M
w??u \v m
? 2
?(x(i) ,x(j) ) (xu , xv )
(i)
(j)
u
v
N i,j=1
qu (xu )qv (xv )

(9)

buv (xv ),
Marginalizing onto xv , we have the following particle approximation to B
N
(j)
(j) c
b uv (xv )M
1 Xm
vu (xv )
b
?x(j) (xv )
Buv (xv ) ?
(j)
v
N j=1
qv (xv )

(10)

where the particle approximated message m
b uv (xv ) from u to v has the form of the message representation in the PBP algorithm (4).
buv .
To determine sensible proposal distributions, we can find qu and qv that are close to the target B
buv kqu qv ) as the measure of closeness, the optimal qu required for the
Using the KL divergence KL(B
u to v message is the node belief,
Y
buv (xu ) ? ?u (xu )
B
m
b wu (xu )
(11)
w??u

thus supporting the claim in [1] that a good proposal to use is the current estimate of the node belief.
As pointed out in Section 2, it is computationally inefficient to use the particle approximated node
belief as the proposal distribution. An idea is to use a tractable exponential family distribution for
qu instead, say
Y
qu (xu ) ? ??u (xu )
?wu (xu )
(12)
w??u

where ??u and ?wu are exponential family approximations of ?u and m
b wu respectively. In Section
4 we use a Gaussian family, but we are not limited to this. Using the framework of expectation
propogation (EP) [13], we can iteratively find good exponential family approximations as follows.
\w
For each w ? ?u , to update the ?wu , we form the cavity distribution qu ? qu /?wu and the corre\w
+
sponding tilted distribution m
b wu qu . The updated ?wu
is the exponential family factor minimising
the KL divergence,

h
i

+
?wu
= arg min KL m
b wu (xu )qu\w (xu )
 ?(xu )qu\w (xu ) .
(13)
??exp.fam.

Geometrically, the update projects the tilted distribution onto the exponential family manifold.
The optimal solution requires computing the moments of the tilted distribution through numeri\w
cal quadrature, and selecting ?wu so that ?wu qu matches the moments of the tilted distribution. In
our scenario the moment computation can be performed crudely on a small number of evaluation
points since it only concerns the updating of the importance sampling proposal. If an optimal ? in
the exponential family does not exist, e.g. in the Gaussian case that the optimal ? has a negative
variance, we simply revert ?wu to its previous value [13]. An analogous update is used for ??u .
In the above derivation, the expectation propagation steps for each incoming message into u and for
the node potential are performed first, to fit the proposal to the current estimated belief at u, before
4

it is used to draw N particles, which can then be used to form the particle approximated messages
from u to each of its neighbours. Alternatively, once each particle approximated message m
b uv (xv )
is formed, we can update its exponential family projection ?uv (xv ) immediately. This alternative
scheme is described in Algorithm 1.
Algorithm 1 Node update
(i)

1: sample {xu } ? qu ( ? )
(i) Q
(i)
bu (x(i)
2: compute B
b wu (xu )
u ) = ?u (xu )
w??u m
3: for v ? ?u do
cuv (x(i)
b (i) b vu (xu(i) )
4:
compute M
u ) := Bu (xu )/m
(i)
(i)
cuv (x(i)
5:
compute the normalized weights wuv ? M
u )/qu (xu )
PN
(i)
(i)
6:
update the estimator of the outgoing message m
b uv (xv ) = i=1 wuv ?uv (xu , xv )
\?

+
compute the cavity distribution qv ? qv /??v , get ??v
in the exponential family such that
\?
+ \?
+
+
??v qv approximates ?v qv , update qv ? ??v and let ??v ? ??v
\u
+
8:
compute the cavity distribution qv ? qv /?uv , get ?uv in the exponential family such that
\u
+ \u
+
+
qv approximates m
b uv qv , update qv ? ?uv
and let ?uv ? ?uv
?uv
9: end for

7:

3.1

Computational complexity and sub-quadratic implementation

Each EP projection step costs O(N ) computations since the message m
b wu is a mixture of N components (see (4)). Drawing N particles from the exponential family proposal qu costs O(N ). The step
with highest computational complexity is in evaluating the particle weights in (4). Indeed, evaluating
the mixture representation of a message on a single point is O(N ), and we need to compute this for
each of N particles. Similarly, evaluating the estimator of the belief on N sampling points at node
u requires O(|?u |N 2 ). This can be reduced since the algorithm still provides consistent estimators
if we consider the evaluation of unbiased estimators of the messages instead. Since the messages
PN
i
i
have the form m
b uv (xv ) =
i=1 wuv ?uv (xv ), we can follow a method presented in [16] where
? M
i
}N
one draws M indices {i` }`=1 from a multinomial with weights {wuv
i=1 and evaluates the correi?
`
sponding M components ?uv
. This reduces the cost of the evaluation of the beliefs to O(|?u |M N )
which leads to an overall sub-quadratic complexity if M is o(N ). We show in the next section how
it compares to the quadratic implementation when M = O(log N ).

4

Experiments

We investigate the performance of our method on MRFs for two simple graphs. This allows us
to compare the performance of EPBP to the performance of PBP in depth. We also illustrate the
behavior of the sub-quadratic version of EPBP. Finally we show that EPBP provides good results in
a simple denoising application.
4.1

Comparison with PBP

We start by comparing EPBP to PBP as implemented by Ihler et al. on a 3 ? 3 grid (figure 1)
with random variables taking values on R. The node and edge potentials are selected such that the
marginals are multimodal, non-Gaussian and skewed with

?u (xu )
= ?1 N (xu ? yu ; ?2, 1) + ?2 G(xu ? yu ; 2, 1.3)
,
(14)
?uv (xu , xv ) = L(xu ? xv ; 0, 2)
where yu denotes the observation at node u, N (x; ?, ?) ? exp(?x2 /2? 2 ) (density of a Normal
distribution), G(x; ?, ?) ? exp(?(x??)/? +exp(?(x??)/?)) (density of a Gumbel distribution)
and L(x; ?, ?) ? exp(?|x ? ?|/?) (density of a Laplace distribution). The parameters ?1 and ?2
are respectively set to 0.6 and 0.4. We compare the two methods after 20 LBP iterations.1
1
The scheduling used alternates between the classical orderings: top-down-left-right, left-right-top-down,
down-up-right-left and right-left-down-up. One ?LBP iteration? implies that all nodes have been updated once.

5

1

4

7

2

5

8

3

6

9

1
2
4

5

3
6

7

8

Figure 1: Illustration of the grid (left) and tree (right) graphs used in the experiments.
PBP as presented in [1] is implemented using the same parameters than those in an implementation
code provided by the authors: the proposal on each node is the last estimated belief and sampled with
a 20-step MCMC chain, the MH proposal is a normal distribution. For EPBP, the approximation of
the messages are Gaussians. The ground truth is approximated by running LBP on a deterministic
equally spaced mesh with 200 points. All simulations were run with Julia on a Mac with 2.5 GHz
Intel Core i5 processor, our code is available online.2
Figure 2 compares the performances of both methods. The error is computed as the mean L1 error
over all nodes between the estimated beliefs and the ground truth evaluated over the same deterministic mesh. One can observe that not only does PBP perform worse than EPBP but also that the
error plateaus with increasing number of samples. This is because the secondampling within PBP
is done approximately and hence the consistency of the estimators is lost. The speed-up offered by
EPBP is very substantial (figure 4 left). Hence, although it would be possible to use more MCMC
(Metropolis-Hastings) iterations within PBP to improve its performance, it would
? make the method
prohibitively expensive to use. Note that for EPBP, one observes the usual 1/ N convergence of
particle methods.
Figure 3 compares the estimator of the beliefs obtained by the two methods for three arbitrarily
picked nodes (node 1, 5 and 9 as illustrated on figure 1). The figure also illustrates the last proposals
constructed with our approach and one notices that their supports match closely the support of the
true beliefs. Figure 4 left illustrates how the estimated beliefs converge as compared to the true
beliefs with increasing number of iterations. One can observe that PBP converges more slowly and
that the results display more variability which might be due to the MCMC runs being too short.
We repeated the experiments on a tree with 8 nodes (figure 1 right) where we know that, at convergence, the beliefs computed using BP are proportional to the true marginals. The node and edge
potentials are again picked such that the marginals are multimodal with

?u (xu )
= ?1 N (xu ? yu ; ?2, 1) + ?2 N (xu ? yu ; 1, 0.5)
,
(15)
?uv (xu , xv ) = L(xu ? xv ; 0, 1)
with ?1 = 0.3 and ?2 = 0.7. On this example, we also show how ?pure EP? with normal distributions performs. We also try using the distributions obtained with EP as proposals for PBP (referred
to as ?PBP after EP? in figures). Both methods underperform compared to EPBP as illustrated visually in Figure 5. In particular one can observe in Figure 3 that ?PBP after EP? converges slower
than EPBP with increasing number of samples.
4.2

Sub-quadratic implementation and denoising application

As outlined in Section 3.1, in the implementation of EPBP one can use an unbiased estimator of
the edge weights based on a draw of M components from a multinomial. The complexity of the
resulting algorithm is O(M N ). We apply this method to the 3 ? 3 grid example in the case where
M is picked to be roughly of order log(N ): i.e., for N = {10, 20, 50, 100, 200, 500}, we pick
M = {5, 6, 8, 10, 11, 13}. The results are illustrated in Figure 6 where one can see that the N log N
implementation compares very well to the original quadratic implementation at a much reduced
cost. We apply this sub-quadratic method on a simple probabilistic model for an image denoising
problem. The aim of this example is to show that the method can be applied to larger graphs and still
provide good results. The model underlined is chosen to showcase the flexibility and applicability
of our method in particular when the edge-potential is non-integrable. It is not claimed to be an
optimal approach to image denoising.3 The node and edge potentials are defined as follows:

?u (xu )
= N (xu ? yu ; 0, 0.1)
,
(16)
?uv (xu , xv ) = L? (xu ? xv ; 0, 0.03)
2
3

https://github.com/tlienart/EPBP.
In this case in particular, an optimization-based method such as [17] is likely to yield better results.

6

where L? (x; ?, ?) = L(x; ?, ?) if |x| ? ? and L(?; ?, ?) otherwise. In this example we set
? = 0.2. The value assigned to each pixel of the reconstruction is the estimated mean obtained over
the corresponding node (figure 7). The image has size 50 ? 50 and the simulation was run with
N = 30 particles per nodes, M = 5 and 10 BP iterations taking under 2 minutes to complete. We
compare it with the result obtained with EP on the same model.
10 0

10 0
EPBP
PBP after EP

Mean L1 error

Mean L1 error

PBP
EPBP

10 -1

10 -2

10 1

10 2

10 -1

10 -2

10 3

10 1

10 2

Number of samples per node

10 3

Number of samples per node

Figure 2: (left) Comparison of the mean L1 error for PBP and EPBP for the 3 ? 3 grid example.
(right) Comparison of the mean L1 error for ?PBP after EP? and EPBP for the tree example. In both
cases, EPBP is more accurate for the same number of samples.
0.35

0.6

0.3

0.3

0.5

0.25

0.4

0.2

0.3

0.15

0.25

True belief
Estimated belief (EPBP)
Estimated belief (PBP)
Proposal (EPBP)

0.2
0.15
0.1
0.05
0
-5

0

5

10

15

0.2

0.1

0.1

0.05

0
-5

0

5

10

15

0
-5

0

5

10

15

Figure 3: Comparison of the beliefs on node 1, 5 and 9 as obtained by evaluating LBP on a deterministic mesh (true belief ), with PBP and with EPBP for the 3 ? 3 grid example. The proposal used
by EPBP at the last step is also illustrated. The results are obtained with N = 100 samples on each
node and 20 BP iterations. One can observe visually that EPBP outperforms PBP.
4.5

4

10

3

10

2

PBP
EPBP

3.5

Wall-clock time [s]

3

Mean L1 error

10
EPBP
PBP

4

2.5
2
1.5
1

10 1

10 0

0.5
0

0

5

10

15

10 -1

20

Number of BP iterations

10 1

10 2

10 3

Number of samples per node

Figure 4: (left) Comparison of the convergence in L1 error with increasing number of BP iterations
for the 3 ? 3 grid example when using N = 30 particles. (right) Comparison of the wall-clock time
needed to perform PBP and EPBP on the 3 ? 3 grid example.

5

Discussion

We have presented an original way to design adaptively efficient and easy-to-sample-from proposals
for a particle implementation of Loopy Belief Propagation. Our proposal is inspired by the Expectation Propagation framework.
We have demonstrated empirically that the resulting algorithm is significantly faster and more accurate than an implementation of PBP using the estimated beliefs as proposals and sampling from
them using MCMC as proposed in [1]. It is also more accurate than EP due to the nonparametric
nature of the messages and offers consistent estimators of the LBP messages. A sub-quadratic version of the method was also outlined and shown to perform almost as well as the original method on
7

mildly multi-modal models, it was also applied successfully in a simple image denoising example
illustrating that the method can be applied on graphical models with several hundred nodes.
We believe that our method could be applied successfully to a wide range of applications such as
smoothing for Hidden Markov Models [18], tracking or computer vision [19, 20]. In future work,
we will look at considering other divergences than the KL and the ?Power EP? framework [21], we
will also look at encapsulating the present algorithm within a sequential Monte Carlo framework
and the recent work of Naesseth et al. [22].
1.2

0.9

0.5

0.8

0.45

0.7

0.4

0.6

0.35

1
0.8

True belief
Est. bel. (EPBP)
Est. bel. (PBP)
Est. bel. (EP)
Est. bel. (PBP after EP)

0.3

0.5

0.25

0.6
0.4

0.2
0.4

0.3

0.15

0.2

0.1

0.1

0.05

0.2
0
-2

0

2

4

0
-2

6

0

2

4

6

0
-2

0

2

4

6

Figure 5: Comparison of the beliefs on node 1, 3 and 8 as obtained by evaluating LBP on a deterministic mesh, using EPBP, PBP, EP and PBP using the results of EP as proposals. This is for the
tree example with N = 100 samples on each node and 20 LBP iterations. Again, one can observe
visually that EPBP outperforms the other methods.
10

0

10

2

10

10

NlogN implementation
Quadratic implementation

Wall-clock time [s]

Mean L1 error

NlogN implementation
Quadratic implementation

-1

-2

10 1

10 2

10 1

10 0

10

10 3

Number of samples

-1

10 1

10 2

10 3

Number of samples per node

Figure 6: Comparison of the mean L1 error for PBP and EPBP on a 3 ? 3 grid (left). For the
same number of samples, EPBP is more accurate. It is also faster by about two orders of magnitude
(right). The simulations were run several times for the same observations to illustrate the variability
of the results.

Figure 7: From left to right: comparison of the original (first), noisy (second) and recovered image
using the sub-quadratic implementation of EPBP (third) and with EP (fourth).
Acknowledgments
We thank Alexander Ihler and Drew Frank for sharing their implementation of Particle Belief Propagation. TL gratefully acknowledges funding from EPSRC (grant 1379622) and the Scatcherd European scholarship scheme. YWT?s research leading to these results has received funding from
EPSRC (grant EP/K009362/1) and ERC under the EU?s FP7 Programme (grant agreement no.
617411). AD?s research was supported by the EPSRC (grant EP/K000276/1, EP/K009850/1) and
by AFOSR/AOARD (grant AOARD-144042).
8

References
[1] Alexander T. Ihler and David A. McAllester. Particle belief propagation. In Proc. 12th AISTATS, pages 256?263, 2009.
[2] Martin J. Wainwright and Michael I. Jordan. Graphical models, exponential families, and
variational inference. Found. and Tr. in Mach. Learn., 1(1?2):1?305, 2008.
[3] Erik B. Sudderth, Alexander T. Ihler, Michael Isard, William T. Freeman, and Alan S. Willsky.
Nonparametric belief propagation. Commun. ACM, 53(10):95?102, 2010.
[4] Jeremy Schiff, Erik B. Sudderth, and Ken Goldberg. Nonparametric belief propagation for
distributed tracking of robot networks with noisy inter-distance measurements. In IROS ?09,
pages 1369?1376, 2009.
[5] Alexander T. Ihler, John W. Fisher, Randolph L. Moses, and Alan S. Willsky. Nonparametric
belief propagation for self-localization of sensor networks. In IEEE Sel. Ar. Comm., volume 23,
pages 809?819, 2005.
[6] Christopher Crick and Avi Pfeffer. Loopy belief propagation as a basis for communication in
sensor networks. In Proc. 19th UAI, pages 159?166, 2003.
[7] Jian Sun, Nan-Ning Zheng, and Heung-Yeung Shum. Stereo matching using belief propagation. In IEEE Trans. Patt. An. Mach. Int., volume 25, pages 787?800, 2003.
[8] Andrea Klaus, Mario Sormann, and Konrad Karner. Segment-based stereo matching using
belief propagation and a self-adapting dissimilarity measure. In Proc. 18th ICPR, volume 3,
pages 15?18, 2006.
[9] Nima Noorshams and Martin J. Wainwright. Belief propagation for continuous state spaces:
Stochastic message-passing with quantitative guarantees. JMLR, 14:2799?2835, 2013.
[10] Judea Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufman, 1988.
[11] Jonathan S. Yedidia, William T. Freeman, and Yair Weiss. Constructing free energy approximations and generalized belief propagation algorithms. MERL Technical Report, 2002.
[12] Erik B. Sudderth, Alexander T. Ihler, William T. Freeman, and Alan S. Willsky. Nonparametric
belief propagation. In Procs. IEEE Comp. Vis. Patt. Rec., volume 1, pages 605?612, 2003.
[13] Thomas P. Minka. Expectation propagation for approximate Bayesian inference. In Proc. 17th
UAI, pages 362?369, 2001.
[14] Kevin P. Murphy, Yair Weiss, and Michael I. Jordan. Loopy belief propagation for approximate
inference: an empirical study. In Proc. 15th UAI, pages 467?475, 1999.
[15] Mila Nikolova. Thresholding implied by truncated quadratic regularization. IEEE Trans. Sig.
Proc., 48(12):3437?3450, 2000.
[16] Mark Briers, Arnaud Doucet, and Sumeetpal S. Singh. Sequential auxiliary particle belief
propagation. In Proc. 8th ICIF, volume 1, pages 705?711, 2005.
[17] Leonid I. Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal algorithms. Physica D, 60(1):259?268, 1992.
[18] M. Briers, A. Doucet, and S. Maskell. Smoothing algorithms for state-space models. Ann. Inst.
Stat. Math., 62(1):61?89, 2010.
[19] Erik B. Sudderth, Michael I. Mandel, William T. Freeman, and Alan S. Willsky. Visual hand
tracking using nonparametric belief propagation. In Procs. IEEE Comp. Vis. Patt. Rec., 2004.
[20] Pedro F. Felzenszwalb and Daniel P. Huttenlocher. Efficient graph-based image segmentation.
Int. Journ. Comp. Vis., 59(2), 2004.
[21] Thomas P. Minka. Power EP. Technical Report MSR-TR-2004-149, 2004.
[22] Christian A. Naesseth, Fredrik Lindsten, and Thomas B. Sch?on. Sequential monte carlo for
graphical models. In Proc. 27th NIPS, pages 1862?1870, 2014.

9

"
2005,A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification,,2763-a-probabilistic-interpretation-of-svms-with-an-application-to-unbalanced-classification.pdf,Abstract Missing,"A Probabilistic Interpretation of SVMs with an
Application to Unbalanced Classification

Yves Grandvalet ?
Heudiasyc, CNRS/UTC
60205 Compi`egne cedex, France
grandval@utc.fr

Johnny Mari?ethoz Samy Bengio
IDIAP Research Institute
1920 Martigny, Switzerland
{marietho,bengio}@idiap.ch

Abstract
In this paper, we show that the hinge loss can be interpreted as the
neg-log-likelihood of a semi-parametric model of posterior probabilities.
From this point of view, SVMs represent the parametric component of a
semi-parametric model fitted by a maximum a posteriori estimation procedure. This connection enables to derive a mapping from SVM scores
to estimated posterior probabilities. Unlike previous proposals, the suggested mapping is interval-valued, providing a set of posterior probabilities compatible with each SVM score. This framework offers a new
way to adapt the SVM optimization problem to unbalanced classification, when decisions result in unequal (asymmetric) losses. Experiments
show improvements over state-of-the-art procedures.

1

Introduction

In this paper, we show that support vector machines (SVMs) are the solution of a relaxed
maximum a posteriori (MAP) estimation problem. This relaxed problem results from fitting
a semi-parametric model of posterior probabilities. This model is decomposed into two
components: the parametric component, which is a function of the SVM score, and the
non-parametric component which we call a nuisance function. Given a proper binding of
the nuisance function adapted to the considered problem, this decomposition enables to
concentrate on selected ranges of the probability spectrum. The estimation process can
thus allocate model capacity to the neighborhoods of decision boundaries.
The connection to semi-parametric models provides a probabilistic interpretation of SVM
scores, which may have several applications, such as estimating confidences over the predictions, or dealing with unbalanced losses. (which occur in domains such as diagnosis,
intruder detection, etc). Several mappings relating SVM scores to probabilities have already been proposed (Sollich 2000, Platt 2000), but they are subject to arbitrary choices,
which are avoided here by their integration to the nuisance function.
The paper is organized as follows. Section 2 presents the semi-parametric modeling approach; Section 3 shows how we reformulate SVM in this framework; Section 4 proposes
several outcomes of this formulation, including a new method to handle unbalanced losses,
which is tested empirically in Section 5. Finally, Section 6 briefly concludes the paper.
?
This work was supported in part by the IST Programme of the European Community, under the
PASCAL Network of Excellence IST-2002-506778. This publication only reflects the authors? views.

2

Semi-Parametric Classification

We address the binary classification problem of estimating a decision rule from a learning
set Ln = {(xi , yi )}ni=1 , where the ith example is described by the pattern xi ? X and
the associated response yi ? {?1, 1}. In the framework of maximum likelihood estimation, classification can be addressed either via generative models, i.e. models of the joint
distribution P (X, Y ), or via discriminative methods modeling the conditional P (Y |X).
2.1

Complete and Marginal Likelihood, Nuisance Functions

Let p(1|x; ?) denote the model of P (Y = 1|X = x), p(x; ?) the model of P (X) and ti
the binary response variable such that ti = 1 when yi = 1 and ti = 0 when yi = ?1.
Assuming independent examples, the complete log-likelihood can be decomposed as
X
L(?, ?; Ln ) =
ti log(p(1|xi ; ?)) + (1 ? ti ) log(1 ? p(1|xi ; ?)) + log(p(xi ; ?)) , (1)
i

where the two first terms of the right-hand side represent the marginal or conditional likelihood, that is, the likelihood of p(1|x; ?).
For classification purposes, the parameter ? is not relevant, and may thus be qualified as a
nuisance parameter (Lindsay 1985). When ? can be estimated independently of ?, maximizing the marginal likelihood provides the estimate returned by maximizing the complete
likelihood with respect to ? and ?. In particular, when no assumption whatsoever is made
on P (X), maximizing the conditional likelihood amounts to maximize the joint likelihood
(McLachlan 1992). The density of inputs is then considered as a nuisance function.
2.2

Semi-Parametric Models

Again, for classification purposes, estimating P (Y |X) may be considered as too demanding. Indeed, taking a decision only requires the knowledge of sign(2P (Y = 1|X = x)?1).
We may thus consider looking for the decision rule minimizing the empirical classification
error, but this problem is intractable for non-trivial models of discriminant functions.
Here, we briefly explore how semi-parametric models (Oakes 1988) may be used to reduce the modelization effort as compared to the standard likelihood approach. For this,
we consider a two-component semi-parametric model of P (Y = 1|X = x), defined as
p(1|x; ?) = g(x; ?) + ?(x), where the parametric component g(x; ?) is the function of interest, and where the non-parametric component ? is a constrained nuisance function. Then,
we address the maximum likelihood estimation of the semi-parametric model p(1|x; ?)
X
?
?
ti log(p(1|xi ; ?)) + (1 ? ti ) log(1 ? p(1|xi ; ?))
min ?
?
?
? ?,?
i
(2)
s. t. p(1|x; ?) = g(x; ?) + ?(x)
?
?
0 ? p(1|x; ?) ? 1
?
?
?? (x) ? ?(x) ? ?+ (x)

where ?? and ?+ are user-defined functions, which place constraints on the non-parametric
component ?. According to these constraints, one pursues different objectives, which can
be interpreted as either weakened or focused versions of the original problem of estimating
precisely P (Y |X) on the whole range [0, 1].
At the one extreme, when ?? = ?+ , one recovers a parametric maximum likelihood problem, where the estimate of posterior probabilities p(1|x; ?) is simply g(x; ?) shifted by the
baseline function ?. At the other extreme, when ?? (x) ? ?g(x) and ?+ (x) ? 1 ? g(x),
p(1|?; ?) perfectly explains (interpolates) any training sample for any ?, and the optimization problem in ? is ill-posed. Note that the optimization problem in ? is always ill-posed,
but this is not of concern as we do not wish to estimate the nuisance function.

+

p(1|x)

? (x)/? (x)

0

0.5

-

-

+

? (x)/? (x)

+?
0
??

1

p(1|x)

0.5

1
1??

0 ?

1?? 1

g(x)

?
0

0 ?

1?? 1

g(x)

?0.5
0

0.5

g(x)

1

0
0

0.5

1

g(x)

Figure 1: Two examples of ?? (x) (dashed) and ?+ (x) (plain) vs. g(x) and resulting ?-tube
of possible values for the estimate of P (Y = 1|X = x) (gray zone) vs. g(x).
Generally, as ? is not estimated, the estimate of posterior probabilities p(1|x; ?) is only
known to lie within the interval [g(x; ?) + ?? (x), g(x; ?) + ?+ (x)]. In what follows, we
only consider functions ?? and ?+ expressed as functions of the argument g(x), for which
the interval can be recovered from g(x) alone. We also require ?? (x) ? 0 ? ?+ (x), in
order to ensure that g(x; ?) is an admissible value of p(1|x; ?).
Two simple examples are displayed in Figure 1. The two first graphs represent ?? and ?+
designed to estimate posterior probabilities up to precision ?, and the corresponding ?-tube
of admissible estimates knowing g(x). The two last graphs represent the same functions
for ?? and ?+ defined to focus on the only relevant piece of information regarding decision:
estimating where P (Y |X) is above 1/2. 1
2.3

Estimation of the Parametric Component

The definitions of ?? and ?+ affect the estimation of the parametric component. Regarding
?, when the values of g(x; ?) + ?? (x) and g(x; ?) + ?+ (x) lie within [0, 1], problem (2)
is equivalent to the following relaxed maximum likelihood problem
?
X
? min ?
ti log(g(xi ; ?) + ?i ) + (1 ? ti ) log(1 ? g(xi ; ?) ? ?i )
?,?
(3)
i
? s. t. ?? (x ) ? ? ? ?+ (x ) i = 1, . . . , n
i
i
i

where ? is an n-dimensional vector of slack variables. The problem is qualified as relaxed
compared to the the maximum likelihood estimation of posterior probabilities by g(xi ; ?),
because modeling posterior probabilities by g(xi ; ?) + ?i is a looser objective.

The monotonicity of the objective function with respect to ?i implies that the constraints
?? (xi ) ? ?i and ?i ? ?+ (xi ) are saturated at the solution of (3) for ti = 0 or ti = 1 respectively. Thus, the loss in (3) is the neg-log-likelihood of the lower or the upper bound on
p(1|xi ; ?) respectively. Provided that g, ?? and ?+ are defined such that ?? (x) ? ?+ (x),
0 ? g(x) + ?? (x) ? 1 and 0 ? g(x) + ?+ (x) ? 1, the optimization problem with respect
to ? reduces to
X
min ?
ti log(g(xi ; ?) + ?+ (xi )) + (1 ? ti ) log(1 ? g(xi ; ?) ? ?? (xi )) . (4)
?

i

Figure 2 displays the losses for positive examples corresponding to the choices of ?? and
?+ depicted in Figure 1 (the losses are symmetrical around 0.5 for negative examples).
Note that the convexity of the objective function with respect to g depends on the choices
of ?? and ?+ . One can show that, providing ?+ and ?? are respectively concave and convex
functions of g, then the loss (4) is convex in g.
When ?? (x) ? 0 ? ?+ (x), g(x) is an admissible estimate of P (Y = 1|x). However,
the relaxed loss (4) is optimistic, below the neg-log-likelihood of g. This optimism usually
1
Of course, this naive attempt to minimize the training classification error is doomed to failure.
Reformulating the problem does not affect its convexity: it remains NP-hard.

L(g(x),1)

L(g(x),1)
0

1?? 1

0

g(x)

0.5

1

g(x)

Figure 2: Losses for positive examples (plain) and neg-log-likelihood of g(x) (dotted) vs.
g(x). Left: for the function ?+ displayed on the left-hand side of Figure 1; right: for the
function ?+ displayed on the right-hand side of Figure 1.
results in a non-consistent estimation of posterior probabilities (i.e g(x) does not converge
towards P (Y = 1|X = x) as the sample size goes to infinity), a common situation in
semi-parametric modeling (Lindsay 1985). This lack of consistency should not be a concern here, since the non-parametric component is purposely introduced to address a looser
estimation problem. We should therefore restrict consistency requirements to the primary
goal of having posterior probabilities in the ?-tube [g(x) + ?? (x), g(x) + ?+ (x)].

3

Semi-Parametric Formulation of SVMs

Several authors pointed the closeness of SVM and the MAP approach to Gaussian processes (Sollich (2000) and references therein). However, this similarity does not provide a
proper mapping from SVM scores to posterior probabilities. Here, we resolve this difficulty
thanks to the additional degrees of freedom provided by semi-parametric modelling.
3.1

SVMs and Gaussian Processes

In its primal Lagrangian formulation, the SVM optimization problem reads
X
1
[1 ? yi (f (xi ) + b)]+ ,
min kf k2H + C
f,b 2
i

(5)

where H is a reproducing kernel Hilbert space with norm k ? kH , C is a regularization
parameter and [f ]+ = max(f, 0).
The penalization term in (5) can be interpreted as a Gaussian prior on f , with a covariance
function proportional to the reproducing kernel of H (Sollich 2000). Then, the interpretation of the hinge loss as a marginal log-likelihood requires to identify an affine function of
the last term of (5) with the two first terms of (1). We thus look for two constants c0 and
c1 6= 0, such that, for all values of f (x) + b, there exists a value 0 ? p(1|x) ? 1 such that

p(1|x) = exp ?(c0 + c1 [1 ? (f (x) + b)]+ )
.
(6)
1 ? p(1|x) = exp ?(c0 + c1 [1 + (f (x) + b)]+ )
The system (6) has a solution over the whole range of possible values of f (x) + b if and
only if c0 = log(2) and c1 = 0. Thus, the SVM optimization problem does not implement
the MAP approach to Gaussian processes.
To proceed with a probabilistic interpretation of SVMs, Sollich (2000) proposed a normalized probability model. The normalization functional was chosen arbitrarily, and the
consequences of this choice on the probabilistic interpretation was not evaluated. In what
follows, we derive an imprecise mapping, with interval-valued estimates of probabilities,
representing the set of all admissible semi-parametric formulations of SVM scores.
3.2

SVMs and Semi-Parametric Models

With the semi-parametric models of Section 2.2, one has to identify an affine function of
the hinge loss with the two terms of (4). Compared to the previous situation, one has the

2.5
2

0.6
0.4
0.2
0
?6

1

1.5

p(1|x)

L(g(x),1)

p(1|x)

1
0.8

1

0.5

0.5
?4

?2

0

2

4

f(x)+b

6

0
?6

?4

?2

0

f(x)+b

2

4

6

0
0

0.25

0.5

0.75

1

g(x)

Figure 3: Left: lower (dashed) and upper (plain) posterior probabilities [g(x) +
?? (x), g(x) + ?+ (x)] vs. SVM scores f (x) + b; center: corresponding neg-log-likelihood
of g(x) for positive examples vs. f (x)+b. right: lower (dashed) and upper (plain) posterior
probabilities vs. g(x), for g defined in (8).

freedom to define the slack functions ?? and ?+ . The identification problem is now
?
g(x) + ?+ (x) = exp ?(c0 + c1 [1 ? (f (x) + b)]+ )
?
?
?
? 1 ? g(x) ? ?? (x) = exp ?(c0 + c1 [1 + (f (x) + b)]+ )
s.t. 0 ? g(x) + ?? (x) ? 1
.
?
+
?
0
?
g(x)
+
?
(x)
?
1
?
?
?? (x) ? ?+ (x)

(7)

Provided c0 = 0 and 0 < c1 ? log(2), there are functions g, ?? and ?+ such that the
above problem has a solution. Hence, we obtain a set of probabilistic interpretations fully
compatible with SVM scores. The solutions indexed by c1 are nested, in the sense that, for
any x, the length of the uncertainty interval, ?+ (x)??? (x), is monotonically decreasing in
c1 : the interpretation of SVM scores as posterior probabilities gets tighter as c1 increases.
The most restricted subset of admissible interpretations, with the shortest uncertainty intervals, obtained for c1 = log(2), is represented in the left-hand side of Figure 3. The loss
incurred by a positive example is represented on the central graph, where the gray zone represents the neg-log-likelihood of all admissible solutions of g(x). Note that the hinge loss
is proportional to the neg-log-likelihood of the upper posterior probability g(x) + ?+ (x),
which is the loss for positive examples in the semi-parametric model in (4). Conversely, the
hinge loss for negative examples is reached for g(x) + ?? (x). An important observation,
that will be useful in Section 4.2 is that the neg-log-likelihood of any admissible functions
g(x) is tangent to the hinge loss at f (x) + b = 0.
The solution is unique in terms of the admissible interval [g + ?? , g + ?+ ], but many
definitions of (?? , ?+ , g) solve (7). For example, g may be defined as

2?[1?(f (x)+b)]+
,
(8)
2?[1+(f (x)+b)]+ + 2?[1?(f (x)+b)]+
which is essentially the posterior probability model proposed by Sollich (2000), represented
dotted in the first two graphs of Figure 3.
g(x; ?) =

The last graph of Figure 3 displays the mapping from g(x) to admissible values of p(1|x)
which results from the choice described in (8). Although the interpretation of SVM scores
does not require to specify g, it may worth to list some features common to all options.
First, g(x) + ?? (x) = 0 for all g(x) below some threshold g0 > 0, and conversely, g(x) +
?+ (x) = 1 for all g(x) above some threshold g1 < 1. These two features are responsible
for the sparsity of the SVM solution. Second, the estimation of posterior probabilities is
accurate at 0.5, and the length of the uncertainty interval on p(1|x) monotonically increases
in [g0 , 0.5] and then monotonically decreases in [0.5, g1 ]. Hence, the training objective of
SVMs is intermediate between the accurate estimation of posterior probabilities on the
whole range [0, 1] and the minimization of the classification risk.

4

Outcomes of the Probabilistic Interpretation

This section gives two consequences of our probabilistic interpretation of SVMs. Further
outcomes, still reserved for future research are listed in Section 6.
4.1

Pointwise Posterior Probabilities from SVM Scores

Platt (2000) proposed to estimate posterior probabilities from SVM scores by fitting a logistic function over the SVM scores. The only logistic function compatible with the most
stringent interpretation of SVMs in the semi-parametric framework,
1
g(x; ?) =
,
(9)
1 + 4?(f (x)+b))
is identical to the model of Sollich (2000) (8) when f (x) + b lies in the interval [?1, 1].
Other logistic functions are compatible with the looser interpretations obtained by letting
c1 < log(2), but their use as pointwise estimates is questionable, since the associated
confidence interval is wider. In particular, the looser interpretations do not ensure that
f (x) + b = 0 corresponds to g(x) = 0.5. Then, the decision function based on the
estimated posterior probabilities by g(x) may differ from the SVM decision function.
Being based on an arbitrary choice of g(x), pointwise estimates of posterior probabilities
derived from SVM scores should be handled with caution. As discussed by Zhang (2004),
they may only be consistent at f (x) + b = 0, where they may converge towards 0.5.
4.2

Unbalanced Classification Losses

SVMs are known to perform well regarding misclassification error, but they provide skewed
decision boundaries for unbalanced classification losses, where the losses associated with
incorrect decisions differ according to the true label. The mainstream approach used to
address this problem consists in using different losses for positive and negative examples
(Morik et al. 1999, Veropoulos et al. 1999), i.e.
X
X
1
[1 ? (f (xi ) + b)]+ +C ?
[1 + (f (xi ) + b)]+ , (10)
min kf k2H +C +
f,b 2
{i|yi =1}

{i|yi =?1}

where the coefficients C + and C ? are constants, whose ratio is equal to the ratio of the
losses ?FN and ?FP pertaining to false negatives and false positives, respectively (Lin et al.
2002).2 Bayes? decision theory defines the optimal decision rule by positive classification
FP
when P (y = 1|x) > P0 , where P0 = ?FP?+?
. We may thus rewrite C + = C ? (1 ? P0 )
FN
?
and C = C ? P0 . With such definitions, the optimization problem may be interpreted
as an upper-bound on the classification risk defined from ?FN and ?FP . However, the machinery of Section 3.2 unveils a major problem: the SVM decision function provided by
sign(f (xi ) + b) is not consistent with the probabilistic interpretation of SVM scores.
We address this problem by deriving another criterion, by requiring that the neg-loglikelihood of any admissible functions g(x) is tangent to the hinge loss at f (x) + b = 0.
This leads to the following problem:
?
X
1
[? log(P0 ) ? (1 ? P0 )(f (xi ) + b)]+ +
min kf k2H + C ?
f,b 2
{i|yi =1}
?
X
[? log(1 ? P0 ) + P0 (f (xi ) + b)]+ ? .
(11)
{i|yi =?1}

2

False negatives/positives respectively designate positive/negative examples incorrectly classified.

5
4

0.6
0.4
0.2
0
?10

1

3

p(1|x)

L(g(x),1)

p(1|x)

1
0.8

2

0.5

1

0

10

f(x)+b

20

0
?10

0

10

f(x)+b

20

0
0

0.25

0.5

0.75

1

g(x)

Figure 4: Left: lower (dashed) and upper (plain) posterior probabilities [g(x) +
?? (x), g(x) + ?+ (x)] vs. SVM scores f (x) + b obtained from (11) with P0 = 0.25; center:
corresponding neg-log-likelihood of g(x) for positive examples vs. f (x) + b. right: lower
(dashed) and upper (plain) posterior probabilities vs. g(x), for g defined by ?+ (x) = 0 for
f (x) + b ? 0 and ?? (x) = 0 for f (x) + b ? 0.

This loss differs from (10), in the respect that the margin for positive examples is smaller
than the one for negative examples when P0 < 0.5. In particular, (10) does not affect
the SVM solution for separable problems, while in (11), the decision boundary moves
towards positive support vectors when P0 decreases. The analogue of Figure 3, displayed
on Figure 4, shows that one recovers the characteristics of the standard SVM loss, except
that the focus is now on the posterior probability P0 defined by Bayes? decision rule.

5

Experiments with Unbalanced Classifications Losses

It is straightforward to implement (11) in standard SVM packages. For experimenting with
difficult unbalanced two-class problems, we used the Forest database, the largest available
UCI dataset (http://kdd.ics.uci.edu/databases/covertype/). We consider the subproblem of discriminating the positive class Krummholz (20510 examples)
against the negative class Spruce/Fir (211840 examples). The ratio of negative to positive
examples is high, a feature commonly encountered with unbalanced classification losses.
The training set was built by random selection of size 11 000 (1000 and 10 000 examples
from the positive and negative class respectively); a validation set, of size 11 000 was drawn
identically among the other examples; finally, the test set, of size 99 000, was drawn among
the remaining examples.
The performance was measured by the weighted risk function R = n1 (NFN ?FN +NFP ?FP ),
where NFN and NFP are the number of false negatives and false positives, respectively. The
loss ?FP was set to one, and ?FN was successively set to 1, 10 and 100, in order to penalize
more and more heavily errors from the under-represented class.
All approaches were tested using SVMs with a Gaussian kernel on normalized data. The
hyper-parameters were tuned on the validation set for each of the ?FN values. We additionally considered three tuning for the bias b: ?b is the bias returned by the algorithm; ?bv the
bias returned by minimizing R on the validation set, which is an optimistic estimate of the
bias that could be computed by cross-validation. We also provide results for b? , the optimal
bias computed on the test set. This ?crystal ball? tuning may not represent an achievable
goal, but it shows how far we are from the optimum. Table 1 compares the risk R obtained
with the three approaches for the different values of ?FN .
The first line, with ?FN = 1 corresponds to the standard classification error, where all
training criteria are equivalent in theory and in practice. The bias returned by the algorithm
is very close to the optimal one. For ?FN = 10 and ?FN = 100, the models obtained
by optimizing C + /C ? (10) and P0 (11) achieve better results than the baseline with the
crystal ball bias. While the solutions returned by C + /C ? can be significantly improved

Table 1: Errors for 3 different criteria and for 3 different models over the Forest database
?FN
1
10
100

Baseline, problem (5)
?b
b?
0.027
0.026
0.167
0.108
1.664
0.406

C + /C ? , problem (10)
?b
?bv
b?
0.027 0.027 0.026
0.105 0.104 0.094
0.403 0.291 0.289

P0 , problem (11)
?b
?bv
b?
0.027 0.027 0.026
0.095 0.104 0.094
0.295 0.291 0.289

by tuning the bias, our criterion provides results that are very close to the optimum, in the
range of the performances obtained with the bias optimized on an independant validation
set. The new optimization criterion can thus outperform standard approaches for highly
unbalanced problems.

6

Conclusion

This paper introduced a semi-parametric model for classification which provides an interesting viewpoint on SVMs. The non-parametric component provides an intuitive means of
transforming the likelihood into a decision-oriented criterion. This framework was used
here to propose a new parameterization of the hinge loss, dedicated to unbalanced classification problems, yielding significant improvements over the classical procedure.
Among other prospectives, we plan to apply the same framework to investigate hinge-like
criteria for decision rules including a reject option, where the classifier abstains when a
pattern is ambiguous. We also aim at defining losses encouraging sparsity in probabilistic
models, such as kernelized logistic regression. We could thus build sparse probabilistic
classifiers, providing an accurate estimation of posterior probabilities on a (limited) predefined range of posterior probabilities. In particular, we could derive decision-oriented
criteria for multi-class probabilistic classifiers. For example, minimizing classification error only requires to find the class with highest posterior probability, and this search does
not require precise estimates of probabilities outside the interval [1/K, 1/2], where K is
the number of classes.

References
Y. Lin, Y. Lee, and G. Wahba. Support vector machines for classification in non-standard situations.
Machine Learning, 46:191?202, 2002.
B. G. Lindsay. Nuisance parameters. In S. Kotz, C. B. Read, and D. L. Banks, editors, Encyclopedia
of Statistical Sciences, volume 6. Wiley, 1985.
G. J. McLachlan. Discriminant analysis and statistical pattern recognition. Wiley, 1992.
K. Morik, P. Brockhausen, and T. Joachims. Combining statistical learning with a knowledge-based
approach - a case study in intensive care monitoring. In Proceedings of ICML, 1999.
D. Oakes. Semi-parametric models. In S. Kotz, C. B. Read, and D. L. Banks, editors, Encyclopedia
of Statistical Sciences, volume 8. Wiley, 1988.
J. C. Platt. Probabilities for SV machines. In A. J. Smola, P. L. Bartlett, B. Sch?olkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers, pages 61?74. MIT Press, 2000.
P. Sollich. Probabilistic methods for support vector machines. In S. A. Solla, T. K. Leen, and K.-R.
M?uller, editors, Advances in Neural Information Processing Systems 12, pages 349?355, 2000.
K. Veropoulos, C. Campbell, and N. Cristianini. Controlling the sensitivity of support vector machines. In T. Dean, editor, Proc. of the IJCAI, pages 55?60, 1999.
T. Zhang. Statistical behavior and consistency of classification methods based on convex risk minimization. Annals of Statistics, 32(1):56?85, 2004.

"
2016,One-vs-Each Approximation to Softmax for Scalable Estimation of Probabilities,Poster,6468-one-vs-each-approximation-to-softmax-for-scalable-estimation-of-probabilities.pdf,"The softmax representation of probabilities for categorical variables plays a prominent role in modern machine learning with numerous applications in areas such as large scale classification, neural language modeling and recommendation systems. However, softmax estimation is very expensive for large scale inference because of the high cost associated with computing the normalizing constant. Here, we introduce an efficient approximation to softmax probabilities which takes the form of a rigorous lower bound on the exact probability. This bound is expressed as a product over pairwise probabilities and it leads to scalable estimation based on stochastic optimization. It allows us to perform doubly stochastic estimation by subsampling both training instances and class labels. We show that the new bound has interesting theoretical properties and we demonstrate its use in classification problems.","One-vs-Each Approximation to Softmax for Scalable
Estimation of Probabilities
Michalis K. Titsias
Department of Informatics
Athens University of Economics and Business
mtitsias@aueb.gr

Abstract
The softmax representation of probabilities for categorical variables plays a prominent role in modern machine learning with numerous applications in areas such as
large scale classification, neural language modeling and recommendation systems.
However, softmax estimation is very expensive for large scale inference because
of the high cost associated with computing the normalizing constant. Here, we
introduce an efficient approximation to softmax probabilities which takes the form
of a rigorous lower bound on the exact probability. This bound is expressed as a
product over pairwise probabilities and it leads to scalable estimation based on
stochastic optimization. It allows us to perform doubly stochastic estimation by
subsampling both training instances and class labels. We show that the new bound
has interesting theoretical properties and we demonstrate its use in classification
problems.

1

Introduction

Based on the softmax representation, the probability of a variable y to take the value k ? {1, . . . , K},
where K is the number of categorical symbols or classes, is modeled by
efk (x;w)
p(y = k|x) = PK
,
fm (x;w)
m=1 e

(1)

where each fk (x; w) is often referred to as the score function and it is a real-valued function indexed
by an input vector x and parameterized by w. The score function measures the compatibility of input
x with symbol y = k so that the higher the score is the more compatible x becomes with y = k. The
most common application of softmax is multiclass classification where x is an observed input vector
and fk (x; w) is often chosen to be a linear function or more generally a non-linear function such as a
neural network [3, 8]. Several other applications of softmax arise, for instance, in neural language
modeling for learning word vector embeddings [15, 14, 18] and also in collaborating filtering for
representing probabilities of (user, item) pairs [17]. In such applications the number of symbols
K could often be very large, e.g. of the order of tens of thousands or millions, which makes the
computation of softmax probabilities very expensive due to the large sum in the normalizing constant
of Eq. (1). Thus, exact training procedures based on maximum likelihood or Bayesian approaches
are computationally prohibitive and approximations are needed. While some rigorous bound-based
approximations to the softmax exists [5], they are not so accurate or scalable and therefore it would
be highly desirable to develop accurate and computationally efficient approximations.
In this paper we introduce a new efficient approximation to softmax probabilities which takes the
form of a lower bound on the probability of Eq. (1). This bound draws an interesting connection
between the exact softmax probability and all its one-vs-each pairwise probabilities, and it has several
desirable properties. Firstly, for the non-parametric estimation case it leads to an approximation of the
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

likelihood that shares the same global optimum with exact maximum likelihood, and thus estimation
based on the approximation is a perfect surrogate for the initial estimation problem. Secondly, the
bound allows for scalable learning through stochastic optimization where data subsampling can be
combined with subsampling categorical symbols. Thirdly, whenever the initial exact softmax cost
function is convex the bound remains also convex.
Regarding related work, there exist several other methods that try to deal with the high cost of softmax
such as methods that attempt to perform the exact computations [9, 19], methods that change the
model based on hierarchical or stick-breaking constructions [16, 13] and sampling-based methods
[1, 14, 7, 11]. Our method is a lower bound based approach that follows the variational inference
framework. Other rigorous variational lower bounds on the softmax have been used before [4, 5],
however they are not easily scalable since they require optimizing data-specific variational parameters.
In contrast, the bound we introduce in this paper does not contain any variational parameter, which
greatly facilitates stochastic minibatch training. At the same time it can be much tighter than previous
bounds [5] as we will demonstrate empirically in several classification datasets.

2

One-vs-each lower bound on the softmax

Here, we derive the new bound on the softmax (Section 2.1) and we prove its optimality property when
performing approximate maximum likelihood estimation (Section 2.2). Such a property holds for the
non-parametric case, where we estimate probabilities of the form p(y = k), without conditioning
on some x, so that the score functions fk (x; w) reduce to unrestricted parameters fk ; see Eq. (2)
below. Finally, we also analyze the related bound derived by Bouchard [5] and we compare it with
our approach (Section 2.3).
2.1

Derivation of the bound

Consider a discrete random variable y ? {1, . . . , K} that takes the value k with probability,
efk
p(y = k) = Softmaxk (f1 , . . . , fK ) = PK
m=1

efm

,

(2)

where each fk is a free real-valued scalar parameter. We wish to express a lower bound on p(y = k)
and the key step of our derivation is to re-write p(y = k) as
p(y = k) =

1+

1
.
?(fk ?fm )
m6=k e

(3)

P

Then, by exploiting the fact that for any non-negative numbers ?1 and ?2 it P
holds 1 + Q
?1 + ?2 ?
1 + ?1 + ?2 + ?1 ?2 = (1 + ?1 )(1 + ?2 ), and more generally it holds (1 + i ?i ) ? i (1 + ?i )
where each ?i ? 0, we obtain the following lower bound on the above probability,
p(y = k) ?

Y
m6=k

1
1 + e?(fk ?fm )

=

Y
m6=k

Y
efk
=
?(fk ? fm ).
f
f
m
k
e +e

(4)

m6=k

where ?(?) denotes the sigmoid function. Clearly, the terms in the product are pairwise probabilities
each corresponding to the event y = k conditional on the union of pairs of events, i.e. y ? {k, m}
where m is one of the remaining values. We will refer to this bound as one-vs-each bound on the
softmax probability, since it involves K ? 1 comparisons of a specific event y = k versus each of the
K ? 1 remaining events. Furthermore, the above result can be stated more generally to define bounds
on arbitrary probabilities as the following statement shows.
Proposition 1. Assume a probability model with state space ? and probability measure P (?). For
any event A ? ? and an associated countable set of disjoint events {Bi } such that ?i Bi = ? \ A, it
holds
Y
P (A) ?
P (A|A ? Bi ).
(5)
i
PP
(A)
Proof. Given that P (A) = PP (A)
(?) = P (A)+ i P (Bi ) , the result follows by applying the inequality
P
Q
(1 + i ?i ) ? i (1 + ?i ) exactly as done above for the softmax parameterization.

2

Remark. If the set {Bi } consists of a single event B then by definition B = ? \ A and the bound is
exact since in such case P (A|A ? B) = P (A).
Furthermore, based on the above construction we can express a full class of hierarchically ordered
bounds. For instance, if we merge two events Bi and Bj into a single one, then the term P (A|A ?
Bi )P (A|A ? Bj ) in the initial bound is replaced with P (A|A ? Bi ? Bj ) and the associated new
bound, obtained after this merge, can only become tighter. To see a more specific example in the
softmax probabilistic model, assume a small subset of categorical symbols Ck , that does not include
k, and denote the remaining symbols excluding k as C?k so that k ? Ck ? C?k = {1, . . . , K}. Then, a
tighter bound, that exists higher in the hierarchy, than the one-vs-each bound (see Eq. 4) takes the
form,
Y
p(y = k) ? Softmaxk (fk , fCk ) ? Softmaxk (fk , fC?k ) ? Softmaxk (fk , fCk ) ?
?(fk ? fm ), (6)
m?C?k

where Softmaxk (fk , fCk ) =

fk
Pe
efk + m?C efm

and Softmaxk (fk , fC?k ) =

k

fk
Pe
efk + m?C? efm

. For sim-

k

plicity of our presentation in the remaining of the paper we do not discuss further these more general
bounds and we focus only on the one-vs-each bound.
The computationally useful aspect of the bound in Eq. (4) is that it factorizes into a product, where
each factor depends only on a pair of parameters (fk , fm ). Crucially, this avoids the evaluation of the
normalizing constant associated with the global probability in Eq. (2) and, as discussed in Section 3, it
leads to scalable training using stochastic optimization that can deal with very large K. Furthermore,
approximate maximum likelihood estimation based on the bound can be very accurate and, as shown
in the next section, it is exact for the non-parametric estimation case.
The fact that the one-vs-each bound in (4) is a product of pairwise probabilities suggests that there
is a connection with Bradley-Terry (BT) models [6, 10] for learning individual skills from paired
comparisons and the associated multiclass classification systems obtained by combining binary
classifiers, such as one-vs-rest and one-vs-one approaches [10]. Our method differs from BT models,
since we do not combine binary probabilistic models to a posteriori form a multiclass model. Instead,
we wish to develop scalable approximate algorithms that can surrogate the training of multiclass
softmax-based models by maximizing lower bounds on the exact likelihoods of these models.
2.2

Optimality of the bound for maximum likelihood estimation

Assume a set of observation (y1 , . . . , yN ) where each yi ? {1, . . . , K}. The log likelihood of the
data takes the form,
N
K
Y
Y
L(f ) = log
p(yi ) = log
p(y = k)Nk ,
(7)
i=1

k=1

where f = (f1 , . . . , fK ) and Nk denotes the number of data points with value k. By substituting
p(y = k) from Eq. (2) and then taking derivatives with respect to f we arrive at the standard stationary
conditions of the maximum likelihood solution,
efk
Nk
, k = 1, . . . , K.
(8)
=
PK
f
m
N
m=1 e
These stationary conditions are satisfied for fk = log Nk + c where c ? R is an arbitrary constant.
What is rather surprising is that the same solutions fk = log Nk + c satisfy also the stationary
conditions when maximizing a lower bound on the exact log likelihood obtained from the product of
one-vs-each probabilities.
More precisely, by replacing p(y = k) with the bound from Eq. (4) we obtain a lower bound on the
exact log likelihood,
?
?Nk
K
fk
Y
Y
X
e
?
? =
F(f ) = log
log P (fk , fm ),
(9)
efk + efm
k=1

where P (fk , fm ) =

h

fk

e
efk +efm

iNk h

m6=k

fm

e
efk +efm

k>m

iNm

is a likelihood involving only the data of the pair

of states (k, m), while there exist K(K ? 1)/2 possible such pairs. If instead of maximizing the exact
log likelihood from Eq. (7) we maximize the lower bound we obtain the same parameter estimates.
3

Proposition 2. The maximum likelihood parameter estimates fk = log Nk + c, k = 1, . . . , K for
the exact log likelihood from Eq. (7) globally also maximize the lower bound from Eq. (9).
Proof. By computing the derivatives of F(f ) we obtain the following stationary conditions
K ?1=

X Nk + Nm
efk
, k = 1, . . . , K,
Nk
efk + efm

(10)

m6=k

which form a system of K non-linear equations over the unknowns (f1 , . . . , fK ). By substituting
the values fk = log Nk + c we can observe that all K equations are simultaneously satisfied which
means that these values are solutions. Furthermore, since F(f ) is a concave function of f we can
conclude that the solutions fk = log Nk + c globally maximize F(f ).
Remark. Not only is F(f ) globally maximized by setting fk = log Nk + c, but also each pairwise
likelihood P (fk , fm ) in Eq. (9) is separately maximized by the same setting of parameters.
2.3

Comparison with Bouchard?s bound

Bouchard [5] proposed a related bound that next we analyze in terms of its ability to approximate the
exact maximum likelihood training in the non-parametric case, and then we compare it against our
method. Bouchard [5] was motivated by the problem of applying variational Bayesian inference to
multiclass classification and he derived the following upper bound on the log-sum-exp function,
log

K
X
m=1

efm ? ? +

K
X


log 1 + efm ?? ,

(11)

m=1

where ? ? R is a variational parameter that needs to be optimized in order for the bound to become
as tight as possible. The above induces a lower bound on the softmax probability p(y = k) from Eq.
(2) that takes the form
efk ??
p(y = k) ? QK
.
(12)
fm ?? )
m=1 (1 + e
This is not the same as Eq. (4), since there is not a value for ? for which the above bound will reduce
to our proposed one. For instance, if we set ? = fk , then Bouchard?s bound becomes half the one
in Eq. (4) due to the extra term 1 + efk ?fk = 2 in the product in the denominator.1 Furthermore,
such a value for ? may not be the optimal one and in practice ? must be chosen by minimizing
the upper bound in Eq. (11). While such an optimization is a convex problem, it requires iterative
optimization since there is not in general an analytical solution for ?. However, for the simple case
where K = 2 we can analytically find the optimal ? and the optimal f parameters. The following
proposition carries out this analysis and provides a clear understanding of how Bouchard?s bound
behaves when applied for approximate maximum likelihood estimation.
Proposition 3. Assume that K = 2 and we approximate the probabilities p(y = 1) and
ef1 ??
p(y = 2) from (2) with the corresponding Bouchard?s bounds given by (1+ef1 ??
and
)(1+ef2 ?? )
ef2 ??
.
(1+ef1 ?? )(1+ef2 ?? )

These bounds are used to approximate the maximum likelihood solution by
maximizing a bound F(f1 , f2 , ?) which is globally maximized for
?=

f1 + f2
, fk = 2 log Nk + c, k = 1, 2.
2

(13)

The proof of the above is given in the Supplementary material. Notice that the above estimates are
biased so that the probability of the most populated class (say the y = 1 for which N1 > N2 ) is
overestimated while the other probability is underestimated. This is due to the factor 2 that multiplies
log N1 and log N2 in (13).
2
Also notice that the solution ? = f1 +f
is not a general trend, i.e. for K > 2 the optimal ? is not the
2
mean of fk s. In such cases approximate maximum likelihood estimation based on Bouchard?s bound
requires iterative optimization. Figure 1a shows some estimated softmax probabilities, using a dataset

1

Notice that the product in Eq. (4) excludes the value k, while Bouchard?s bound includes it.

4

of 200 points each taking one out of ten values, where f is found by exact maximum likelihood, the
proposed one-vs-each bound and Bouchard?s method. As expected estimation based on the bound in
Eq. (4) gives the exact probabilities, while Bouchard?s bound tends to overestimate large probabilities
and underestimate small ones.
0.25
0.2

1

0.15

0

?100

0.1

Lower bound

Estimated Probability

?50
2

?150
?200

?1
?250

0.05
?2
0

1

2

3

4

5 6 7
Values

8

9

?300
0

10
?3

?2

?1

(a)

0

1

2

(b)

2000

4000
6000
Iterations

8000

10000

(c)

Figure 1: (a) shows the probabilities estimated by exact softmax (blue bar), one-vs-each approximation (red bar)
and Bouchard?s method (green bar). (b) shows the 5-class artificial data together with the decision boundaries
found by exact softmax (blue line), one-vs-each (red line) and Bouchard?s bound (green line). (c) shows the
maximized (approximate) log likelihoods for the different approaches when applied to the data of panel (b)
(see Section 3). Notice that the blue line in (c) is the exact maximized log likelihood while the remaining lines
correspond to lower bounds.

3

Stochastic optimization for extreme classification

Here, we return to the general form of the softmax probabilities as defined by Eq. (1) where the
score functions are indexed by input x and parameterized by w. We consider a classification task
where given a training set {xn , yn }N
n=1 , where yn ? {1, . . . , K}, we wish to fit the parameters w by
maximizing the log likelihood,
L = log

N
Y

efyn (xn ;w)
.
PK
fm (xn ;w)
n=1
m=1 e

(14)

When the number of training instances is very large, the above maximization can be carried out by
applying stochastic gradient descent (by minimizing ?L) where we cycle over minibatches. However,
this stochastic optimization procedure cannot deal with large values of K because the normalizing
constant in the softmax couples all scores functions so that the log likelihood cannot be expressed as
a sum across class labels. To overcome this, we can use the one-vs-each lower bound on the softmax
probability from Eq. (4) and obtain the following lower bound on the previous log likelihood,
F = log

N
Y
Y
n=1 m6=yn

1
1+

e?[fyn (xn ;w)?fm (xn ;w)]

=?

N X
X



log 1 + e?[fyn (xn ;w)?fm (xn ;w)]

n=1 m6=yn

(15)
which now
P consists of a sum over both data points and labels. Interestingly, the sum over the
labels, m6=yn , runs over all remaining classes that are different from the label yn assigned to xn .
Each term in the sum is a logistic regression cost, that depends on the pairwise score difference
fyn (xn ; w)?fm (xn ; w), and encourages the n-th data point to get separated from the m-th remaining
class. The above lower bound can be optimized by stochastic gradient descent by subsampling terms
in the double sum in Eq. (15), thus resulting in a doubly stochastic approximation scheme. Next we
further discuss the stochasticity associated with subsampling remaining classes.
The gradient for the cost associated with a single training instance (xn , yn ) is
X
?Fn =
? (fm (xn ; w) ? fyn (xn ; w)) [?w fyn (xn ; w) ? ?w fm (xn ; w)] .

(16)

m6=yn

This gradient consists of a weighted sum where the sigmoidal weights ? (fm (xn ; w) ? fyn (xn ; w))
quantify the contribution of the remaining classes to the whole gradient; the more a remaining
class overlaps with yn (given xn ) the higher its contribution is. A simple way to get an unbiased
stochastic estimate of (16) is to randomly subsample a small subset of remaining classes from the set
{m|m 6= yn }. More advanced schemes could be based on importance sampling where we introduce
5

a proposal distribution pn (m) defined on the set {m|m 6= yn } that could favor selecting classes with
large sigmoidal weights. While such more advanced schemes could reduce variance, they require
prior knowledge (or on-the-fly learning) about how classes overlap with one another. Thus, in Section
4 we shall experiment only with the simple random subsampling approach and leave the above
advanced schemes for future work.
To illustrate the above stochastic gradient descent algorithm we simulated a two-dimensional data set
of 200 instances, shown in Figure 1b, that belong to five classes. We consider a linear classification
model where the score functions take the form fk (xn , w) = wkT xn and where the full setPof
parameters is w = (w1 , . . . , wK ). We consider minibatchesP
of size ten to approximate the sum n
and subsets of remaining classes of size one to approximate m6=yn . Figure 1c shows the stochastic
evolution of the approximate log likelihood (dashed red line), i.e. the unbiased subsampling based
approximation of (15), together with the maximized exact softmax log likelihood (blue line), the
non-stochastically maximized approximate lower bound from (15) (red solid line) and Bouchard?s
method (green line). To apply Bouchard?s method we construct a lower bound on the log likelihood
by replacing each softmax probability with the bound from (12) where we also need to optimize a
separate variational parameter ?n for each data point. As shown in Figure 1c our method provides a
tighter lower bound than Bouchard?s method despite the fact that it does not contain any variational
parameters. Also, Bouchard?s method can become very slow when combined with stochastic gradient
descent since it requires tuning a separate variational parameter ?n for each training instance. Figure
1b also shows the decision boundaries discovered by the exact softmax, one-vs-each bound and
Bouchard?s bound. Finally, the actual parameters values found by maximizing the one-vs-each bound
were remarkably close (although not identical) to the parameters found by the exact softmax.

4

Experiments

4.1

Toy example in large scale non-parametric estimation

Here, we illustrate the ability to stochastically maximize the bound in Eq. (9) for the simple nonparametric estimation case. In such case, we can also maximize the bound based on the analytic
formulas and therefore we will be able to test how well the stochastic algorithm can approximate
the optimal/known solution. We consider a data set of N = 106 instances each taking one out of
K = 104 possible categorical values. The data were generated from a distribution p(k) ? u2k , where
each uk was randomly chosen in [0, 1]. The probabilities estimated based on the analytic formulas
are shown in Figure 2a. To stochastically estimate these probabilities we follow the doubly stochastic
framework of Section 3 so that we subsample data instances of minibatch size b = 100 and for each
instance we subsample 10 remaining categorical values. We use a learning rate initialized to 0.5/b
(and then decrease it by a factor of 0.9 after each epoch) and performed 2 ? 105 iterations. Figure
2b shows the final values for the estimated probabilities, while Figure 2c shows the evolution of the
estimation error during the optimization iterations. We can observe that the algorithm performs well
and exhibits a typical stochastic approximation convergence.
?4

?4

x 10

x 10

0.7
3.5

3
2.5
2
1.5
1
0.5
0
0

0.6

3

0.5

2.5
Error

Estimated Probability

Estimated Probability

3.5

2

4000
6000
Values

(a)

8000

10000

0.3
0.2

1

0.1

0.5
2000

0.4

1.5

0
0

2000

4000
6000
Values

(b)

8000

10000

0
0

0.5

1
Iterations

1.5

2
5

x 10

(c)

Figure 2: (a) shows the optimally estimated probabilities which have been sorted for visualizations purposes. (b)
shows the corresponding probabilities estimated by stochastic optimization. (c) shows the absolute norm for the
vector of differences between exact estimates and stochastic estimates.

4.2 Classification
Small scale classification comparisons. Here, we wish to investigate whether the proposed lower
bound on the softmax is a good surrogate for exact softmax training in classification. More precisely,
we wish to compare the parameter estimates obtained by the one-vs-each bound with the estimates
6

obtained by exact softmax training. To quantify closeness we use the normalized absolute norm
norm =

|wsoftmax ? w? |
,
|wsoftmax |

(17)

where wsoftmax denotes the parameters obtained by exact softmax training and w? denotes estimates
obtained by approximate training. Further, we will also report predictive performance measured by
classification error and negative log predictive density (nlpd) averaged across test data,
error = (1/Ntest )

N
test
X

I(yi 6= ti ),

nlpd = (1/Ntest )

i=1

N
test
X

? log p(ti |xi ),

(18)

i=1

where ti denotes the true label of a test point and yi the predicted one. We trained the linear multiclass
model of Section 3 with the following alternative methods: exact softmax training (SOFT), the onevs-each bound (OVE), the stochastically optimized one-vs-each bound (OVE - SGD) and Bouchard?s
bound (BOUCHARD). For all approaches, the associated cost function was maximized together with
an added regularization penalty term, ? 12 ?||w||2 , which ensures that the global maximum of the cost
function is achieved for finite w. Since we want to investigate how well we surrogate exact softmax
training, we used the same fixed value ? = 1 in all experiments.
We considered three small scale multiclass classification datasets: MNIST2 , 20 NEWS3 and BIBTEX
[12]; see Table 1 for details. Notice that BIBTEX is originally a multi-label classification dataset [2].
where each example may have more than one labels. Here, we maintained only a single label for each
data point in order to apply standard multiclass classification. The maintained label was the first label
appearing in each data entry in the repository files4 from which we obtained the data.
Figure 3 displays convergence of the lower bounds (and for the exact softmax cost) for all methods.
Recall, that the methods SOFT, OVE and BOUCHARD are non-stochastic and therefore their optimization can be carried out by standard gradient descent. Notice that in all three datasets the one-vs-each
bound gets much closer to the exact softmax cost compared to Bouchard?s bound. Thus, OVE tends to
give a tighter bound despite that it does not contain any variational parameters, while BOUCHARD has
N extra variational parameters, i.e. as many as the training instances. The application of OVE - SGD
method (the stochastic version of OVE) is based on a doubly stochastic scheme where we subsample
minibatches of size 200 and subsample remaining classes of size one. We can observe that OVE - SGD
is able to stochastically approach its maximum value which corresponds to OVE.
Table 2 shows the parameter closeness score from Eq. (17) as well as the classification predictive
scores. We can observe that OVE and OVE - SGD provide parameters closer to those of SOFT than the
parameters provided by BOUCHARD. Also, the predictive scores for OVE and OVE - SGD are similar to
SOFT, although they tend to be slightly worse. Interestingly, BOUCHARD gives the best classification
error, even better than the exact softmax training, but at the same time it always gives the worst nlpd
which suggests sensitivity to overfitting. However, recall that the regularization parameter ? was
fixed to the value one and it was not optimized separately for each method using cross validation.
Also notice that BOUCHARD cannot be easily scaled up (with stochastic optimization) to massive
datasets since it introduces an extra variational parameter for each training instance.
Large scale classification. Here, we consider AMAZONCAT-13 K (see footnote 4) which is a large
scale classification dataset. This dataset is originally multi-labelled [2] and here we maintained only
a single label, as done for the BIBTEX dataset, in order to apply standard multiclass classification.
This dataset is also highly imbalanced since there are about 15 classes having the half of the training
instances while they are many classes having very few (or just a single) training instances.
Further, notice that in this large dataset the number of parameters we need to estimate for the linear
classification model is very large: K ? (D + 1) = 2919 ? 203883 parameters where the plus one
accounts for the biases. All methods apart from OVE - SGD are practically very slow in this massive
dataset, and therefore we consider OVE - SGD which is scalable.
We applied OVE - SGD where at each stochastic gradient update we consider a single training instance
(i.e. the minibatch size was one) and for that instance we randomly select five remaining classes. This
2

http://yann.lecun.com/exdb/mnist
http://qwone.com/~jason/20Newsgroups/
4
http://research.microsoft.com/en-us/um/people/manik/downloads/XC/XMLRepository.
html
3

7

Table 1: Summaries of the classification datasets.

Name

Dimensionality

Classes

Training examples

Test examples

MNIST
20 NEWS
BIBTEX
AMAZONCAT-13 K

784
61188
1836
203882

10
20
148
2919

60000
11269
4880
1186239

10000
7505
2515
306759

Table 2: Score measures for the small scale classification datasets.

MNIST
20 NEWS
BIBTEX

SOFT

BOUCHARD

OVE

(error, nlpd)

(norm, error, nlpd)

(norm, error, nlpd)

OVE - SGD
(norm, error, nlpd)

(0.074, 0.271)
(0.272, 1.263)
(0.622, 2.793)

(0.64, 0.073, 0.333)
(0.65, 0.249, 1.337)
(0.25, 0.621, 2.955)

(0.50, 0.082, 0.287)
(0.05, 0.276, 1.297)
(0.09, 0.636, 2.888)

(0.53, 0.080, 0.278)
(0.14, 0.276, 1.312)
(0.10, 0.633, 2.875)

4

x 10

0

?6
?7
0

0.5

1
1.5
2
5
Iterations
x 10

(a)

?2000
?2500
?3000

Lower bound

SOFT
OVE
OVE?SGD
BOUCHARD

Lower bound

?4

Lower bound

Lower bound

?3

?5

?3000

?1500

?2

?4000
?5000

?400
?600
?800

?3500
?4000
0

?200

5
Iterations

?6000
0

10
5
x 10

(b)

5
Iterations

(c)

10
5
x 10

?1000
0

5
Iterations

10
5
x 10

(d)

Figure 3: (a) shows the evolution of the lower bound values for MNIST, (b) for 20 NEWS and (c) for BIBTEX. For
more clear visualization the bounds of the stochastic OVE - SGD have been smoothed using a rolling window of
400 previous values. (d) shows the evolution of the OVE - SGD lower bound (scaled to correspond to a single data
point) in the large scale AMAZONCAT-13 K dataset. Here, the plotted values have been also smoothed using a
rolling window of size 4000 and then thinned by a factor of 5.

leads to sparse parameter updates, where the score function parameters of only six classes (the class
of the current training instance plus the remaining five ones) are updated at each iteration. We used a
very small learning rate having value 10?8 and we performed five epochs across the full dataset, that
is we performed in total 5 ? 1186239 stochastic gradient updates. After each epoch we halve the
value of the learning rate before next epoch starts. By taking into account also the sparsity of the input
vectors each iteration is very fast and full training is completed in just 26 minutes in a stand-alone
PC. The evolution of the variational lower bound that indicates convergence is shown in Figure 3d.
Finally, the classification error in test data was 53.11% which is significantly better than random
guessing or by a method that decides always the most populated class (where in AMAZONCAT-13 K
the most populated class occupies the 19% of the data so the error of that method is around 79%).

5

Discussion

We have presented the one-vs-each lower bound on softmax probabilities and we have analyzed
its theoretical properties. This bound is just the most extreme case of a full family of hierarchically ordered bounds. We have explored the ability of the bound to perform parameter estimation
through stochastic optimization in models having large number of categorical symbols, and we have
demonstrated this ability to classification problems.
There are several directions for future research. Firstly, it is worth investigating the usefulness of the
bound in different applications from classification, such as for learning word embeddings in natural
language processing and for training recommendation systems. Another interesting direction is to
consider the bound not for point estimation, as done in this paper, but for Bayesian estimation using
variational inference.
Acknowledgments
We thank the reviewers for insightful comments. We would like also to thank Francisco J. R. Ruiz for
useful discussions and David Blei for suggesting the name one-vs-each for the proposed method.
8

References
[1] Yoshua Bengio and Jean-S?bastien S?n?cal. Quick training of probabilistic neural nets by importance
sampling. In Proceedings of the conference on Artificial Intelligence and Statistics (AISTATS), 2003.
[2] Kush Bhatia, Himanshu Jain, Purushottam Kar, Manik Varma, and Prateek Jain. Sparse local embeddings
for extreme multi-label classification. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and
R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 730?738. Curran
Associates, Inc., 2015.
[3] Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics).
Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2006.
[4] D. Bohning. Multinomial logistic regression algorithm. Annals of the Inst. of Statistical Math, 44:197?200,
1992.
[5] Guillaume Bouchard. Efficient bounds for the softmax function and applications to approximate inference
in hybrid models. Technical report, 2007.
[6] R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. The method of paired
comparisons. Biometrika, 39(3/4):324?345, 1952.
[7] Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul.
Fast and robust neural network joint models for statistical machine translation. In Proceedings of the
52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
1370?1380, Baltimore, Maryland, June 2014. Association for Computational Linguistics.
[8] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. Book in preparation for MIT Press,
2016.
[9] Siddharth Gopal and Yiming Yang. Distributed training of large-scale logistic models. In Sanjoy Dasgupta
and David Mcallester, editors, Proceedings of the 30th International Conference on Machine Learning
(ICML-13), pages 289?297. JMLR Workshop and Conference Proceedings, 2013.
[10] Tzu-Kuo Huang, Ruby C. Weng, and Chih-Jen Lin. Generalized Bradley-Terry models and multi-class
probability estimates. J. Mach. Learn. Res., 7:85?115, December 2006.
[11] Shihao Ji, S. V. N. Vishwanathan, Nadathur Satish, Michael J. Anderson, and Pradeep Dubey. Blackout:
Speeding up recurrent neural network language models with very large vocabularies. 2015.
[12] Ioannis Katakis, Grigorios Tsoumakas, and Ioannis Vlahavas. Multilabel text classification for automated
tag suggestion. In In: Proceedings of the ECML/PKDD-08 Workshop on Discovery Challenge, 2008.
[13] Mohammad Emtiyaz Khan, Shakir Mohamed, Benjamin M. Marlin, and Kevin P. Murphy. A stickbreaking likelihood for categorical data analysis with latent Gaussian models. In Proceedings of the
Fifteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2012, La Palma,
Canary Islands, April 21-23, 2012, pages 610?618, 2012.
[14] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of
words and phrases and their compositionality. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani,
and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3111?3119.
Curran Associates, Inc., 2013.
[15] Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language
models. In Proceedings of the 29th International Conference on Machine Learning, pages 1751?1758,
2012.
[16] F. Morin and Y. Bengio. Hierarchical probabilistic neural network language model. In Proceedings of the
Tenth International Workshop on Artificial Intelligence and Statistics, pages 246?252. Citeseer, 2005.
[17] Ulrich Paquet, Noam Koenigstein, and Ole Winther. Scalable Bayesian modelling of paired symbols.
CoRR, abs/1409.2824, 2012.
[18] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pages 1532?1543, Doha, Qatar, October 2014. Association for Computational Linguistics.
[19] Sudheendra Vijayanarasimhan, Jonathon Shlens, Rajat Monga, and Jay Yagnik. Deep networks with large
output spaces. CoRR, abs/1412.7479, 2014.

9

"
2016,Incremental Variational Sparse Gaussian Process Regression,Poster,6473-incremental-variational-sparse-gaussian-process-regression.pdf,"Recent work on scaling up Gaussian process regression (GPR) to large datasets has primarily focused on sparse GPR, which leverages a small set of basis functions to approximate the full Gaussian process during inference.  However, the majority of these approaches are batch methods that operate on the entire training dataset at once, precluding the use of datasets that are streaming or too large to fit into memory. Although previous work has considered incrementally solving variational sparse GPR, most algorithms fail to update the basis functions and therefore perform suboptimally. We propose a novel incremental learning algorithm for variational sparse GPR based on stochastic mirror ascent of probability densities in reproducing kernel Hilbert space. This new formulation allows our algorithm to update basis functions online in accordance with the manifold structure of probability densities for fast convergence. We conduct several experiments and show that our proposed approach achieves better empirical performance in terms of prediction error than  the recent state-of-the-art incremental solutions to variational sparse GPR.","Incremental Variational Sparse Gaussian Process
Regression

Ching-An Cheng
Institute for Robotics and Intelligent Machines
Georgia Institute of Technology
Atlanta, GA 30332
cacheng@gatech.edu

Byron Boots
Institute for Robotics and Intelligent Machines
Georgia Institute of Technology
Atlanta, GA 30332
bboots@cc.gatech.edu

Abstract
Recent work on scaling up Gaussian process regression (GPR) to large datasets has
primarily focused on sparse GPR, which leverages a small set of basis functions
to approximate the full Gaussian process during inference. However, the majority
of these approaches are batch methods that operate on the entire training dataset
at once, precluding the use of datasets that are streaming or too large to fit into
memory. Although previous work has considered incrementally solving variational
sparse GPR, most algorithms fail to update the basis functions and therefore
perform suboptimally. We propose a novel incremental learning algorithm for
variational sparse GPR based on stochastic mirror ascent of probability densities
in reproducing kernel Hilbert space. This new formulation allows our algorithm
to update basis functions online in accordance with the manifold structure of
probability densities for fast convergence. We conduct several experiments and
show that our proposed approach achieves better empirical performance in terms of
prediction error than the recent state-of-the-art incremental solutions to variational
sparse GPR.

1

Introduction

Gaussian processes (GPs) are nonparametric statistical models widely used for probabilistic reasoning
about functions. Gaussian process regression (GPR) can be used to infer the distribution of a latent
function f from data. The merit of GPR is that it finds the maximum a posteriori estimate of
the function while providing the profile of the remaining uncertainty. However, GPR also has
drawbacks: like most nonparametric learning techniques the time and space complexity of GPR
scale polynomially with the amount of training data. Given N observations, inference of GPR
involves inverting an N ? N covariance matrix which requires O(N 3 ) operations and O(N 2 ) storage.
Therefore, GPR for large N is infeasible in practice.
Sparse Gaussian process regression is a pragmatic solution that trades accuracy against computational complexity. Instead of parameterizing the posterior using all N observations, the idea is
to approximate the full GP using the statistics of finite M  N function values and leverage the
induced low-rank structure to reduce the complexity to O(M 2 N + M 3 ) and the memory to O(M 2 ).
? = {?
Often sparse GPRs are expressed in terms of the distribution of f (?
xi ), where X
xi ? X } M
i=1 are
called inducing points or pseudo-inputs [21, 23, 18, 26]. A more general representation leverages the
information about the inducing function (Li f )(?
xi ) defined by indirect measurement of f through a
bounded linear operator Li (e.g. integral) to more compactly capture the full GP [27, 8]. In this work,
we embrace the general notion of inducing functions, which trivially includes f (?
xi ) by choosing Li
? to denote the parameters
to be identity. With abuse of notation, we reuse the term inducing points X
that define the inducing functions.
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

Learning a sparse GP representation in regression can be summarized as inference of the hyperparameters, the inducing points, and the statistics of inducing functions. One approach to learning is
to treat all of the parameters as hyperparameters and find the solution that maximizes the marginal
likelihood [21, 23, 18]. An alternative approach is to view the inducing points and the statistics of
inducing functions as variational parameters of a class of full GPs, to approximate the true posterior of
f , and solve the problem via variational inference, which has been shown robust to over-fitting [26, 1].
All of the above methods are designed for the batch setting, where all of the data is collected in
advance and used at once. However, if the training dataset is extremely large or the data are streaming
and encountered in sequence, we may want to incrementally update the approximate posterior of the
latent function f . Early work by Csat? and Opper [6] proposed an online version of GPR, which
greedily performs moment matching of the true posterior given one sample instead of the posterior of
all samples. More recently, several attempts have been made to modify variational batch algorithms
to incremental algorithms for learning sparse GPs [1, 9, 10]. Most of these methods rely on the
fact that variational sparse GPR with fixed inducing points and hyperparameters is equivalent to
inference of the conjugate exponential family: Hensman et al. [9] propose a stochastic approximation
of the variational sparse GPR problem [26] based on stochastic natural gradient ascent [11]; Hoang
et al. [10] generalizes this approach to the case with general Gaussian process priors. Unlike the
original variational algorithm for sparse GPR [26], which finds the optimal inducing points and
hyperparameters, these algorithms only update the statistics of the inducing functions fX? .
In this paper, we propose an incremental learning algorithm for variational sparse GPR, which
we denote as iVSGPR. Leveraging the dual formulation of variational sparse GPR in reproducing
kernel Hilbert space (RKHS), iVSGPR performs stochastic mirror ascent in the space of probability
densities [17] to update the approximate posterior of f , and stochastic gradient ascent to update the
hyperparameters. Stochastic mirror ascent, similar to stochastic natural gradient ascent, considers the
manifold structure of probability functions and therefore converges faster than the naive gradient approach. In each iteration, iVSGPR solves a variational sparse GPR problem of the size of a minibatch.
As a result, iVSGPR has constant complexity per iteration and can learn all the hyperparameters, the
inducing points, and the associated statistics online.

2

Background

In this section, we provide a brief summary of Gaussian process regression and sparse Gaussian
process regression for efficient inference before proceeding to introduce our incremental algorithm
for variational sparse Gaussian process regression in Section 3.
2.1

Gaussian Processes Regression

Let F be a family of real-valued continuous functions f : X 7? R. A GP is a distribution of
functions f in F such that, for any finite set X ? X , {f (x)|x ? X} is Gaussian distributed
N (f (x)|m(x), k(x, x0 )): for any x, x0 ? X , m(x) and k(x, x0 ) represent the mean of f (x) and the
covariance between f (x) and f (x0 ), respectively. In shorthand, we write f ? GP(m, k).
The mean m(x) and the covariance k(x, x0 ) (the kernel function) are often parametrized by a set of
hyperparameters which encode our prior belief of the unknown function f . In this work, for simplicity,
we assume that m(x) = 0 and the kernel can be parameterized as k(x, x0 ) = ?2 gs (x, x0 ), where
gs (x, x0 ) is a positive definite kernel, ?2 is a scaling factor and s denotes other hyperparameters [20].
The objective of GPR is to infer the posterior probability of the function f given data D =
{(xi , yi )}N
i=1 . In learning, the function value f (xi ) is treated as a latent variable and the observation yi = f (xi ) + i is modeled as the function corrupted by i.i.d. noise i ? N (|0, ? 2 ). Let
X = {xi }N
i=1 . The posterior probability distribution p(f |y) can be compactly summarized as
GP(m|D , k|D ):
m|D (x) = kx,X (KX + ? 2 I)?1 y
0

(1)
2

?1

k|D (x, x ) = kx,x0 ? kx,X (KX + ? I)

kX,x0

(2)

N
1?N
where y = (yi )N
denotes the vector of the cross-covariance between x and X,
i=1 ? R , kx,X ? R
N ?N
and KX ? R
denotes the empirical covariance matrix of the training set. The hyperparameters

2

? := (s, ?, ?) in the GP are learned by maximizing the log-likelihood of the observation y
max log p(y) = max log N (y|0, KX + ? 2 I).
?

2.2

?

(3)

Sparse Gaussian Processes Regression

A straightforward approach to sparse GPR is to approximate the GP prior of interest with a degenerate
GP [21, 23, 18]. Formally, for any xi , xj ? X , it assumes that
f (xi ) ? yi |fX? ,

f (xi ) ? f (xj )|fX? ,

(4)

M

where fX? denotes ((Li f )(?
xi ))i=1 and ? denotes probabilistic independence between two random
variables. That is, the original empirical covariance matrix KX is replaced by a rank-M approxi? X := K ? K ?1 K ? , where K ? is the covariance of f ? and K ? ? RN ?M is the
mation K
?
X,X X
X,X
X
X
X,X
? are
cross-covariance between fX and fX? . Let ? ? RN ?N be diagonal. The inducing points X
treated as hyperparameters and can be found by jointly maximizing the log-likelihood with ?
? X + ? 2 I + ?),
max log N (y|0, K
?
?,X

(5)

Several approaches to sparse GPR can be viewed as special cases of this problem [18]: the Deterministic Training Conditional (DTC) [21] approximation sets ? as zero. To heal the degeneracy in p(fX ),
the Fully Independent Training Conditional (FITC) approximation [23] includes heteroscedastic
? X ). As a result, their sum ? + K
? X matches the true covariance
noise, setting ? = diag(KX ? K
KX in the diagonal term. This general maximum likelihood scheme for finding the inducing points
is adopted with variations in [24, 27, 8, 2]. A major drawback of all of these approaches is that they
? in the prior parametrization [26].
can over-fit due to the high degrees-of-freedom X
Variational sparse GPR can alternatively be formulated to approximate the posterior of the latent
function by a full GP parameterized by the inducing points and the statistics of inducing functions [1,
26]. Specifically, Titsias [26] proposes to use
q(fX , fX? ) = p(fX |fX? )q(fX? )

(6)

? is the Gaussian approximation of p(f ? |y)
to approximate p(fX , fX? |y), where q(fX? ) = N (fX? |m,
? S)
X
?1
?
and p(fX |fX? ) = N (fX |KX,X? KX? fX? , KX ? KX ) is the conditional probability in the full GP. The
novelty here is that q(fX , fX? ), despite parametrization by finite parameters, is still a full GP, which,
unlike its predecessor [21], can be infinite-dimensional.
The inference problem of variational sparse GPR is solved by minimizing the KL-divergence
KL[q(fX , fX? )||p(fX , fX? |y)]. In practice, the minimization problem is transformed into the maximization of the lower bound of the log-likelihood [26]:
Z
p(y|fX )p(fX |fX? )p(fX? )
max log p(y) ? max
q(fX , fX? ) log
dfX dfX?
? m,
?
?
q(fX , fX? )
?,X,
? S
Z
p(y|fX )p(fX? )
= max
p(fX |fX? )q(fX? ) log
dfX dfX?
?
?
q(fX? )
?,X,m,
? S
? X + ? 2 I) ? 1 Tr(KX ? K
? X ).
= max log N (y|0, K
(7)
?
2? 2
?,X
? for treatment of non-conjugate
The last equality results from exact maximization over m
? and S;
? whereas p(f ? ) and p(fX |f ? )
likelihoods, see [22]. We note that q(fX? ) is a function of m
? and S,
X
X
? As a result, X
? become variational parameters that can be optimized without
are functions of X.
over-fitting. Compared with (5), the variational approach in (7) regularizes the learning with penalty
? X ) and therefore exhibits better generalization performance. Several subsequent works
Tr(KX ? K
employ similar strategies: Alvarez et al. [3] adopt the same variational approach in the multi-output
regression setting with scaled basis functions, and Abdel-Gawad et al. [1] use expectation propagation
to solve for the approximate posterior under the same factorization.
3

3

Incremental Variational Sparse Gaussian Process Regression

Despite leveraging sparsity, the batch solution to the variational objective in (7) requires O(M 2 N )
operations and access to all of the training data during each optimization step [26], which means
that learning from large datasets is still infeasible. Recently, several attempts have been made to
incrementally solve the variational sparse GPR problem in order to learn better models from large
datasets [1, 9, 10]. The key idea is to rewrite (7) explicitly into the sum of individual observations:
Z
p(y|fX )p(fX? )
max
p(fX |fX? )q(fX? ) log
dfX dfX?
?
?
q(fX? )
?,X,m,
? S
!
Z
N
X
p(fX? )
dfX? .
(8)
= max
q(fX? )
Ep(fxi |fX? ) [log p(yi |fxi )] + log
? m,
?
q(fX? )
?,X,
? S
i=1
? is identical to the problem of stochastic variational
The objective function in (8), with fixed X,
inference [11] of conjugate exponential families. Hensman et al. [9] exploit this idea to incrementally
update the statistics m
? and S? via stochastic natural gradient ascent,1 which, at the tth iteration, takes
the direction derived from the limit of maximizing (8) subject to KLsym (qt (fX? )||qt?1 (fX? )) <  as
 ? 0. Natural gradient ascent considers the manifold structure of probability distribution derived
?
from KL divergence and is known to be Fisher efficient [4]. Although the optimal inducing points X,
? should be updated given new observations, it is difficult to design natural
like the statistics m
? and S,
? online. Because p(fX |f ? ) in (8) depends on all
gradient ascent for learning the inducing points X
X
the observations, evaluating the divergence with respect to p(fX |fX? )q(fX? ) over iterations becomes
infeasible.
We propose a novel approach to incremental variational sparse GPR, iVSGPR, that works by reformulating (7) in its RKHS dual form. This avoids the issue of the posterior approximation
p(fX |fX? )q(fX? ) referring to all observations. As a result, we can perform stochastic approximation
of (7) while monitoring the KL divergence between the posterior approximates due to the change
? and X
? across iterations. Specifically, we use stochastic mirror ascent [17] in the space
of m,
? S,
of probability densities in RKHS, which was recently proven to be as efficient as stochastic natural
gradient ascent [19]. In each iteration, iVSGPR solves a subproblem of fractional Bayesian inference,
which we show can be formulated into a standard variational sparse GPR of the size of a minibatch in
O(M 2 Nm + M 3 ) operations, where Nm is the size of a minibatch.
3.1

Dual Representations of Gaussian Processes in RKHS

An RKHS H is a Hilbert space of functions satisfying the reproducing property: ?kx ? H such that
?f ? H, f (x) = hf, kx iH . In general, H can be infinite-dimensional and uniformly approximate
continuous functions on a compact set [16]. To simplify the notation we write kxT f for hf, kx iH , and
f T Lg for hf, Lgi, where f, g ? H and L : H 7? H, even if H is infinite-dimensional.
A Gaussian process GP(m, k) has a dual representation in an RKHS H [12]: there exists ? ? H and
a positive semi-definite linear operator ? : H 7? H such that for any x, x0 ? X , ??x , ?x0 ? H,
m(x) = ??Tx ?,

k(x, x0 ) = ?2 ?Tx ??x0 .

(9)

That is, the mean function has a realization ? in H, which is defined by the reproducing kernel
? x0 ) = ?2 ?T ?x0 ; the covariance function can be equivalently represented by a linear operator
k(x,
x
?. In shorthand, with abuse of notation, we write N (f |?, ?).2 Note that we do not assume the
samples from GP(m, k) are in H. In the following, without loss of generality, we assume the GP
prior considered in regression has ? = 0 and ? = I. That is, m(x) = 0 and k(x, x0 ) = ?2 ?Tx ?x0 .
3.1.1

Subspace Parametrization of the Approximate Posterior

The full GP posterior approximation p(fX |fX? )q(fX? ) in (7) can be written equivalently in a subspace
? M :
parametrization using {?x?i ? H|?
xi ? X}
i=1
?
? = ?X? a,

? = I + ? ? A?T? ,
?
X
X

(10)

? was fixed in their experiments, it can potentially be updated by stochastic gradient ascent.
Although X
Because a GP can be infinite-dimensional, it cannot define a density but only a Gaussian measure. The
notation N (f |?, ?) is used to indicate that the Gaussian measure can be defined, equivalently, by ? and ?.
1

2

4

?  0, and ? ? : RM 7? H is defined as ? ? a =
where a ? RM , A ? RM ?M such that ?
X
X
PM
? and define ?x? to satisfy ?T ?
? S)
?
=
m.
?
By
(10),
? ) = N (fX
? |m,
?i . Suppose q(fX
i
?
i=1 ai ?x
X
?
m
? = KX? a and S = KX? + KX? AKX? , which implies the relationship
?1
a = KX
?
? m,

?1 ? ?1
?1
A = KX
? SKX
? ? KX
? ,

(11)

where the covariances related to the inducing functions are defined as KX? = ?TX? ?X? and KX,X? =
?1
?1 ? ?1
??TX ?X? . The sparse structure results in f (x) ? GP(kx,X? KX
? kx,x + kx,X? (KX
? m,
? SKX
? ?
R
?1
KX? )kX,x
p(f (x)|fX? )q(fX? )dfX? , the posterior GP found in (7), where
? ), which is the same as
kx,x = k(x, x) and kx,X? = ??Tx ?X? . We note that the scaling factor ? is associated with the
evaluation of f (x), not the inducing functions fX? . In addition, we distinguish the hyperparameter s
?
(e.g. length scale) that controls the measurement basis ?x from the parameters in inducing points X.
?  0. More precisely, because (10) is
A subspace parametrization corresponds to a full GP if ?
? and the inducing points X,
? the family of subspace
completely determined by the statistics m,
? S,
parametrized GPs lie on a nonlinear submanifold in the space of all GPs (the degenerate GP in (4) is
a special case if we allow I in (10) to be ignored).
3.1.2

Sparse Gaussian Processes Regression in RKHS

We now reformulate the variational inference problem (7) in RKHS3 . Following the previous section,
the sparse GP structure on the posterior approximate q(fX , fX? ) in (6) has a corresponding dual
? Specially, q(f ) and q(fX , f ? ) are related as follows:
representation in RKHS q(f ) = N (f |?
?, ?).
X
? X |1/2 ,
q(f ) ? p(fX |fX? )q(fX? )|KX? |1/2 |KX ? K

(12)

in which the determinant is due to the change of measure. The equality (12) allows us to rewrite (7)
in terms of q(f ) simply as
Z
p(y|f )p(f )
max L(q(f )) = max q(f ) log
df,
(13)
q(f )
q(f )
q(f )
or equivalently as minq(f ) KL[q(f )||p(f |y)]. That is, the heuristically motivated variational problem (7) is indeed minimizing a proper KL-divergence between two Gaussian measures. A similar
justification on (7) is given rigorously in [14] in terms of KL-divergence minimization between
Gaussian processes, which can be viewed as a dual of our approach. Due to space limitations, the
proofs of (12) and the equivalence between (7) and (13) can be found in the Appendix.
The benefit of the formulation of (13) is that in its sampling form,
!
Z
N
X
p(f )
max q(f )
log p(yi |f ) + log
df,
q(f )
q(f )
i=1

(14)

? m,
the approximate posterior q(f ) nicely summarizes all the variational parameters X,
? and S? without
referring to the samples as in p(fX |fX? )q(fX? ). Therefore, the KL-divergence of q(f ) across iterations
can be used to regulate online learning.
3.2

Incremental Learning

Stochastic mirror ascent [17] considers (non-)Euclidean structure on variables induced by a Bregman
divergence (or prox-function) [5] in convex optimization. We apply it to solve the variational
inference problem in (14), because (14) is convex in the space of probabilities [17]. Here, we ignore
the dependency of q(f ) on f for simplicity. At the tth iteration, stochastic mirror ascent solves the
subproblem
Z
? t , yt )q(f )df ? KL[q||qt ],
qt+1 = arg max ?t ?L(q
(15)
q

3

Here we assume the set X is finite and countable. This assumption suffices in learning and allows us to
restrict H be the finite-dimensional span of ?X . Rigorously, for infinite-dimensional H, the equivalence can be
written in terms of Radon?Nikodym derivative between q(f )df and normal Gaussian measure, where q(f )df
denotes a Gaussian measure that has an RKHS representation given as q(f )

5

? t , yt ) is the sampled subgradient of L with respect to q when the
where ?t is the step size, ?L(q
observation is
converges in O(t?1/2 ) if (15) is solved within numerical error
P(xt , yt ). The
P algorithm
2
t such that t ? O( ?t ) [7].
The subproblem (15) is actually equivalent to sparse variational GP regression with a general Gaussian
prior. By definition of L(q) in (14), (15) can be derived as


Z
p(f )
qt+1 = arg max ?t q(f ) N log p(yt |f ) + log
df ? KL[q||qt ]
q
qt (f )
Z
p(yt |f )N ?t p(f )?t qt1??t (f )
= arg max q(f ) log
df.
(16)
q
q(f )
This equation is equivalent to (13) but with the prior modified to p(f )?t qt (f )1??t and the likelihood
modified to p(yi |f )N ?t . Because p(f ) is an isotropic zero-mean Gaussian, p(f )?t qt (f )1??t has the
subspace parametrization expressed in the same basis functions as qt . Suppose qt has mean ?
?t and
?t
1??t
? ?1
?
precision ?
.
Then
p(f
)
q
(f
)
is
equivalent
to
N
(f
|?
?
,
?)
up
to
a
constant
factor,
where
t
t
?1
?1
?1
?1
?1
?1
?
?
?
?
?
?
?t = (1 ? ?t )?t ?t ?
?t and ?t = (1 ? ?t )?t + ?t I. By (10), ?t = I ? ?X? (At + KX? ) ?X?
?1
? ?1
for some At , and therefore ?
= I ? (1 ? ?t )?X? (A?1
?X? , which is expressed in the
?)
t
t + KX
same basis. In addition, by (12), (16) can be further written in the form of (7) and therefore solved by
a standard sparse variational GPR program with modified m
? and S? (Please see Appendix for details).
Although we derived the equations for a single observation, minibatchs can be used with the same
N ?t
QNm
p(yti |f ) Nm . The
convergence rate and reduced variance by changing the factor p(yt |f )N ?t to i=1
hyperparameters ? = (s, ?, ?) in the GP can be updated along with the variational parameters using
R
)p(f )
stochastic gradient ascent along the gradient of qt (f ) log p(yqt |f
df .
t (f )
3.3

Related Work

The subproblem (16) is equivalent to first performing stochastic natural gradient ascent [11] of q(f )
in (14) and then projecting the distribution back to the low-dimensional manifold specified by the
subspace parametrization. At the tth iteration, define qt0 (f ) := p(yt |f )N ?t p(f )?t qt (f )1??t . Because
a GP can be viewed as Gaussian measure in an infinite-dimensional RKHS, qt0 (f ) (16) can be viewed
as the result of taking natural stochastic gradient ascent with step size ?t from qt (f ). Then (16)
becomes minq KL[q||qt0 ] in order to project qt0 back to subspace parametrization specified by M basis
functions. Therefore, (16) can also be viewed as performing stochastic natural gradient ascent with a
? which controls the inducing
KL divergence projection. From this perspective, we can see that if X,
functions, are fixed in the subproblem (16), iVSGPR degenerates to the algorithm of Hensman et
al. [9].
Recently, several researches have considered the manifold structure induced by KL divergence in
Bayesian inference [7, 25, 13]. Theis and Hoffman [25] use trust regions to mitigate the sensitivity of
stochastic variational inference to choices of hyperparameters and initialization. Let ?t be the size
of the trust region. At the tth iteration, it solves the objective maxq L(q) ? ?t KL[q||qt ], which is
the same as subproblem (16) if applied to (14). The difference is that in (16) ?t is a decaying step
sequence in stochastic mirror ascent, whereas ?t is manually selected. A similar formulation also
appears in [13], where the part of L(q) non-convex to the variational parameters is linearized. Dai et
? in the setting with
al. [7] use particles or a kernel density estimator to approximate the posterior of X
low-rank GP prior. By contrast, we follow Titsias?s variational approach [26] to adopt a full GP as
? and
the approximate posterior, and therefore avoid the difficulties in estimating the posterior of X
focus on the approximate posterior q(f ) related to the function values.
The stochastic mirror ascent framework sheds light on the convergence condition of the algorithm.
P As
pointed out
in
Dai
et
al.
[7],
the
subproblem
(15)
can
be
solved
up
to

accuracy
as
long
as
t is
t
?
P
order O( ?t2 ), where ?t ? O(1/ t) [17]. Also, Khan et al. [13] solves a linearized approximation
of (15) in each step and reports satisfactory empirical results. Although variational sparse GPR (16) is
? and is often solved by nonlinear conjugate gradient ascent, empirically
a nonconvex optimization in X
the objective function increases most significantly in the first few iterations. Therefore, based on the
results in [7], we argue that in online learning (16) can be solved approximately by performing a
small fixed number of line searches.
6

4

Experiments

We compare our method iVSGPR with VSGPRsvi the state-of-the-art variational sparse GPR method
based on stochastic variational inference [9], in which i.i.d. data are sampled from the training
dataset to update the models. We consider a zero-mean GP prior generated by a squared-exponential
QD
?(xd ?x0d )2
),
with automatic relevance determination (SE-ARD) kernel [20] k(x, x0 ) = ?2 d=1 exp(
2s2d
where sd > 0 is the length scale of dimension d and D is the dimensionality of the input. For the
inducing functions, we modified the multi-scale kernel in [27] to
?xT ?x0

=

D
Y
i=d

2lx,d lx0 ,d
2 + l2
lx,d
x0 ,d

!1/2
exp ?

D
X
kxd ? x0 k2
d=1

2 +
lx,d

d
lx2 0 ,d

!
,

(17)

where lx,d is the length-scale parameter. The definition (17) includes the SE-ARD kernel as a special
D
case, which can be recovered by identifying ?x = ?x and (lx,d )D
d=1 = (sd )d=1 , and hence their cross
covariance can be computed.
In the following experiments, we set the number inducing functions to 512. All models were
initialized with the same hyperparameters and inducing points: the hyperparameters were selected as
the optimal ones in the batch variational sparse GPR [26] trained on subset of the training dataset
of size 2048; the inducing points were initialized
as random samples from the first minibatch. We
?
chose the learning rate to be ?t = (1 + t)?1 , for stochastic mirror ascent to update the posterior
approximation; the learning rate for the stochastic gradient ascent to update the hyperparameters is
set to 10?4 ?t . We evaluate the models in terms of the normalized mean squared error (nMSE) on a
held-out test set after 500 iterations.
We performed experiments on three real-world robotic datasets datasets, kin40k4 , SARCOS5 , KUKA6 ,
and three variations of iVSGPR: iVSGPR5 , iVSGPR10 , and iVSGPRada .7 For the kin40k and SARCOS
datasets, we also implemented VSGPR?svi , which uses stochastic variational inference to update m
?
and S? but fixes hyperparameters and inducing points as the solution to the batch variational sparse
GPR [26] with all of the training data. Because VSGPR?svi reflects the perfect scenario of performing
stochastic approximation under the selected learning rate, we consider it as the optimal goal we want
to approach.
The experimental results of kin40k and SARCOS are summarized in Table 1a. In general, the adaptive
scheme iVSGPRada performs the best, but we observe that even performing a small fixed number of
iterations ( iVSGPR5 , iVSGPR10 ) results in performance that is close to, if not better than VSGPR?svi .
Possible explanations are that the change of objective function in gradient-based algorithms is
dominant in the first few iterations and that the found inducing points and hyper-parameters have
finite numerical resolution in batch optimization. For example, Figure 1a shows the change of test
error over iterations in learning joint 2 of SARCOS dataset. For all methods, the convergence rate
improves with a larger minibatch. In addition, from Figure 1b, we observe that the required number
of steps iVSGPRada needed to solve (16) decays with the number of iterations; only a small number
line searches is required after the first few iterations.
Table 1b and Table 1c show the experimental results on two larger datasets. In the experiments, we
mixed the offline and online partitions in the original KUKA dataset and then split 90% into training
and 10% into testing datasets in order to create an online i.i.d. streaming scenario. We did not
compare to VSGPR?svi on these datasets, since computing the inducing points and hyperparameters
in batch is infeasible. As above, iVSGPRada stands out from other models, closely followed by
iVSGPR10 . We found that the difference between VSGPRsvi and iVSGPRs is much greater on these
larger real-world benchmarks.
Auxiliary experimental results illustrating convergence for all experiments summarized in Tables 1a, 1b, and 1c are included in the Appendix.
4

kin40k: 10000 training data, 30000 testing data, 8 attributes [23]
SARCOS: 44484 training data, 4449 testing data, 28 attributes. http://www.gaussianprocess.org/gpml/data/
6
KUKA1&KUKA2: 17560 offline data, 180360 online data, 28 attributes. [15]
7
The number in the subscript denotes the number of function calls allowed in nonlinear conjugate gradient
descent [20] to solve subproblems (16) and ada denotes (16) is solved until the relative function change is less
than 10?5 .
5

7

kin40k
SARCOS J1
SARCOS J2
SARCOS J3
SARCOS J4
SARCOS J5
SARCOS J6
SARCOS J7

VSGPRsvi

iVSGPR5

iVSGPR10

iVSGPRada

VSGPR?
svi

0.0959
0.0247
0.0193
0.0125
0.0048
0.0267
0.0300
0.0101

0.0648
0.0228
0.0176
0.0112
0.0044
0.0243
0.0259
0.0090

0.0608
0.0214
0.0159
0.0104
0.0040
0.0229
0.0235
0.0082

0.0607
0.0210
0.0156
0.0103
0.0038
0.0226
0.0229
0.0081

0.0535
0.0208
0.0156
0.0104
0.0039
0.0230
0.0230
0.0101

(a) kin40k and SARCOS

J1
J2
J3
J4
J5
J6
J7

VSGPRsvi

iVSGPR5

iVSGPR10

iVSGPRada

0.1699
0.1530
0.1873
0.1376
0.1955
0.1766
0.1374

0.1455
0.1305
0.1554
0.1216
0.1668
0.1645
0.1357

0.1257
0.1221
0.1403
0.1151
0.1487
0.1573
0.1342

0.1176
0.1138
0.1252
0.1108
0.1398
0.1506
0.1333

J1
J2
J3
J4
J5
J6
J7

(b) KUKA1

VSGPRsvi

iVSGPR5

iVSGPR10

iVSGPRada

0.1737
0.1517
0.2108
0.1357
0.2082
0.1925
0.1329

0.1452
0.1312
0.1818
0.1171
0.1846
0.1890
0.1309

0.1284
0.1183
0.1652
0.1104
0.1697
0.1855
0.1287

0.1214
0.1081
0.1544
0.1046
0.1598
0.1809
0.1275

(c) KUKA2

Table 1: Testing error (nMSE) after 500 iterations. Nm = 2048; Ji denotes the ith joint.

(a) Test error

(b) Functions calls of iVSGPRada

Figure 1: Online learning results of SARCOS joint 2. (a) nMSE evaluated on the held out test set; the
dash lines and the solid lines denote the results with Nm = 512 and Nm = 2048, respectively. (b)
Number of function calls used by iVSGPRada in solving (16) (A maximum of 100 calls is imposed )

5

Conclusion

We propose a stochastic approximation of variational sparse GPR [26], iVSGPR. By reformulating
the variational inference in RKHS, the update of the statistics of the inducing functions and the
inducing points can be unified as stochastic mirror ascent on probability densities to consider the
manifold structure. In our experiments, iVSGPR shows better performance than the direct adoption of
stochastic variational inference to solve variational sparse GPs. As iVSGPR executes a fixed number
of operations for each minibatch, it is suitable for applications where training data is abundant, e.g.
sensory data in robotics. In future work, we are interested in applying iVSGPR to extensions of sparse
Gaussian processes such as GP-LVMs and dynamical system modeling.

References
[1] Ahmed H Abdel-Gawad, Thomas P Minka, et al. Sparse-posterior gaussian processes for general likelihoods. arXiv preprint arXiv:1203.3507, 2012.
[2] Mauricio Alvarez and Neil D Lawrence. Sparse convolved gaussian processes for multi-output regression.
In Advances in neural information processing systems, pages 57?64, 2009.
[3] Mauricio A Alvarez, David Luengo, Michalis K Titsias, and Neil D Lawrence. Efficient multioutput gaussian processes through variational inducing kernels. In International Conference on Artificial Intelligence
and Statistics, pages 25?32, 2010.

8

[4] Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251?276,
1998.
[5] Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, and Joydeep Ghosh. Clustering with bregman
divergences. The Journal of Machine Learning Research, 6:1705?1749, 2005.
[6] Lehel Csat? and Manfred Opper. Sparse on-line gaussian processes. Neural computation, 14(3):641?668,
2002.
[7] Bo Dai, Niao He, Hanjun Dai, and Le Song. Scalable bayesian inference via particle mirror descent. arXiv
preprint arXiv:1506.03101, 2015.
[8] Anibal Figueiras-vidal and Miguel L?zaro-gredilla. Inter-domain gaussian processes for sparse inference
using inducing features. In Advances in Neural Information Processing Systems, pages 1087?1095, 2009.
[9] James Hensman, Nicolo Fusi, and Neil D Lawrence. Gaussian processes for big data. arXiv preprint
arXiv:1309.6835, 2013.
[10] Trong Nghia Hoang, Quang Minh Hoang, and Kian Hsiang Low. A unifying framework of anytime sparse
gaussian process regression models with stochastic variational inference for big data. In Proc. ICML, pages
569?578, 2015.
[11] Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. The
Journal of Machine Learning Research, 14(1):1303?1347, 2013.
[12] Irina Holmes and Ambar N Sengupta. The gaussian radon transform and machine learning. Infinite
Dimensional Analysis, Quantum Probability and Related Topics, 18(03):1550019, 2015.
[13] Mohammad E Khan, Pierre Baqu?, Fran?ois Fleuret, and Pascal Fua. Kullback-leibler proximal variational
inference. In Advances in Neural Information Processing Systems, pages 3384?3392, 2015.
[14] Alexander G de G Matthews, James Hensman, Richard E Turner, and Zoubin Ghahramani. On sparse
variational methods and the kullback-leibler divergence between stochastic processes. In Proceedings of
the Nineteenth International Conference on Artificial Intelligence and Statistics, 2016.
[15] Franziska Meier, Philipp Hennig, and Stefan Schaal. Incremental local gaussian regression. In Advances
in Neural Information Processing Systems, pages 972?980, 2014.
[16] Charles A Micchelli, Yuesheng Xu, and Haizhang Zhang. Universal kernels. The Journal of Machine
Learning Research, 7:2651?2667, 2006.
[17] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574?1609,
2009.
[18] Joaquin Qui?onero-Candela and Carl Edward Rasmussen. A unifying view of sparse approximate gaussian
process regression. The Journal of Machine Learning Research, 6:1939?1959, 2005.
[19] Garvesh Raskutti and Sayan Mukherjee. The information geometry of mirror descent. Information Theory,
IEEE Transactions on, 61(3):1451?1457, 2015.
[20] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine learning. 2006.
[21] Matthias Seeger, Christopher Williams, and Neil Lawrence. Fast forward selection to speed up sparse
gaussian process regression. In Artificial Intelligence and Statistics 9, number EPFL-CONF-161318, 2003.
[22] Rishit Sheth, Yuyang Wang, and Roni Khardon. Sparse variational inference for generalized gp models. In
Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1302?1311,
2015.
[23] Edward Snelson and Zoubin Ghahramani. Sparse gaussian processes using pseudo-inputs. In Advances in
neural information processing systems, pages 1257?1264, 2005.
[24] Edward Snelson and Zoubin Ghahramani. Local and global sparse gaussian process approximations. In
International Conference on Artificial Intelligence and Statistics, pages 524?531, 2007.
[25] Lucas Theis and Matthew D Hoffman. A trust-region method for stochastic variational inference with
applications to streaming data. arXiv preprint arXiv:1505.07649, 2015.
[26] Michalis K Titsias. Variational learning of inducing variables in sparse gaussian processes. In International
Conference on Artificial Intelligence and Statistics, pages 567?574, 2009.
[27] Christian Walder, Kwang In Kim, and Bernhard Sch?lkopf. Sparse multiscale gaussian process regression.
In Proceedings of the 25th international conference on Machine learning, pages 1112?1119. ACM, 2008.

9

"
2014,Design Principles of the Hippocampal Cognitive Map,Spotlight,5340-design-principles-of-the-hippocampal-cognitive-map.pdf,"Hippocampal place fields have been shown to reflect behaviorally relevant aspects of space. For instance, place fields tend to be skewed along commonly traveled directions, they cluster around rewarded locations, and they are constrained by the geometric structure of the environment. We hypothesize a set of design principles for the hippocampal cognitive map that explain how place fields represent space in a way that facilitates navigation and reinforcement learning. In particular, we suggest that place fields encode not just information about the current location, but also predictions about future locations under the current transition distribution. Under this model, a variety of place field phenomena arise naturally from the structure of rewards, barriers, and directional biases as reflected in the transition policy. Furthermore, we demonstrate that this representation of space can support efficient reinforcement learning. We also propose that grid cells compute the eigendecomposition of place fields in part because is useful for segmenting an enclosure along natural boundaries. When applied recursively, this segmentation can be used to discover a hierarchical decomposition of space. Thus, grid cells might be involved in computing subgoals for hierarchical reinforcement learning.","Design Principles of the Hippocampal Cognitive Map

Kimberly L. Stachenfeld1 , Matthew M. Botvinick1 , and Samuel J. Gershman2
Princeton Neuroscience Institute and Department of Psychology, Princeton University
2
Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology
kls4@princeton.edu, matthewb@princeton.edu, sjgershm@mit.edu
1

Abstract
Hippocampal place fields have been shown to reflect behaviorally relevant aspects
of space. For instance, place fields tend to be skewed along commonly traveled
directions, they cluster around rewarded locations, and they are constrained by the
geometric structure of the environment. We hypothesize a set of design principles
for the hippocampal cognitive map that explain how place fields represent space
in a way that facilitates navigation and reinforcement learning. In particular, we
suggest that place fields encode not just information about the current location,
but also predictions about future locations under the current transition distribution. Under this model, a variety of place field phenomena arise naturally from
the structure of rewards, barriers, and directional biases as reflected in the transition policy. Furthermore, we demonstrate that this representation of space can
support efficient reinforcement learning. We also propose that grid cells compute
the eigendecomposition of place fields in part because is useful for segmenting an
enclosure along natural boundaries. When applied recursively, this segmentation
can be used to discover a hierarchical decomposition of space. Thus, grid cells
might be involved in computing subgoals for hierarchical reinforcement learning.

1

Introduction

A cognitive map, as originally conceived by Tolman [46], is a geometric representation of the environment that can support sophisticated navigational behavior. Tolman was led to this hypothesis
by the observation that rats can acquire knowledge about the spatial structure of a maze even in the
absence of direct reinforcement (latent learning; [46]). Subsequent work has sought to formalize the
representational content of the cognitive map [13], the algorithms that operate on it [33, 35], and its
neural implementation [34, 27]. Much of this work was galvanized by the discovery of place cells
in the hippocampus [34], which selectively respond when an animal is in a particular location, thus
supporting the notion that the brain contains an explicit map of space. The later discovery of grid
cells in the entorhinal cortex [16], which respond periodically over the entire environment, indicated
a possible neural substrate for encoding metric information about space.
Metric information is very useful if one considers the problem of spatial navigation to be computing the shortest path from a starting point to a goal. A mechanism that accumulates a record of
displacements can easily compute the shortest path back to the origin, a technique known as path
integration. Considerable empirical evidence supports the idea that animals use this technique for
navigation [13]. Many authors have proposed theories of how grid cells and place cells can be used
to carry out the necessary computations [27].
However, the navigational problems faced by humans and animals are inextricably tied to the more
general problem of reward maximization, which cannot be reduced to the problem of finding the
shortest path between two points. This raises the question: does the brain employ the same machinery for spatial navigation and reinforcement learning (RL)? A number of authors have suggested
how RL mechanisms can support spatial learning, where spatial representations (e.g., place cells or
1

grid cells), serve as the input to the learning system [11, 15]. In contrast to the view that spatial representation is extrinsic to the RL system, we pursue the idea that the brain?s spatial representations
are designed to support RL. In particular, we show how spatial representations resembling place
cells and grid cells emerge as the solution to the problem of optimizing spatial representation in the
service of RL.
We first review the formal definition of the RL problem, along with several algorithmic solutions.
Special attention is paid to the successor representation (SR) [6], which enables a computationally
convenient decomposition of value functions. We then show how the successor representation naturally comes to represent place cells when applied to spatial domains. The eigendecomposition of
the successor representation reveals properties of an environment?s spectral graph structure, which
is particularly useful for discovering hierarchical decompositions of space. We demonstrate that the
eigenvectors resemble grid cells, and suggest that one function of the entorhinal cortex may be to
encode a compressed representation of space that aids hierarchical RL [3].

2

The reinforcement learning problem

Here we consider the problem of RL in a Markov decision process, which consists of the following
elements: a set of states S, a set of actions A, a transition distribution P (s0 |s, a) specifying the
probability of transitioning to state s0 ? S from state s ? S after taking action a ? A, a reward
function R(s) specifying the expected reward in state s, and a discount factor ? ? [0, 1]. An agent
chooses actions according to a policy ?(a|s) and collects rewards as it moves through the state space.
The standard RL problem
P? is to choose a policy that maximizes the value (expected discounted future
return), V (s) = E? [ t=0 ? t R(st ) | s0 = s]. Our focus here is on policy evaluation (computing V ).
In our simulations we feed the agent the optimal policy; in the Supplementary Materials we discuss
algorithms for policy improvement. To simplify notation,
P we assume implicit dependence on ? and
define the state transition matrix T , where T (s, s0 ) = a ?(a|s)P (s0 |s, a).
Most work on RL has focused on two classes of algorithms for policy evaluation: ?model-free?
algorithms that estimate V directly from sample paths, and ?model-based? algorithms that estimate
T and R from sample paths and then compute V by some form of dynamic programming or tree
search [44, 5]. However, there exists a third class that has received less attention. As shown by
Dayan [6], the value function can be decomposed into the inner product of the reward function with
the SR, denoted by M :
P
V (s) = s0 M (s, s0 )R(s0 ),
M = (I ? ?T )?1
(1)
where I denotes the identity matrix. The SR encodes the expected discounted future occupancy of
state s0 along a trajectory initiated in state s:
P?
M (s, s0 ) = E [ t=0 ? t I{st = s0 } | s0 = s] ,
(2)
where I{?} = 1 if its argument is true, and 0 otherwise.
The SR obeys a recursion analogous to the Bellman equation for value functions:
P
M (s, j) = I{s = j} + ? s0 T (s, s0 )M (s0 , j).
(3)
This recursion can be harnessed to derive a temporal difference learning algorithm for incrementally
? of the SR [6, 14]. After observing a transition s ? s0 , the estimate is
updating an estimate M
updated according to:
h
i
? (s, j) ? M
? (s, j) + ? I{s = j} + ? M
? (s0 , j) ? M
? (s, j) ,
M
(4)
where ? is a learning rate (unless specified otherwise, ? = 0.1 in our simulations). The SR combines
some of the advantages of model-free and model-based algorithms: like model-free algorithms,
policy evaluation is computationally efficient, but at the same time the SR provides some of the same
flexibility as model-based algorithms. As we illustrate later, an agent using the SR will be sensitive
to distal changes in reward, whereas a model-free agent will be insensitive to these changes.

3

The successor representation and place cells

In this section, we explore the neural implications of using the SR for policy evaluation: if the brain
encoded the SR, what would the receptive fields of the encoding population look like, and what
2

Empty Room
1.8

1.8

Single Barrier

c

e

1.8

1.8

2.1
1.2

5.6

1.9

1.8

1.8

1.2

1.3

1.2

1.4

1.8

f
1.6

1.3
1.3

Reward (+)

d
5.6

1.2

1.9

1.8

1.8

b

Multiple Rooms
No Reward

a

1.4

Discounted expected visiations (SR)

Figure 1: SR place fields. Top two rows show place fields without reward, bottom two show
retrospective place fields with reward (marked by +). Maximum firing rate (a.u.) indicated for each
plot. (a, b) Empty room. (c, d) Single barrier. (e, f) Multiple rooms.
Direction Selectivity

4

0.2
0.4
0.6
0.8
1

3

Figure 2: Direction selectivity along a
track. Direction selectivity arises in SR
place fields when the probability p?
of transitioning in the preferred left-toright direction along a linear track is
greater than the probability p? of transitioning in the non-preferred direction.
The legend shows the ratio of p? to p?
for each simulation.

2

1

0
100

150

200

250

300

Distance along Track

350

400

would the population look like at any point in time? This question is most easily addressed in spatial
domains, where states index spatial locations (see Supplementary Materials for simulation details).
For an open field with uniformly distributed rewards we assume a random walk policy, and the
resulting SR for a particular location is an approximately symmetric, gradually decaying halo around
that location (Fig. 1a)?the canonical description of a hippocampal place cell. In order for the
population to encode the expected visitations to each state in the domain from the current starting
state (i.e. a row of M ), each receptive field corresponds to a column of the SR matrix. This allows
the current state?s value to be computed by taking the dot product of its population vector with the
reward vector. The receptive field (i.e. column of M ) will encode the discounted expected number
of times that state was visited for each starting state, and will therefore skew in the direction of the
states that likely preceded the current state.
More interesting predictions can be made when we examine the effects of obstacles and direction
preference that shape the transition structure. For instance, when barriers are inserted into the environment, the probability of transitioning across these obstacles will go to zero. SR place fields
are therefore constrained by environmental geometry, and the receptive field will be discontinuous
across barriers (Fig. 1c,e). Consistent with this idea, experiments have shown that place fields become distorted around barriers [32, 40]. When an animal has been trained to travel in a preferred
direction along a linear track, we expect the response of place fields to become skewed opposite the
direction of travel (Fig. 2), a result that has been observed experimentally [28, 29].
Another way to alter the transition policy is by introducing a goal, which induces a tendency to move
in the direction that maximizes reward. Under these conditions, we expect firing fields centered near
rewarded locations to expand to include the surrounding locations and to increase their firing rate,
as has been observed experimentally [10, 21]. Meanwhile, we expect the majority of place fields
3

Percentage of Neurons Firing

f

a4
2
0

0.4

Depth

b

0.2
0

Distance around annular track

Figure 3: Reward clustering in annular maze. (a) Histogram of number of cells firing above
baseline at each displacement around an annular track. (b) Heat map of number of firing cells at
each location on unwrapped annular maze. Reward is centered on track. Baseline firing rate set to
10% maximum.

late detour

early detour

no detour

a

b

c

d

1.25

1.15

1.49

1.60

1.15

1.49

2.36

1.08

1.49

Firing Fields

Value

Figure 4: Tolman detour task. The starting location is at the bottom of the maze where the
three paths meet, and the reward is at the top. Barriers are shown as black horizontal lines. Three
conditions are shown: No detour, early detour, and late detour. (a, b, c) SR place fields centered near
and far from detours. Maximum firing rate (a.u.) indicated by each plot. (d) Value function.

that encode non-rewarded states to skew slightly away from the reward. Under certain settings
for what firing rate constitutes baseline (see Supplementary Materials), the spread of the rewarded
locations? fields compensates for the skew of surrounding fields away from the reward, and we
observe ?clustering? around rewarded locations (Fig. 3), as has been observed experimentally in the
annular water maze task [18]. This parameterization sensitivity may explain why goal-related firing
is not observed in all tasks [25, 24, 41].
As another illustration of the model?s response to barriers, we simulated place fields in a version
of the Tolman detour task [46], as described in [1]. Rats are trained to move from the start to the
rewarded location. At some point, an ?early? or a ?late? transparent barrier is placed in the maze
so that the rat must take a detour. For the early barrier, a short detour is available, and for the later
barrier, the only detour is a longer one. Place fields near the detour are more strongly affected than
places far away from the detour (Fig. 4a,b,c), consistent with experimental findings [1]. Fig. 4d
shows the value function in each of these detour conditions.

4

Behavioral predictions: distance estimation and latent learning

In this section, we examine some of the behavioral consequences of using the SR for RL. We first
show that the SR anticipates biases in distance estimation induced by semi-permeable boundaries.
We then explore the ability of the SR to support latent learning in contextual fear conditioning.
4

b

75

4

50

3
2

25

1

0
0

a 18
16
14

0.5

0

1

Permeability

SR Distance

b
Lesion
Control

12
10
6
4
2
0

1

2

Preexposure Duration (steps)

3

x 10

5

Figure 6: Context preexposure facilitation
effect. (a) Simulated conditioned response
(CR) to the context following one-trial contextual fear conditioning, shown as a function of
preexposure duration. The CR was approximated as the negative value summed over the
environment. The ?Lesion? corresponds to
agents with hippocampal damage, simulated by
setting the SR learning rate to 0.01. The ?Control? group has a learning rate of 0.1. (b) value
for a single location after preexposure in a control agent. (c) same as (b) in a lesioned agent.

0
?0.2
?0.4
?0.6
?0.8

c

8

0

Control

0
?0.1

Value

Conditioned Response

Figure 5: Distance estimates. (a) Increase in
the perceived distance between two points on
opposite sides of a semipermeable boundary
(marked with + and ? in 5b) as a function of
barrier permeability. (b) Perceived distance between destination (market with +) and all other
locations in the space (barrier permeability =
0.05).

5

Value

Distance (% Increase)

a

?0.2

Lesion

?0.3

Stevens and Coupe [43] reported that people overestimated the distance between two locations when
they were separated by a boundary (e.g., a state or country line). This bias was hypothesized to arise
from a hierarchical organization of space (see also [17]). We showp(Fig. 5) how distance estimates
(using the Euclidean distance between SR state representations, (M (s0 ) ? M (s))2 , as a proxy
for the perceived distance between s and s0 ) between points in different regions of the environment
are altered when an enclosure is divided by a soft (semi-permeable) boundary. We see that as the
permeability of the barrier decreases (making the boundary harder), the percent increase in perceived
distance between locations increases without bound. This gives rise to a discontinuity in perceived
travel time at the soft boundary. Interestingly, the hippocampus is directly involved in distance
estimation [31], suggesting the hippocampal cognitive map as a neural substrate for distance biases
(although a direct link has yet to be established).
The context preexposure facilitation effect refers to the finding that placing an animal inside a conditioning chamber prior to shocking it facilitates the acquisition of contextual fear [9]. In essence, this
is a form of latent learning [46]. The facilitation effect is thought to arise from the development of a
conjunctive representation of the context in the hippocampus, though areas outside the hippocampus
may also develop a conjunctive representation in the absence of the hippocampus, albeit less efficiently [48]. The SR provides a somewhat different interpretation: over the course of preexposure,
the hippocampus develops a predictive representation of the context, such that subsequent learning
is rapidly propagated across space. Fig. 6 shows a simulation of this process and how it accounts
for the facilitation effect. We simulated hippocampal lesions by reducing the SR learning rate from
0.1 to 0.01, resulting in a more punctate SR following preexposure and a reduced facilitation effect.

5

Eigendecomposition of the successor representation: hierarchical
decomposition and grid cells

Reinforcement learning and navigation can often be made more efficient by decomposing the environment hierarchically. For example, the options framework [45] utilizes a set of subgoals to divide
and conquer a complex learning environment. Recent experimental work suggests that the brain may
exploit a similar strategy [3, 36, 8]. A key problem, however, is discovering useful subgoals; while
progress has been made on this problem in machine learning, we still know very little about how the
brain solves it (but see [37]). In this section, we show how the eigendecomposition of the SR can
be used to discover subgoals. The resulting eigenvectors strikingly resemble grid cells observed in
entorhinal cortex.
5

a

Open Room

b

Single Barrier

c

Multiple Rooms

Figure 7: Eigendecomposition of the SR. Each panel shows the same 20 eigenvectors randomly
sampled from the top 100 (excluding the constant first eigenvector) for the environmental geometries
shown in Fig. 1 (no reward). (a) Empty room. (b) Single barrier. (c) Multiple rooms.
Eigendecomposition

Figure 8: Eigendecomposition of the SR in a
hairpin maze. Since the walls of the maze effectively elongate a dimension of travel (the track
of the maze), the low frequency eigenvectors resemble one-dimensional sinusoids that have been
folded to match the space. Meanwhile, the low
frequency eigenvectors exhibit the compartmentalization shown by [7].

A number of authors have used graph partitioning techniques to discover subgoals [30, 39]. These
approaches cluster states according to their community membership (a community is defined as a
highly interconnected set of nodes with relatively few outgoing edges). Transition points between
communities (bottleneck states) are then used as subgoals. One important graph partitioning technique, used by [39] to find subgoals, is the normalized cuts algorithm [38], which recursively thresholds the second smallest eigenvector (the Fiedler vector) of the normalized graph Laplacian to obtain
a graph partition. Given an undirected graph with symmetric weight matrix W , the graph Laplacian
is given by L = D ? W . The normalized graph Laplacian
is given by L = I ? D?1/2 W D?1/2 ,
P
where D is a diagonal degree matrix with D(s, s) = s0 W (s, s0 ). When states are projected onto
the second eigenvector, they are pulled along orthogonal dimensions according to their community
membership. Locations in distinct regions but close in Euclidean distance ? for instance, nearby
points on opposite sides of a boundary ? will be represented as distant in the eigenspace.
The normalized graph Laplacian is closely related to the SR [26]. Under a random walk policy,
the transition matrix is given by T = D?1 W . If ? is an eigenvector of the random walk?s graph
Laplacian I?T , then D1/2 ? is an eigenvector of the normalized graph Laplacian. The corresponding
eigenvector for the discounted Laplacian, I ? ?T , is ??. Since the matrix inverse preserves the
eigenvectors, the normalized graph Laplacian has the same eigenvectors as the SR, M = (I??T )?1 ,
scaled by ?D?1/2 . These spectral eigenvectors can be approximated by slow feature analysis [42].
Applying hierarchical slow feature analysis to streams of simulated visual inputs produces feature
representations that resemble hippocampal receptive fields [12].
A number of representative SR eigenvectors are shown in Fig. 7, for three different room topologies.
The higher frequency eigenvectors display the latticing characteristic of grid cells [16]. The eigendecomposition is often discontinuous at barriers, and in many cases different rooms are represented
by independent sinusoids. Fig. 8 shows the eigendecomposition for a hairpin maze. The eigenvectors resemble folded up one-dimensional sinusoids, and high frequency eigenvectors appear as
repeating phase-locked ?submaps? with firing selective to a subset of hallways, much like the grid
cells observed by Derdikman and Moser [7].
In the multiple rooms environment, visual inspection reveals that the SR eigenvector with the second
smallest eigenvalue (the Fiedler vector) divides the enclosure along the vertical barrier: the left half
is almost entirely blue and the right half almost entirely red, with a smooth but steep transition
at the doorway (Fig. 9a). As discussed above, this second eigenvector can therefore be used to
segment the enclosure along the vertical boundary. Applying this segmentation recursively, as in
the normalized cuts algorithm, produces a hierarchical decomposition of the environment (Figure
6

Segmentation

b

Figure 9: Segmentation using normalized cuts.
(a) The results of segmentation by thresholding
the second eigenvector of the multiple rooms environment in Fig. 1. Dotted lines indicate the
segment boundaries. (b, c) Eigenvector segmentation applied recursively to fully parse the enclosure into the four rooms.

a
c

First Level

Second Level

9b,c). By identifying useful subgoals from the environmental topology, this decomposition can be
exploited by hierarchical learning algorithms [3, 37].
One might reasonably question why the brain should represent high frequency eigenvectors (like
grid cells) if only the low frequency eigenvectors are useful for hierarchical decomposition. Spectral
features also serve as generally useful representations [26, 22], and high frequency components are
important for representing detail in the value function. The progressive increase in grid cell spacing
along the dorsal-ventral axis of the entorhinal cortex may function as a multi-scale representation
that supports both fine and coarse detail [2].

6

Discussion

We have shown how many empirically observed properties of spatial representation in the brain,
such as changes in place fields induced by manipulations of environmental geometry and reward,
can be explained by a predictive representation of the environment. This predictive representation
is intimately tied to the problem of RL: in a certain sense, it is the optimal representation of space
for the purpose of computing value functions, since it reduces value computation to a simple matrix
multiplication [6]. Moreover, this optimality principle is closely connected to ideas from manifold
learning and spectral graph theory [26]. Our work thus sheds new computational light on Tolman?s
cognitive map [46].
Our work is connected to several lines of previous work. Most relevant is Gustafson and Daw
[15], who showed how topologically-sensitive spatial representations recapitulate many aspects of
place cells and grid cells that are difficult to reconcile with a purely Euclidean representation of
space. They also showed how encoding topological structure greatly aids reinforcement learning in
complex spatial environments. Earlier work by Foster and colleagues [11] also used place cells as
features for RL, although the spatial representation did not explicitly encode topological structure.
While these theoretical precedents highlight the importance of spatial representation, they leave
open the deeper question of why particular representations are better than others. We showed that
the SR naturally encodes topological structure in a format that enables efficient RL.
Spectral graph theory provides insight into the topological structure encoded by the SR. In particular,
we showed that eigenvectors of the SR can be used to discover a hierarchical decomposition of the
environment for use in hierarchical RL. These eigenvectors may also be useful as a representational
basis for RL, encoding multi-scale spatial structure in the value function. Spectral analysis has
frequently been invoked as a computational motivation for entorhinal grid cells (e.g., [23]). The
fact that any function can be reconstructed by sums of sinusoids suggested that the entorhinal cortex
implements a kind of Fourier transform of space, and that place cells are the result of reconstructing
spatial signals from their spectral decomposition. Two problems face this interpretation. Fist, recent
evidence suggests that the emergence of place cells does not depend on grid cell input [4, 47].
Second, and more importantly for our purposes, Fourier analysis is not the right mathematical tool
when dealing with spatial representation in a topologically structured environment, since we do not
expect functions to be smooth over boundaries in the environment. This is precisely the purpose of
spectral graph theory: the eigenvectors of the graph Laplacian encode the smoothest approximation
of a function that respects the graph topology [26].
Recent work has elucidated connections between models of episodic memory and the SR. Specifically, in [14] it was shown that the SR is closely related to the Temporal Context Model (TCM)
of episodic memory [20]. The core idea of TCM is that items are bound to their temporal context
(a running average of recently experienced items), and the currently active temporal context is used
7

to cue retrieval of other items, which in turn cause their temporal context to be retrieved. The SR
can be seen as encoding a set of item-context associations. The connection to episodic memory is
especially interesting given the crucial mnemonic role played by the hippocampus and entorhinal
cortex in episodic memory. Howard and colleagues [19] have laid out a detailed mapping between
TCM and the medial temporal lobe (including entorhinal and hippocampal regions).
An important question for future work concerns how biologically plausible mechanisms can implement the computations posited in our paper. We described a simple error-driven updating rule for
learning the SR, and in the Supplementary Materials we derive a stochastic gradient learning rule
that also uses a simple error-driven update. Considerable attention has been devoted to the implementation of error-driven learning rules in the brain, so we expect that these learning rules can be
implemented in a biologically plausible manner.

References
[1] A. Alvernhe, E. Save, and B. Poucet. Local remapping of place cell firing in the tolman detour task.
European Journal of Neuroscience, 33:1696?1705, 2011.
[2] H. T. Blair, A. C. Welday, and K. Zhang. Scale-invariant memory representations emerge from moire
interference between grid fields that produce theta oscillations: a computational model. The Journal of
Neuroscience, 27:3211?3229, 2007.
[3] M. M. Botvinick, Y. Niv, and A. C. Barto. Hierarchically organized behavior and its neural foundations:
A reinforcement learning perspective. Cognition, 113:262?280, 2009.
[4] M. P. Brandon, J. Koenig, J. K. Leutgeb, and S. Leutgeb. New and distinct hippocampal place codes are
generated in a new environment during septal inactivation. Neuron, 82:789?796, 2014.
[5] N. D. Daw, Y. Niv, and P. Dayan. Uncertainty-based competition between prefrontal and dorsolateral
striatal systems for behavioral control. Nature Neuroscience, 8:1704?1711, 2005.
[6] P. Dayan. Improving generalization for temporal difference learning: The successor representation. Neural Computation, 5:613?624, 1993.
[7] D. Derdikman, J. R. Whitlock, A. Tsao, M. Fyhn, T. Hafting, M.-B. Moser, and E. I. Moser. Fragmentation
of grid cell maps in a multicompartment environment. Nature Neuroscience, 12:1325?1332, 2009.
[8] C. Diuk, K. Tsai, J. Wallis, M. Botvinick, and Y. Niv. Hierarchical learning induces two simultaneous, but
separable, prediction errors in human basal ganglia. The Journal of Neuroscience, 33:5797?5805, 2013.
[9] M. S. Fanselow. From contextual fear to a dynamic view of memory systems. Trends in Cognitive
Sciences, 14:7?15, 2010.
[10] A. Fenton, L. Zinyuk, and J. Bures. Place cell discharge along search and goal-directed trajectories.
European Journal of Neuroscience, 12:3450, 2001.
[11] D. Foster, R. Morris, and P. Dayan. A model of hippocampally dependent navigation, using the temporal
difference learning rule. Hippocampus, 10:1?16, 2000.
[12] M. Franzius, H. Sprekeler, and L. Wiskott. Slowness and sparseness lead to place, head-direction, and
spatial-view cells. PLoS Computational Biology, 3:3287?3302, 2007.
[13] C. R. Gallistel. The Organization of Learning. The MIT Press, 1990.
[14] S. J. Gershman, C. D. Moore, M. T. Todd, K. A. Norman, and P. B. Sederberg. The successor representation and temporal context. Neural Computation, 24:1553?1568, 2012.
[15] N. J. Gustafson and N. D. Daw. Grid cells, place cells, and geodesic generalization for spatial reinforcement learning. PLoS Computational Biology, 7:e1002235, 2011.
[16] T. Hafting, M. Fyhn, S. Molden, M.-B. Moser, and E. I. Moser. Microstructure of a spatial map in the
entorhinal cortex. Nature, 436:801?806, 2005.
[17] S. C. Hirtle and J. Jonides. Evidence of hierarchies in cognitive maps. Memory & Cognition, 13:208?217,
1985.
[18] S. A. Hollup, S. Molden, J. G. Donnett, M. B. Moser, and E. I. Moser. Accumulation of hippocampal
place fields at the goal location in an annular watermaze task. Journal of Neuroscience, 21:1635?1644,
2001.
[19] M. W. Howard, M. S. Fotedar, A. V. Datey, and M. E. Hasselmo. The temporal context model in spatial
navigation and relational learning: toward a common explanation of medial temporal lobe function across
domains. Psychological Review, 112:75?116, 2005.
[20] M. W. Howard and M. J. Kahana. A distributed representation of temporal context. Journal of Mathematical Psychology, 46:269?299, 2002.

8

[21] T. Kobayashi, A. Tran, H. Nishijo, T. Ono, and G. Matsumoto. Contribution of hippocampal place cell
activity to learning and formation of goal-directed navigation in rats. Neuroscience, 117:1025?35, 2003.
[22] G. Konidaris, S. Osentoski, and P. S. Thomas. Value function approximation in reinforcement learning
using the Fourier basis. In AAAI, 2011.
[23] J. Krupic, N. Burgess, and J. O?oeefe. Neural representations of location composed of spatially periodic
bands. Science, 337:853?857, 2012.
[24] P. Lenck-Santini, R. Muller, E. Save, and B. Poucet. Relationships between place cell firing fields and
navigational decisions by rats. The Journal of Neuroscience, 22:9035?47, 2002.
[25] P. Lenck-Santini, E. Save, and B. Poucet. Place-cell firing does not depend on the direction of turn in a
y-maze alternation task. European Journal of Neuroscience, 13(5):1055?8, 2001.
[26] S. Mahadevan. Learning representation and control in markov decision processes: New frontiers. Foundations and Trends in Machine Learning, 1:403?565, 2009.
[27] B. L. McNaughton, F. P. Battaglia, O. Jensen, E. I. Moser, and M.-B. Moser. Path integration and the
neural basis of the ?cognitive map?. Nature Reviews Neuroscience, 7:663?678, 2006.
[28] M. R. Mehta, C. A. Barnes, and B. L. McNaughton. Experience-dependent, asymmetric expansion of
hippocampal place fields. Proceedings of the National Academy of Sciences, 94:8918?8921, 1997.
[29] M. R. Mehta, M. C. Quirk, and M. A. Wilson. Experience-dependent asymmetric shape of hippocampal
receptive fields. Neuron, 25:707?715, 2000.
[30] I. Menache, S. Mannor, and N. Shimkin. Q-cut?dynamic discovery of sub-goals in reinforcement learning. In European Conference on Machine Learning, pages 295?306. Springer, 2002.
[31] L. K. Morgan, S. P. MacEvoy, G. K. Aguirre, and R. A. Epstein. Distances between real-world locations
are represented in the human hippocampus. The Journal of Neuroscience, 31:1238?1245, 2011.
[32] R. U. Muller and J. L. Kubie. The effects of changes in the environment on the spatial firing of hippocampal complex-spike cells. The Journal of Neuroscience, 7:1951?1968, 1987.
[33] R. U. Muller, M. Stead, and J. Pach. The hippocampus as a cognitive graph. The Journal of General
Physiology, 107:663?694, 1996.
[34] J. O?Keefe and L. Nadel. The Hippocampus as a Cognitive Map. Clarendon Press Oxford, 1978.
[35] A. K. Reid and J. R. Staddon. A dynamic route finder for the cognitive map. Psychological Review,
105:585?601, 1998.
[36] J. J. Ribas-Fernandes, A. Solway, C. Diuk, J. T. McGuire, A. G. Barto, Y. Niv, and M. M. Botvinick. A
neural signature of hierarchical reinforcement learning. Neuron, 71:370?379, 2011.
[37] A. C. Schapiro, T. T. Rogers, N. I. Cordova, N. B. Turk-Browne, and M. M. Botvinick. Neural representations of events arise from temporal community structure. Nature Neuroscience, 16:486492, 2013.
[38] J. Shi and J. Malik. Normalized cuts and image segmentation. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 22:888?905, 2000.
? S?ims?ek, A. P. Wolfe, and A. G. Barto. Identifying useful subgoals in reinforcement learning by local
[39] O.
graph partitioning. In Proceedings of the 22nd International Conference on Machine Learning, pages
816?823. ACM, 2005.
[40] W. E. Skaggs and B. L. McNaughton. Spatial firing properties of hippocampal ca1 populations in an
environment containing two visually identical regions. The Journal of Neuroscience, 18:8455?8466,
1998.
[41] A. Speakman and J. O?Keefe. Hippocampal complex spike cells do not change their place fields if the
goal is moved within a cue controlled environment. European Journal of Neuroscience, 2:544?5, 1990.
[42] H. Sprekeler. On the relation of slow feature analysis and laplacian eigenmaps. Neural computation,
23:3287?3302, 2011.
[43] A. Stevens and P. Coupe. Distortions in judged spatial relations. Cognitive Psychology, 10:422 ? 437,
1978.
[44] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT press, 1998.
[45] R. S. Sutton, D. Precup, and S. Singh. Between MDPs and semi-MDPs: A framework for temporal
abstraction in reinforcement learning. Artificial Intelligence, 112:181?211, 1999.
[46] E. C. Tolman. Cognitive maps in rats and men. Psychological Review, 55:189?208, 1948.
[47] T. J. Wills, F. Cacucci, N. Burgess, and J. O?Keefe. Development of the hippocampal cognitive map in
preweanling rats. Science, 328:1573?1576, 2010.
[48] B. J. Wiltgen, M. J. Sanders, S. G. Anagnostaras, J. R. Sage, and M. S. Fanselow. Context fear learning
in the absence of the hippocampus. The Journal of Neuroscience, 26:5484?5491, 2006.

9

"
2015,Recursive Training of 2D-3D Convolutional Networks for Neuronal Boundary Prediction,Poster,5636-recursive-training-of-2d-3d-convolutional-networks-for-neuronal-boundary-prediction.pdf,"Efforts to automate the reconstruction of neural circuits from 3D electron microscopic (EM) brain images are critical for the field of connectomics. An important computation for reconstruction is the detection of neuronal boundaries. Images acquired by serial section EM, a leading 3D EM technique, are highly anisotropic, with inferior quality along the third dimension. For such images, the 2D max-pooling convolutional network has set the standard for performance at boundary detection. Here we achieve a substantial gain in accuracy through three innovations. Following the trend towards deeper networks for object recognition, we use a much deeper network than previously employed for boundary detection. Second, we incorporate 3D as well as 2D filters, to enable computations that use 3D context. Finally, we adopt a recursively trained architecture in which a first network generates a preliminary boundary map that is provided as input along with the original image to a second network that generates a final boundary map. Backpropagation training is accelerated by ZNN, a new implementation of 3D convolutional networks that uses multicore CPU parallelism for speed. Our hybrid 2D-3D architecture could be more generally applicable to other types of anisotropic 3D images, including video, and our recursive framework for any image labeling problem.","Recursive Training of 2D-3D Convolutional Networks
for Neuronal Boundary Detection

Kisuk Lee, Aleksandar Zlateski
Massachusetts Institute of Technology
{kisuklee,zlateski}@mit.edu

Ashwin Vishwanathan, H. Sebastian Seung
Princeton University
{ashwinv,sseung}@princeton.edu

Abstract
Efforts to automate the reconstruction of neural circuits from 3D electron microscopic (EM) brain images are critical for the field of connectomics. An important
computation for reconstruction is the detection of neuronal boundaries. Images acquired by serial section EM, a leading 3D EM technique, are highly anisotropic,
with inferior quality along the third dimension. For such images, the 2D maxpooling convolutional network has set the standard for performance at boundary
detection. Here we achieve a substantial gain in accuracy through three innovations. Following the trend towards deeper networks for object recognition, we use
a much deeper network than previously employed for boundary detection. Second, we incorporate 3D as well as 2D filters, to enable computations that use 3D
context. Finally, we adopt a recursively trained architecture in which a first network generates a preliminary boundary map that is provided as input along with
the original image to a second network that generates a final boundary map. Backpropagation training is accelerated by ZNN, a new implementation of 3D convolutional networks that uses multicore CPU parallelism for speed. Our hybrid 2D3D architecture could be more generally applicable to other types of anisotropic
3D images, including video, and our recursive framework for any image labeling
problem.

1

Introduction

Neural circuits can be reconstructed by analyzing 3D brain images from electron microscopy (EM).
Image analysis has been accelerated by semiautomated systems that use computer vision to reduce
the amount of human labor required [1, 2, 3]. However, analysis of large image datasets is still
laborious [4], so it is critical to increase automation by improving the accuracy of computer vision
algorithms.
A variety of machine learning approaches have been explored for the 3D reconstruction of neurons,
a problem that can be formulated as image segmentation or boundary detection [5, 6]. This paper
focuses on neuronal boundary detection in images from serial section EM, the most widespread kind
of 3D EM [7]. The technique starts by cutting and collecting ultrathin (30 to 100 nm) sections of
brain tissue. A 2D image is acquired from each section, and then the 2D images are aligned. The
spatial resolution of the resulting 3D image stack along the z direction (perpendicular to the cutting
plane) is set by the thickness of the sections. This is generally much worse than the resolution that
EM yields in the xy plane. In addition, alignment errors may corrupt the image along the z direction.
Due to these issues with the z direction of the image stack [6, 8], most existing analysis pipelines
begin with 2D processing and only later transition to 3D. The stages are: (1) neuronal boundary
detection within each 2D image, (2) segmentation of neuron cross sections within each 2D image,
and (3) 3D reconstruction of individual neurons by linking across multiple 2D images [1, 9].
1

Boundary detection in serial section EM images is done by a variety of algorithms. Many algorithms
were compared in the ISBI?12 2D EM segmentation challenge, a publicly available dataset and
benchmark [10]. The winning submission was an ensemble of max-pooling convolutional networks
(ConvNets) created by IDSIA [11]. One of the ConvNet architectures shown in Figure 1 (N4) is the
largest architecture from [11], and serves as a performance baseline for the research reported here.
We improve upon N4 by adding several new elements (Fig. 1):
Increased depth Our VD2D architecture is deeper than N4 (Figure 1), and borrows other nowstandard practices from the literature, such as rectified linear units (ReLUs), small filter sizes, and
multiple convolution layers between pooling layers. VD2D already outperforms N4, without any
use of 3D context. VD2D is motivated by the principle ?the deeper, the better,? which has become
popular for ConvNets applied to object recognition [12, 13].
3D as well as 2D When human experts detect boundaries in EM images, they use 3D context
to disambiguate certain locations. VD2D3D is also able to use 3D context, because it contains
3D filters in its later layers. ConvNets with 3D filters were previously applied to block face EM
images [2, 3, 14]. Block face EM is another class of 3D EM techniques, and produces nearly
isotropic images, unlike serial section EM. VD2D3D also contains 2D filters in its earlier layers.
This novel hybrid use of 2D and 3D filters is suited for the highly anisotropic nature of serial
section EM images.
Recursive training of ConvNets VD2D and VD2D3D are concatenated to create an extremely
deep network. The output of VD2D is a preliminary boundary map, which is provided as input
to VD2D3D in addition to the original image (Fig. 1). Based on these two inputs, VD2D3D is
trained to compute the final boundary map. Such ?recursive? training has previously been applied
to neural networks for boundary detection [8, 15, 16], but not to ConvNets.
ZNN for 3D deep learning Very deep ConvNets with 3D filters are computationally expensive,
so an efficient software implementation is critical. We trained our networks with ZNN (https:
//github.com/seung-lab/znn-release, [17]), which uses multicore CPU parallelism
for speed. ZNN is one of the few deep learning implementations that is well-optimized for 3D.

2D
2DConvNet
ConvNet
VD2D
VD2D

1st input
193x193x5

boundary prediction
1x1x1

2D-3D ConvNet
VD2D3D
2nd input
85x85x5

initialize with
learned 2D representations

1st stage

tanh Pool1 tanh Pool2 tanh
48
2x2x1
48
2x2x1
48
Conv1
Conv2
Conv3
4x4x1
5x5x1
4x4x1

N4

Input
95x95x1

VD2D

Input
109x109x1

Conv1a
3x3x1

Conv1b
3x3x1

Conv1c
2x2x1

Input
85x85x5

Conv1a
3x3x1

Conv1b
3x3x1

Conv1c
2x2x1

VD2D3D

recursive input
85x85x5

ReLU
24

ReLU
24

tanh
24

Pool1
2x2x1

Pool3 tanh Pool4
2x2x1 48
2x2x1
Conv4
4x4x1

Conv2a
3x3x1

Conv2b
3x3x1

Conv2a
3x3x1

Conv2b
3x3x1

ReLU
36

tanh
36

Pool2
2x2x1

2nd stage

tanh
200
Conv5
3x3x1

Softmax
2

Conv3a
3x3x1

Conv3b
3x3x1

Pool3 ReLU
2x2x1
60
Conv4a
3x3x1

Conv3a
3x3x1

Conv3b
3x3x1

Conv4a
3x3x2

ReLU
48

Output
1x1x1

tanh
48

Pool3
2x2x2

ReLU
60

tanh Pool4 ReLU
2x2x1 200
60
Conv4b
Conv5
3x3x1
3x3x1

Conv4b Conv4c
3x3x2
3x3x2
ReLU
60

ReLU
100

Softmax
2
Output
1x1x1

Output
1x1x1

Softmax
2

Figure 1: An overview of our proposed framework (top) and model architectures (bottom). The
number of trainable parameters in each model is 220K (N4), 230K (VD2D), 310K (VD2D3D).
2

While we have applied the above elements to serial section EM images, they are likely to be generally useful for other types of images. The hybrid use of 2D and 3D filters may be useful for video,
which can also be viewed as an anisotropic 3D image. Previous 3D ConvNets applied to video
processing [18, 19] have used 3D filters exclusively.
Recursively trained ConvNets are potentially useful for any image labeling problem. The approach
is very similar to recurrent ConvNets [20], which iterate the same ConvNet. The recursive approach
uses different ConvNets for the successive iterations. The recursive approach has been justified in
several ways. In MRF/CRF image labeling, it is viewed as the sequential refinement of the posterior
probability of a pixel being assigned a label, given both an input image and recursive input from the
previous step [21]. Another viewpoint on recursive training is that statistical dependencies in label
(category) space can be directly modeled from the recursive input [15]. From the neurobiological
viewpoint, using a preliminary boundary map for an image to guide the computation of a better
boundary map for the image can be interpreted as employing a top-down or attentional mechanism.
We expect ZNN to have applications far beyond the one considered in this paper. ZNN can train very
large networks, because CPUs can access more memory than GPUs. Task parallelism, rather than
the SIMD parallelism of GPUs, allows for efficient training of ConvNets with arbitrary topology. A
self-tuning capability automatically optimizes each layer by choosing between direct and FFT-based
convolution. FFT convolution may be more efficient for wider layers or larger filter size [22, 23].
Finally, ZNN may incur less software development cost, owing to the relative ease of the generalpurpose CPU programming model.
Finally, we applied our ConvNets to images from a new serial section EM dataset from the mouse
piriform cortex. This dataset is important to us, because we are interested in conducting neuroscience research concerning this brain region. Even to those with no interest in piriform cortex,
the dataset could be useful for research on image segmentation algorithms. Therefore we make the
annotated dataset publicly available (http://seunglab.org/data/).

2

Dataset and evaluation

Images of mouse piriform cortex The datasets described here were acquired from the piriform cortex of an adult mouse prepared with aldehyde fixation and reduced osmium staining [24].
The tissue was sectioned using the automatic tape collecting ultramicrotome (ATUM) [25] and
sections were imaged on a Zeiss field emission scanning electron microscope [26]. The 2D images were assembled into 3D stacks using custom MATLAB routines and TrakEM2, and each
stack was manually annotated using VAST (https://software.rc.fas.harvard.edu/
lichtman/vast/, [25]) (Figure 2). Then each stack was checked and corrected by another annotator.
The properties of the four image stacks are detailed in Table 1. It should be noted that image
quality varies across the stacks, due to aging of the field emission source in the microscope. In all
experiments we used stack1 for testing, stack2 and stack3 for training, and stack4 as an
additional training data for recursive training.

Figure 2: Example dataset (stack1, Table 1) and results of each architecture on stack1.
3

Name
Resolution (nm3 )
Dimension (voxel3 )
# samples
Usage

Table 1: Piriform cortex datasets
stack1
stack2
stack3
7 ? 7 ? 40
7 ? 7 ? 40
7 ? 7 ? 40
255 ? 255 ? 168 512 ? 512 ? 170 512 ? 512 ? 169
10.9M
44.6 M
44.3 M
Test
Training
Training

stack4
10 ? 10 ? 40
256 ? 256 ? 121
7.9 M
Training (extra)

Pixel error We use softmax activation in the output layer of our networks to produce per-pixel
real-valued outputs between 0 and 1, each of which is interpreted as the probability of an output pixel
being boundary, or vice versa. This real-valued ?boundary map? can be thresholded to generate a
binary boundary map, from which the pixel-wise classification error is computed. We report the best
classification error obtained by optimizing the binarization threshold with line search.
Rand score We evaluate 2D segmentation performance with the Rand scoring system [27, 28].
Let nij denote the number of pixels simultaneously in the ith segment of the proposal segmentation
and the j th segment of the ground truth segmentation. The Rand merge score and the Rand split
score
P 2
P 2
ij nij
ij nij
Rand
Rand
Vmerge = P P
, Vsplit = P P
.
2
2
i(
j(
j nij )
i nij )
are closer to one when there are fewer merge and split errors, respectively. The Rand F-score is the
Rand
Rand
and Vsplit
.
harmonic mean of Vmerge
To compute the Rand scores, we need to first obtain 2D neuronal segmentation based on the realvalued boundary map. To this end, we apply two segmentation algorithms with different levels
of sophistication: (1) simple thresholding followed by computing 2D connected components, and
(2) modified graph-based watershed algorithm [29]. We report the best Rand F-score obtained by
optimizing parameters for each algorithm with line search, as well as the precision-recall curve for
the Rand scores.

3

Training with ZNN

ZNN [17] was built for 3D ConvNets. 2D convolution is regarded as a special case of 3D convolution, in which one of the three filter dimensions has size 1. For the details on how ZNN implements
task parallelism on multicore CPUs, we refer interested readers to [17]. Here we describe only aspects of ZNN that are helpful for understanding how it was used to implement the ConvNets of this
paper.
Dense output with maximum filtering In object recognition, a ConvNet is commonly applied
to produce a single output value for an entire input image. However, there are many applications in
which ?dense output? is required, i.e., the ConvNet should produce an output image with the same
resolution as the original input image. Such applications include boundary detection [11], image
labeling [30], and object localization [31].
ZNN was built from the ground up for dense output and also for dense feature maps.1 ZNN employs
max-filtering, which slides a window across the image and applies the maximum operation to the
window (Figure 3). Max-filtering is the dense variant of max-pooling. Consequently all feature
maps remain intact as dense 3D volumes during both forward and backward passes, making them
straightforward for visualization and manipulation.
On the other hand, all filtering operations are sparse, in the sense that the sliding window samples sparsely from a regularly spaced set of voxels in the image (Figure 3). ZNN can control the
spacing/sparsity of any filtering operation, either convolution or max-filtering.
ZNN can efficiently compute the dense output of a sliding window max-pooling ConvNet by making
filter sparsity depend on the number of prior max-filterings. More specifically, each max-filtering
1
Feature maps with the same resolution as the original input image. See Figure 5 for example. Note that the
feature maps shown in Figure 5 keep the original resolution even after a couple of max-pooling layers.

4

Convolution

Max-Pool

Convolution

Convolution

Max-Filter

Sparse Convolution

Figure 3: Sliding window max-pooling ConvNet (left) applied on three color-coded adjacent input
windows producing three outputs. Equivalent outputs produced by a max-filtering ConvNet with
sparse filters (right) applied on a larger window. Computation is minimized by reusing the intermediate values for computing multiple outputs (as color coded).

increases the sparsity of all subsequent filterings by a factor equal to the size of the max-pooling
window. This approach, which we employ for the paper, is also called ?skip-kernels? [31] or ?filter
rarefaction? [30], and is equivalent in its results to ?max-fragmentation-pooling? [32, 33]. Note
however that ZNN is more general, as the sparsity of filters need not depend on max-filtering, but
can be controlled independently.
Output patch training Training in ZNN is based on loss computed over a dense output patch of
arbitrary size. The patch can be arbitrarily large, limited only by memory. This includes the case of a
patch that spans the entire image [30, 33]. Although large patch sizes reduce the computational cost
per output pixel, neighboring pixels in the patch may provide redundant information. In practice,
we choose an intermediate output patch size.

4

Network architecture

N4 As a baseline for performance comparisons, we adopted the largest 2D ConvNet architecture
(named N4) from Cires?an et al. [11] (Figure 1).
VD2D The architecture of VD2D (?Very Deep 2D?) is shown in Figure 1. Multiple convolution
layers are between each max-pooling layer. All convolution filters are 3?3?1, except that Conv1c
uses a 2 ? 2 ? 1 filter to make the ?field of view? or ?receptive field? for a single output pixel have
an odd-numbered size and therefore centerable around the output pixel. Due to the use of smaller
filters, the number of trainable parameters in VD2D (230K) is roughly the same as in the shallower
N4 (220K).
VD2D3D The architecture of VD2D3D (?Very Deep 2D-3D?) is initially identical to VD2D (Figure 1), except that later convolution layers switch to 3 ? 3 ? 2 filters. This causes the number of
trainable parameters to increase, so we compensate by trimming the size of Conv4c to just 100
feature maps. The 3D filters in the later layers should enable the network to use 3D context to detect
neuronal boundaries. The use of 2D filters in the initial layers makes the network faster to run and
train.
Recursive training It is possible to apply VD2D3D by itself to boundary detection, giving the
raw image as the only input. However, we use a recursive approach in which VD2D3D receives an
extra input, the output of VD2D. As we will see below, this produces a significant improvement in
performance. It should be noted that instead of providing the recursive input directly to VD2D3D,
we added new layers 2 dedicated to processing it. This separate, parallel processing stream for
recursive input joins the main stream at Conv1c, allowing for more complex, highly nonlinear
interaction between the low-level features and the contextual information in the recursive input.
2

These layers are identical to Conv1a, Conv1b, and Conv1c.

5

5

Training procedures

Networks were trained using backpropagation with the cross-entropy loss function. We first trained
VD2D, and then trained VD2D3D. The 2D layers of VD2D3D were initialized using trained weights
from VD2D. This initialization meant that our recursive approach bore some similarity to recurrent
ConvNets, in which the first and second stage networks are constrained to be identical [20]. However, we did not enforce exact weight sharing, but fine-tuned the weights of VD2D3D.
Output patch As mentioned earlier, training with ZNN is done by dense output patch-based
gradient update with per-pixel loss. During training, an output patch of specified size is randomly
drawn from the training stacks at the beginning of each forward pass.
Class rebalancing In dense output patch-based training, imbalance between the number of training samples in different classes (e.g. boundary/non-boundary) can be handled by either sampling
a balanced number of pixels from an output patch, or by differentially weighting the per-pixel
loss [30]. In our experiment, we adopted the latter approach (loss weighting) to deal with the high
imbalance between boundary and non-boundary pixels.
Data augmentation We used the same data augmentation method used in [11], randomly rotating and flipping 2D image patches.
Hyperparameter We always used the fixed learning rate of 0.01 with the momentum of 0.9.
When updating weights we divided the gradient by the total number of pixels in an output patch,
similar to the typical minibatch averaging.
We first trained N4 with an output patch of size 200 ? 200 ? 1 for 90K gradient updates. Next, we
trained VD2D with 150 ? 150 ? 1 output patches, reflecting the increased size of model compared to
N4. After 60K updates, we evaluated the trained VD2D on the training stacks to obtain preliminary
boundary maps, and started training VD2D3D with 100?100?1 output patches, again reflecting the
increased model complexity. We trained VD2D3D for 90K updates. In this recursive training stage
we additionally used stack4 to prevent VD2D3D from being overly dependent on the good-quality
boundary maps for training stacks. It should be noted that stack4 has slightly lower xy-resolution
than other stacks (Table 1), which we think is helpful in terms of learning multi-scale representation.
Our proposed recursive framework is different from the training of recurrent ConvNets [20] in that
recursive input is not dynamically produced by the first ConvNet during training, but evaluated
before and being fixed throughout the recursive training stage. However, it is also possible to further
train the first ConvNet even after evaluating its preliminary output as recursive input to the second
ConvNet. We further trained VD2D for another 30K updates while VD2D3D is being trained. We
report the final performance of VD2D after a total of 90K updates. We also replaced the initial
VD2D boundary map with the final one when evaluating VD2D3D results. With ZNN, it took two
days to train both N4 and VD2D for 90K updates, and three days to train VD2D3D for 90K updates.

6

Results

In this section, we show both quantitative and qualitative results obtained by the three architectures
shown in Figure 1, namely N4, VD2D, and VD2D3D. The pixel-wise classification error of each
model on test set was 10.63% (N4), 9.77% (VD2D), and 8.76% (VD2D3D).
Quantitative comparison Figure 4 compares the result of each architecture on test set (stack1),
both quantitatively and qualitatively. The leftmost bar graph shows the best 2D Rand F-score of
each model obtained by 2D segmentation with (1) simpler connected component clustering and
(2) more sophisticated watershed-based segmentation. The middle and rightmost graphs show the
precision-recall curve of each model for the Rand scores obtained with the connected component and
watershed-based segmentation, respectively. We observe that VD2D performs significantly better
than N4, and also VD2D3D outperforms VD2D by a significant margin in terms of both best Rand
F-score and overall precision-recall curve.
Qualitative comparison Figure 2 shows the visualization of boundary detection results of each
model on test set, along with the original EM images and ground truth boundary map. We observe
that false detection of boundary on intracellular regions was significantly reduced in VD2D3D,
6

Rand score (connected component)

0.7

Connected
component

image data

Watershed

0.6
0.6

ground-truth

Rand score (watershed)

0.8

0.8

0.85

1
0.9

0.9

0.9

0.8

1

Rand merge score

Rand F-score

0.95

Best Rand F-score
N4
VD2D
VD2D3D

Rand merge score

1

0.7

N4
VD2D
VD2D3D

0.7

0.8

0.9

Rand split score

N4

1

0.6
0.6

VD2D

N4
VD2D
VD2D3D

0.7

0.8

0.9

Rand split score

1

VD2D3D

Figure 4: Quantitative (top) and qualitative (middle and bottom) evaluation of results.

which demonstrates the effectiveness of the proposed 2D-3D ConvNet combined with recursive
approach. The middle and bottom rows in Figure 4 show some example locations in test set where
both 2D models (N4 and VD2D) failed to correctly detect the boundary, or erroneously detected false
boundaries, whereas VD2D3D correctly predicted on those ambiguous locations. Visual analysis on
the boundary detection results of each model again demonstrates the superior performance of the
proposed recursively trained 2D-3D ConvNet over 2D models.

7

Discussion

Biologically-inspired recursive framework Our proposed recursive framework is greatly inspired by the work of Chen et al. [34]. In this work, they examined the close interplay between
neurons in the primary and higher visual cortical areas (V1 and V4, respectively) of monkeys performing contour detection tasks. In this task, monkeys were trained to detect a global contour pattern
that consists of multiple collinearly aligned bars in a cluttered background.
The main discovery of their work is as follows: initially, V4 neurons responded to the global contour
pattern. After a short time delay (?40 ms), the activity of V1 neurons responding to each bar composing the global contour pattern was greatly enhanced, whereas those responding to the background
was largely suppressed, despite the fact that those ?foreground? and ?background? V1 neurons have
similar response properties. They referred to it as ?push-pull response mode? of V1 neurons between foreground and background, which is attributable to the top-down influence from the higher
level V4 neurons. This process is also referred to as ?countercurrent disambiguating process? [34].
This experimental result readily suggests a mechanistic interpretation on the recursive training of
deep ConvNets for neuronal boundary detection. We can roughly think of V1 responses as lower
level feature maps in a deep ConvNet, and V4 responses as higher level feature maps or output activations. Once the overall ?contour? of neuonal boundaries is detected by the feedforward processing
of VD2D, this preliminary boundary map can then be recursively fed to VD2D3D. This process
7

VD2D
Conv2a

VD2D
Conv3b

VD2D3D
Conv2a

VD2D3D
Conv3b

Figure 5: Visualization of the effect of recursive training. Left: an example feature map from the
lower layer Conv2a in VD2D, and its corresponding feature map in VD2D3D. Right: an example feature map from the higher layer Conv3b in VD2D, and its corresponding feature map in
VD2D3D. Note that recursive training greatly enhances the signal-to-noise ratio of boundary representations.
can be thought of as corresponding to the initial detection of global contour patterns by V4 and its
top-down influence on V1.
During recursive training, VD2D3D will learn how to integrate the pixel-level contextual information in the recursive input with the low-level features, presumably in such a way that feature
activations on the boundary location are enhanced, whereas activations unrelated to the neuronal
boundary (intracellular space, mitochondria, etc.) are suppressed. Here the recursive input can also
be viewed as the modulatory ?gate? through which only the signals relevant to the given task of neuronal boundary detection can pass. This is convincingly demonstrated by visualizing and comparing
the feature maps of VD2D and VD2D3D.
In Figure 5, the noisy representations of oriented boundary segments in VD2D (first and third volumes) are greatly enhanced in VD2D3D (second and fourth volumes), with signals near boundary
being preserved or amplified, and noises in the background being largely suppressed. This is exactly
what we expected from the proposed interpretation of our recursive framework.
Potential of ZNN
We have shown that ZNN can serve as a viable alternative to the mainstream
GPU-based deep learning frameworks, especially when processing 3D volume data with 3D ConvNets. ZNN?s unique features including the large output patch-based training and the dense computation of feature maps can be further utilized for additional computations to better perform the given
task. In theory, we can perform any kind of computation on the dense output prediction between
each forward and backward passes. For instance, objective functions that consider topological constraints (e.g. MALIS [35]) or sampling of topologically relevant locations (e.g. LED weighting [15])
can be applied to the dense output patch, in addition to loss computation, before each backward pass.
Dense feature maps also enable the straighforward implementation of multi-level feature integration
for fine-grained segmentation. Long et al. [30] resorted to upsampling of the higher level features
with lower resolution in order to integrate them with the lower level features with higher resolution.
Since ZNN maintains every feature map at the original resolution of input, it is straighforward
enough to combine feature maps from any level, removing the need for upsampling.
Acknowledgments
We thank Juan C. Tapia, Gloria Choi and Dan Stettler for initial help with tissue handling and Jeff
Lichtman and Richard Schalek with help in setting up tape collection. Kisuk Lee was supported
by a Samsung Scholarship. The recursive approach proposed in this paper was partially motivated
by Matthew J. Greene?s preliminary experiments. We are grateful for funding from the Mathers
Foundation, Keating Fund for Innovation, Simons Center for the Social Brain, DARPA (HR001114-2-0004), and ARO (W911NF-12-1-0594).
References
[1] S. Takemura et al. A visual motion detection circuit suggested by Drosophila connectomics. Nature,
500:175-181, 2013.

8

[2] M. Helmstaedter, K. L. Briggman, S. C. Turaga, V. Jain, H. S. Seung, and W. Denk. Connectomic reconstruction of the inner plexiform layer in the mouse retina. Nature, 500:168-174, 2013.
[3] J. S. Kim et al. Space-time wiring specificity supports direction selectivity in the retina. Nature, 509:331336, 2014.
[4] M. Helmstaedter. Cellular-resolution connectomics: challenges of dense neural circuit reconstruction. Nature Methods, 10(6):501-507, 2013.
[5] V. Jain, H. S. Seung, and S. C. Turaga. Machines that learn to segment images: a crucial technology for
connectomics. Current Opinion in Neurobiology, 20:653-666, 2010.
[6] T. Tasdizen et al. Image segmentation for connectomics using machine learning. In Computational Intelligence in Biomedical Imaging, pp. 237-278, ed. K. Suzuki, Springer New York, 2014.
[7] K. L. Briggman and D. D. Bock. Volume electron microscopy for neuronal circuit reconstruction. Current
Opinion in Neurobiology, 22(1):154-61, 2012.
[8] E. Jurrus et al. Detection of neuron membranes in electron microscopy images using a serial neural network
architecture. Medical Image Analysis, 14(6):770-783, 2010.
[9] T. Liu, C. Jones, M. Seyedhosseini, and T. Tasdizen. A modular hierarchical approach to 3D electron
microscopy image segmentation. Journal of Neuroscience Methods, 26:88-102, 2014.
[10] Segmentation of neuronal structures in EM stacks challenge - ISBI 2012. http://brainiac2.mit.
edu/isbi_challenge/.
[11] D. C. Cires?an, A. Giusti, L. M. Gambardella, and J. Schmidhuber. Deep neural networks segment neuronal
membranes in electron microscopy images. In NIPS, 2012.
[12] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
[13] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In
ICLR, 2015.
[14] S. C. Turaga et al. Convolutional networks can learn to generate affinity graphs for image segmentation.
Neural Computation, 22:511-538, 2010.
[15] G. B. Huang and V. Jain. Deep and wide multiscale recursive networks for robust image labeling. In ICLR,
2014.
[16] M. Seyedhosseini and T. Tasdizen. Multi-class multi-scale series contextual model for image segmentation. Image Processing, IEEE Transactions on, 22(11):4486-4496, 2013.
[17] A. Zlateski, K. Lee, and H. S. Seung. ZNN - A fast and scalable algorithm for training 3D convolutional
networks on multi-core and many-core shared memory machines. arXiv:1510.06706, 2015.
[18] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri. Learning Spatiotemporal Features with 3D
Convolutional Networks. arXiv:1412.0767, 2014.
[19] L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle, and A. Courville. Describing Videos by
Exploiting Temporal Structure. arXiv:1502.08029, 2015.
[20] P. O. Pinheiro and R. Collobert. Recurrent convolutional neural networks for scene labeling. In ICML,
2014.
[21] Z. Tu. Auto-context and its application to high-level vision tasks. In CVPR, 2008.
[22] M. Mathieu, M. Henaff, and Y. LeCun. Fast training of convolutional networks through FFTs. In ICLR,
2014.
[23] N. Vasilache, J. Johnson, M. Mathieu, S. Chintala, S. Piantino, and Y. LeCun. Fast convolutional nets
with fbfft: a GPU performance evaluation. In ICLR, 2015.
[24] J. C. Tapia et al. High-contrast en bloc staining of neuronal tissue for field emission scanning electron
microscopy. Nature Protocols, 7(2):193-206, 2012.
[25] N. Kasthuri et al. Saturated reconstruction of a volume of neocortex. Cell 162, 648-61, 2015.
[26] K. J. Hayworth et al. Imaging ATUM ultrathin section libraries with WaferMapper: a multi-scale approach
to EM reconstruction of neural circuits. Frontiers in Neural Circuits, 8, 2014.
[27] W. M. Rand. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association, 66(336):847-850, 1971.
[28] R. Unnikrishnan, C. Pantofaru, and M. Hebert. Toward objective evaluation of image segmentation algorithms. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(6):929-944, 2007.
[29] A. Zlateski and H. S. Seung. Image segmentation by size-dependent single linkage clustering of a watershed basin graph. arXiv:1505.00249, 2015.
[30] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR,
2015.
[31] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. OverFeat: Integrated recognition,
localization and detection using convolutional networks. In ICLR, 2014.
[32] A. Giusti, D. C. Cires?an, J. Masci, L. M. Gambardella, and J. Schmidhuber. Fast image scanning with
deep max-pooling convolutional neural networks. In ICIP, 2013.
[33] J. Masci, A. Giusti, D. C. Cires?an, G. Fricout, and J. Schmidhuber. A fast learning algorithm for image
segmentation with max-pooling convolutional networks. In ICIP, 2013.
[34] M. Chen, Y. Yan, X. Gong, C. D. Gilbert, H. Liang, and W. Li. Incremental integration of global contours
through interplay between visual cortical areas. Neuron, 82(3):682-694, 2014.
[35] S. C. Turaga et al. Maximin affinity learning of image segmentation. In NIPS, 2009.

9

"
2011,Adaptive Hedge,,4191-adaptive-hedge.pdf,"Most methods for decision-theoretic online learning are based on the Hedge algorithm, which takes a parameter called the learning rate. In most previous analyses the learning rate was carefully tuned to obtain optimal worst-case performance, leading to suboptimal performance on easy instances, for example when there exists an action that is significantly better than all others. We propose a new way of setting the learning rate, which adapts to the difficulty of the learning problem: in the worst case our procedure still guarantees optimal performance, but on easy instances it achieves much smaller regret. In particular, our adaptive method achieves constant regret in a probabilistic setting, when there exists an action that on average obtains strictly smaller loss than all other actions. We also provide a simulation study comparing our approach to existing methods.","Adaptive Hedge

?
Peter Grunwald

Tim van Erven
Department of Mathematics
VU University
De Boelelaan 1081a
1081 HV Amsterdam, the Netherlands
tim@timvanerven.nl

Centrum Wiskunde & Informatica (CWI)
Science Park 123, P.O. Box 94079
1090 GB Amsterdam, the Netherlands
pdg@cwi.nl

Wouter M. Koolen
CWI and Department of Computer Science
Royal Holloway, University of London
Egham Hill, Egham, Surrey
TW20 0EX, United Kingdom
wouter@cs.rhul.ac.uk

Steven de Rooij
Centrum Wiskunde & Informatica (CWI)
Science Park 123, P.O. Box 94079
1090 GB Amsterdam, the Netherlands
s.de.rooij@cwi.nl

Abstract
Most methods for decision-theoretic online learning are based on the Hedge algorithm, which takes a parameter called the learning rate. In most previous analyses
the learning rate was carefully tuned to obtain optimal worst-case performance,
leading to suboptimal performance on easy instances, for example when there exists an action that is significantly better than all others. We propose a new way
of setting the learning rate, which adapts to the difficulty of the learning problem: in the worst case our procedure still guarantees optimal performance, but on
easy instances it achieves much smaller regret. In particular, our adaptive method
achieves constant regret in a probabilistic setting, when there exists an action that
on average obtains strictly smaller loss than all other actions. We also provide a
simulation study comparing our approach to existing methods.

1

Introduction

Decision-theoretic online learning (DTOL) is a framework to capture learning problems that proceed
in rounds. It was introduced by Freund and Schapire [1] and is closely related to the paradigm of
prediction with expert advice [2, 3, 4]. In DTOL an agent is given access to a fixed set of K actions,
and at the start of each round must make a decision by assigning a probability to every action. Then
all actions incur a loss from the range [0, 1], and the agent?s loss is the expected loss of the actions
under the probability distribution it produced. Losses add up over rounds and the goal for the agent
is to minimize its regret after T rounds, which is the difference in accumulated loss between the
agent and the action that has accumulated the least amount of loss.
The most commonly studied strategy for the agent is called the Hedge algorithm [1, 5]. Its performance crucially depends on a parameter ? called the learning rate. Different ways of tuning
the learning rate have been proposed, which all aim to minimize the regret for the worst possible sequence of losses the actions might incur. If T is known
to the agent, then the learning rate
p
may be tuned to achieve worst-case regret bounded by T ln(K)/2, which is known to be optimal as T and K become large [4]. Nevertheless, by slightly relaxing the problem, one can obtain
better guarantees. Suppose for example that the cumulative loss L?T of the best action is known
to
p the? agent beforehand. Then, if the learning rate is set appropriately, the regret is bounded by
2LT ln(K) + ln(K) [4], which has the same asymptotics as the previous bound in the worst case
1

(because L?T ? T ) but may p
be much better when L?T turns out to be small. Similarly, Hazan and
Kale [6] obtain a bound of 8 VARmax
T ln(K) + 10 ln(K) for a modification of Hedge if the cumulative empirical variance VARmax
T of the best expert is known. In applications it may be unrealistic to
assume that T or (especially) L?T or VARmax
is known beforehand, but at the cost of slightly worse
T
constants such problems may be circumvented using either the doubling trick (setting a budget on
the unknown quantity and restarting the algorithm with a double budget when the budget is depleted)
[4, 7, 6], or a variable learning rate that is adjusted each round [4, 8].
Bounding the regret in terms of L?T or VARmax
is based on the idea that worst-case performance is
T
not the only property of interest: such bounds give essentially the same guarantee in the worst case,
but a much better guarantee in a plausible favourable case (when L?T or VARmax
is small). In this
T
paper, we pursue the same goal for a different favourable case. To illustrate our approach, consider
the following simplistic example with two actions: let 0 < a < b < 1 be such that b ? a > 2. Then
in odd rounds the first action gets loss a +  and the second action gets loss b ? ; in even rounds
the actions get losses a ?  and b + , respectively. Informally, this seems like a very easy instance
of DTOL, because the cumulative losses of the actions diverge and it is easy to see from the losses
which action is the best one. In fact, the Follow-the-Leader strategy, which puts all probability mass
on the action
p 1 in this case ? the worst-case
p with smallest cumulative loss, gives a regret of at most
bound O( L?T ln(K)) is very loose by comparison, and so is O( VARmax
T ln(K)), which is of the
p
same order T ln(K). On the other hand, for Follow-the-Leader one cannot guarantee sublinear
regret for worst-case instances. (For example, if one out of two actions yields losses 12 , 0, 1, 0, 1, . . .
and the other action yields losses 0, 1, 0, 1, 0, . . ., its regret will be at least T /2 ? 1.) To get the best
of both worlds, we introduce an adaptive version of Hedge, called AdaHedge, that automatically
adapts to the difficulty of the problem by varying the learning rate appropriately. As a result we
obtain constant regret for the simplistic
example above and other ?easy? instances of DTOL, while
p
at the same time guaranteeing O( L?T ln(K)) regret in the worst case.
It remains to characterise what we consider easy problems, which we will do in terms of the probabilities produced by Hedge. As explained below, these may be interpreted as a generalisation of
Bayesian posterior probabilities. We measure the difficulty of the problem in terms of the speed at
which the posterior probability of the best action converges to one. In the previous example, this
happens at an exponential rate, whereas for worst-case instances the posterior probability of the best
action does not converge to one at all.
Outline In the next section we describe a new way of tuning the learning rate, and show that it
yields essentially optimal performance guarantees in the worst case. To construct the AdaHedge
algorithm, we then add the doubling trick to this idea in Section 3, and analyse its worst-case regret.
In Section 4 we show that AdaHedge in fact incurs much smaller regret on easy problems. We
compare AdaHedge to other instances of Hedge by means of a simulation study in Section 5. The
proof of our main technical lemma is postponed to Section 6, and open questions are discussed in
the concluding Section 7. Finally, longer proofs are only available as Additional Material in the full
version at arXiv.org.

2

Tuning the Learning Rate

Setting Let the available actions be indexed by k ? {1, . . . , K}. At the start of each round
t = 1, 2, . . . the agent A is to assign a probability wtk to each action k by producing a vector
wt = (wt1 , . . . , wtK ) with nonnegative components that sum up to 1. Then every action k incurs
a loss `kt ? [0, 1], which we collect in the loss vector `t = (`1t , . . . , `K
loss of the agent
t ), and theP
PK
T
k k
k
is wt ? `t = k=1 wt `t . After T rounds action k has accumulated loss LT = t=1 `kt , and the
agent?s regret is
T
X
RA (T ) =
wt ? `t ? L?T ,
t=1

where L?T = min1?k?K LkT is the cumulative loss of the best action.

2

k

k
Hedge The Hedge algorithm chooses the weights wt+1
proportional to e??Lt , where ? > 0 is
the learning rate. As is well-known, these weights may essentially be interpreted as Bayesian posk
terior probabilities on actions, relative to a uniform prior and pseudo-likelihoods Ptk = e??Lt =
Qt
k
??`s
[9, 10, 4]:
s=1 e
k
1
k
e??Lt
k
K ? Pt
,
=
wt+1
=P
0
??Lk
Bt
t
k0 e

where
Bt =

X

1
K

? Ptk =

k

X

1
K

k

? e??Lt

(1)

k

is a generalisation of the Bayesian marginal likelihood. And like the ordinary marginal likelihood,
Bt factorizes into sequential per-round contributions:
Bt =

t
Y

ws ? e??`s .

(2)

s=1

We will sometimes write wt (?) and Bt (?) instead of wt and Bt in order to emphasize the dependence of these quantities on ?.
The Learning Rate and the Mixability Gap A key quantity in our and previous [4] analyses is
the gap between the per-round loss of the Hedge algorithm and the per-round contribution to the
negative logarithm of the ?marginal likelihood? BT , which we call the mixability gap:


?t (?) = wt (?) ? `t ? ? ?1 ln(wt (?) ? e??`t ) .
In the setting of prediction with expert advice, the subtracted term coincides with the loss incurred
by the Aggregating Pseudo-Algorithm (APA) which, by allowing the losses of the actions to be
mixed with optimal efficiency, provides an idealised lower bound for the actual loss of any prediction strategy [9]. The mixability gap measures how closely we approach this ideal. As the same
interpretation still holds in the more general DTOL setting of this paper, we can measure the difficulty of the problem, and tune ?, in terms of the cumulative mixability gap:
?T (?) =

T
X
t=1

?t (?) =

T
X

wt (?) ? `t +

1
?

ln BT (?).

t=1

We proceed to list some basic properties of the mixability gap. First, it is nonnegative and bounded
above by a constant that depends on ?:
Lemma 1. For any t and ? > 0 we have 0 ? ?t (?) ? ?/8.
Proof. The lower bound follows by applying Jensen?s inequality to the concave function ln, the
upper bound from Hoeffding?s bound on the cumulant generating function [4, Lemma A.1].
Further, the cumulative mixability gap ?T (?) can be related to L?T via the following upper bound,
proved in the Additional Material:
?L?T + ln(K)
Lemma 2. For any T and ? ? (0, 1] we have ?T (?) ?
.
e?1
This relationship will make it possible to provide worst-case guarantees similar to what is possible
when ? is tuned in terms of L?T . However, for easy instances of DTOL this inequality is very loose,
in which case we can prove substantially better regret bounds. We could now proceed by optimizing
the learning rate ? given the rather awkward assumption that ?T (?) is bounded by a known constant
b for all ?, which would be the natural counterpart to an analysis that optimizes ? when a bound on
L?T is known. However, as ?T (?) varies with ? and is unknown a priori anyway, it makes more
sense to turn the analysis on its head and start by fixing ?. We can then simply run the Hedge
algorithm until the smallest T such that ?T (?) exceeds an appropriate budget b(?), which we set to


1
b(?) = ?1 + e?1
ln(K).
(3)
3

When at some point the budget is depleted, i.e. ?T (?) ? b(?), Lemma 2 implies that
q
? ? (e ? 1) ln(K)/L?T ,

(4)

so that, up to a constant
pfactor, the learning rate used by AdaHedge is at least as large as the learning
rates proportional to ln(K)/L?T that are used in the literature. On the other hand, it is not too
p
large, because we can still provide a bound of order O( L?T ln(K)) on the worst-case regret:
Theorem 3. Suppose the agent runs Hedge with learning rate ? ? (0, 1], and after T rounds has
just used up the budget (3), i.e. b(?) ? ?T (?) < b(?) + ?/8. Then its regret is bounded by
q
4
1
RHedge(?) (T ) < e?1
L?T ln(K) + e?1
ln(K) + 18 .
Proof. The cumulative loss of Hedge is bounded by
T
X

wt ? `t = ?T (?) ?

1
?

ln BT < b(?) + ?/8 ?

1
?

ln BT ?

1
e?1

ln(K) + 18 +

2
?

ln(K) + L?T , (5)

t=1

where we have used the bound BT ?

3

1 ??L?
T.
Ke

Plugging in (4) completes the proof.

The AdaHedge Algorithm

We now introduce the AdaHedge algorithm by adding the doubling trick to the analysis of the
previous section. The doubling trick divides the rounds in segments i = 1, 2, . . ., and on each
segment restarts Hedge with a different learning rate ?i . For AdaHedge we set ?1 = 1 initially, and
scale down the learning rate by a factor of ? > 1 for every new segment, such that ?i = ?1?i . We
monitor ?t (?i ), measured only on the losses in the i-th segment, and when it exceeds its budget
bi = b(?i ) a new segment is started. The factor ? is a parameter
of the algorithm. Theorem 5 below
?
suggests setting its value to the golden ratio ? = (1 + 5)/2 ? 1.62 or simply to ? = 2.
Algorithm 1 AdaHedge(?)
. Requires ? > 1
???
for t = 1, 2, . . . do
if t = 1 or ? ? b then
. Start a new segment
1
+ ?1 ) ln(K)
? ? ?/?; b ? ( e?1
1
1
? ? 0; w = (w1 , . . . , wK ) ? ( K
,..., K
)
end if
. Make a decision
Output probabilities w for round t
Actions receive losses `t
. Prepare for the next round
? ? ? + w ? `t + ?1 ln(w ? e??`t )
1

K

w ? (w1 ? e??`t , . . . , wK ? e??`t )/(w ? e??`t )
end for
end
The regret of AdaHedge is determined by the number of segments it creates: the fewer segments
there are, the smaller the regret.
Lemma 4. Suppose that after T rounds, the AdaHedge algorithm has started m new segments.
Then its regret is bounded by


 ?m ? 1 
1
RAdaHedge (T ) < 2 ln(K)
+ m e?1
ln(K) + 18 .
??1
Proof. The regret per segment is bounded as in (5). Summing over all m segments, and plugging in
Pm
Pm?1 i
m
i=1 1/?i =
i=0 ? = (? ? 1)/(? ? 1) gives the required inequality.
4

Using (4), one can obtain an upper bound on the number of segments that leads to the following
guarantee for AdaHedge:
Theorem 5. Suppose the agent runs AdaHedge for T rounds. Then its regret is bounded by
p

? ?2 ? 1 q 4 ?
RAdaHedge (T ) ?
LT ln(K) + O ln(L?T + 2) ln(K) ,
e?1
??1
For details see the proof in the Additional
Material. p
The value for ? that minimizes the leading
?
for
which
?
?2 ? 1/(? ? 1) ? 3.33, but simply taking
factor is the golden ratio ? = (1 + 5)/2,
p
? = 2 leads to a very similar factor of ? ?2 ? 1/(? ? 1) ? 3.46.

4

Easy Instances

While the previous sections reassure us that AdaHedge performs well for the worst possible sequence of losses, we are also interested in its behaviour when the losses are not maximally antagonistic. We will characterise such sequences in terms of convergence of the Hedge posterior
probability of the best action:
wt? (?) = max wtk (?).
1?k?K

??Lk
t?1

is proportional to e
(Recall that
, so wt? corresponds to the posterior probability of the
action with smallest cumulative loss.) Technically, this is expressed by the following refinement of
Lemma 1, which is proved in Section 6.

Lemma 6. For any t and ? ? (0, 1] we have ?t (?) ? (e ? 2)? 1 ? wt? (?) .
wtk

This lemma, which may be of independent interest, is a variation on Hoeffding?s bound on the
cumulant generating function. While Lemma 1 leads to a bound on ?T (?) that grows linearly
in T , Lemma 6 shows that ?T (?) may grow much slower. In fact, if the posterior probabilities wt?
converge to 1 sufficiently quickly, then ?T (?) is bounded, as shown by the following lemma. Recall
that L?T = min1?k?K LkT .
Lemma 7. Let ? and ? be positive constants, and let ? ? Z+ . Suppose that for t = ?, ? + 1, . . . , T
?
there exists a single action k ? that achieves minimal cumulative loss Lkt = L?t , and for k 6= k ? the
cumulative losses diverge as Lkt ? L?t ? ?t? . Then for all ? > 0
T
X


?
1 ? wt+1
(?) ? CK ? ?1/? ,

t=?

where CK = (K ? 1)?

?1/?

?(1 + ?1 ) is a constant that does not depend on ?, ? or T .

The lemma is proved in the Additional Material. Together with Lemmas 1 and 6, it gives an upper
bound on ?T (?), which may be used to bound the number of segments started by AdaHedge. This
leads to the following result, whose proof is also delegated to the Additional Material.
Let s(m) denote the round in which AdaHedge starts its m-th segment, and let Lkr (m) =
Lks(m)+r?1 ? Lks(m)?1 denote the cumulative loss of action k in that segment.
Lemma 8. Let ? > 0 and ? > 1/2 be constants, and let CK be as in Lemma 7. Suppose there
?
exists a segment m? ? Z+ started by AdaHedge, such that ? := b8 ln(K)?(m ?1)(2?1/?) ? 8(e ?
?
?
2)CK + 1c ? 1 and for some action k the cumulative losses in segment m diverge as
?

Lkr (m? ) ? Lkr (m? ) ? ?r?

for all r ? ? and k 6= k ? .

(6)

?

Then AdaHedge starts at most m segments, and hence by Lemma 4 its regret is bounded by a
constant:
RAdaHedge (T ) = O(1).
In the simplistic example from the introduction, we may take ? = b ? a ? 2 and ? = 1, such that
(6) is satisfied for any ? ? 1. Taking m? large enough to ensure that ? ? 1, we find that AdaHedge
1
never starts more than m? = 1 + dlog? ( ?e?2
ln(2) + 8 ln(2) )e segments. Let us also give an example of
a probabilistic setting in which Lemma 8 applies:
5

Theorem 9. Let ? > 0 and ? ? (0, 1] be constants, and let k ? be a fixed action. Suppose the loss
vectors `t are independent random variables such that the expected differences in loss satisfy
?

min? E[`kt ? `kt ] ? 2?

k6=k

for all t ? Z+ .

Then, with probability at least 1 ? ?, AdaHedge starts at most
 (K ? 1)(e ? 2) ln 2K/(?2 ?)
l
1 m
m? = 1 + log?
+
+
2
? ln(K)
4? ln(K)
8 ln(K)

(7)

(8)

segments and consequently its regret is bounded by a constant:

RAdaHedge (T ) = O K + log(1/?) .
This shows that the probabilistic setting of
pthe theorem is much easier than the worst case, for which
only a bound on the regret of order O( T ln(K)) is possible, and that AdaHedge automatically
adapts to this easier setting. The proof of Theorem 9 is in the Additional Material. It verifies that the
conditions of Lemma 8 hold with sufficient probability for ? = 1, and ? and m? as in the theorem.

5

Experiments

We compare AdaHedge to other hedging algorithms in two experiments involving simulated losses.
5.1

Hedging Algorithms

Follow-the-Leader. This algorithm is included because it is simple and very effective if the losses
are not antagonistic, although as mentioned in the introduction its regret is linear in the worst case.
Hedge with fixed learning rate. We also include Hedge with a fixed learning rate
q
? = 2 ln(K)/L?T ,
(9)
p
which achieves the regret bound 2 ln(K)L?T + ln(K)1 . Since ? is a function of L?T , the agent
needs to use post-hoc knowledge to use this strategy.
Hedge with doubling trick. The common way to apply the doubling trick to L?T is to set a budget on
L?T and multiply it by some constant ?0 at the start of each new segment, after which ? is optimized
for the new budget [4, 7]. Instead, we proceed the other way around and with each new segment
first divide ? by ? = 2 and then calculate the new budget such that (9) holds when ?t (?) reaches
the budget. This way we keep the same invariant (? is never larger than the right-hand side of (9),
with equality when the budget is depleted), and the frequency of doubling remains logarithmic in
L?T with a constant determined by ?, so both approaches are equally valid. However, controlling the
sequence of values of ? allows for easier comparison to AdaHedge.
AdaHedge (Algorithm 1). Like in the previous algorithm, we set ? = 2. Because of how we set up
the doubling, both algorithms now use the same sequence of learning rates 1, 1/2, 1/4, . . . ; the only
difference is when they decide to start a new segment.
Hedge with variable learning rate. Rather than using the doubling trick, this algorithm, described
in [8], changes the learning rate each round as a function of L?t . This way there is no need to relearn
the weights of the actions in each block, which leads to a better worst-case bound and potentially
better performance in practice. Its behaviour on easy problems, as we are currently interested in, has
not been studied.
5.2

Generating the Losses

In both experiments we choose losses in {0, 1}. The experiments are set up as follows.
1

Cesa-Bianchi and Lugosi use ? = ln(1 +
simplified expression we use.

p

2 ln K/L?T ) [4], but the same bound can be obtained for the

6

100

20

90

18
Hedge (doubling)
Hedge (fixed learning rate)
Hedge (variable learning rate)
AdaHedge
Follow the leader

80
70

16
14
12
Regret

Regret

60
50

10

40

8

30

6

20

4

10

2

0
0

Hedge (doubling)
Hedge (fixed learning rate)
Hedge (variable learning rate)
AdaHedge
Follow the leader

1000

2000

3000

4000 5000 6000
Number of Rounds

7000

8000

9000

0
0

10000

1000

2000

(a) I.I.D. losses

3000

4000 5000 6000
Number of Rounds

7000

8000

9000

10000

(b) Correlated losses

Figure 1: Simulation results
I.I.D. losses. In the first experiment, all T = 10 000 losses for all K = 4 actions are independent,
with distribution depending only on the action: the probabilities of incurring loss 1 are 0.35, 0.4,
0.45 and 0.5, respectively. The results are then averaged over 50 repetitions of the experiment.
Correlated losses. In the second experiment, the T = 10 000 loss vectors are still independent,
but no longer identically distributed. In addition there are dependencies within the loss vectors `t ,
between the losses for the K = 2 available actions: each round is hard with probability 0.3, and
easy otherwise. If round t is hard, then action 1 yields loss 1 with probability 1 ? 0.01/t and action
2 yields loss 1 with probability 1 ? 0.02/t. If the round is easy, then the probabilities are flipped and
the actions yield loss 0 with the same probabilities. The results are averaged over 200 repetitions.
5.3

Discussion and Results

Figure 1 shows the results of the experiments above. We plot the regret (averaged over repetitions
of the experiment) as a function of the number of rounds, for each of the considered algorithms.
I.I.D. Losses. In the first considered regime, the accumulated losses for each action diverge linearly with high probability, so that the regret of Follow-the-Leader is bounded. Based on Theorem 9
we expect AdaHedge to incur bounded regret also; this is confirmed in Figure 1(a). Hedge with a
fixed learning rate shows much larger regret. This happens because the learning rate, while it optimizes the worst-case bound, is much too small for this easy regime. In fact, if we would include
more rounds, the learning rate would be set to an even smaller value, clearly showing the need to
determine the learning rate adaptively. The doubling trick provides one way to adapt the learning
rate; indeed, we observe that the regret of Hedge with the doubling trick is initially smaller than the
regret of Hedge with fixed learning rate. However, unlike AdaHedge, the algorithm never detects
that its current value of ? is working well; instead it keeps exhausting its budget, which leads to a
sequence of clearly visible bumps in its regret. Finally, it appears that the Hedge algorithm with
variable learning rate also achieves bounded regret. This is surprising, as the existing theory for
this algorithm only considers its worst-case behaviour, and the algorithm was not designed to do
specifically well in easy regimes.
Correlated Losses. In the second simulation we investigate the case where the mean cumulative
loss of two actions is extremely close ? within O(log t) of one another. If the losses of the actions
where independent, such a small difference
? would be dwarfed by random fluctuations in the cumulative losses, which would be of order O( t). Thus the two actions can only be distinguished because
we have made their losses dependent. Depending on the application, this may actually be a more natural scenario than complete independence as in the first simulation; for example, we can think of the
losses as mistakes of two binary classifiers, say, two naive Bayes classifiers with different smoothing parameters. In such a scenario,
losses will be dependent, and the difference in cumulative loss
?
will be much smaller than O( t). In the previous experiment, the posterior weights of the actions

7

converged relatively quickly for a large range of learning rates, so that the exact value of the learning
rate was most important at the start (e.g., from 3000 rounds onward Hedge with fixed learning rate
does not incur much additional regret any more). In this second setting, using a high learning rate
remains important throughout. This explains why in this case Hedge with variable learning rate can
no longer keep up with Follow-the-Leader. The results for AdaHedge are also interesting: although
Theorem 9 does not apply in this case, we may still hope that ?t (?) grows slowly enough that the
algorithm does not start too many segments. This turns out to be the case: over the 200 repetitions
of the experiment, AdaHedge started only 2.265 segments on average, which explains its excellent
performance in this simulation.

6

Proof of Lemma 6

Our main technical tool is Lemma 6. Its proof requires the following intermediate result:


Lemma 10. For any ? > 0 and any time t, the function f (`t ) = ln wt ? e??`t is convex.
This may be proved by observing that f is the convex conjugate of the Kullback-Leibler divergence.
An alternative proof based on log-convexity is provided in the Additional Material.
Proof of Lemma 6. We need to bound ?t = wt (?) ? `t + ?1 ln(wt (?) ? e??`t ), which is a convex
function of `t by Lemma 10. As a consequence, its maximum is achieved when `t lies on the
boundary of its domain, such that the losses `kt are either 0 or 1 for all k, and in the remainder of the
proof we will assume (without loss of generality) that this is the case. Now let ?t = wt ? `t be the
posterior probability of the actions with loss 1. Then


1
1
?t = ?t + ln (1 ? ?t ) + ?t e?? = ?t + ln 1 + ?t (e?? ? 1) .
?
?
Using ln x ? x ? 1 and e?? ? 1 ? ? + 12 ? 2 , we get ?t ? 12 ?t ?, which is tight for ?t near 0. For ?t
near 1, rewrite
1
?t = ?t ? 1 + ln(e? (1 ? ?t ) + ?t )
?
and use ln x ? x ? 1 and e? ? 1 + ? + (e ? 2)? 2 for ? ? 1 to obtain ?t ? (e ? 2)(1 ? ?t )?.
Combining the bounds, we find
?t ? (e ? 2)? min{?t , 1 ? ?t }.
?

?

Now, let k ? be an action such that wt? = wtk . Then `kt = 0 implies ?t ? 1 ? wt? . On the other
?
hand, if `kt = 1, then ?t ? wt? so 1??t ? 1?wt? . Hence, in both cases min{?t , 1??t } ? 1?wt? ,
which completes the proof.

7

Conclusion and Future Work

We have presented a new algorithm, AdaHedge, that adapts to the difficulty of the DTOL learning
problem. This difficulty was characterised in terms of convergence of the posterior probability of the
best action. For hard instances of DTOL, for which thepposterior does not converge, it was shown
that the regret of AdaHedge is of the optimal order O( L?T ln(K)); for easy instances, for which
the posterior converges sufficiently fast, the regret was bounded by a constant. This behaviour was
confirmed in a simulation study, where the algorithm outperformed existing versions of Hedge.
A surprising observation in the experiments was the good performance of Hedge with a variable
learning rate on some easy instances. It would be interesting to obtain matching theoretical guarantees, like those presented here for AdaHedge. A starting point might be to consider how fast the
posterior probability of the best action converges to one, and plug that into Lemma 6.
Acknowledgments
The authors would like to thank Wojciech Kot?owski for useful discussions. This work was supported in part by the IST Programme of the European Community, under the PASCAL2 Network
of Excellence, IST-2007-216886, and by NWO Rubicon grant 680-50-1010. This publication only
reflects the authors? views.
8

References
[1] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting. Journal of Computer and System Sciences, 55:119?139, 1997.
[2] N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Information and Computation, 108(2):212?261, 1994.
[3] V. Vovk. A game of prediction with expert advice. Journal of Computer and System Sciences,
56(2):153?173, 1998.
[4] N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press,
2006.
[5] Y. Freund and R. E. Schapire. Adaptive game playing using multiplicative weights. Games
and Economic Behavior, 29:79?103, 1999.
[6] E. Hazan and S. Kale. Extracting certainty from uncertainty: Regret bounded by variation
in costs. In Proceedings of the 21st Annual Conference on Learning Theory (COLT), pages
57?67, 2008.
[7] N. Cesa-Bianchi, Y. Freund, D. Haussler, D. P. Helmbold, R. E. Schapire, and M. K. Warmuth.
How to use expert advice. Journal of the ACM, 44(3):427?485, 1997.
[8] P. Auer, N. Cesa-Bianchi, and C. Gentile. Adaptive and self-confident on-line learning algorithms. Journal of Computer and System Sciences, 64:48?75, 2002.
[9] V. Vovk. Competitive on-line statistics. International Statistical Review, 69(2):213?248, 2001.
[10] D. Haussler, J. Kivinen, and M. K. Warmuth. Sequential prediction of individual sequences
under general loss functions. IEEE Transactions on Information Theory, 44(5):1906?1925,
1998.
[11] A. N. Shiryaev. Probability. Springer-Verlag, 1996.

9

"
2017,Multiscale Quantization for Fast Similarity Search,Poster,7157-multiscale-quantization-for-fast-similarity-search.pdf,"We propose a multiscale quantization approach for fast similarity search on large, high-dimensional datasets. The key insight of the approach is that quantization methods, in particular product quantization, perform poorly when there is large variance in the norms of the data points. This is a common scenario for real- world datasets, especially when doing product quantization of residuals obtained from coarse vector quantization. To address this issue, we propose a multiscale formulation where we learn a separate scalar quantizer of the residual norm scales. All parameters are learned jointly in a stochastic gradient descent framework to minimize the overall quantization error. We provide theoretical motivation for the proposed technique and conduct comprehensive experiments on two large-scale public datasets, demonstrating substantial improvements in recall over existing state-of-the-art methods.","Multiscale Quantization for Fast Similarity Search
Xiang Wu Ruiqi Guo Ananda Theertha Suresh Sanjiv Kumar
Dan Holtmann-Rice David Simcha Felix X. Yu
Google Research, New York
{wuxiang, guorq, theertha, sanjivk, dhr, dsimcha, felixyu}@google.com

Abstract
We propose a multiscale quantization approach for fast similarity search on large,
high-dimensional datasets. The key insight of the approach is that quantization
methods, in particular product quantization, perform poorly when there is large
variance in the norms of the data points. This is a common scenario for realworld datasets, especially when doing product quantization of residuals obtained
from coarse vector quantization. To address this issue, we propose a multiscale
formulation where we learn a separate scalar quantizer of the residual norm scales.
All parameters are learned jointly in a stochastic gradient descent framework to
minimize the overall quantization error. We provide theoretical motivation for the
proposed technique and conduct comprehensive experiments on two large-scale
public datasets, demonstrating substantial improvements in recall over existing
state-of-the-art methods.

1

Introduction

Large-scale similarity search is central to information retrieval and recommendation systems for
images, audio, video, and textual information. For high-dimensional data, several hashing based
methods have been proposed, including randomized [19, 1, 32] and learning-based techniques
[34, 35, 15]. Another set of techniques, based on quantization, have become popular recently due to
their strong performance on real-world data. In particular, product quantization (PQ) [12, 20] and its
variants have regularly claimed top spots on public benchmarks such as GIST1M, SIFT1B [20] and
DEEP10M [3].
In product quantization, the original vector space is decomposed into a Cartesian product of lower
dimensional subspaces, and vector quantization is performed in each subspace independently. Vector
quantization (VQ) approximates a vector x 2 Rdim(x) by finding the closest quantizer in a codebook
C:
ck2
V Q (x; C) = argmin kx
c2{Cj }

dim(x)?m

where C 2 R
is a vector quantization codebook with m codewords, and the j-th column Cj
represents the j-th quantizer. Similarly, product quantization (PQ) with K subspaces can be defined
as following concatenation:
P Q (x;

S = {S(k) }) = [

V Q (x

(1)

; S(1) ); ? ? ? ;

V Q (x

(K)

; S(K) )]

where x(k) denotes the subvector of x in the k-th subspace, and S(k) 2 Rdim(x
K product quantization codebooks, each with l sub-quantizers.

(k)

)?l

(1)
is a collection of

Product quantization works well in large part due to the fact that it permits asymmetric distance
computation [20], in which only dataset vectors are quantized while the query remains unquantized.
This is more precise than techniques based on Hamming distances (which generally require hashing
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

2 1/2
2

""#
	/	'
2

! ""#

(a)

(b)

Figure 1: Variance in data point norms poses a challenge to product quantization. (a) PQ quantization
error on a synthetic dataset X 2 Rd?N grows as the standard deviation of data point norms (kxk2 )
increases. The mean of the dataset is zero ?(x) = 0, and the average squared norm is fixed,
?(kxk22 ) = 1. In both settings, m = 16 codes are generated per data point and one with l = 16
sub-quantizers per subspace, the other
p with l = 256. (b) Ratio between the standard deviation
(krx k2 ) and normalization factor ?(krx k22 ), where rx represents the residual after vector (coarse)
quantization on the real-world dataset of SIFT1M.
the query), while still being efficient to compute using lookup table operations. We will give a more
detailed background on product quantization variants in Section 1.2.
1.1

Motivation of Multiscale

Empirically, product quantization works the best when the variance in each subspace is roughly
balanced [20]. To ensure this, a rotation matrix is often applied to the data prior to performing
quantization. This rotation can be either random [20] or learned [11, 30, 39].
In this work, however, we argue that the quality of the product quantization codebook also degenerates
when there is variance in the norms of the data points being encoded?even when the variance is
relatively moderate. We illustrate this point by generating synthetic datasets such that: (1) the dataset
mean is zero; (2) data point direction is chosen uniformly at random; (3) the average squared norm
of the data points is fixed. In Figure 1a, we plot quantization error (MSE) of product quantization
against the standard deviation of the norms of the data points. Clearly, quantization error increases
with the variance of the data point norms. In real-world settings (Figure 1b), the residuals of a coarse
vector quantization of the data also tend to have highly varying norms.
To compensate for the case when there is large variance in norms, we modify the formulation of
product quantization by separately scalar quantizing data point norms, and then unit-normalizing the
data points before applying product quantization. When computing asymmetric distances, this simply
requires a scalar multiplication of the PQ codebook once per scalar quantizer, which has negligible
computational cost in practice.
To scale quantization based search techniques to massive datasets, a popular strategy is to first
vector quantize the input vectors in the original space (coarse quantization), and then apply product
quantization on the vector quantization residuals [20]. However, in such a ?VQ-PQ? style approach,
the norms of the residuals exhibit significant variance. Therefore, the proposed multiscale approach
provides significant gains for massive search even when the original data is fully normalized.
1.2

Related Works

The original idea of product quantization traces back to early works of signal processing [14, 12].
J?gou et al. [20] first introduced efficient asymmetric distance computation (ADC) and applied it to
the approximate nearest neighbor (ANN) search problem. Since then, there have been multiple lines
of work focused on improving PQ.
Coarse Quantizer. Also termed inverted file (IVF) indexing structure in J?gou et al. [20], this
approach learns a vector quantization of the data points via clustering, using the cluster indices to
form an inverted index storing all data points corresponding to a given cluster index consecutively.
A data point is encoded via PQ codes associated with the residual (offset) of the data point from its
closet cluster center. This design enables non-exhaustive search by searching only a subset of the m
2

clusters/partitions in the inverted index. However, previous works have learned coarse quantizers as a
separate preprocessing step, without training the coarse quantizers jointly with the PQ codebooks.
Rotation Matrix. Since PQ quantizes each subspace independently, a rotation matrix can be applied
to reduce the intra-subspace statistical dependence. Researchers have proposed multiple ways to
estimate such a rotation matrix: Norouzi and Fleet [30] use ITQ [13] style alternating quantization;
Optimized PQ [11] also applied a simple strategy to minimize the quantization error; Locally
Optimized PQ [22] learns a separate R for each coarse partition (and incurs the extra overhead
of multiplying each local rotation with the query vector to compute lookup tables specific to each
partition). In high-dimensional setup, Zhang et al. [39] address the scalability issue in learning the
d ? d rotation matrix by imposing a Kronecker product structure. While learning such orthogonal
transformations is a good strategy in general, it does not change the norm of data points. Thus it still
suffers from norm variance as discussed in Section 1.1.
Additive Codebooks. Another line of research is focused on learning additive codebooks instead of
subspace codebooks. This includes additive quantization [5, 6, 26], composite quantization [37, 38]
and stacked quantization [27]. Since they do not work in subspaces, additive codebooks don?t require
rotation, although they are harder to learn and more expensive to encode. Empirically, such additive
codebooks are more expressive, and outperform OPQ at lower bitrates. However, OPQ achieves
similar performance at higher bitrates. Since additive codebooks don?t address the variance of data
point norms, the proposed multiscale approach can also be applied to additive codebooks as well.
Implementation Improvements. Much effort has been put into optimizing the implementation of
ADC, as it is computationally critical. Douze et al. [10] propose using Hamming distance for fast
pruning. Johnson et al. [21] come up with an efficient GPU implementation for ADC lookup. Andr?
et al. [2] propose to use SIMD-based computation to compute lower bounds for ADC. Our method is
compatible with all of these improvements. We also discuss our ADC implementation in Section 4.4.
Non-quantization Techniques. There is a large body of similarity search literature on nonquantization based methods in both inner product search and nearest neighbor search. Tree based
methods [7, 29, 9], graph based methods [16] and locality sensitive hashing style algorithms [19, 1, 32]
focus on non-exhaustive search by partitioning the search space. In practice, these often lead to
random memory accesses, and are often combined with exhaustive methods in ways similar to
IVFADC [20, 4, 31, 28]. Binary embedding based approaches [36, 24, 18, 13, 17, 25] focus on
learning short binary codes, and can be searched efficiently in Hamming space. However, there
is typically a large gap between the precision of distance computations in Hamming vs. product
codes under the same bitrate, and ADC can be computed with similar speed ([2, 21], Section 4.4).
Therefore, we focus on comparison to ADC based techniques in this paper.
1.3

Contributions

We propose a complete end-to-end training algorithm to learn coarse quantizers, a rotation matrix,
and product quantization codebooks, together with scalar quantizers to capture coarse quantization
residual norms. This differs from prior work in that it (a) identifies and addresses the problem of
variance in data point norms; (b) includes coarse quantizers as a part of the optimization; and (c) is endto-end trainable using stochastic gradient descent (SGD), which leads to a significant improvement
in quantization error compared to previous methods using alternating optimization [30, 11]. We
also present ablation tests demonstrating the importance of each component of the algorithm in
Section 4.2. In addition, we present theoretical motivation for our approach in Section 3.

2

Methodology

We focus on minimizing quantization error kx x
?k2 , where x is a data point and x
? is its quantized approximation, as a proxy for minimizing query-database distance approximation error
|kq xk2 kq x
?k2 |. State-of-the-art quantization techniques take a hierarchical approach [11, 27].
For instance, one or more ?coarse? quantization stages (VQ) can be followed by product quantization
(PQ) of the vector quantization residuals. A learned rotation is often applied to the residuals prior to
product quantization to further reduce quantization error [11].
This style of approach provides two key benefits:
3

1. Real world data is often clusterable, with the diameter of clusters substantially lower than the
diameter of the dataset as a whole. The vector quantization can thus be used to obtain a ?residual
dataset? with much smaller diameter, yielding significant reductions in quantization error when
quantized with only a product code [15].
2. By additionally learning a rotation of the VQ residuals, the variance within each PQ subspace can
be significantly reduced for many real world datasets, yielding substantially lower quantization
error and correspondingly higher recall.
As noted in Section 1.1, an additional source of quantization error when performing product quantization is the variance of data point norms. We extend the above strategy by explicitly representing the
norm of VQ residuals, learning a PQ codebook only on the unit-normalized rotated VQ residuals,
while separately scalar quantizing the residual norm scales. Specifically, multiscale quantization
employs the following steps: (1) vector quantization of the dataset; (2) learned rotation of the vector
quantization residuals; (3) reparameterization of the rotated residuals into direction and scale components; (4) product quantization of the direction component; (5) scalar quantization of the scale
component.
Formally, in multiscale quantization, the rotated residual rx and its `2 normalized version r?x are
defined as:
rx = R(x
r?x = rx /krx k2
V Q (x)),
And a data point x 2 Rd is approximated by
x?x
?=

V Q (x)

+ r?x , where r?x =

SQ ( x )R

T

rx )
P Q (?

and

x

= krx k2 /k

rx )k2
P Q (?

(2)

From above, V Q (x) = argminc2{Cj } kx ck2 returns the closest vector quantization codeword for
x; C 2 Rd?m is a vector quantization codebook with m codewords; Cj is its j-th codeword (i.e. the
j-th column of C); And the matrix R 2 Rd?d is a learned rotation, applied to the residuals of vector
quantization; The residual norm scale x is a scalar multiplier to the product quantized P Q (?
rx ) that
helps preserve the `2 norm of the rotated residual rx ; And SQ returns the nearest scalar quantizer
from a scalar quantization codebook W 2 Rp with p codewords (equivalent to one-dimensional
vector quantization). The product quantizer P Q (rx ) is given by
0 (1) (1) 1
0
1
(1)
rx )
r?x
P Q (?
B (2) (2) C
B (2) C
rx ) C
B P Q (?
B r?x C
B
C
B . C
(?
r
)
=
,
r
?
=
PQ x
x
..
B
C
B . C
@
A
@ . A
.
(K) (K)
rx )
P Q (?

(K)

r?x

as the concatenation of codewords obtained by dividing the rotated and normalized residuals r?x
(k)
into K subvectors r?x , k = 1, 2, ? ? ? , K, and quantizing the subvectors independently by vector
(k)
quantizers P Q (?) to minimize quantization error:
(k)
rx(k) )
P Q (?

= argmin k?
rx(k)

sk2 .

(k)

s2{Sj }

(k)

Hence, S(k) 2 Rd ?l is the vector quantization codebook for the k-th subspace (with l codewords).
d
Frequently, d(k) , the dimension of the k-th subvector, is simply K
, although subvectors of varying
size are also possible.
(k)

(k)

The quantized, normalized residuals are represented by the K indices of index( P Q (?
rx )), k =
1, ? ? ? , K. This representation has an overall bitrate of K log2 l, where K is the number of subspaces,
and l is the number of product quantizers in each subspace. The residual norm scales are maintained by
organizing the residuals associated with a VQ partition into groups, where within a group all residuals
have the same quantized norm scale. The groups are ordered by quantized norm scale, and thus
only the indices of group boundaries need to be maintained. The total storage cost incluiding group
boundaries and scalar quantization levels is thus O(mp), where m is number of vector quantizers and
p is the number of scalar quantizers. In our experiments, we set p to 8, which we find has a negligible
effect on recall compared with using unquantized norm scales.
4

2.1

Efficient Search under Multiscale Quantization

The multiscale quantization model enables nearest neighbor search to be carried out efficiently. For
a query q, we compute the squared `2 distance of q with each codeword in the vector quantization
codebook C, and search further within the nearest VQ partition. Suppose the corresponding quantizer
is c?q = argminc2{Cj } kq ck2 , and the corresponding quantization partition is Pq? = {x 2
{Xj }[N ] | V Q (x) = c?q }. Then, the approximate squared `2 distance between the query and database
points in Pq? are computed using a lookup table. The final prediction is made by taking the database
point with the smallest approximate distance, i.e.
?
?
xpred
= argmin kq c?q k22 2 R(q c?q ) ? [ SQ ( x ) P Q (?
rx )] + k SQ ( x ) P Q (?
rx )k22 .
q
x2Pq?

We use a lookup table to compute the quantized inner product between subvectors of the query?s
rotated VQ residual q? = R(q
c?q ) and the scaled product quantized data point residuals
rx ). Letting q?(k) be the k-th subvector of q? and wx = SQ ( x ) the quantized norm
SQ ( x ) P Q (?
scale, we first precompute inner products and the squared quantized `2 norm with the PQ codebook S
(k)
(k)
(k)
as vj = 2?
q (k) ? wx Sj + wx2 kSj k22 for all j and k, giving K lookup tables v (1) , . . . , v (K) each
of dimension l. We can then compute
2?
q ? wx

P Q (rx )

+ wx2 k

2
P Q (rx )k2

=

K
X

k=1

v

(k)
index(

(k)
P Q (rx ))

In practice, instead of searching only one vector quantization partition, one can use soft vector
quantization and search the t partitions with the lowest kq Cj k2 . The final complexity of the search
tK
is O( Nm
).

In our implementation, since all the data points with the same quantized norm scale are stored in
consecutive groups, we need only create a new lookup table at the beginning of a new group, by
(k)
(k)
combining scale independent lookup tables of 2?
q (k) ? Sj and kSj k22 (multiplied by wx and wx2 ,
respectively) using hardware optimized fused multiply-add instructions. We incur this computation
cost only p times for a VQ partition, where p is the number of scalar quantizers. In our experiment, we
set p = 8 and the number of VQ partitions to search t = 8, maintaining relatively low performance
overhead. We discuss more on the lookup table implementation in Section 4.4.
2.2

Optimization Procedure

We can explicitly formulate the mean squared loss as a function of our parameter vector ? =
(C; {S(k) }[K] ; R; {Wi }[m] ) per our approximation formulation (2). Wi here represents the parameter vector for the scalar quantizer of norm scales in partition i. To jointly train the parameters
of the model, we use stochastic gradient descent. To optimize the orthogonal transformation of
vector quantization residuals while maintaining orthogonality, we parameterize it via the Cayley
characterization of orthogonal matrices [8]:
R = (I

A)(I + A)

1

,

(3)

where A is a skew-symmetric matrix, i.e. A = AT . Note that (3) is differentiable w.r.t. the
d(d 1)/2 parameters of A. Computing the gradient requires an inversion of a d ? d matrix at each
iteration. However we found this tradeoff to be acceptable for datasets with dimensionalities in the
hundreds to thousands. When applying this method on high-dimensional datasets, one can restrict the
number of parameters of A to trade off capacity and computational cost.
The codebook for vector quantization is initialized using random samples from the dataset, while
the codebook for product quantization is initialized using the residuals (after vector quantization,
normalization and rotation) of a set of independent samples. To allow the vector quantization a
chance to partition the space, we optimize only the vector quantization error for several epochs before
initializing the product codes and doing full joint training. The parameters of the skew-symmetric
matrix A were initialized by sampling from N (0, 1e 3).

All optimization parameters were fixed for all datasets (although we note it would be possible to
improve results slightly with more extensive per-dataset tuning). We used the Adam optimization
algorithm [23] with the parameters suggested by the authors, minibatch sizes of 2000, and a learning
rate of 1e 4 during joint training (and 1e 3 when training only the vector quantizers).
5

To learn the scalar quantizer for residual norm scales and capture their local distribution within a
VQ partition, we jointly optimize the assignment of PQ codes and the scalar quantizer for all data
points within the same partition. Leaving the PQ codebook and rotation fixed, we alternate between
following two steps until convergence:
1. Fix all assigned PQ codes and scalar quantize the norm scales
only within the partition.

x

= krx k2 /k

2. Fix all quantized norm scales within the partition and reassign PQ codes for rx /

rx )k2
P Q (?
SQ ( x ).

In practice, it only takes a few iterations to converge to a local minimum for every VQ partition.

3

Analysis

Below we provide theoretical motivation and analysis for the components of the proposed quantization
approach, including for multiscale, learned rotation, and coarse quantization stages.
3.1

Multiscale

We first show that adding a scalar quantizer further increases the recall when the norms of the residuals
exhibit large variance. For a query q and a given partition with center Cj , if we define qj = q Cj ,
then the `2 error caused by residual quantization is
|kqj

rx k22

kqj

r?x k22 | = |

2qj ? (rx

? |2qj ? (rx

r?x ) + krx k22

r?x )| + |krx k22

The first query dependent term can be further transformed as
q
q
|2qj ? (rx r?x )| = 2 [(rx r?x )T qj ][qjT (rx r?x )] = 2 (rx
Taking expectation w.r.t q yields
q
Eq |2qj ? (rx r?x )| ? 2 Eq [(rx

r?x )T (qj qjT )(rx

q
r?x )] = 2 (rx

k?
rx k22 |

k?
rx k22 |.

r?x )T (qj qjT )(rx

r?x )

r?x )T Eq (qj qjT )(rx

r?x ),

where the inequality follows from Jensen?s inequality. If q is the largest eigen value of the covariance
matrix Eq (qj qjT ), then
p
Eq |kqj rx k22 kqj r?x k22 | ? 2
r?x k2 + |krx k22 k?
rx k22 |.
q krx

Existing quantization methods have focused on the first term in the error of `2 distance. However for
VQ residuals with large variance in krx k2 , the second quadratic term becomes dominant. By scalar
quantizing the residual norm scales, especially within each VQ partition locally, we can reduce the
second term substantially and thus improve recall on real datasets.
3.2

Rotation Matrix

Performing quantization after a learned rotation has been found to work well in practice [13, 30].
Here we show rotation is required in some scenarios. Let xi = Ryi , 1 ? i ? n. We show that
there exist simple examples, where the yi ?s have a product code with small codebook size and MSE
0, whereas to get any small MSE on xi s one may need to use exponentially many codewords. On
real-world datasets, this difference might not be quite so pronounced, but it is still significant and
hence undoing the rotation can yield significantly better MSE. We provide the following Lemma (see
the supplementary material for a proof).
Lemma 1. Let X = RY, i.e., for 1 ? i ? n, xi = Ryi . There exists a dataset Y and a rotation
matrix R such that a canonical basis product code of size 2 is sufficient to achieve MSE of 0 for Y,
whereas any product code on X requires 2c?min(d/K,K)? codewords to achieve MSE ?kxkmax , where
c is some universal constant and kxkmax is the maximum `2 norm of any data point.
6

(a)

(b)

Figure 2: (a) Break down by contribution to MSE reduction from each component in our model
on SIFT1M and DEEP10M datasets with different bitrates. The baseline is the original IVFADC
setup with no rotation or norm scale quantization. (b) Time spent per query by different distance
computation methods on linear search of a database of size |X| = 27 , 28 , 29 , ? ? ? 216 under 128 bits.
Lower curves indicate faster search time.
3.3

Coarse Quantization

We analyze the proposed vector and product quantization when the data is generated by a K-subspace
mixture model that captures two properties observed in many real-world data sets: samples belong to
one of several underlying categories, also referred to as components, and within each component the
residuals are generated independently in K subspaces. The precise model is defined in Appendix B.
For a query q, let x?q be the sample that minimizes kq xk2 . Let xVq Q be the output of the hierarchical
nearest neighbor algorithm that first finds the nearest cluster center and then searches within that
cluster. We show that if q is generated independently of x, then with high probability it returns an
xVq Q that is near-optimal.
Theorem 1. Given n samples from an underlying K-subspace mixture model that has been clustered
correctly and an independently generated query q, with probability 1
,
r
r
dr2
4n
d2
2n
kq x?q k22 kq xVq Q k22 ? 8b
log
+ 4r2
log
.
2K
2K
See Appendix B for a proof. Note that r = maxx2X krx k1 is the maximum value of the residual
in any coordinate and offers a natural scaling for our problem and b = maxx2X kq xk2 is the
maximum distance between q and any data point.

4
4.1

Experiments
Evaluation Datasets

We evaluate the performance of end-to-end trained multiscale quantization (MSQ) on the SIFT1M [20]
and DEEP10M [3] datasets, which are often used in benchmarking the performance of nearest
neighbor search. SIFT1M [20] contains 1 million, 128 dimensional SIFT descriptors extracted from
Flickr images. DEEP10M is introduced in [3], by extracting 96 PCA components from the final
hidden layer activations of GoogLeNet [33].
At training time, each dataset is indexed with 1024 VQ coarse quantizers. At query time, quantized
residuals from the 8 partitions closest to the query are further searched using ADC to generate the
final nearest neighbors. We report results on both quantization error (MSE, Section 4.2) and in terms
of retrieval recall (Recall1@N, Section 4.3). Often, the two metrics are strongly correlated.
4.2

Ablation Tests

Compared to IVFADC [20], which uses plain PQ with coarse quantizers, our end-to-end trained MSQ
reduces quantization error by 15-20% on SIFT1M, and 20-25% on DEEP10M, which is a substantial
reduction. Multiple components contribute to this reduction: (1) learned rotation of the VQ residuals;
(2) separate quantization of the residual norms into multiple scales; and (3) end-to-end training of all
parameters.
7

Figure 3: Recall curves when retrieving Top-1 neighbors (Recall1@N) on the SIFT1M dataset with
varying numbers of codebooks and centers. We search t = 8 out of m = 1024 VQ partitions.
In order to understand the effect of each component, we plot the MSE reduction relative to IVFADC [20] for several ablation tests (Figure 2a). On DEEP10M, the proposed multiscale approach
and the end-to-end learning contribute an additional 5-10% MSE reduction on top of learned rotation, while they contribute 10-15% on SIFT1M. It is important to note that on SIFT1M, multiscale
quantization and end-to-end training have a bigger impact than learned rotation, which is itself often
considered to yield a significant improvement.
4.3 Recall Experiments
We compare the proposed end-to-end trained multiscale quantization method against three baselines
methods: product quantization (PQ) [20], optimized product quantization (OPQ) [11] and stacked
quantizers (SQ) [27]. We generate ground-truth results using brute force search, and compare the
results of each method against ground-truth in fixed-bitrate settings.
For fixed-bitrate experiments, we show recall curves for varying numbers of PQ codebooks from the
range {8, 16, 32} for the SIFT1M dataset and {6, 12, 24} for the DEEP10M dataset. For each number
of codebooks, we experimented with both 16 centers for in-register table lookup and 256 centers
for in-memory table lookup in Figure 3 and 4. From the recall curves, it is clear that multiscale
quantization performs better than all baselines across both datasets in all settings.
4.4

Speed Benchmarks

We use the same indexing structure (IVF), and the same ADC computation implementation for all
baselines (PQ [20], OPQ [11], SQ [27]). Thus the speed of all baselines are essentially identical at the
same bitrate, meaning Figure 3 and 4 are both fixed-memory and fixed-time, and thus directly comparable. For codebooks with 256 centers, we implemented in-memory lookup table (LUT256) [20]; for
codebooks with 16 centers, we implemented in-register lookup table (LUT16) using the VPSHUFB
instruction from AVX2, which performs 32 lookups in parallel.
Also, we notice that there have been different implementations of ADC. The original algorithm
proposed in [20] uses in-memory lookup tables. We place tables in SIMD registers and leverage
SIMD instructions for fast lookup. Similar ideas are also reported in recent literature [10, 17, 2].
Here we put them on equal footing and provide a comparison of different approaches. In Figure 2b,
we plot the time for distance computation at the same bitrate. Clearly, VPSHUFB based LUT16
achieves almost the same speed compared to POPCNT based Hamming, and they are both 5x faster
than in-memory based ADC. As a practical observation, when the number of neighbors to be retrieved
is large, Recall1@N of LUT256 and LUT16 is often comparable at the same bitrate, and LUT16
with 5x speed up is almost always preferred.
8

Figure 4: Recall curves when retrieving Top-1 neighbors (Recall1@N) on the DEEP10M datasets
varying numbers of codebooks and centers. We search t = 8 out of m = 1024 VQ partitions.

5

Conclusions

We have proposed an end-to-end trainable multiscale quantization method that minimizes overall
quantization loss. We introduce a novel scalar quantization approach to account for the variances in
data point norms, which is both empirically and theoretically motivated. Together with the end-to-end
training, this contributes to large reduction in quantization error over existing competing methods that
already employ optimized rotation and coarse quantization. Finally, we conducted comprehensive
nearest neighbor search retrieval experiments on two large-scale, publicly available benchmark
datasets, and achieve considerable improvement over state-of-the-art.

6

Acknowledgements

We thank Jeffrey Pennington and Chen Wang for their helpful comments and discussions.

References
[1] Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. Practical and
optimal lsh for angular distance. In Advances in Neural Information Processing Systems, pages 1225?1233,
2015.
[2] Fabien Andr?, Anne-Marie Kermarrec, and Nicolas Le Scouarnec. Cache locality is not enough: highperformance nearest neighbor search with product quantization fast scan. Proceedings of the VLDB
Endowment, 9(4):288?299, 2015.
[3] A. Babenko and V. Lempitsky. Efficient indexing of billion-scale datasets of deep descriptors. In 2016
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2055?2063, June 2016.
[4] Artem Babenko and Victor Lempitsky. The inverted multi-index. In Computer Vision and Pattern
Recognition (CVPR), 2012 IEEE Conference on, pages 3069?3076. IEEE, 2012.
[5] Artem Babenko and Victor Lempitsky. Additive quantization for extreme vector compression. In Computer
Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 931?938. IEEE, 2014.
[6] Artem Babenko and Victor Lempitsky. Tree quantization for large-scale similarity search and classification.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4240?4248,
2015.
[7] Jon Louis Bentley. Multidimensional binary search trees used for associative searching. Communications
of the ACM, 18(9):509?517, 1975.
[8] Arthur Cayley. Sur quelques propri?t?s des d?terminants gauches. Journal f?r die reine und angewandte
Mathematik, 32:119?123, 1846.

9

"
2016,Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations,Poster,6214-path-normalized-optimization-of-recurrent-neural-networks-with-relu-activations.pdf,"We investigate the parameter-space geometry of recurrent neural networks (RNNs), and develop an adaptation of path-SGD optimization method, attuned to this geometry, that can learn plain RNNs with ReLU activations. On several datasets that require capturing long-term dependency structure, we show that path-SGD can significantly improve trainability of ReLU RNNs compared to RNNs trained with SGD, even with various recently suggested initialization schemes.","Path-Normalized Optimization of Recurrent Neural
Networks with ReLU Activations
Behnam Neyshabur?
Toyota Technological Institute at Chicago

Yuhuai Wu?
University of Toronto

bneyshabur@ttic.edu

ywu@cs.toronto.edu

Ruslan Salakhutdinov
Carnegie Mellon University

Nathan Srebro
Toyota Technological Institute at Chicago

rsalakhu@cs.cmu.edu

nati@ttic.edu

Abstract
We investigate the parameter-space geometry of recurrent neural networks (RNNs),
and develop an adaptation of path-SGD optimization method, attuned to this
geometry, that can learn plain RNNs with ReLU activations. On several datasets
that require capturing long-term dependency structure, we show that path-SGD can
significantly improve trainability of ReLU RNNs compared to RNNs trained with
SGD, even with various recently suggested initialization schemes.

1

Introduction

Recurrent Neural Networks (RNNs) have been found to be successful in a variety of sequence learning
problems [4, 3, 9], including those involving long term dependencies (e.g., [1, 23]). However, most
of the empirical success has not been with ?plain? RNNs but rather with alternate, more complex
structures, such as Long Short-Term Memory (LSTM) networks [7] or Gated Recurrent Units (GRUs)
[3]. Much of the motivation for these more complex models is not so much because of their modeling
richness, but perhaps more because they seem to be easier to optimize. As we discuss in Section
3, training plain RNNs using gradient-descent variants seems problematic, and the choice of the
activation function could cause a problem of vanishing gradients or of exploding gradients.
In this paper our goal is to better understand the geometry of plain RNNs, and develop better
optimization methods, adapted to this geometry, that directly learn plain RNNs with ReLU activations.
One motivation for insisting on plain RNNs, as opposed to LSTMs or GRUs, is because they
are simpler and might be more appropriate for applications that require low-complexity design
such as in mobile computing platforms [22, 5]. In other applications, it might be better to solve
optimization issues by better optimization methods rather than reverting to more complex models.
Better understanding optimization of plain RNNs can also assist us in designing, optimizing and
intelligently using more complex RNN extensions.
Improving training RNNs with ReLU activations has been the subject of some recent attention,
with most research focusing on different initialization strategies [12, 22]. While initialization can
certainly have a strong effect on the success of the method, it generally can at most delay the problem
of gradient explosion during optimization. In this paper we take a different approach that can be
combined with any initialization choice, and focus on the dynamics of the optimization itself.
Any local search method is inherently tied to some notion of geometry over the search space (e.g.
the space of RNNs). For example, gradient descent (including stochastic gradient descent) is tied to
the Euclidean geometry and can be viewed as steepest descent with respect to the Euclidean norm.
Changing the norm (even to a different quadratic norm, e.g. by representing the weights with respect
to a different basis in parameter space) results in different optimization dynamics. We build on prior
work on the geometry and optimization in feed-forward networks, which uses the path-norm [16]
?

Contributed equally.

30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

Input nodes
FF (shared weights)

hv = x[v]

RNN notation

h0t = xt , hi0 = 0

Internal nodes
hP
i
(u?v)?E wu?v hu
 i i?1
+
i
hit = Win
ht + Wrec
hit?1 +
hv =

hv =

Output nodes
P
(u?v)?E wu?v hu

hdt = Wout hd?1
t

Table 1: Forward computations for feedforward nets with shared weights.
(defined in Section 4) to determine a geometry leading to the path-SGD optimization method. To
do so, we investigate the geometry of RNNs as feedforward networks with shared weights (Section
2) and extend a line of work on Path-Normalized optimization to include networks with shared
weights. We show that the resulting algorithm (Section 4) has similar invariance properties on RNNs
as those of standard path-SGD on feedforward networks, and can result in better optimization with
less sensitivity to the scale of the weights.

2

Recurrent Neural Nets as Feedforward Nets with Shared Weights

We view Recurrent Neural Networks (RNNs) as feedforward networks with shared weights.
We denote a general feedforward network with ReLU activations and shared weights is indicated
by N (G, ?, p) where G(V, E) is a directed acyclic graph over the set of nodes V that corresponds
to units v ? V in the network, including special subsets of input and output nodes Vin , Vout ? V ,
p ? Rm is a parameter vector and ? : E ? {1, . . . , m} is a mapping from edges in G to parameters
indices. For any edge e ? E, the weight of the edge e is indicated by we = p?(e) . We refer to the
set of edges that share the ith parameter pi by Ei = {e ? E|?(e) = i}. That is, for any e1 , e2 ? Ei ,
?(e1 ) = ?(e2 ) and hence we1 = we2 = p?(e1 ) .
Such a feedforward network represents a function fN (G,?,p) : R|Vin | ? R|Vout | as follows: For any
input node v ? Vin , its output hv is the corresponding coordinate
of the input vector x i? R|Vin | .
hP
For each internal node v, the output is defined recursively as hv =
where
(u?v)?E wu?v ? hu
+

[z]+ = max(z, 0) is the ReLU P
activation function2 . For output nodes v ? Vout , no non-linearity is
applied and their output hv = (u?v)?E wu?v ? hu determines the corresponding coordinate of
the computed function fN (G,?,p) (x). Since we will fix the graph G and the mapping ? and learn
the parameters p, we use the shorthand fp = fN (G,?,p) to refer to the function implemented by
parameters p. The goal of training is to find parameters p that minimize some error functional L(fp )
that depends on p only through the function fp . E.g. in supervised learning L(f ) = E [loss(f (x), y)]
and this is typically done by minimizing an empirical estimate of this expectation.

If the mapping ? is a one-to-one mapping, then there is no weight sharing and it corresponds to
standard feedforward networks. On the other hand, weight sharing exists if ? is a many-to-one
mapping. Two well-known examples of feedforward networks with shared weights are convolutional
and recurrent networks. We mostly use the general notation of feedforward networks with shared
weights throughout the paper as this will be more general and simplifies the development and notation.
However, when focusing on RNNs, it is helpful to discuss them using a more familiar notation which
we briefly introduce next.
Recurrent Neural Networks Time-unfolded RNNs are feedforward networks with shared weights
that map an input sequence to an output sequence. Each input node corresponds to either a coordinate
of the input vector at a particular time step or a hidden unit at time 0. Each output node also
corresponds to a coordinate of the output at a specific time step. Finally, each internal node refers
to some hidden unit at time t ? 1. When discussing RNNs, it is useful to refer to different layers
and the values calculated at different time-steps. We use a notation for RNN structures in which
the nodes are partitioned into layers and hit denotes the output of nodes in layer i at time step t.
Let x = (x1 , . . . , xT ) be the input at different time steps where T is the maximum number of
i
propagations through time and we refer to it as the length of the RNN. For 0 ? i < d, let Win
and
i
Wrec be the input and recurrent parameter matrices of layer i and Wout be the output parameter
matrix. Table 1 shows forward computations for RNNs.The output of the function implemented by
RNN can then be calculated as fW,t (x) = hdt . Note that in this notations, weight matrices Win ,
Wrec and Wout correspond to ?free? parameters of the model that are shared in different time steps.
2

The bias terms can be modeled by having an additional special node vbias that is connected to all internal
and output nodes, where hvbias = 1.

2

3

Non-Saturating Activation Functions

The choice of activation function for neural networks can have a large impact on optimization. We
are particularly concerned with the distinction between ?saturating? and ?non-starting? activation
functions. We consider only monotone activation functions and say that a function is ?saturating? if it
is bounded?this includes, e.g. sigmoid, hyperbolic tangent and the piecewise-linear ramp activation
functions. Boundedness necessarily implies that the function values converge to finite values at
negative and positive infinity, and hence asymptote to horizontal lines on both sides. That is, the
derivative of the activation converges to zero as the input goes to both ?? and +?. Networks with
saturating activations therefore have a major shortcoming: the vanishing gradient problem [6]. The
problem here is that the gradient disappears when the magnitude of the input to an activation is large
(whether the unit is very ?active? or very ?inactive?) which makes the optimization very challenging.
While sigmoid and hyperbolic tangent have historically been popular choices for fully connected
feedforward and convolutional neural networks, more recent works have shown undeniable advantages
of non-saturating activations such as ReLU, which is now the standard choice for fully connected and
Convolutional networks [15, 10]. Non-saturating activations, including the ReLU, are typically still
bounded from below and asymptote to a horizontal line, with a vanishing derivative, at ??. But
they are unbounded from above, enabling their derivative to remain bounded away from zero as the
input goes to +?. Using ReLUs enables gradients to not vanish along activated paths and thus can
provide a stronger signal for training.
However, for recurrent neural networks, using ReLU activations is challenging in a different way, as
even a small change in the direction of the leading eigenvector of the recurrent weights could get
amplified and potentially lead to the explosion in forward or backward propagation [1].
To understand this, consider a long path from an input in the first element of the sequence to an output
of the last element, which passes through the same RNN edge at each step (i.e. through many edges
in some Ei in the shared-parameter representation). The length of this path, and the number of times
it passes through edges associated with a single parameter, is proportional to the sequence length,
which could easily be a few hundred or more. The effect of this parameter on the path is therefore
exponentiated by the sequence length, as are gradient updates for this parameter, which could lead to
parameter explosion unless an extremely small step size is used.
Understanding the geometry of RNNs with ReLUs could helps us deal with the above issues more
effectively. We next investigate some properties of geometry of RNNs with ReLU activations.
Invariances in Feedforward Nets with Shared Weights
Feedforward networks (with or without shared weights) are highly over-parameterized, i.e. there
are many parameter settings p that represent the same function fp . Since our true object of interest
is the function f , and not the identity p of the parameters, it would be beneficial if optimization
would depend only on fp and not get ?distracted? by difference in p that does not affect fp . It is
therefore helpful to study the transformations on the parameters that will not change the function
presented by the network and come up with methods that their performance is not affected by such
transformations.
Definition 1. We say a network N is invariant to a transformation T if for any parameter setting p,
fp = fT (p) . Similarly, we say an update rule A is invariant to T if for any p, fA(p) = fA(T (p)) .
Invariances have also been studied as different mappings from the parameter space to the same
function space [19] while we define the transformation as a mapping inside a fixed parameter space.
A very important invariance in feedforward networks is node-wise rescaling [17]. For any internal
node v and any scalar ? > 0, we can multiply all incoming weights into v (i.e. wu?v for any
(u ? v) ? E) by ? and all the outgoing weights (i.e. wv?u for any (v ? u) ? E) by 1/? without
changing the function computed by the network. Not all node-wise rescaling transformations can be
applied in feedforward nets with shared weights. This is due to the fact that some weights are forced
to be equal and therefore, we are only allowed to change them by the same scaling factor.
Definition 2. Given a network N , we say an invariant transformation Te that is defined over edge
weights (rather than parameters) is feasible for parameter mapping ? if the shared weights remain
equal after the transformation, i.e. for any i and for any e, e0 ? Ei , Te (w)e = Te (w)e0 .
3

1

1

1

1#
?

1

1#
?

1

1
1

1

1

1

T

1
1

?#
?

?#
?

1

1
1

?#
?

??
?

1

1

1
1

?#
?
?#
?

1

1

Figure 1: An example of invari-

1#
?

?#
?

1

1

1#
?

?

1

?

?#
?

?

?

ances in an RNN with two hidden
layers each of which has 2 hidden
units. The dashed lines correspond
to recurrent weights. The network
on the left hand side is equivalent
(i.e. represents the same function)
to the network on the right for any
nonzero ?11 = a, ?21 = b, ?12 = c,
?22 = d.

Therefore, it is helpful to understand what are the feasible node-wise rescalings for RNNs. In the
following theorem, we characterize all feasible node-wise invariances in RNNs.
Theorem 1. For any ? such that ?ji > 0, any Recurrent Neural Network with ReLU activation is invariant to the transformation T? ([Win , Wrec , Wout ]) = [Tin,? (Win ) , Trec,? (Wrec ) , Tout,? (Wout )]
where for any i, j, k:
 i i
?j Win [j, k]
i = 1,
 i
Tin,? (Win )i [j, k] =
(1)
?ji /?ki?1 Win
[j, k] 1 < i < d,
 i

Trec,? (Wrec )i [j, k] = ?ji /?ki Wrec
[j, k],
Tout,? (Wout )[j, k] = 1/?kd?1 Wout [j, k].
Furthermore, any feasible node-wise rescaling transformation can be presented in the above form.
The proofs of all theorems and lemmas are given in the supplementary material. The above theorem
shows that there are many transformations under which RNNs represent the same function. An
example of such invariances is shown in Fig. 1. Therefore, we would like to have optimization
algorithms that are invariant to these transformations and in order to do so, we need to look at
measures that are invariant to such mappings.

4

Path-SGD for Networks with Shared Weights

As we discussed, optimization is inherently tied to a choice of geometry, here represented by a choice
of complexity measure or ?norm?3 . Furthermore, we prefer using an invariant measure which could
then lead to an invariant optimization method. In Section 4.1 we introduce the path-regularizer and in
Section 4.2, the derived Path-SGD optimization algorithm for standard feed-forward networks. Then
in Section 4.3 we extend these notions also to networks with shared weights, including RNNs, and
present two invariant optimization algorithms based on it. In Section 4.4 we show how these can be
implemented efficiently using forward and backward propagations.
4.1

Path-regularizer

The path-regularizer is the sum over all paths from input nodes to output nodes of the product of
squared weights along the path. To define it formally,let P be the set of directed paths from input to
output units so that for any path ? = ?0 , . . . , ?len(?) ? P of length len(?), we have that ?0 ? Vin ,
?len(?) ? Vout and for any 0 ? i ? len(?) ? 1, (?i ? ?i+1 ) ? E. We also abuse the notation and
denote e ? ? if for some i, e = (?i , ?i+1 ). Then the path regularizer can be written as:
2
?net
(w) =

X len(?)?1
Y

w?2i ??i+1

(2)

i=0

??P

Equivalently, the path-regularizer can be defined recursively on the nodes of the network as:
X
X
2
2
?v2 (w) =
?u2 (w)wu?v
,
?net
(w) =
?u2 (w)

(3)

u?Vout

(u?v)?E
3

The path-norm which we define is a norm on functions, not on weights, but as we prefer not getting into this
technical discussion here, we use the term ?norm? very loosely to indicate some measure of magnitude [18].

4

4.2

Path-SGD for Feedforward Networks

Path-SGD is an approximate steepest descent step with respect to the path-norm. More formally, for
a network without shared weights, where the parameters are the weights themselves, consider the
diagonal quadratic approximation of the path-regularizer about the current iterate w(t) :
D
E 1


2
2
2
2
??net
(w(t) + ?w) = ?net
(w(t) ) + ??net
(w(t) ), ?w + ?w> diag ?2 ?net
(w(t) ) ?w (4)
2
P
?2?2
0 2
0 2
Using the corresponding quadratic norm kw ? w k?? 2 (w(t) +?w) = 12 e?E ?wnet
2 (we ? we ) , we
net
e
can define an approximate steepest descent step as:

2
D
E 


w(t+1) = min ? ?L(w), w ? w(t) + 
w ? w(t) 
 2
.
(5)
(t)
w

?
?net (w

+?w)

Solving (5) yields the update:
we(t+1) = we(t) ?

?
?L
(w(t) )
(t)
?e (w ) ?we

where: ?e (w) =

2
(w)
1 ? 2 ?net
.
2
2
?we

(6)

(w(t) ) is called
The stochastic version that uses a subset of training examples to estimate ?w?L
u?v
Path-SGD [16]. We now show how Path-SGD can be extended to networks with shared weights.
4.3

Extending to Networks with Shared Weights

When the networks has shared weights, the path-regularizer is a function of parameters p and
therefore the quadratic approximation should also be with respect to the iterate p(t) instead of w(t)
which results in the following update rule:

D
E 


p(t+1) = min ? ?L(p), p ? p(t) + 
p ? p(t) 
 2 (t)
.
(7)
p

where kp ?

2
p0 k?? 2 (p(t) +?p)
net

?
?net (p

=

2
? 2 ?net
i=1 ?p2i

Pm
1
2

(pi ?

2
p0i ) .

+?p)

Solving (7) gives the following update:

2
(p)
?
?L (t)
1 ? 2 ?net
.
(p
)
where:
?
(p)
=
i
2
(t)
2
?pi
?i (p ) ?pi
The second derivative terms ?i are specified in terms of their path structure as follows:
(t+1)

pi

(t)

= pi ?
(1)

(8)

(2)

Lemma 1. ?i (p) = ?i (p) + ?i (p) where
len(?)?1
(1)

?i (p) =

X X

1e??

Y

p2?(?j ??j+1 ) =

j=0
e6=(?j ??j+1 )

e?Ei ??P

X

?e (w),

(9)

e?Ei
len(?)?1

(2)

?i (p) = p2i

X

X

e1,e2?Ei
e1 6=e2

??P

1e1 ,e2 ??

Y

p2?(?j ??j+1 ) ,

(10)

j=0
e1 6=(?j ??j+1 )
e2 6=(?j ??j+1 )

and ?e (w) is defined in (6).
(2)
The second term ?i (p) measures the effect of interactions between edges corresponding to the
same parameter (edges from the same Ei ) on the same path from input to output. In particular, if for
any path from an input unit to an output unit, no two edges along the path share the same parameter,
then ?(2) (p) = 0. For example, for any feedforward or Convolutional neural network, ?(2) (p) = 0.
But for RNNs, there certainly are multiple edges sharing a single parameter on the same path, and so
we could have ?(2) (p) 6= 0.
The above lemma gives us a precise update rule for the approximate steepest descent with respect to
the path-regularizer. The following theorem confirms that the steepest descent with respect to this
regularizer is also invariant to all feasible node-wise rescaling for networks with shared weights.
Theorem 2. For any feedforward networks with shared weights, the update (8) is invariant to all
(1)
feasible node-wise rescalings. Moreover, a simpler update rule that only uses ?i (p) in place of
?i (p) is also invariant to all feasible node-wise rescalings.
Equations (9) and (10) involve a sum over all paths in the network which is exponential in depth of
the network. However, we next show that both of these equations can be calculated efficiently.
5

&

' = 400, , = 10

0.00014

' = 400, , = 40

0.00022

' = 100, , = 10

0.00037

' = 100, , = 10

0.00048

Training Error

500
#

400
300
200

SGD
Path-SGD:5(1)
Path-SGD:5(1)+5(2)

400
300
200
100

100
0

Test Error

500

SGD
Path-SGD:5(1)
Path-SGD:5(1)+5(2)

Perplexity

#

! (%)

Perplexity

! (#)

0

50

100
Epoch

150

200

0

0

50

100
Epoch

150

200

Figure 2: Path-SGD with/without the second term in word-level language modeling on PTB. We use the
standard split (929k training, 73k validation and 82k test) and the vocabulary size of 10k words. We initialize
the weights by sampling from the uniform distribution with range [?0.1, 0.1]. The table on the left shows the
ratio of magnitude of first and second term for different lengths T and number of hidden units H. The plots
compare the training and test errors using a mini-batch of size 32 and backpropagating through T = 20 time
steps and using a mini-batch of size 32 where the step-size is chosen by a grid search.

4.4

Simple and Efficient Computations for RNNs
(1)

(2)

We show how to calculate ?i (p) and ?i (p) by considering a network with the same architecture
but with squared weights:
Theorem 3. For any network N (G, ?, p), consider N (G, ?, p?) where for any i, p?i = p2i . Define the
P|Vout |
function g : R|Vin | ? R to be the sum of outputs of this network: g(x) = i=1
fp? (x)[i]. Then ?(1)
(2)
and ? can be calculated as follows where 1 is the all-ones input vector:
X
?g(1) ?hu0 (?
p)
(2)
hu (?
p).
(11)
?(1) (p) = ?p? g(1),
?i (p) =
p?i
0
?h
(?
p
)
?h
(?
p
)
v
v
(u?v),(u0 ?v 0 )?E
(u?v)6=(u0 ?v 0 )

i

In the process of calculating the gradient ?p? g(1), we need to calculate hu (?
p) and ?g(1)/?hv (?
p)
for any u, v. Therefore, the only remaining term to calculate (besides ?p?g(1)) is ?hu0 (?
p)/?hv (?
p).

Recall that T is the length (maximum number of propagations through time) and d is the number
of layers in an RNN. Let H be the number of hidden units in each layer and B be the size of the
mini-batch. Then calculating the gradient of the loss at all points in the minibatch (the standard
work required for any mini-batch gradient approach) requires time O(BdT H 2 ). In order to calculate
(1)
?i (p), we need to calculate the gradient ?p? g(1) of a similar network at a single input?so the
time complexity is just an additional O(dT H 2 ). The second term ?(2) (p) can also be calculated
for RNNs in O(dT H 2 (T + H)). For an RNN, ?(2) (Win ) = 0 and ?(2) (Wout ) = 0 because only
recurrent weights are shared multiple times along an input-output path. ?(2) (Wrec ) can be written
and calculated in the matrix form:
#
""
T
?3 
1 ?1
X
X
 > T ?t
>
?g(1)
(2)
i
0i
0i t1
i
Wrec

h (?
p)
? (Wrec ) = Wrec 
?hit1 +t2 +1 (?
p) t2
t1 =0
t2 =2
2
0i
i
where for any i, j, k we have Wrec
[j, k] = Wrec
[j, k] . The only terms that require extra computation are powers of Wrec which can be done in O(dT H 3 ) and the rest of the matrix computations need
O(dT 2 H 2 ). Therefore, the ratio of time complexity of calculating the first term and second term with
respect to the gradient over mini-batch is O(1/B) and O((T + H)/B) respectively. Calculating only
(1)
(2)
?i (p) is therefore very cheap with minimal per-minibatch cost, while calculating ?i (p) might be
(1)
expensive for large networks. Beyond the low computational cost, calculating ?i (p) is also very
easy to implement as it requires only taking the gradient with respect to a standard feed-forward
calculation in a network with slightly modified weights?with most deep learning libraries it can be
implemented very easily with only a few lines of code.

5
5.1

Experiments
The Contribution of the Second Term

As we discussed in section 4.4, the second term ?(2) in the update rule can be computationally
expensive for large networks. In this section we investigate the significance of the second term
6

0.20

MSE

0.15

Adding 100
IRNN
RNN Path

Adding 400

0.20

IRNN
RNN Path

0.20
0.15

0.10

0.00
0

IRNN
RNN Path

0.15

0.05

0.05
15
30
45
number of Epochs

Adding 750

0.10

0.10

0.05
0.00
0

0.25

80
160
number of Epochs

240

0.00
0

100
200
300
number of Epochs

400

Figure 3: Test errors for the addition problem of different lengths.
and show that at least in our experiments, the contribution of the second term is negligible. To
compare the two terms ?(1) and ?(2) , we train a single layer RNN with H = 200 hidden units for the
task of word-level language modeling on Penn Treebank (PTB) Corpus [13]. Fig. 2 compares the
performance of SGD vs. Path-SGD with/without ?(2) . We clearly see that both versions of Path-SGD
are performing very similarly and both of them outperform SGD significantly. This results in Fig. 2
suggest that the first term is more significant and therefore we can ignore the second term.
To
better
the importance of the two terms, we compared the ratio of the norms

 (2)

 
understand


? 
 / 
?(1) 
 for different RNN lengths T and number of hidden units H. The table in Fig. 2
2
2
shows that the contribution of the second term is bigger when the network has fewer number of
hidden units and the length of the RNN is larger (H is small and T is large). However, in many cases,
it appears that the first term has a much bigger contribution in the update step and hence the second
term can be safely ignored. Therefore, in the rest of our experiments, we calculate the Path-SGD
updates only using the first term ?(1) .
5.2

Synthetic Problems with Long-term Dependencies

Training Recurrent Neural Networks is known to be hard for modeling long-term dependencies due to
the gradient vanishing/exploding problem [6, 2]. In this section, we consider synthetic problems that
are specifically designed to test the ability of a model to capture the long-term dependency structure.
Specifically, we consider the addition problem and the sequential MNIST problem.
Addition problem: The addition problem was introduced in [7]. Here, each input consists of two
sequences of length T , one of which includes numbers sampled from the uniform distribution with
range [0, 1] and the other sequence serves as a mask which is filled with zeros except for two entries.
These two entries indicate which of the two numbers in the first sequence we need to add and the task
is to output the result of this addition.
Sequential MNIST: In sequential MNIST, each digit image is reshaped into a sequence of length 784,
turning the digit classification task into sequence classification with long-term dependencies [12, 1].
For both tasks, we closely follow the experimental protocol in [12]. We train a single-layer RNN
consisting of 100 hidden units with path-SGD, referred to as RNN-Path. We also train an RNN of
the same size with identity initialization, as was proposed in [12], using SGD as our baseline model,
referred to as IRNN. We performed grid search for the learning rates over {10?2 , 10?3 , 10?4 }
for both our model and the baseline. Non-recurrent weights were initialized from the uniform
distribution with range [?0.01, 0.01]. Similar to [1], we found the IRNN to be fairly unstable (with
SGD optimization typically diverging). Therefore for IRNN, we ran 10 different initializations and
picked the one that did not explode to show its performance.
In our first experiment, we evaluate Path-SGD on the addition problem. The results are shown in
Fig. 3 with increasing the length T of the sequence: {100, 400, 750}. We note that this problem
becomes much harder as T increases because the dependency between the output (the sum of two
numbers) and the corresponding inputs becomes more distant. We also compare RNN-Path with
the previously published results, including identity initialized RNN [12] (IRNN), unitary RNN [1]
(uRNN), and np-RNN4 introduced by [22]. Table 2 shows the effectiveness of using Path-SGD.
Perhaps more surprisingly, with the help of path-normalization, a simple RNN with the identity
initialization is able to achieve a 0% error on the sequences of length 750, whereas all the other
methods, including LSTMs, fail. This shows that Path-SGD may help stabilize the training and
alleviate the gradient problem, so as to perform well on longer sequence. We next tried to model
4
The original paper does not include any result for 750, so we implemented np-RNN for comparison.
However, in our implementation the np-RNN is not able to even learn sequences of length of 200. Thus we put
?>2? for length of 750.

7

Adding
100

Adding
400

Adding
750

sMNIST

IRNN [12]
uRNN [1]
LSTM [1]
np-RNN[22]

0
0
0
0

16.7
3
2
2

16.7
16.7
16.7
>2

5.0
4.9
1.8
3.1

IRNN
RNN-Path

0
0

0
0

16.7
0

7.1
3.1

Table 2: Test error (MSE) for the adding problem with
different input sequence lengths and test classification
error for the sequential MNIST.

PTB

text8

RNN+smoothReLU [20]
HF-MRNN [14]
RNN-ReLU[11]
RNN-tanh[11]
TRec,? = 500[11]

1.42
1.65
1.55
1.48

1.55
1.54
-

RNN-ReLU
RNN-tanh
RNN-Path
LSTM

1.55
1.58
1.47
1.41

1.65
1.70
1.58
1.52

Table 3: Test BPC for PTB and text8.

the sequences length of 1000, but we found that for such very long sequences RNNs, even with
Path-SGD, fail to learn.
Next, we evaluate Path-SGD on the Sequential MNIST problem. Table 2, right column, reports
test error rates achieved by RNN-Path compared to the previously published results. Clearly, using
Path-SGD helps RNNs achieve better generalization. In many cases, RNN-Path outperforms other
RNN methods (except for LSTMs), even for such a long-term dependency problem.
5.3

Language Modeling Tasks

In this section we evaluate Path-SGD on a language modeling task. We consider two datasets, Penn
Treebank (PTB-c) and text8 5 . PTB-c: We performed experiments on a tokenized Penn Treebank
Corpus, following the experimental protocol of [11]. The training, validations and test data contain
5017k, 393k and 442k characters respectively. The alphabet size is 50, and each training sequence is
of length 50. text8: The text8 dataset contains 100M characters from Wikipedia with an alphabet
size of 27. We follow the data partition of [14], where each training sequence has a length of 180.
Performance is evaluated using bits-per-character (BPC) metric, which is log2 of perplexity.
Similar to the experiments on the synthetic datasets, for both tasks, we train a single-layer RNN
consisting of 2048 hidden units with path-SGD (RNN-Path). Due to the large dimension of hidden
space, SGD can take a fairly long time to converge. Instead, we use Adam optimizer [8] to help speed
up the training, where we simply use the path-SGD gradient as input to the Adam optimizer.
We also train three additional baseline models: a ReLU RNN with 2048 hidden units, a tanh RNN
with 2048 hidden units, and an LSTM with 1024 hidden units, all trained using Adam. We performed
grid search for learning rate over {10?3 , 5 ? 10?4 , 10?4 } for all of our models. For ReLU RNNs,
we initialize the recurrent matrices from uniform[?0.01, 0.01], and uniform[?0.2, 0.2] for nonrecurrent weights. For LSTMs, we use orthogonal initialization [21] for the recurrent matrices and
uniform[?0.01, 0.01] for non-recurrent weights. The results are summarized in Table 3.
We also compare our results to an RNN that uses hidden activation regularizer [11] (TRec,? = 500),
Multiplicative RNNs trained by Hessian Free methods [14] (HF-MRNN), and an RNN with smooth
version of ReLU [20]. Table 3 shows that path-normalization is able to outperform RNN-ReLU and
RNN-tanh, while at the same time shortening the performance gap between plain RNN and other
more complicated models (e.g. LSTM by 57% on PTB and 54% on text8 datasets). This demonstrates
the efficacy of path-normalized optimization for training RNNs with ReLU activation.

6

Conclusion

We investigated the geometry of RNNs in a broader class of feedforward networks with shared
weights and showed how understanding the geometry can lead to significant improvements on
different learning tasks. Designing an optimization algorithm with a geometry that is well-suited
for RNNs, we closed over half of the performance gap between vanilla RNNs and LSTMs. This is
particularly useful for applications in which we seek compressed models with fast prediction time
that requires minimum storage; and also a step toward bridging the gap between LSTMs and RNNs.
Acknowledgments
This research was supported in part by NSF RI/AF grant 1302662, an Intel ICRI-CI award, ONR
Grant N000141310721, and ADeLAIDE grant FA8750-16C-0130-001. We thank Saizheng Zhang
for sharing a base code for RNNs.
5

http://mattmahoney.net/dc/textdata

8

References
[1] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. arXiv
preprint arXiv:1511.06464, 2015.
[2] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient
descent is difficult. Neural Networks, IEEE Transactions on, 5(2):157?166, 1994.
[3] Kyunghyun Cho, Bart Van Merri?nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder?decoder for statistical
machine translation. In Proceeding of the 2015 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1724?1734, 2014.
[4] Alex Graves and Navdeep Jaitly. Towards end-to-end speech recognition with recurrent neural networks.
In Proceeding of the International Conference on Machine Learning (ICML), pages 1764?1772, 2014.
[5] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with
pruning, trained quantization and huffman coding. In Proceeding of the International Conference on
Learning Representations, 2016.
[6] Sepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem
solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02), 1998.
[7] Sepp Hochreiter and J?rgen Schmidhuber. Long short-term memory. Neural computation, 9(8), 1997.
[8] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceeding of the
International Conference on Learning Representations, 2015.
[9] Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel. Unifying visual-semantic embeddings with
multimodal neural language models. Transactions of the Association for Computational Linguistics, 2015.
[10] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep convolutional
neural networks. In Advances in neural information processing systems (NIPS), pages 1097?1105, 2012.
[11] David Krueger and Roland Memisevic. Regularizing RNNs by stabilizing activations. In Proceeding of
the International Conference on Learning Representations, 2016.
[12] Quoc V Le, Navdeep Jaitly, and Geoffrey E Hinton. A simple way to initialize recurrent networks of
rectified linear units. arXiv preprint arXiv:1504.00941, 2015.
[13] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus
of english: The penn treebank. Computational linguistics, 19(2):313?330, 1993.
[14] Tom?? Mikolov, Ilya Sutskever, Anoop Deoras, Hai-Son Le, Stefan Kombrink, and J Cernocky. Subword
language modeling with neural networks. (http://www.fit.vutbr.cz/ imikolov/rnnlm/char.pdf), 2012.
[15] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In
Proceedings of the International Conference on Machine Learning (ICML), pages 807?814, 2010.
[16] Behnam Neyshabur, Ruslan Salakhutdinov, and Nathan Srebro. Path-SGD: Path-normalized optimization
in deep neural networks. In Advanced in Neural Information Processsing Systems (NIPS), 2015.
[17] Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro. Data-dependent path
normalization in neural networks. In the International Conference on Learning Representations, 2016.
[18] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks.
In Proceeding of the 28th Conference on Learning Theory (COLT), 2015.
[19] Yann Ollivier. Riemannian metrics for neural networks ii: recurrent networks and learning symbolic data
sequences. Information and Inference, page iav007, 2015.
[20] Marius Pachitariu and Maneesh Sahani. Regularization and nonlinearities for neural language models:
when are they needed? arXiv preprint arXiv:1301.5650, 2013.
[21] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of
learning in deep linear neural networks. In International Conference on Learning Representations, 2014.
[22] Sachin S. Talathi and Aniket Vartak. Improving performance of recurrent neural network with relu
nonlinearity. In the International Conference on Learning Representations workshop track, 2014.
[23] Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin, Roland Memisevic, Ruslan Salakhutdinov,
and Yoshua Bengio. Architectural complexity measures of recurrent neural networks. arXiv preprint
arXiv:1602.08210, 2016.

9

"
1995,A Smoothing Regularizer for Recurrent Neural Networks,,1119-a-smoothing-regularizer-for-recurrent-neural-networks.pdf,Abstract Missing,"A Smoothing Regularizer for Recurrent
Neural Networks
Lizhong Wu and John Moody
Oregon Graduate Institute, Computer Science Dept., Portland, OR 97291-1000

Abstract
We derive a smoothing regularizer for recurrent network models by
requiring robustness in prediction performance to perturbations of
the training data. The regularizer can be viewed as a generalization of the first order Tikhonov stabilizer to dynamic models. The
closed-form expression of the regularizer covers both time-lagged
and simultaneous recurrent nets, with feedforward nets and onelayer linear nets as special cases. We have successfully tested this
regularizer in a number of case studies and found that it performs
better than standard quadratic weight decay.

1

Introd uction

One technique for preventing a neural network from overfitting noisy data is to add
a regularizer to the error function being minimized. Regularizers typically smooth
the fit to noisy data. Well-established techniques include ridge regression, see (Hoerl & Kennard 1970), and more generally spline smoothing functions or Tikhonov
stabilizers that penalize the mth-order squared derivatives of the function being fit,
as in (Tikhonov & Arsenin 1977), (Eubank 1988), (Hastie & Tibshirani 1990) and
(Wahba 1990). Thes( -ilethods have recently been extended to networks of radial
basis functions (Girosi, Jones & Poggio 1995), and several heuristic approaches have
been developed for sigmoidal neural networks, for example, quadratic weight decay
(Plaut, Nowlan & Hinton 1986), weight elimination (Scalettar & Zee 1988),(Chauvin 1990),(Weigend, Rumelhart & Huberman 1990) and soft weight sharing (Nowlan
& Hinton 1992). 1 All previous studies on regularization have concentrated on feedforward neural networks. To our knowledge, recurrent learning with regularization
has not been reported before.
ITwo additional papers related to ours, but dealing only with feed forward networks,
came to our attention or were written after our work was completed. These are (Bishop
1995) and (Leen 1995). Also, Moody & Rognvaldsson (1995) have recently proposed
several new classes of smoothing regularizers for feed forward nets.

459

A Smoothing Regularizer for Recurrent Neural Networks

In Section 2 of this paper, we develop a smoothing regularizer for general dynamic
models which is derived by considering perturbations of the training data. We
present a closed-form expression for our regularizer for two layer feedforward and
recurrent neural networks, with standard weight decay being a special case. In
Section 3, we evaluate our regularizer's performance on predicting the U.S. Index
of Industrial Production. The advantage of our regularizer is demonstrated by
comparing to standard weight decay in both feedforward and recurrent modeling.
Finally, we conclude our paper in Section 4.

2
2.1

Smoothing Regularization
Prediction Error for Perturbed Data Sets

Consider a training data set {P: Z(t),X(t)}, where the targets Z(t) are assumed to
be generated by an unknown dynamical system F*(I(t)) and an unobserved noise
process:
Z(t) = F*(I(t? + E*(t) with I(t) = {X(s), s = 1,2,???, t} .
(1)
Here, I(t) is, the information set containing both current and past inputs X(s), and
the E*(t) are independent random noise variables with zero mean and variance (F*2.
Consider next a dynamic network model Z(t) = F(~, I(t)) to be trained on data set
P, where ~ represents a set of network parameters, and F( ) is a network transfer
function which is assumed to be nonlinear and dynamic. We assume that F( ) has
good approximation capabilities, such that F(~p,I(t)) ~ F*(I(t)) for learnable
parameters ~ p.
Our goal is to derive a smoothing regularizer for a network trained on the actual
data set P that in effect optimizes the expected network performance (prediction
risk) on perturbed test data sets of form {Q : Z(t),X(t)}. The elements of Q are
related to the elements of P via small random perturbations Ez(t) and Ez(t), so that

Z(t) = Z(t) + Ez(t) ,
(2)
X(t) = X(t) + Ez(t) .
(3)
The Ez(t) and Ez(t) have zero mean and variances (Fz2 and (Fz2 respectively. The
training and test errors for the data sets P and Q are
N

Dp =

~ L [Z(t) - F(~p,I(t))]2

(4)

t=l

N

DQ =

~ L[Z(t) - F(~p,i(t)W

,

(5)

t=l

~p

denotes the network parameters obtained by training on data set P, and
l(t)
{X(s),s = 1,2,??? ,t} is the perturbed information set of Q. With this
notation, our goal is to minimize the expected value of DQ, while training on D p.

where

=

Consider the prediction error for the perturbed data point at time t:
d(t) = [Z(t) - F(~p,i(t)W .
With Eqn (2), we obtain

d(t)

=

-

[Z(t) + Ez(t) - F(~p,I(t)) + F(~p,I(t)) - F(~p,i(t)W,
[Z(t) - F(~p,I(t)W + [F(~p,I(t)) - F(~p,l(t)W + [Ez(t)]2
+2[Z(t) - F(~p,I(t))JIF(~p,I(t)) - F(~p,i(t))]
+2Ez(t)lZ(t) - F(~p,l(t))].

(6)

(7)

L. WU. 1. MOODY

460

Assuming that C:z(t) is uncorrelated with [Z(t) - F(~p,i(t?] and averaging over
the exemplars of data sets P and Q, Eqn(7) becomes
DQ

=

1

1

N

N

Dp+ NL[F(~p,I(t?-F(~p,i(t)W+ NL[c: z(t)]2
t=1

t=1

2 N

+ N L[Z(t) - F(~p,I(t?)][F(~p,I(t? - F(~p,i(t?].

(8)

t=l

The third term, 2:::'1 [C: z (t)]2, in Eqn(8) is independent of the weights, so it can
be neglected during the learning process. The fourth term in Eqn(8) is the crosscovariance between [Z~t) - F(~p,I(t?] and [F(~p,I(t? - F(~p,i(t?]. Using
the inequality 2ab ~ a + b2 , we can see that minimizing the first term D p and
the second term ~ 2:~I[F(~p,I(t? - F(~p,i(t?]2 in Eqn (8) during training
will automatically decrease the effect of the cross-covariance term. Therefore, we
exclude the cross-covariance term from the training criterion.
The above analysis shows that the expected test error DQ can be minimized by
minimizing the objective function D:
1 N

D

= N L[Z(t) -

F(~, I(t?]2

1 N
L[F(~p, I(t? - F(~ p,i(t?]2.

+N

t=l

(9)

t=l

In Eqn (9), the second term is the time average of the squared disturbance

IIZ(t) - Z(t)1I2 of the trained network output due to the input perturbation
lIi(t) - I(t)W. Minimizing this term demands that small changes in the input
variables yield correspondingly small changes in the output. This is the standard
smoothness prior, nanlely that if nothing else is known about the function to be
approximated, a good option is to assume a high degree of smoothness. Without
knowing the correct functional form of the dynamical system F- or using such prior
assumptions, the data fitting problem is ill-posed. In (Wu & Moody 1996), we have
shown that the second term in Eqn (9) is a dynamic generalization of the first order
Tikhonov stabilizer.
2.2

Form of the Proposed Smoothing Regularizer

Consider a general, two layer, nonlinear, dynamic network with recurrent connections on the internal layer 2 as described by

Yet)

= f (WY(t -

T)

+ V X(t? ,Z(t) = UY(t)

(10)

where X(t), Yet) and Z(t) are respectively the network input vector, the hidden
output vector and the network output; ~ = {U, V, W} is the output, input and
recurrent connections of the network; f( ) is the vector-valued nonlinear transfer
function of the hidden units; and T is a time delay in the feedback connections of
hidden layer which is pre-defined by a user and will not be changed during learning.
T can be zero, a fraction, or an integer, but we are interested in the cases with a
small T.3
20 ur derivation can easily be extended to other network structures.
3When the time delay T exceeds some critical value, a recurrent network becomes
unstable and lies in oscillatory modes. See, for example, (Marcus & Westervelt 1989).

461

A Smoothing Regularizer for Recurrent Neural Networks

When T = 1, our model is a recurrent network as described by (Elman 1990) and
(Rumelhart, Hinton & Williams 1986) (see Figure 17 on page 355). When T is equal
to some fraction smaller than one, the network evolves ~ times within each input
time interval. When T decreases and approaches zero, our model is the same as the
network studied by (Pineda 1989), and earlier, widely-studied additive networks. In
(Pineda 1989), T was referred to as the network relaxation time scale. (Werbos 1992)
distinguished the recurrent networks with zero T and non-zero T by calling them
simultaneous recurrent networks and time-lagged recurrent networks respectively.
We have found that minimizing the second term of Eqn(9) can be obtained by
smoothing the output response to an input perturbation at every time step. This
yields, see (Wu & Moody 1996):

IIZ(t)-Z(t)W~p/(~p)IIX(t)-X(t)W for t=1,2, ... ,N.

(11)

We call PT 2 (~ p) the output sensitivity of the trained network ~ p to an input perturbation. PT 2 ( ~ p) is determined by the network parameters only and is independent
of the time variable t.
We obtain our new regularizer by training directly on the expected prediction error
for perturbed data sets Q. Based on the analysis leading to Eqns (9) and (11), the
training criterion thus becomes
1 N
D = N 2:[Z(t) - F(~,I(t)W

+ .\p/(~)

.

(12)

t=l

The coefficient .\ in Eqn(12) is a regularization parameter that measures the degree
of input perturbation lIi(t) - I(t)W. The algebraic form for PT(~) as derived in
(Wu & Moody 1996) is:

,IIUIIIIVII
PT ( ~)1 _ ,IIWII

{1-

exp

(,IIWTIl

-l)}

'

(13)

for time-lagged recurrent networks (T > 0). Here, 1111 denotes the Euclidean matrix
norm. The factor, depends upon the maximal value of the first derivatives of the
activation functions of the hidden units and is given by:

, = m~ II/(oj(t))
t ,]

I,

(14)

where j is the index of hidden units and OJ(t) is the input to the ph unit. In general,
, ~ 1. 4 To insure stability and that the effects of small input perturbations are
damped out, it is required, see (Wu & Moody 1996), that

,IIWII < 1

.

(15)

The regularizer Eqn(13) can be deduced for the simultaneous recurrent networks in
the limit THO by:

p( ~) -= P0 (~) = ,IIUIIIIVII
1 - ,IIWII .
If the network is feedforward, W

(16)

= 0 and T = 0, Eqns (13) and (16) become

p(~) =

,11U1I11V1l .

(17)

Moreover, if there is no hidden layer and the inputs are directly connected to the
outputs via U, the network is an ordinary linear model, and we obtain
p(~)

=

IIUII ,

4For instance, f'(x} = [1- f(x})f(x} if f(x)

= l+!-z.

(18)
Then, ""'{

= max 1f'(x}} 1= t.

462

L. WU, J. MOODY

which is standard quadratic weight decay (Plaut et al. 1986) as is used in ridge
regression (Hoerl & Kennard 1970).
The regularizer (Eqn(17) for feedforward networks and Eqn (13) for recurrent networks) was obtained by requiring smoothness of the network output to perturbations
of data. We therefore refer to it as a smoothing regularizer. Several approaches can
be applied to estimate the regularization parameter..x, as in (Eubank 1988), (Hastie
& Tibshirani 1990) and (Wahba 1990). We will not discuss this subject in this
paper.
In the next section, we evaluate the new regularizer for the task of predicting the
U.S. Index of Industrial Production. Additional empirical tests can be found in
(Wu & Moody 1996).

3

Predicting the U.S. Index of Industrial Production

The Index of Industrial Production (IP) is one of the key measures of economic
activity. It is computed and published monthly. Our task is to predict the onemonth rate of change of the index from January 1980 to December 1989 for models
trained from January 1950 to December 1979. The exogenous inputs we have used
include 8 time series such as the index of leading indicators, housing starts, the
money supply M2, the S&P 500 Index. These 8 series are also recorded monthly.
In previous studies by (Moody, Levin & Rehfuss 1993), with the same defined
training and test data sets, the normalized prediction errors of the one month rate
of change were 0.81 with the neuz neural network simulator, and 0.75 with the
proj neural network simulator.
We have simulated feedforward and recurrent neural network models. Both models
consist of two layers. There are 9 input units in the recurrent model, which receive the 8 exogenous series and the previous month IP index change. We set the
time-delayed length in the recurrent connections T = 1. The feedforward model is
constructed with 36 input units, which receive 4 time-delayed versions of each input
series. The time-delay lengths a,re 1, 3, 6 and 12, respectively. The activation functions of hidden units in both feedforward and recurrent models are tanh functions.
The number of hidden units varies from 2 to 6. Each model has one linear output
unit.
We have divided the data from January 1950 to December 1979 into four nonoverlapping sub-sets. One sub-set consists of 70% of the original data and each of
the other three subsets consists of 10% of the original data. The larger sub-set is
used as training data and the three smaller sub-sets are used as validation data.
These three validation data sets are respectively used for determination of early
stopped training, selecting the regularization parameter and selecting the number
of hidden units.
We have formed 10 random training-validation partitions. For each trainingvalidation partition, three networks with different initial weight parameters are
trained. Therefore, our prediction committee is formed by 30 networks.
The committee error is the average of the errors of all committee members. All
networks in the committee are trained simultaneously and stopped at the same
time based on the committee error of a validation set. The value of the regularization parameter and the number of hidden units are determined by minimizing the
committee error on separate validation sets.
Table 1 compares the out-of-sample performance of recurrent networks and feedfor-

463

A Smoothing Regularizer for Recurrent Neural Networks

Table 1: Nonnalized prediction errors for the one-month rate of return on the U.S.
Index of Industrial Production (Jan. 1980 - Dec. 1989). Each result is based on 30
networks.
Model

Regularizer

Mean ? Std

Median

Max

Min

Committee

Recurrent
Networks

Smoothing
Weight Decay

0.646?0.008
0.734?0.018

0.647
0.737

0.657
0.767

0.632
0.704

0.639
0.734

Feedforward
Networks

Smoothing
Weight Decay

0.700?0.023
0.745?0.043

0.707
0.748

0.729
0.805

0.654
0.676

0.693
0.731

ward networks trained with our smoothing regularizer to that of networks trained
with standard weight decay. The results are based on 30 networks. As shown, the
smoothing regularizer again outperfonns standard weight decay with 95% confidence (in t-distribution hypothesis) in both cases of recurrent networks and feedforward networks. We also list the median, maximal and minimal prediction errors
over 30 predictors. The last column gives the committee results, which are based on
the simple average of 30 network predictions. We see that the median, maximal and
minimal values and the committee results obtained with the smoothing regularizer
are all smaller than those obtained with standard weight decay, in both recurrent
and feedforward network models.

4

Concluding Remarks

Regularization in learning can prevent a network from overtraining. Several techniques have been developed in recent years, but all these are specialized for feedforward networks. To our best knowledge, a regularizer for a recurrent network has
not been reported previously.
We have developed a smoothing regularizer for recurrent neural networks that captures the dependencies of input, output, and feedback weight values on each other.
The regularizer covers both simultaneous and time-lagged recurrent networks, with
feedforward networks and single layer, linear networks as special cases. Our smoothing regularizer for linear networks has the same fonn as standard weight decay. The
regularizer developed depends on only the network parameters, and can easily be
used. A more detailed description of this work appears in (Wu & Moody 1996).

References
Bishop, C. (1995), 'Training with noise is equivalent to Tikhonov regularization',
Neural Computation 7(1), 108-116.
Chauvin, Y. (1990), Dynamic behavior of constrained back-propagation networks,
in D. Touretzky, ed., 'Advances in Neural Infonnation Processing Systems 2',
Morgan Kaufmann Publishers, San Francisco, CA, pp. 642-649.
Elman, J. (1990), 'Finding structure in time', Cognition Science 14, 179-211.
Eubank, R. L. (1988), Spline Smoothing and Nonparametric Regression, Marcel
Dekker, Inc.
Girosi, F., Jones, M. & Poggio, T. (1995), 'Regularization theory and neural networks architectures', Neural Computation 7, 219-269.

464

L. WU, J. MOODY

Hastie, T. J. & Tibshirani, R. J. (1990), Generalized Additive Models, Vol. 43 of
Monographs on Statistics and Applied Probability, Chapman and Hall.
Hoerl, A. & Kennard, R. (1970), 'Ridge regression: biased estimation for nonorthogonal problems', Technometrics 12, 55-67.
Leen, T. (1995), 'From data distributions to regularization in invariant learning',
Neural Computation 7(5), 974-98l.
Marcus, C. & Westervelt, R. (1989), Dynamics of analog neural networks with
time delay, in D. Touretzky, ed., 'Advances in Neural Information Processing
Systems 1', Morgan Kaufmann Publishers, San Francisco, CA.
Moody, J. & Rognvaldsson, T. (1995), Smoothing regularizers for feed-forward neural networks, Oregon Graduate Institute Computer Science Dept. Technical
Report, submitted for publication, 1995.
Moody, J., Levin, U. & Rehfuss, S. (1993), 'Predicting the U.S. index of industrial production', In proceedings of the 1993 Parallel Applications in Statistics
and Economics Conference, Zeist, The Netherlands. Special issue of Neural
Network World 3(6), 791-794.
Nowlan, S. & Hinton, G. (1992), 'Simplifying neural networks by soft weightsharing', Neural Computation 4(4), 473-493.
Pineda, F. (1989), 'Recurrent backpropagation and the dynamical approach to
adaptive neural computation', Neural Computation 1(2), 161-172.
Plaut, D., Nowlan, S. & Hinton, G. (1986), Experiments on learning by back propagation, Technical Report CMU-CS-86-126, Carnegie-Mellon University.
Rumelhart, D., Hinton, G. & Williams, R. (1986), Learning internal representations by error propagation, in D. Rumelhart & J. McClelland, eds, 'Parallel
Distributed Processing: Exploration in the microstructure of cognition', MIT
Press, Cambridge, MA, chapter 8, pp. 319-362.
Scalettar, R. & Zee, A. (1988), Emergence of grandmother memory in feed forward
networks: learning with noise and forgetfulness, in D. Waltz & J. Feldman,
eds, 'Connectionist Models and Their Implications: Readings from Cognitive
Science', Ablex Pub. Corp.
Tikhonov, A. N. & Arsenin, V. 1. (1977), Solutions of Ill-posed Problems, Winston;
New York: distributed solely by Halsted Press. Scripta series in mathematics.
Translation editor, Fritz John.
Wahba, G. (1990), Spline models for observational data, CBMS-NSF Regional Conference Series in Applied Mathematics.
Weigend, A., Rumelhart, D. & Huberman, B. (1990), Back-propagation, weightelimination and time series prediction, in T. Sejnowski, G. Hinton & D. Touretzky, eds, 'Proceedings of the connectionist models summer school', Morgan
Kaufmann Publishers, San Mateo, CA, pp. 105-116.
Werbos, P. (1992), Neurocontrol and supervised learning: An overview and evaluation, in D. White & D. Sofge, eds, 'Handbook of Intelligent Control', Van
Nostrand Reinhold, New York.
Wu, L. & Moody, J. (1996), 'A smoothing regularizer for feedforward and recurrent
neural networks', Neural Computation 8(3), 463-491.

"
1992,Stimulus Encoding by Multidimensional Receptive Fields in Single Cells and Cell Populations in V1 of Awake Monkey,,639-stimulus-encoding-by-multidimensional-receptive-fields-in-single-cells-and-cell-populations-in-v1-of-awake-monkey.pdf,Abstract Missing,"STIMULUS ENCODING BY
MULTIDIMENSIONAL RECEPTIVE FIELDS
IN SINGLE CELLS AND CELL POPULATIONS
IN VI OF AWAKE MONKEY
Edward Stern
Center for Neural Computation
and Department of Neurobiology
Life Sciences Institute
Hebrew University
Jerusalem, Israel

Eilon Vaadia
Center for Neural Computation
and Physiology Department
Hadassah Medical School
Hebrew University
Jerusalem, Israel

Ad Aertsen
Institut fur Neuroinfonnatik
Ruhr-Universitat-Bochum
Bochum, Gennany

Shaul Hochstein
Center for Neural Computation
and Department of Neurobiology,
Life Sciences Institute
Hebrew University
Jerusalem, Israel

ABSTRACT
Multiple single neuron responses were recorded
from a single electrode in VI of alert, behaving
monkeys. Drifting sinusoidal gratings were
presented in the cells' overlapping receptive
fields, and the stimulus was varied along several
visual dimensions. The degree of dimensional
separability was calculated for a large population
of neurons, and found to be a continuum. Several
cells showed different temporal response
dependencies to variation of different stimulus
dimensions, i.e. the tuning of the modulated
firing was not necessarily the same as that of the
mean firing rate. We describe a multidimensional
receptive field, and use simultaneously recorded
responses to compute a multi-neuron receptive
field, describing the information processing
capabilities of a group of cells. Using dynamic
correlation analysis, we propose several
computational schemes for multidimensional
spatiotemporal tuning for groups of cells. The
implications for neuronal coding of stimuli are
discussed.
377

378

Stern, Aensen, Vaadia, and Hochstein

INTRODUCTION
The receptive field is perhaps the most useful concept for understanding neuronal
information processing. The ideal definition of the receptive field is that set of stimuli
which cause a change in the neuron's firing properties. However, as with many such
concepts, the use of the receptive field in describing the behavior of sensory neurons falls
short of the ideal. The classical method for describing the receptive field has been to
measure the ""tuning curve"" i.e. the response of the neuron as a function of the value of
one dimension of the stimulus. This presents a problem because the sensory world is
multidimensional; For example, even a simple visual stimulus, such as a patch of a
sinusoidal grating, may vary in location, orientation, spatial frequency, temporal
frequency, movement direction and speed, phase, contrast, color, etc. Does the tuning to
one dimension remain constant when other dimensions are varied? i.e. are the dimensions
linearly separable? It is not unreasonable to expect inseparability: Consider an oriented,
spatially discrete receptive field. The excitation generated by passing a bar through the
receptive field will of course change with orientation. However, the shape of this tuning
curve will depend upon the bar width, related to the spatial frequency. This effect has not
been studied quantitatively, however. If interactions among dimensions exist, do they
account for a large portion of the cell's response variance? Are there discrete populations
of cells, with some cells showing interactions among dimensions and others not? These
question have clear implications for the problem of neural coding.
Related to the question of dimensional separability is that of stimulus encoding: Given
that the receptive field is multidimensional in nature, how can the cell maximize the
amount of stimulus information it encodes? Does the neuron use a single code to
represent all the stimulus dimensions? It is possible that interactions lead to greater
uncertainty in stimulus identification. Does the small number of visual cortical cells
encode all the possible combinations of stimuli using only spike rate as the dependent
variable? We present data indicating that more information is indeed present in the
neuronal response, and propose a new approach for its utilization.
The final problem that we address is the following: Clearly, many cells participate in the
stimulus encoding process. Arriving at a valid concept of a multidimensional receptive
field, can we generalize this concept to more than one cell introducing the notion of a
multi-cellular receptive field?

METHODS
Drifting sinusoidal gratings were presented for 500 msec to the central 10 degrees of the
visual field of monkeys performing a fixation task. The gratings were varied in
orientation, spatial frequency,temporal frequency, and movement direction. We recorded
from up to 3 cells simultaneously with a single electrode in the monkey's primary visual
cortex (VI). The cells described in this study were well separated, using a templatematching procedure. The responses of the neurons were plotted as Peri-Stimulus Time
Histograms (PSTHs) and their parameters quantified (Abeles, 1982), and offline Fourier
analysis and time-dependent crosscorrelation analysis (Aertsen et ai, 1989) were
performed.

Stimulus Encoding by Multidimensional Receptive Fields in Single Cells and Cell Populations

RESULTS
Recording the responses of visual cortical neurons to stimuli varied over a number of
dimensions, we found that in some cases, the tuning curve to one dimension depended on
the value of another dimension. Figure lA shows the spatial-frequency tuning curve of a
single cell measured at 2 different stimulus orientations. When the orientation of the
stimulus is 72 degrees, the peak response is at a spatial frequency of 4.5 cycles/degree
(cpd), while at an orientation of216 degrees, the spatial frequency of peak response is 2.3
cpd. If the responses to different visual dimensions were truly linearly separable, the
tuning curve to any single dimension would have the same shape and, in particular,
position of peak, despite any variations in other dimensions. If the tuning curves are not
parallel, then interactions must exist between dimensions. Clearly, this is an example of
a cell whose responses are not linearly separable. In order to quantify the inseparability
phenomenon, analyses of variance were performed, using spike rate as the dependent
variable, and the visual dimensions of the stimuli as the independent variables. We then
measured the amount of interaction as a percentage of the total between-conditions

A.

Spatial

B. Interaction effects between

Frequency

stimulus dimensions:

Tuning Dependence upon
Orientation

Percentage or total variance

30'.....-------------.
<1)

0.9

~

0.8

25

bO O.7

s::

'C 0.6

u::
""0 0 .5
~ 0.4

E0.3
a

0.2

s::

0.1

O~--~~~~n---P-~~~
0.1
1
10

!?1I11~
1I11~
II
oooooo~
_
N
f""""I
\I""')
110

--- --of non-residual variance
~

,

Spatial Frequency (cpd)

I

_

%

'

N

1

,

1""""'11

""'""

,

It/')

__ ORl=72 - 6 - ORl=216

nfac=34

nfac=45

Figure 1: Dimensional Inseparability or Visual Cortical Neurons. A:
An example or dimensionsional inseparability in the response or a single
cell; B: Histogram or dimensional inseparability as a percentage or
total response variance.

379

380

Stern, Aertsen, Vaadia, and Hochstein
variance divided by the residuals. The resulting histogram for 69 cells is shown in Figure
lB. Although there are several cells with non-significant interactions, i.e. linearly
separable dimensions, this is not the majority of cells. The amount of dimensional
inseparability seems to be a continuum. We suggest that separability is a significant
variable in the coding capability of the neurons, which must be taken into account when
modeling the representation of sensory information by cortical neural networks.
We found that the time course of the response was not always constant, but varied with
stimulus parameters. Cortical cell responses may have components which are sustained
(constant over time), transient (with a peak near stimulus onset and/or offset), or
modulated (varying with the stimulus period). For example, Figure 2 shows the responses
of a single neuron in VI to 50 stimuli, varying in orientation and spatial frequency. Each
response is plotted as a PSTH, and the stippled bar under the PSTH indicates the time of

Orientation

DD~DD

4.5

-

D

[:j Eaij tJ D

D~EJijjG5D
23DG!5~GjD
D~[MjE5E:5
G:J [;J CiIIJ ~ c:::J
Ej~~~E:::J
liM ! f.ll !!u ? D.I

11m.,

f Iq lip ? B.'

p.i

!

Ill.'

f 1.'1

1.5

....t

I 1.1 I Ft.2

! U .. t

u .s lI!.t !

I.t

J

I!.' !

i.4

J

O.8D~~~D

DG5~tJD

04oEJt5DCj
II ?'

!

U

I flU t f.4 I 117.1 ? f .1 I 11'"" ! 1.0 I p.i

: u!

Figure 2: Spatial Frequency/Orientation Tuning of Responses
of VI Cell

Stimulus Encoding by Multidimensional Receptive Fields in Single Cells and Cell Populations
the stimulus presentation (500 msec). The numbers beneath each PSTH are the firing rate
averaged over the response time. and the standard deviations of the response over
repetitions of the stimulus (in this case 40). Clearly. the cell is orientation selective, and
the neuronal response is also tuned to spatial frequency. The stimulus eliciting the
highest firing rate is ORI=252 degrees; SF=3.2 cycles/degree (cpd). However, when
looking at the responses to lower spatial frequencies, we see a modulation in the PSTH.
The modulation, when present, has 2 peaks, corresponding to the temporal frequency of
the stimulus grating (4 cycles/second). Therefore, although the response rate of the cell is
lower at low spatial frequencies than for other stimuli, the spike train carries additional
information about another stimulus dimension.
If the visual neuron is considered as a linear system, the predicted response to a drifting
sinusoidal grating would be a (rectified) sinusoid of the same (temporal) frequency as that
of the stimulus, i.e. a modulated response (Enroth-Cugell & Robson, 1966; Hochstein &
Shapley, 1976; Spitzer & Hochstein. 1988). However, as seen in Figure 2, in some
stimulus regimes the cell's response deviates from linearity. We conclude that the
linearity or nonlinearity of the response is dependent upon the stimulus conditions
(Spitzer & Hochstein, 1985). A modulated response is one that would be expected from
simple cells, while the sustained response seen at higher spatial frequencies is that
expected from complex cells. Our data therefore suggest that the simple/complex cell
categorization is not complete.
A further example of response time-course dependence on stimulus parameters is seen in
Figure 3A. In this case, the stimulus was varied in spatial frequency and temporal
frequency, while other dimensions were held constant. Again, as spatial frequency is
raised. the modulation of the PSTH gives way to a more sustained response. Funhennore,
as temporal frequency is raised. both the sustained and the modulated responses are
replaced by a single transient response. When present, the frequency of the modulation
follows that of the temporal frequency of the stimulus. Fourier analysis of the response
histograms (Figure 3B) reveals that the DC and fundamental component (FC) are not
tuned to the same stimulus values (arrows indicating peaks). We propose that this
information may be available to the cell readout, enabling the single cell to encode
multiple stimulus dimensions simultaneously.
Thus, a complete description of the receptive field must be multidimensional in nature.
Furthermore, in light of the evidence that the spike train is not constant, one of the
dimensions which must be used to display the receptive field must be time.
Figure 4 shows one method of displaying a multidimensional response map, with time
along the abscissa (in 10 msec bins) and orientation along the ordinate. In the top two
figures, the z axis, represented in gray-scale, is the number of counts (spikes) per bin.
Therefore, each line is a PSTH, with counts (bin height) coded by shading. In this
example. cell 2 (upper picture) is tuned to orientation, with peaks at 90 and 270 degrees.
The cell is only slightly direction selective, as represented by the fact that the 2 areas of
high activity are similarly shaded. However, there is a transient peak at270 degrees which

381

382

Stern, Aertsen, Vaadia, and Hochstein

A.

Spatial Frequency (cpd)
0.6

lb

0.11

1.5

1.1

2.1

DG:J~G:J~
!
p..
III""
111.1
,.0

4.21

!'"" 1

! I.e 1 p.l ! I.S 1

! 1.11

RCJ~~~~
1.51 pi., m.s
lIZ.'
Fi.S
I
~

/II.! ??.?

!

"" .4 :

! n.I

! is.1

C:W~~CitJ~
!

III""! ""'I ~i.I !
2

tl.S

pu ! 14.1 p.1

IU "".S ! 14.1

UJ~CWJ[MJ~
!
!
IA.'

pi ?? ? D.C [fT.'

1M FO.I

IS! tl.O ! 1 .1

! 1M

B.
nonnalized values

1

o

1

2
16

TF (cycles/second)

0.6

0.8

1.1

SF (cpd)

Figure 3: A. TF/SF Tuning of response of VI cell.
B. Tuning of DC and FC of response to stimulus parameters.
is absent at 90 degrees. The middle picture. representing a simultaneously recorded cell
shows a different pattern of activity. The orientation tuning of this cell is similar to that
of cell 2, but it has slIonger directional selectivity. (towards 90 degrees). In this case, the
lIansient is also at 90 degrees. The bottom picture shows the joint activity of these 2
cells. Rather than each line being a PSTH, each line is a Joint PSTH (JPSTH; Aertsen et
al. 1989). This histogram represents the time-dependent correlated activity of a pair of
cells. It is equivalent to sliding a window across a spike lIain of one neuron and

Stimulus Encoding by Multidimensional Receptive Fields in Single Cells and Cell Populations

324

cell 2

counts/bin

250
200
150
\00
50

216
108

o

o
cell 3

-

324

~

216

I II

t

Q,I

:s

-

.?

108

S

I:

.~

...=>

0
cell 2,3 coincidence

324

60

216

20

108

o

50
40

30
10

o
o

250

500

time (msec)

750

1000

SF=4.S cpd

Figure 4: Response Maps.
Top, Middle: Single-cell Multidimensional Receptive Fields;
Bottom: Multi-Cell Multidimensional Receptive Field
asking when a spike from another neuron falls within the window. The size of the
window can be varied; here we used 2 msec. Therefore, we are asking when these cells
fire within 2 msec of each other, and how this is connected to the stimulus. The z axis is
now coincidences per bin. We may consider this the logical AND activity of these cells;
if there is a cell receiving infonnation from both of these neurons, this is the receptive
field which would describe its input. Clearly. it is different from the each of the 2
individual cells. In our results. it is more narrowly tuned. and the tuning can not be
predicted from the individual components. We emphasize that this is the ""raw"" JPSTH.
which is not corrected for stimulus effects. common input. or normalized. This is because
we want a measure comparable to the PSTHs themselves, to compare a multi-unit

383

384

Stern, Aertsen, Vaadia, and Hochstein
receptive field to its single unit components. In this case, however, a significant (p<O.01;
Palm et ai, 1988) ""mono-directional"" interaction is present. For a more complete
description of the receptive field, this type of figure, shown here for one spatial frequency
only, can be shown for all spatial frequencies as ""slices"" along a fourth axis. However,
space limitations prevent us from presenting this multidimensional aspect of the
multicellular receptive field.

CONCLUSIONS
We have shown that interactions among stimulus dimensions account for a significant
proponion of the response variance of V 1 cells. The variance of the interactions itself
may be a useful parameter when considering a population response, as the amount and
location of the dimensional inseparability varies among cells. We have also shown that
different temporal characteristics of the spike trains can be tuned to different dimensions,
and add to the encoding capabilities of the cell in a neurobioiogically realistic manner.
Finally, we use these results to generate multidimensional receptive fields, for single cells
and small groups of cells. We emphasize that this can be generalized to larger populations
of cells, and to compute the population responses of cells that may be meaningful for the
cone x as a biological neuronal network.

Acknowledgements
We thank Israel Nelken, Hagai Bergman, Volodya Yakovlev, Moshe Abeles, Peter
Hillman, Roben Shapley and Valentino Braitenberg for helpful discussions. This study
was supponed by grants from the U.S.-Israel Bi-National Science Foundation (BSF) and
the Israel Academy of Sciences.

References
1. Abeles, M. Quantification, Smoothing, and Confidence Limits for Single Units'
Histograms 1. Neurosci . Melhods 5 ,317-325,1982.

2. Aertsen, A.M .H.J., Gerstein, G. L., Habib, M.K., and Palm, G. Dynamics of
Neuronal Firing Correlation: Modulation of ""Effective Connectivity"" 1. Neurophysio151
(5),900-917, 1989.
3. Enroth-CugeU, C. and Robson, J.G. The Contrast Sensitivity of Retinal Ganglion
Cells of the Call Physiol. Lond 187, 517-552,1966.
4. Hochstein, S. and Shapley, R. M. Linear and Nonlinear Spatial Subunits in Y Cat
Retinal Ganglion Cells 1 Physiol. Lond 262, 265-284, 1976.
5. Palm, G ., Aensen, A.M.H.J. and Gerstein, G.L. On the Significance of Correlations
Among Neuronal Spike Trains Bioi. Cybern. 59, 1-11, 1988.
6. Spitzer, H. and Hochstein , S. Simple and Complex-Cell Response Dependencies on
Stimulation Parameters 1.Neurophysiol 53,1244-1265,1985.
7. Spitzer, H. and Hochstein, S. Complex Cell Receptive Field Models Prog. in
Neurobiology, 31 ,285-309, 1988.

"
2012,Fiedler Random Fields: A Large-Scale Spectral Approach to Statistical Network Modeling,,4607-fiedler-random-fields-a-large-scale-spectral-approach-to-statistical-network-modeling.pdf,"Statistical models for networks have been typically committed to strong prior assumptions concerning the form of the modeled distributions. Moreover, the vast majority of currently available models are explicitly designed for capturing some specific graph properties (such as power-law degree distributions), which makes them unsuitable for application to domains where the behavior of the target quantities is not known a priori. The key contribution of this paper is twofold. First, we introduce the Fiedler delta statistic, based on the Laplacian spectrum of graphs, which allows to dispense with any parametric assumption concerning the modeled network properties. Second, we use the defined statistic to develop the Fiedler random field model, which allows for efficient estimation of edge distributions over large-scale random networks. After analyzing the dependence structure involved in Fiedler random fields, we estimate them over several real-world networks, showing that they achieve a much higher modeling accuracy than other well-known statistical approaches.","Fiedler Random Fields: A Large-Scale Spectral
Approach to Statistical Network Modeling

Mikaela Keller?
Marc Tommasi?
INRIA Lille ? Nord Europe
40 avenue Halley ? B?at A ? Park Plaza
59650 Villeneuve d?Ascq (France)
{antonino.freno, mikaela.keller, marc.tommasi}@inria.fr
Antonino Freno

Abstract
Statistical models for networks have been typically committed to strong prior assumptions concerning the form of the modeled distributions. Moreover, the vast
majority of currently available models are explicitly designed for capturing some
specific graph properties (such as power-law degree distributions), which makes
them unsuitable for application to domains where the behavior of the target quantities is not known a priori. The key contribution of this paper is twofold. First,
we introduce the Fiedler delta statistic, based on the Laplacian spectrum of graphs,
which allows to dispense with any parametric assumption concerning the modeled
network properties. Second, we use the defined statistic to develop the Fiedler
random field model, which allows for efficient estimation of edge distributions
over large-scale random networks. After analyzing the dependence structure involved in Fiedler random fields, we estimate them over several real-world networks, showing that they achieve a much higher modeling accuracy than other
well-known statistical approaches.

1 Introduction
Arising from domains as diverse as bioinformatics and web mining, large-scale data exhibiting network structure are becoming increasingly available. Network models are commonly used to represent the relations among data units and their structural interactions. Recent studies, especially
targeted at social network modeling, have focused on random graph models of those networks. In
the simplest form, a random network is a configuration of binary random variables Xuv such that
the value of Xuv stands for the presence or absence of a link between nodes u and v in the network.
The general idea underlying random graph modeling is that network configurations are generated
by a stochastic process governed by specific probability laws, so that different models correspond to
different families of distributions over graphs.
The simplest random graph model is the Erd?os-R?enyi (ER) model [1], which assumes that the probability of observing a link between two nodes in a given graph is constant for any pair of nodes in
that graph, and it is independent of which other edges are being observed. In preferential attachment
models [2], the probability of linking to any specified node in a graph is proportional to the degree
of the node in the graph, leading to ?rich get richer? effects. Small-world models [3] try to capture
instead such phenomena often observed in real networks as small diameters and high clustering coefficients. An attempt to model potentially complex dependencies between graph edges in the form
of Gibbs-Boltzmann distributions is made by exponential random graph (ERG) models [4], which
subsume the ER model as a special case. Finally, a recent attempt at modeling real networks through
?
Universit?e Charles de Gaulle ? Lille 3, Domaine Universitaire du Pont de Bois ? BP 60149, 59653 Villeneuve d?Ascq (France).

1

a stochastic generative process is made by Kronecker graphs [5], which try to capture phenomena
such as heavy-tailed degree distributions and shrinking diameter properties while paying attention
to the temporal dynamics of network growth.
While some of these models behave better than others in terms of computational tractability, one
basic limitation affecting all of them is a sort of parametric assumption concerning the probability
laws underlying the observed network properties. In other words, currently available models of network structure assume that the shape of the probability distribution generating the network is known
a priori. For example, typical formulations of ERG models assume that the building blocks of real
networks are given by such structures as k-stars and k-triangles, with different weights assigned to
different structures, whereas the preferential attachment model is committed to the assumption that
the observed degree distributions obey a power law. In such frameworks, estimating the model from
data reduces to fitting the model parameters, where the parametric form of the target distribution is
fixed a priori. Clearly, in order for such models to deliver accurate estimates of the distributions at
hand, their prior assumptions concerning the behavior of the target quantities must be satisfied by
the given data. But unfortunately, this is something that we can rarely assess a priori. To date, the
knowledge we have concerning large-scale real-world networks does not allow to assess whether
any particular parametric assumption is capturing in depth the target generative process, although
some observed network properties may happen to be modeled fairly well.
The aim of this paper is twofold. On the one hand, we take a first step toward nonparametric
modeling of random networks by developing a novel network statistic, which we call the Fiedler
delta statistic. The Fiedler delta function allows to model different graph properties at once in an
extremely compact form. This statistic is based on the spectral analysis of the graph, and in particular
on the smallest non-zero eigenvalue of the Laplacian matrix, which is known as Fiedler value [6, 7].
On the other hand, we use the Fiedler delta statistic to define a Boltzmann distribution over graphs,
leading to the Fiedler random field (FRF) model. Roughly speaking, for each binary edge variable
Xuv , potentials in a FRF are functions of the difference determined in the Fiedler value by flipping
the value of Xuv , where the spectral decomposition is restricted to a suitable subgraph incident to
nodes u, v. The intuition is that the information encapsulated in the Fiedler delta for Xuv gives
a measure of the role of Xuv in determining the algebraic connectivity of its neighborhood. As
a first step in the theoretical analysis of FRFs, we prove that these models allow to capture edge
correlations at any distance within a given neighborhood, hence defining a fairly general class of
conditional independence structures over networks.
The paper is organized as follows. Sec. 2 reviews some theoretical background concerning the
Laplacian spectrum of graphs. FRFs are then introduced in Sec. 3, where we also analyze their
dependence structure and present an efficient approach for learning them from data. To avoid unwarranted prior assumptions concerning the statistical behavior of the Fiedler delta, potentials are
modeled by non-linear functions, which we estimate from data by minimizing a contrastive divergence objective. FRFs are evaluated experimentally in Sec. 4, showing that they are well suited for
large-scale estimation problems over real-world networks, while Sec. 5 draws some conclusions and
sketches a few directions for further work.

2 Graphs, Laplacians, and eigenvalues
Let G = (V, E) be an undirected graph with n nodes. In the following we assume that the graph is
unweighted with adjacency matrix A. The degree du of a node u ? V is defined as the number of
connections of u to other nodes, that is du = |{v: {u, v} ? E}|. Accordingly, the degree matrix D of
a graph G corresponds to the diagonal matrix with the vertex degrees d1 , . . . , dn on the diagonal. The
main tools exploited by the random graph model proposed here are the graph Laplacian matrices.
Different graph Laplacians have been defined in the literature. In this work, we use consistently the
unnormalized graph Laplacian, given by L = D ? A. Some basic facts related to the unnormalized
Laplacian matrix can be summarized as follows [7]:
Proposition 1. The unnormalized graph Laplacian L of an undirected graph G has the following
properties: (i) L is symmetric and positive semi-definite; (ii) the smallest eigenvalue of L is 0; (iii)
L has n non-negative, real-valued eigenvalues 0 = ?1 ? . . . ? ?n ; (iv) the multiplicity of the
eigenvalue 0 of L equals the number of connected components in the graph, that is, ?1 = 0 and
?2 > 0 if and only if G is connected.
2

In the following, the (algebraic) multiplicity of an eigenvalue ?i will be denoted by M (?i , G).
If the graph has one single connected component, then M (0, G) = 1, and the second smallest eigenvalue ?2 (G) > 0 is called, in this case, the Fiedler eigenvalue. The Fiedler eigenvalue provides
insight into several graph properties: when there is a nontrivial spectral gap, i.e. ?2 (G) is clearly
separated from 0, the graph has good expansion properties, stronger connectivity, and rapid convergence of random walks in the graph. For example, it is known that ?2 (G) ? ?(G), where ?(G) is the
edge connectivity of the graph (i.e. the size of the smallest edge cut whose removal makes the graph
disconnected [7]). Notice that if the graph has more than one connected component, then ?2 (G) will
be also equal to zero, thus implying that the graph is not connected. Without loss of generality, we
abuse the term Fiedler eigenvalue to denote the smallest eigenvalue different from zero, regardless
of the number of connected components. In this paper, by Fiedler value we mean the eigenvalue
?k+1 (G), where k = M (0, G).
+

For any pair of nodes u and v in a graph G = (V, E), we define two corresponding graphs G uv and
?
+
?
G uv in the following way: G uv = (V, E ? {{u, v}}), and G uv = (V, E \ {{u, v}}). Clearly, we
+
?
have that either G uv = G or G uv = G. A basic property concerning the Laplacian eigenvalues of
+
?
G uv and G uv is the following [7, 8, 9]:
+

?

+

Lemma 1. If G uv and G uv are two graphs with n nodes, such that {u, v} ? V, G uv = (V, E ?
Pn
?
+
?
{{u, v}}), and G uv = (V, E \ {{u, v}}), then we have that: (i) i=1 ?i (G uv ) ? ?i (G uv ) = 2;
+
?
(ii) ?i (G uv ) ? ?i (G uv ) for any i such that 1 ? i ? n.

3 Fiedler random fields
Fiedler random fields are introduced in Sec. 3.1, while in Secs. 3.2?3.3 we discuss their dependence
structure and explain how to estimate them from data respectively.
3.1 Probability distribution
Using the notions reviewed above, we define the Fiedler delta function ??2 in the following way:
+

Definition 1. Given graph G, let k = M (0, G uv ). Then,

+
?
?k+1 (G uv ) ? ?k+1 (G uv ) if Xuv = 1
??2 (u, v, G) =
?
+
?k+1 (G uv ) ? ?k+1 (G uv ) otherwise

(1)

In other words, ??2 (u, v, G) is the variation in the Fiedler eigenvalue of the graph Laplacian that
would result from flipping the value of Xuv in G. Concerning the range of the Fiedler delta function,
we can easily prove the following proposition:
Proposition 2. For any graph G = (V, E) and any pair of nodes {u, v} such that Xuv = 1, we have
that 0 ? ??2 (u, v, G) ? 2.
Proof. Let k = M (0, G). The proposition follows straightforwardly from Lemma 1, given that
?
??2 (u, v, G) = ?k+1 (G) ? ?k+1 (G uv ).
We now proceed to define FRFs. Given a graph G = (V, E), for each (unordered) pair of nodes
{u, v} such that u 6= v, we take Xuv to denote a binary random variable such that Xuv = 1 if
{u, v} ? E, and Xuv = 0 otherwise. Since the graph is undirected,SXuv = Xvu . We also say that a
subgraph GS of G with edge set ES is incident to Xuv if {u, v} ? e?ES e. Then:
Definition 2. Given a graph G, let XG denote the set of random variables defined on G, i.e. XG =
{Xuv : u 6= v ? {u, v} ? V}. For any Xuv ? XG , let Guv be a subgraph of G which is incident
to Xuv and ?uv be a two-place real-valued function with parameter vector ?. We say that the
probability distribution of XG is a Fiedler random field if it factorizes as
?
?
X

1
P (XG | ?) =
exp ?
(2)
?uv Xuv , ??2 (u, v, Guv ); ? ?
Z(?)
Xuv ?XG

3

where Z(?) is the partition function.
In other words, a FRF is a Gibbs-Boltzmann distribution over graphs, with potential functions defined for each node pair {u, v} along with some neighboring subgraph Guv . In particular, in order
to model the dependence of each variable Xuv on Guv , potentials take as argument both the value of
Xuv and the Fiedler delta corresponding to {u, v} in Guv . The idea is to treat the Fiedler delta statistic as a (real-valued) random variable defined over subgraph configurations, and to exploit this random variable as a compact representation of those configurations. This means that the dependence
structure of a FRF is fixed by the particular choice of subgraphs Guv , so that the set XGuv \ {Xuv }
makes Xuv independent of XG \ XGuv . Three fundamental questions are then the following. First,
how do we fix the subgraph Guv for each pair of nodes {u, v}? Second, how do we choose a shape
for the potential functions, so as to fully exploit the information contained in the Fiedler delta, while
avoiding unwarranted assumptions concerning their parametric form? Third, how does the Fiedler
delta statistic behave with respect to the Markov dependence property for random graphs? One basic
result related to the third question is presented in Sec. 3.2, while Sec. 3.3 will address the first two
points.
3.2 Dependence structure
We first recall the definition of Markov dependence for random graphs [10]. Let N (Xuv ) denote
the set {Xwz : {w, z} ? E ? |{w, z} ? {u, v}| = 1}. Then:
Definition 3. A random graph G is said to be a Markov graph (or to have a Markov dependence
structure) if, for any pair of variables Xuv and Xwz in G such that {u, v} ? {w, z} = ?, we have
that P (Xuv | Xwz , N (Xuv )) = P (Xuv | N (Xuv )).
Based on Def. 3, we say that the dependence structure of a random graph G is non-Markovian if,
for disjoint pairs of nodes {u, v} and {w, z}, it does not imply that P (Xuv | Xwz , N (Xuv )) =
P (Xuv | N (Xuv )), i.e. if it is consistent with the inequality P (Xuv | Xwz , N (Xuv )) 6=
P (Xuv | N (Xuv )). We can then prove the following proposition:
Proposition 3. There exist Fiedler random fields with non-Markovian dependence structure.
Proof sketch. Consider a graph G = (V, E) such that V = {u, v, w, z} and E =
{{u, v}, {v, w}, {w, z}, {u, z}}. The proof relies on the following result [6]: if graphs G1
and G2 are, respectively, a path and a circuit of size n, then ?2 (G1 ) = 2 (1 ? cos(?/n))
and ?2 (G2 ) = 2 (1 ? cos(2?/n)). Since adding exactly one edge to a path of size 4 can
yield a circuit of the same size, this property allows to derive analytic forms for the Fiedler
delta statistic in such graphs, showing that there exist parameterizations of ?uv such that
?uv (Xuv , ??2 (u, v, G); ?) 6= ?uv (Xuv , ??2 (u, v, GS ); ?). This means that the dependence structure of G is non-Markovian.1
Note that the proof of Prop. 3 can be straightforwardly generalized to the dependence between two
variables Xuv and Xwz in circuits/paths of arbitrary size n, since the expression used for the Fiedler
eigenvalues of such graphs holds for any n. This fact suggests that FRFs allow to model edge
correlations at virtually any distance within G, provided that each subgraph Guv is chosen in such a
way as to encompass the relevant correlation.
3.3 Model estimation
The problem of learning a FRF from an observed network can be split into the task of estimating
the potential functions once the network distribution has been factorized into a particular set of
subgraphs, and the task of factorizing the distribution through a suitable set of subgraphs, which
corresponds to estimating the dependence structure of the FRF. Here we focus on the problem of
learning the FRF potentials, while suggesting a heuristic way to fix the dependence structure of the
model.
In order to estimate the FRF potentials, we need to specify on the one hand a suitable architecture
for such functions, and on the other hand the objective function that we want to optimize. As a
1

For a complete proof, see the supplementary material.

4

preliminary step, we tested experimentally a variety of shapes for the potential functions. The tests
indicated the importance of avoiding limiting assumptions concerning the form of the potentials,
which motivated us to model them by a feed-forward multilayer perceptron (MLP), due to its wellknown capabilities of approximating functions of arbitrary shape [12]. In particular, throughout
the applications described in this paper we use a simple MLP architecture with one hidden layer
and hyperbolic tangent activation functions. Therefore, our parameter vector ? simply consists of
the weights of the chosen MLP architecture. Notice that, as far as the estimation of potentials is
concerned, any regression model offering approximation capabilities analogous to the MLP family
could be used as well. Here, the only requirement is to avoid unwarranted prior assumptions with
respect to the shape of the potential functions. In this respect, we take our approach to be genuinely
nonparametric, since it does not require the parametric form of the target functions to be specified
a priori in order to estimate them accurately. Concerning instead the learning objective, the main
difficulty we want to avoid is the complexity of computing the partition function involved in the
Gibbs-Boltzmann distribution. The approach we adopt to this aim is to minimize a contrastive
divergence objective [13]. If G = (V, E) is the network that we want to fit our model to, and
?
Guv = (Vuv , Euv ) is a subgraph of G such that {u, v} ? Vuv , let Guv
denote the graph that we obtain
by resampling the value of Xuv in Guv according to the conditional distribution Pb (Xuv | xGuv \
?
{xuv }; ?) predicted by our model. In other words, Guv
is the result of performing just one iteration
of Gibbs sampling on Xuv using ?, where the configuration xGuv of Guv is used to initialize the
(single-step) Markov chain. Then, our goal is to minimize the function ?CD (?; G), given by:
?
??
?
? 1
?
X

?
?CD (?; G) = log
exp ?
? x?uv , ??2 (u, v, Guv
); ? ? ? log Pb (xG | ?)
? Z(?)
?
Xuv ?XG
(3)

X 


?
?
? xuv , ??2 (u, v, Guv ); ? ? ? xuv , ??2 (u, v, Guv ); ?
=
Xuv ?XG

where ? is the function computed by our MLP architecture. The appeal of contrastive divergence
learning is that, while it does not require to compute the partition function, it is known to converge
to points which are very close to maximum-likelihood solutions [14].
If we want our learning objective to be usable in the large-scale setting, then it is not feasible to
sum over all node pairs {u, v} in the network, since the number of such pairs grows quadratically
with |V|. In this respect, a straightforward approach for scaling to very large networks consists in
sampling n objects from the set of all possible pairs of nodes, taking care that the sample contains a
good balance between linked and unlinked pairs. Another issue we need to address concerns the way
we sample a suitable set of subgraphs Gu1 v1 , . . . , Gun vn for the selected pairs of nodes. Although
different sampling techniques could be used in principle [15], our goal is to model correlations
between each variable Xuv and some neighboring region Guv in G. Such a neighborhood should be
large enough to make ??2 (u, v, Guv ) sufficiently informative with respect to the overall network, but
also small enough to keep the spectral decomposition of Guv computationally tractable. Therefore,
in order to sample Guv , we propose to draw Vuv by performing k ?snowball waves? on G [16], using
u and v as seeds, and then setting Euv to be the edge set induced by Vuv in G (see Algorithm 1
for the details). In this way, we can empirically tune the k hyperparameter in order to trade-off the
informativeness of Guv for the tractability of its spectral decomposition, where it is known that the
complexity of computing ??2 (u, v, Guv ) is cubic with respect to the number of nodes in Guv [17].
Algorithm 1 SampleSubgraph: Sampling a neighboring subgraph for a given node pair
Input: Undirected graph G = (V, E); node pair {u, v}; number k of snowball waves.
Output: Undirected graph Guv = (Vuv , Euv ).
SampleSubgraph(G, {u, v}, k):
1. Vuv = {u, v}
2. for(i = 1 to Sk)
3.
Vuv = Vuv ? w?Vuv {z ? V: {w, z} ? E}
4. Euv = {{w, z} ? E: {w, z} ? Vuv }
5. return (Vuv , Euv )

5

	

Once sampled our training set D = (xu1 v1 , Gu1 v1 ), . . . , (xun vn , Gun vn ) , we learn the MLP
weights by minimizing the objective ?CD (?; D), which which we obtain from ?CD (?; G) by restricting the summation in Eq. 3 to the elements of D. Minimization is performed by iterative
gradient descent, using standard backpropagation for updating the MLP weights.

4 Experimental evaluation
In order to investigate the empirical behavior of FRFs as models of large-scale networks, we design
two different groups of experiments (in link prediction and graph generation respectively), using collaboration networks drawn from the arXiv e-print repository (http://snap.stanford.edu/
data/index.html), where nodes represent scientists and edges represent paper coauthorships.
Some basic network statistics are reported in Table 1.
Link prediction. In the first kind of experiments, given a random network G = (V, E), our
goal is to measure the accuracy of FRFs at estimating the conditional distribution of variables
Xuv given the configuration of neighboring subgraphs Guv of G. This can be seen as a link
prediction problem where only local information (given by Guv ) can be used for predicting the
presence of a link {u, v}. At the same time, we want to understand whether the overall network size (in terms of the number of nodes) has an impact on the number of training examples
that will be necessary for FRFs to converge to stable prediction accuracy. Recall that FRFs are
	

trained on a data sample D = (xu1 v1 , Gu1 v1 ), . . . , (xun vn , Gun vn ) , where n ? |V| (|V|?1)
.
2
Given this, converging to stable predictions for values of n which do not depend on |V| is a crucial requirement for achieving large-scale applicability. Let us sample our training set D by first
drawing n node pairs from V in such a way that linked and unlinked pairs from G are equally
represented in D, and then extracting the corresponding subgraphs Gui ,vi by Algorithm 1 using
one snowball wave. We then learn our model from D as described in Sec. 3.3. In all the experiments reported in this work, the number of hidden units in our MLP architecture is set to
5. A test set T containing m objects (xu1 v1 , GS1 ), . . . , (xum vm , GSm ) is also sampled from G
so that T ? D = ?, where pairs {ui , vi } in T are drawn uniformly at random from V ? V.
Predictions are derived from the learned model
by first computing the conditional probability of observing a link for each pair of nodes
{uj , vj } in T , and then making a decision on
the presence/absence of links by thresholding
the predicted probability (where the threshold is
tuned by cross-validation). Prediction accuracy
is measured by averaging the recognition accuracy for linked and unlinked pairs in T respectively (where |T | = 10, 000). In Fig. 1, the accuracy of FRFs on the test set is plotted against
a growing size n of D (where 12 ? n ? 48).
Interestingly, the number of training examples
required for the accuracy curve to stabilize does
not seem to depend at all on the overall network
size. Indeed, fastest convergence is achieved Figure 1: Prediction accuracy of FRFs on the
for the average-sized and the second largest arXiv networks for a growing training set size.
networks, i.e. HepPh and AstroPh respectively.
Notice how a training sample containing an extremely small percentage of node pairs is sufficient
for our learning approach to converge to stable prediction accuracy. This result encourages to think
of FRFs as a convenient modeling option for the large-scale setting.
0.95

0.9

Prediction accuracy on test set

0.85

0.8

0.75

0.7

0.65

0.6

0.55

GrQc (5,242 nodes)
HepTh (9,877 nodes)
HepPh (12,008 nodes)
AstroPh (18,772 nodes)
CondMat (23,133 nodes)

0.5

0.45

10

15

20

25

30
Training set size

35

40

45

50

Besides assessing whether the network size affects the number of training samples needed to accurately learn FRFs, we want to evaluate the usefulness of the dependence structure involved in our
model in predicting the conditional distributions of edges given their neighboring subgraphs. That
is, we want to ascertain whether the effort of modeling the conditional independence structure of
the overall network through the FRF formalism is justified by a suitable gain in prediction accuracy
with respect to statistical models that do not focus explicitly on such dependence structure. To this
aim, we compare FRFs to two popular statistical models for large-scale networks, namely the WattsStrogatz (WS) and the Barab?asi-Albert (BA) models [3, 2]. The WS formalism is mainly aimed
6

at modeling the short-diameter property often observed in real-world networks. Interestingly, the
degree distribution of WS networks can be expressed in closed form in terms of two parameters ?
and ?, related to the average degree distribution and a network rewiring process respectively [18].
On the other hand, the BA model is aimed at explaining the emergence of power-law degree distributions, where such distributions can be expressed in terms of an adaptive parameter ? [19]. The
parameters of both the WS and the BA model can be estimated by standard maximum-likelihood
approaches and then used to predict conditional edge distributions, exploiting information from the
degrees observed in the given subgraphs [20, 21]. The ER model is not considered in this group
of experiments, since the involved independence assumption makes it unusable (i.e. equivalent to
random guessing) for the purposes of conditional estimation tasks. On the other hand, ERG models
are not suitable for application to the large-scale setting. We tried them out using edge, k-star and
k-triangle statistics [4], and the tests confirmed this point. Although the prohibitive cost of fitting the
models and computing the involved feature functions could be overcome in principle by sampling
strategies similar to the ones we employ for FRFs, the potentials used in ERGs become numerically
unstable in the large-scale setting, leading to numerical representation issues for which we are not
aware of any off-the-shelf solution. Accuracy values for the different models are reported in Table 1. FRFs dramatically outperform the other two models on all networks. Since both the BA and
the WS model do not show relevant improvements over simple random guessing, this result clearly
suggests that exploiting the dependence structure involved in network edge configurations is crucial
to accurately predict the presence/absence of links.

Table 1: Edge prediction results on the arXiv networks. General network statistics are also reported,
where CCG and DG stand for average clustering coefficient and network diameter respectively.

Dataset
AstroPh
CondMat
GrQc
HepPh
HepTh

|V|
18,772
23,133
5,242
12,008
9,877

Network Statistics
|E | CCG
396,160
0.63
186,936
0.63
28,980
0.52
237,010
0.61
51,971
0.47

DG
14
15
17
13
17

Prediction Accuracy
BA
FRF
WS
50.98% 89.97% 50.14%
50.15% 91.62% 56.71%
52.57% 91.14% 53.72%
51.61% 86.57% 54.33%
58.33% 92.25% 50.30%

Graph generation. A second group of experiments is aimed at assessing whether the FRFs learned
on the arXiv networks can be considered as plausible models of the degree distribution (DD) and
the clustering coefficient distribution (CC) observed in each network [15]. To this aim, we use the
estimated FRF models to generate artificial graphs of various size, using Gibbs sampling, and then
we compare the DD and CC observed in the artificial graphs with those estimated on the whole
networks. For scale-free networks such as the ones considered here, the BA model is known to be
the most accurate model currently available with respect to DD. On the other hand, for CC both BA
and WS are known to be more realistic models than ER random graphs. Therefore, we compare the
graphs generated by FRFs to those generated by the BA, ER, and WS models for the same networks.
The distance in DD and CC between the artificial graphs on the one hand and the corresponding real
network on the other hand is measured using the Kolmogorov-Smirnov D-statistic, following a
common use in graph mining research [15]. Here we only plot results for the CondMat and HepTh
networks, noticing that the results we collected on the other arXiv networks lend themselves to the
same interpretation as the ones displayed in Fig. 2. Values are averaged over 100 samples for each
considered graph size, where the standard deviation is typically in the order of 10?2 . The outcome
motivates the following considerations. Concerning DD, FRFs are able to improve (at least slightly)
the accuracy of the state-of-the-art BA model, while they are very close that model with respect
to clustering coefficient. In all cases, both BA and FRFs prove to be far more accurate than ER
or WS, where the only advantage of using WS is limited to improving CC over ER. These results
are particularly encouraging, since they show how the nonparametric approach motivating the FRF
model allows to accurately estimate network properties (such as DD) that are not aimed for explicitly
in the model design. This suggests that the Fiedler delta statistic is a promising direction for building
generative models capable of capturing different network properties through a unified approach.
7

1

0.9
BA
ER
FRF
WS

0.9

BA
ER
FRF
WS
0.8

D-statistic for CC

D-statistic for DD

0.8

0.7

0.6

0.7

0.6

0.5
0.5
0.4

0.3

0.4
40

60

80

100
Artificial graph size

120

140

160

40

60

80

(a)

100
Artificial graph size

120

140

160

(b)

1

0.9
BA
ER
FRF
WS

0.9

BA
ER
FRF
WS
0.8

D-statistic for CC

D-statistic for DD

0.8

0.7

0.6

0.7

0.6

0.5
0.5
0.4

0.3

0.4
40

60

80

100
Artificial graph size

120

140

160

40

(c)

60

80

100
Artificial graph size

120

140

160

(d)

Figure 2: D-statistic values for DD and CC on the CondMat (a?b) and HepTh (c?d) networks.

5 Conclusions and future work
The main motivation inspiring this work was the observation that statistical modeling of networks
cries for genuinely nonparametric estimation, because of the inaccuracy often resulting from unwarranted parametric assumptions. In this respect, we showed how the Fiedler delta statistic offers a
powerful building block for designing a nonparametric estimator, which we developed in the form
of the FRF model. Since here we only applied FRFs to collaboration networks, which are typically
scale-free, an important option for future work is to assess the flexibility of FRFs in modeling networks from different families. In the second place, since we only addressed in a heuristic way the
problem of learning the dependence structure of FRFs, a stimulating direction for further research
consists in designing clever techniques for learning the structure of FRFs, e.g. considering the use
of alternative subgraph sampling techniques. Finally, we would like to assess the possibility of
modeling networks through mixtures of FRFs, so as to fit different network regions (with possibly
conflicting properties) through specialized components of the mixture.
Acknowledgments
This work has been supported by the French National Research Agency (ANR-09-EMER-007). The
authors are grateful to Gemma Garriga, R?emi Gilleron, Liva Ralaivola, and Michal Valko for their
useful suggestions and comments.

References
[1] P. Erd?os and A. R?enyi, ?On Random Graphs, I,? Publicationes Mathematicae Debrecen, vol. 6,
pp. 290?297, 1959.
[2] A.-L. Barab?asi and R. Albert, ?Emergence of scaling in random networks,? Science, vol. 286,
pp. 509?512, 1999.
8

[3] D. J. Watts and S. H. Strogatz, ?Collective dynamics of ?small-world? networks,? Nature,
vol. 393, pp. 440?442, 1998.
[4] T. A. B. Snijders, P. E. Pattison, G. L. Robins, and M. S. Handcock, ?New Specifications for
Exponential Random Graph Models,? Sociological Methodology, vol. 36, pp. 99?153, 2006.
[5] J. Leskovec, D. Chakrabarti, J. Kleinberg, C. Faloutsos, and Z. Ghahramani, ?Kronecker
graphs: An approach to modeling networks,? Journal of Machine Learning Research, vol. 11,
pp. 985?1042, 2010.
[6] M. Fiedler, ?Algebraic connectivity of graphs,? Czechoslovak Mathematical Journal, vol. 23,
pp. 298?305, 1973.
[7] B. Mohar, ?The Laplacian Spectrum of Graphs,? in Graph Theory, Combinatorics, and Applications (Y. Alavi, G. Chartrand, O. R. Oellermann, and A. J. Schwenk, eds.), pp. 871?898,
Wiley, 1991.
[8] W. N. Anderson and T. D. Morley, ?Eigenvalues of the Laplacian of a graph,? Linear and
Multilinear Algebra, vol. 18, pp. 141?145, 1985.
[9] D. M. Cvetkovi?c, M. Doob, and H. Sachs, eds., Spectra of Graphs: Theory and Application.
New York (NY): Academic Press, 1979.
[10] O. Frank and D. Strauss, ?Markov Graphs,? Journal of the American Statistical Association,
vol. 81, pp. 832?842, 1986.
[11] J. Besag, ?Spatial Interaction and the Statistical Analysis of Lattice Systems,? Journal of the
Royal Statistical Society. Series B, vol. 36, pp. 192?236, 1974.
[12] K. Hornik, ?Approximation capabilities of multilayer feedforward networks,? Neural Networks, vol. 4, no. 2, pp. 251?257, 1991.
[13] G. E. Hinton, ?Training Products of Experts by Minimizing Contrastive Divergence,? Neural
Computation, vol. 14, no. 8, pp. 1771?1800, 2002.
? Carreira-Perpi?na? n and G. E. Hinton, ?On Contrastive Divergence Learning,? in Pro[14] M. A.
ceedings of the Tenth International Workshop on Articial Intelligence and Statistics (AISTATS
2005), pp. 33?40, 2005.
[15] J. Leskovec and C. Faloutsos, ?Sampling from large graphs,? in Proceedings of the Twelfth
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD
2006), pp. 631?636, 2006.
[16] E. D. Kolaczyk, Statistical Analysis of Network Data. Methods and Models. New York (NY):
Springer, 2009.
[17] Z. Bai, J. Demmel, J. Dongarra, A. Ruhe, and H. van der Vorst, eds., Templates for the Solution
of Algebraic Eigenvalue Problems: A Practical Guide. Philadelphia (PA): SIAM, 2000.
[18] A. Barrat and M. Weigt, ?On the properties of small-world network models,? The European
Physical Journal B, vol. 13, pp. 547?560, 2000.
[19] R. Albert and A.-L. Barab?asi, ?Statistical mechanics of complex networks,? Reviews of Modern
Physics, vol. 74, pp. 47?97, 2002.
[20] M. E. J. Newman, ?Clustering and preferential attachment in growing networks,? Physical
Review E, vol. 64, p. 025102, 2001.
[21] A. Barab?asi, H. Jeong, Z. N?eda, E. Ravasz, A. Schubert, and T. Vicsek, ?Evolution of the social
network of scientic collaborations,? Physica A, vol. 311, pp. 590?614, 2002.

9

"
1987,On the Power of Neural Networks for Solving Hard Problems,,70-on-the-power-of-neural-networks-for-solving-hard-problems.pdf,Abstract Missing,"137

On the Power of Neural Networks for
Solving Hard Problems
J ehoshua Bruck
Joseph W. Goodman
Information Systems Laboratory
Departmen t of Electrical Engineering
Stanford University
Stanford, CA 94305
Abstract
This paper deals with a neural network model in which each neuron
performs a threshold logic function. An important property of the model
is that it always converges to a stable state when operating in a serial
mode [2,5]. This property is the basis of the potential applications of the
model such as associative memory devices and combinatorial optimization

[3,6].
One of the motivations for use of the model for solving hard combinatorial
problems is the fact that it can be implemented by optical devices and
thus operate at a higher speed than conventional electronics.
The main theme in this work is to investigate the power of the model for
solving NP-hard problems [4,8], and to understand the relation between
speed of operation and the size of a neural network. In particular, it will
be shown that for any NP-hard problem the existence of a polynomial
size network that solves it implies that NP=co-NP. Also, for Traveling
Salesman Problem (TSP), even a polynomial size network that gets an
?-approximate solution does not exist unless P=NP.
The above results are of great practical interest, because right now it is
possible to build neural networks which will operate fast but are limited
in the number of neurons.

1

Background

The neural network model is a discrete time system that can be represented by
a weighted and undirected graph. There is a weight attached to each edge of
the graph and a threshold value attached to each node (neuron) of the graph.

? American Institute of Physics 1988

138

The order of the network is the number of nodes in the corresponding graph.
Let N be a neural network of order n; then N is uniquely defined by (W, T)
where:

? W is an n X n symmetric matrix, Wii is equal to the weight attached to
edge (i, j) .
? T is a vector of dimension n, Ti denotes the threshold attached to node i.
Every node (neuron) can be in one of two possible states, either 1 or -1. The
state of node i at time t is denoted by Vi(t). The state of the neural network at
time t is the vector V(t).
The next state of a node is computed by:

Vi(t + 1) = sgn(H,(t)) = {
where

~1 ~t~;2i~ 0

(1)

n

Hi(t) =

L

WiiVj(t) - Ti

i=l

The next state of the network, i.e. V(t + 1), is computed from the current
state by performing the evaluation (1) at a subset of the nodes of the network,
to be denoted by S. The modes of operation are determined by the method
by which the set S is selected in each time interval. If the computation is
performed at a single node in any time interval, i.e. 1S 1= 1, then we will say
that the network is operating in a serial mode; if 1S 1= n then we will say that
that the network is operating in a fully parallel mode. All the other cases, i.e.
1 <I S 1< n will be called parallel modes of operation. The set S can be chosen
at random or according to some deterministic rule.
A state V(t) is called stable iff V(t) = sgn(WV(t) - T), i.e. there is no
change in the state of the network no matter what the mode of operation is.
One of the most important properties of the model is the fact that it always
converges to a stable state while operating in a serial mode. The main idea in
the proof of the convergence property is to define a so called energy function
and to show that this energy function is nondecreasing when the state of the
network changes. The energy function is:

(2)
An important note is that originally the energy function was defined such that
it is nonincreasing [5]; we changed it such that it will comply with some known
graph problems (e.g. Min Cut).
A neural network will always get to a stable state which corresponds to a
local maximum in the energy function. This suggests the use of the network as a

139

device for performing a local search algorithm for finding a maximal value of the
energy function [6]. Thus, the network will perform a local search by operating
in a random and serial mode. It is also known [2,9] that maximization of E
associated with a given network N in which T = 0 is equivalent to finding
the Minimum Cut in N. Actually, many hard problems can be formulated as
maximization of a quadratic form (e.g. TSP [6)) and thus can be mapped to a
neural network.
.

2

The Main Results

The set of stable states is the set of possible final solutions that one will get
using the above approach. These final solutions correspond to local maxima of
the energy function but do not necessarily correspond to global optima of the
corresponding problem. The main question is: suppose we allow the network to
operate for a very long time until it converges; can we do better than just getting
some local optimum? i.e., is it possible to design a network which will always
find the exact solution (or some guaranteed approximation) of the problem?
Definition: Let X be an instance of problem. Then 1 X 1 denotes the size of
X, that is, the number of bits required to represent X. For example, for X
being an instance of TSP, 1 X I is the number of bits needed to represent the
matrix of the distances between cities.
Definition: Let N be a neural network. Then 1 N 1 denotes the size of the
network N. Namely, the number of bits needed to represent Wand T.
Let us start by defining the desired setup for using the neural network as a
model for solving hard problems.
Consider an optimization problem L, we would like to have for every instance
X of L a neural network N x with the following properties:
? Every local maximum of the energy function associated with N x corresponds to a global optimum of X .
? The network N x is small, that is,
in 1X I.

I

Nx

1

is bounded by some polynomial

Moreover, we would like to have an algorithm, to be denoted by A L , which given
an instance X E L, generates the description for N x in polynomial (in I X I)
time.
Now, we will define the desired setup for using the neural network as a model
for finding approximate solutions for hard problems.
Definition: Let

Eglo

be the global maximum of the energy function. Let

Eloc

140

be a local maximum of the energy function. We will say that a local maximum
is an f-approximate of the global iff:
Eglo - Eloc
--:;.--<
Eglo

f

-

The setup for finding approximate solutions is similar to the one for finding
exact solutions. For fo> 0 being some fixed number. We would like to have a
network N x~ in which every local maximum is an f-approximate of the global
and that the global corresponds to an optimum of X. The network N x? should
be small, namely, 1 N x~ 1 should be bounded by a polynomial in 1 X I. Also,
we would like to have an algorithm AL~, such that, given an instance X E L, it
generates the description for N x? in polynomial (in 1 X I) time.
Note that in both the exact case and the approximate case we do not put any
restriction on the time it takes the network to converge to a solution (it can be
exponential) .
A t this point the reader should convince himself that the above description is
what he imagined as the setup for using the neural network model for solving
hard problems, because that is what the following definition is about.
Definition: We will say that a neural network for solving (or finding an fapproximation of) a problem L exists if the algorithm AL (or ALJ which generates the description of N x (or Nx~) exists.
The main results in the paper are summarized by the following two propositions. The first one deals with exact solutions of NP-hard problems while the
second deals with approximate solutions to TSP.
Proposition 1 Let L be an NP-hard problem. Then the existence of a neural
network for solving L implies that NP = co-NP.
Proposition 2 Let f > 0 be some fixed number. The existence of a neural
network for finding an f-approximate solution to TSP implies that P=NP.
Both (P=NP) and (NP=co-NP) are believed to be false statements, hence,
we can not use the model in the way we imagine.

The key observation for proving the above propositions is the fact that a
single iteration in a neural network takes time which is bounded by a polynomial
in the size of the instance of the corresponding problem. The proofs of the above
two propositions follow directly from known results in complexity theory and
should not be considered as new results in complexity theory.

141

3

The Proofs

Proof of Proposition 1: The proof follows from the definition of the classes
NP and co-NP, and Lemma 1. The definitions and the lemma appear in Chapters 15 and 16 in [8] and also in Chapters 2 and 7 in [4].

Lemma 1 If the complement of an NP-complete problem is in NP,
then NP=co-NP.
Let L be an NP-hard problem. Suppose there exists a neural network that solves
L. Let 1 be an NP-complete problem. By definition, 1 can be polynomialy
reduced to L. Thus, for every instance X E 1, we have a neural network such
that from any of its global maxima we can efficiently recognize whether X is a
'yes' or a 'no' instance of 1.
We claim that we have a nondeterministic polynomial time algorithm to decide
that a given instance X E 1 is a 'no' instance. Here is how we do it: for X E 1
we construct the neural network that solves it by using the reduction to L. We
then check every state of the network to see if it is a local maximum (that is
done in polynomial time). In case it is a local maximum, we check if the instance
is a 'yes' or a 'no' instance (this is also done in polynomial time).
Thus, we have a nondeterministic polynomial time algorithm to recognize any
'no' instance of 1. Thus, the complement of the problem 1 is in NP. But 1 is
an NP-complete problem, hence, from Lemma 1 it follows that NP=co-NP. 0

Proof of Proposition 2: The result is a corollary of the results in [7], the
reader can refer to it for a more complete presentation.
The proof uses the fact that the Restricted Hamiltonian Circuit (RHC) is an
NP-complete problem.
Definiton of RHC: Given a graph G = (V, E) and a Hamiltonian path in G.
The question is whether there is a Hamiltonian circuit in G?
It is proven in [7] that RHC is NP-complete.
Suppose there exists a polynomial size neural network for finding an
f-approximate solution to TSP. Then it can be shown that an instance X E
RHC can be reduced to an instance X E TSP, such that in the network N x
the following holds: if the Hamiltonian path that is given in X corresponds to a
local maximum in N x? then X is a 'no' instance; else, if it does not correspond
to a local maximum in N x? then X is a 'yes' instance. Note that we can check
for locality in polynomial time.
Hence, the existence of N xe for all X E TSP implies that we have a polynomial
time algorithm for RHC. 0
?

142

4

Concluding Remarks
1. In Proposition 1 we let I W I and I T I be arbitrary but bounded by a
polynomial in the size of a given instance of a problem. If we assume
that I W I and I T I are fixed for all instances then a similar result to
Proposition 1 can be proved without using complexity theory; this result
appears in [1].
2. The network which corresponds to TSP, as suggested in [6], can not solve
the TSP with guaranteed quality. However, one should note that all the
analysis in this paper is a worst case type of analysis. So, it might be that
there exist networks that have good behavior on the average.
3. Proposition 1 is general to all NP-hard problems while Proposition 2 is
specific to TSP. Both propositions hold for any type of networks in which
an iteration takes polynomial time.
4. Clearly, every network has an algorithm which is equivalent to it, but an
algorithm does not necessarily have a corresponding network. Thus, if we
do not know of an algorithmic solution to a problem we also will not be able
to find a network which solves the problem. If one believes that the neural
network model is a good model (e.g. it is amenable to implementation with
optics), one should develop techniques to program the network to perform
an algorithm that is known to have some guaranteed good behavior.

Acknowledgement: Support of the U.S. Air Force Office of Scientific Research
is gratefully acknowledged.

References
[1] Y. Abu Mostafa, Neural Networks for Computing? in Neural Networks
for Computing, edited by J. Denker (AlP Conference Proceedings no. 151,
1986).
[2] J. Bruck and J. Sanz, A Study on Neural Networks, IBM Tech Rep, RJ
5403, 1986. To appear in International Journal of Intelligent Systems, 1988.
[3] J. Bruck and J. W. Goodman, A Generalized Convergence Theorem for
Neural Networks and its Applications in Combinatorial Optimization, IEEE
First ICNN, San-Diego, June 1987.
[4] M. R. Garey and D. S. Johnson, Computers and Intractability: A Guide to
the Theory of NP-Completeness, W. H. Freeman and Company, 1979.

143

[5] J. J. Hopfield, Neural Networks and Physical Systems with Emergent Collective Computational Abilities, Proc. Nat. Acad. Sci .. USA, Vol. 79, pp.
2554-2558, 1982.
[6] J. J. Hopfield and D. W. Tank, Neural Computations of Decisions in Optimization Problems, BioI. Cybern. 52, pp. 141-152, 1985.
[7] C. H. Papadimitriou and K. Steiglitz, On the Complexity of Local Search
for the Traveling Salesman Problem, SIAM J. on Comp., Vol. 6, No.1, pp.
76-83, 1977.
[8] C. H. Papadimitriou and K. Steiglitz, Combinatorial Optimization: Algo:rithms and Complexity, Prentice-Hall, Inc., 1982.
[9] J. C. Picard and H. D. Ratliff, Minimum Cuts and Related Problems, Networks, Vol 5, pp. 357-370, 1974.

"
2014,Real-Time Decoding of an Integrate and Fire Encoder,Poster,5622-real-time-decoding-of-an-integrate-and-fire-encoder.pdf,"Neuronal encoding models range from the detailed biophysically-based Hodgkin Huxley model, to the statistical linear time invariant model specifying firing rates in terms of the extrinsic signal. Decoding the former becomes intractable, while the latter does not adequately capture the nonlinearities present in the neuronal encoding system. For use in practical applications, we wish to record the output of neurons, namely spikes, and decode this signal fast in order to drive a machine, for example a prosthetic device. Here, we introduce a causal, real-time decoder of the biophysically-based Integrate and Fire encoding neuron model. We show that the upper bound of the real-time reconstruction error decreases polynomially in time, and that the L2 norm of the error is bounded by a constant that depends on the density of the spikes, as well as the bandwidth and the decay of the input signal. We numerically validate the effect of these parameters on the reconstruction error.","Real-Time Decoding of an Integrate and Fire Encoder
Shreya Saxena and Munther Dahleh
Department of Electrical Engineering and Computer Sciences
Massachusetts Institute of Technology
Cambridge, MA 02139
{ssaxena,dahleh}@mit.edu

Abstract
Neuronal encoding models range from the detailed biophysically-based Hodgkin
Huxley model, to the statistical linear time invariant model specifying firing rates
in terms of the extrinsic signal. Decoding the former becomes intractable, while
the latter does not adequately capture the nonlinearities present in the neuronal
encoding system. For use in practical applications, we wish to record the output
of neurons, namely spikes, and decode this signal fast in order to act on this signal,
for example to drive a prosthetic device. Here, we introduce a causal, real-time
decoder of the biophysically-based Integrate and Fire encoding neuron model. We
show that the upper bound of the real-time reconstruction error decreases polynomially in time, and that the L2 norm of the error is bounded by a constant that
depends on the density of the spikes, as well as the bandwidth and the decay of
the input signal. We numerically validate the effect of these parameters on the
reconstruction error.

1

Introduction

One of the most detailed and widely accepted models of the neuron is the Hodgkin Huxley (HH)
model [1]. It is a complex nonlinear model comprising of four differential equations governing
the membrane potential dynamics as well as the dynamics of the sodium, potassium and calcium
currents found in a neuron. We assume in the practical setting that we are recording multiple neurons
using an extracellular electrode, and thus that the observable postprocessed outputs of each neuron
are the time points at which the membrane voltage crosses a threshold, also known as spikes. Even
with complete knowledge of the HH model parameters, it is intractable to decode the extrinsic
signal applied to the neuron given only the spike times. Model reduction techniques are accurate in
certain regimes [2]; theoretical studies have also guaranteed an input-output equivalence between a
multiplicative or additive extrinsic signal applied to the HH model, and the same signal applied to
an Integrate and Fire (IAF) neuron model with variable thresholds [3].
Specifically, take the example of a decoder in a brain machine interface (BMI) device, where the
decoded signal drives a prosthetic limb in order to produce movement. Given the complications
involved in decoding an extrinsic signal using a realistic neuron model, current practices include
decoding using a Kalman filter, which assumes a linear time invariant (LTI) encoding with the extrinsic signal as an input and the firing rate of the neuron as the output [4?6]. Although extremely
tractable for decoding, this approach ignores the nonlinear processing of the extrinsic current by
the neuron. Moreover, assuming firing rates as the output of the neuron averages out the data and
incurs inherent delays in the decoding process. Decoding of spike trains has also been performed
using stochastic jump models such as point process models [7, 8], and we are currently exploring
relationships between these and our work.
1

f (t)

IAF Encoder

{ti }i:|ti |?t

Real-Time Decoder

f?t (t)

Figure 1: IAF Encoder and a Real-Time Decoder.
We consider a biophysically inspired IAF neuron model with variable thresholds as the encoding
model. It has been shown that, given the parameters of the model and given the spikes for all
time, a bandlimited signal driving the IAF model can be perfectly reconstructed if the spikes are
?dense enough? [9?11]. This is a Nyquist-type reconstruction formula. However, for this theory
to be applicable to a real-time setting, as in the case of BMI, we need a causal real-time decoder
that estimates the signal at every time t, and an estimate of the time taken for the convergence
of the reconstructed signal to the real signal. There have also been some approaches for causal
reconstruction of a signal encoded by an IAF encoder, such as in [12]. However, these do not show
the convergence of the estimate to the real signal with the advent of time.
In this paper, we introduce a causal real-time decoder (Figure 1) that, given the parameters of the
IAF encoding process, provides an estimate of the signal at every time, without the need to wait for
a minimum amount of time to start decoding. We show that, under certain conditions on the input
signal, the upper bound of the error between the estimated signal and the input signal decreases
polynomially in time, leading to perfect reconstruction as t ! 1, or a bounded error if a finite
number of iterations are used. The bounded input bounded output (BIBO) stability of a decoder is
extremely important to analyze for the application of a BMI. Here, we show that the L2 norm of the
error is bounded, with an upper bound that depends on the bandwidth of the signal, the density of
the spikes, and the decay of the input signal.
We numerically show the utility of the theory developed here. We first provide example reconstructions using the real-time decoder and compare our results with reconstructions obtained using
existing methods. We then show the dependence of the decoding error on the properties of the input
signal.
The theory and algorithm presented in this paper can be applied to any system that uses an IAF
encoding device, for example in pluviometry. We introduce some preliminary definitions in Section
2, and then present our theoretical results in Section 3. We use a model IAF system to numerically
simulate the output of an IAF encoder and provide causal real-time reconstruction in Section 4, and
end with conclusions in Section 5.

2

Preliminaries

?
We first define the subsets of the L2 space that we consider. L?
2 and L2, are defined as the following.
n
o
?(!) = 0 8! 2
L?
=
f
2
L
|
f
/
[
?,
?]
(1)
2
2
n
o
L?
=
f g 2 L2 | f?(!) = 0 8! 2
/ [ ?, ?]
(2)
2,

, where g (t) = (1+|t|) and f?(!) = (Ff )(!) is the Fourier transform of f . We will only consider
signals in L?
0.
2, for
Next, we define sinc? (t) and
of signals.

[a,b] (t),

both of which will play an integral part in the reconstruction

sinc? (t) =

[a,b] (t)

=

(
?

sin(?t)
?t

1

t 6= 0
t=0

(3)

1
0

t 2 [a, b]
otherwise

(4)

Finally, we define the encoding system based on an IAF neuron model; we term this the IAF Encoder.
We consider that this model has variable thresholds in its most general form, which may be useful if
2

Rt
it is the result of a model reduction technique such as in [3], or in approaches where tii+1 f (? )d?
can be calculated through other means, such as in [9]. A typical IAF Encoder is defined in the
following way: given the thresholds {qi } where qi > 0 8i, the spikes {ti } are such that
Z ti+1
f (? )d? = ?qi
(5)
ti

Rt
This signifies that the encoder outputs a spike at time ti+1 every time the integral ti f (? )d? reaches
the threshold qi or qi . We assume that the decoder has knowledge of the value of the integral
as well as the time at which the integral was reached. For a physical representation with neurons
whose dynamics can faithfully be modeled using IAF neurons, we can imagine two neurons with
the same input f ; one neuron spikes when the positive threshold is reached while the other spikes
when the negative threshold is reached. The decoder views the activity of both of these neurons
and, with knowledge of the corresponding thresholds, decodes the signal accordingly. We can also
take the approach of limiting ourselves to positive fn(t). In order tooremain general in the following
Rt
treatment, we assume that we have knowledge of tii+1 f (? )d? , as well as the corresponding
spike times {ti }.

3

Theoretical Results

The following is a theorem introduced in [11], which was also applied to IAF Encoders in [10,13,14].
We will later use the operators and concepts introduced in this theorem.
Theorem 1. Perfect Reconstruction: Given a sampling set {ti }i2Z and the corresponding samples
R ti+1
?
f (? )d? , we can perfectly reconstruct f 2 L?
ti ) = for some < ?
.
2 if supi2Z (ti+1
ti
Moreover, f can be reconstructed iteratively in the following way, such that
? ?k+1
?
k
kf f k2 ?
kf k2
(6)
?
, and limk!1 f k = f in L2 .
f0
f1

=
=

Af
(I A)f 0 + Af = (I

fk

=

(I

A)f k

1

+ Af =

A)Af + Af

k
X

n=0

, where the operator Af is defined as the following.
1 Z ti+1
X
Af =
f (? )d? sinc? (t
i=1

and si =

ti +ti+1
,
2

A)n Af

(I

si )

(7)
(8)
(9)

(10)

ti

the midpoint of each pair of spikes.

Proof. Provided in [11].
The above theorem requires an infinite number of spikes in order to start decoding. However, we
would like a real-time decoder that outputs the ?best guess? at every time t in order for us to act on
the estimate of the signal. In this paper, we introduce one such decoder; we first provide a high-level
description of the real-time decoder, then a recursive algorithm to apply in the practical case, and
finally we will provide error bounds for its performance.
Real-Time Decoder
At every time t, the decoder outputs an estimate of the input signal f?t (t), where f?t (t) is an estimate
of the signal calculated using all the spikes from time 0 to t. Since there is no new information
between spikes, this is essentially the same as calculating an estimate after every spike ti , f?ti (t),
and using this estimate till the next spike, i.e. for time t 2 [ti , ti+1 ] (see Figure 2).
3

f (t)

f?t1 (t)

f?t2 (t) = f?t1 (t) + gt2 (t)

f?t (t)

f?t3 (t)

0
t0

t1

t3

t2

t4

t5

t6

t7

t

Figure 2: A visualization of the decoding process. The original signal f (t) is shown in black and the
spikes {ti } are shown in blue. As each spike ti arrives, a new estimate f?ti (t) of the signal is formed
(shown in green), which is modified after the next spike ti+1 by the innovation function gti+1 . The
P
output of the decoder f?t (t) = i2Z f?ti (t) [ti ,ti+1 ) (t) is shown in red.
We will show that we can calculate the estimate after every spike f?ti+1 as the sum of the previous
estimate f?ti and an innovation gti+1 . This procedure is captured in the algorithm given in Equations
11 and 12.
Recursive Algorithm
f?t0i+1

=

f?t0i + gt0i+1

f?tki+1

=

f?tki + gtki+1 = f?tki + gtki+11 + gt0i+1

Here, f?t00 = 0, and gt0i+1 (t) =

?R

ti+1
ti

?

?
f (? )d? sinc(t

Ati+1 gtki+11

?

(11)
(12)

si ). We denote f?ti (t) = limk!1 f?tki (t) and

gti+1 (t) = limk!1 gtki+1 (t). We define the operator AT f used in Equation 12 as the following.
X Z ti+1
AT f =
f (? )d? sinc? (t si )
(13)
i:|ti |?T

ti

P
The output of our causal real-time decoder can also be written as f?t (t) = i2Z f?ti (t) [ti ,ti+1 ) (t).
In the case of a decoder that uses a finite number of iterations K at every step, i.e. calculates f?tKi
P
after every spike ti , the decoded signal is f?tK (t) = i2Z f?tKi (t) [ti ,ti+1 ) (t). {f?tki }k are stored after
every spike ti , and thus do not need to be recomputed at the arrival of the next spike. Thus, when a
new spike arrives at ti+1 , each f?tki can be modified by adding the innovation functions gtki+1 .
Next, we show an upper bound on the error incurred by the decoder.
Theorem 2. Real-time reconstruction: Given a signal f 2 L?
2, passed through an IAF encoder
with known thresholds, and given that the spikes satisfy a certain minimum density supi2Z (ti+1
ti ) = for some < ?
? , we can construct a causal real-time decoder that reconstructs a function
?
ft (t) using the recursive algorithm in Equations 11 and 12, s.t.
|f (t)

f?t (t)| ?

c
1

?
?

4

kf k2, (1 + t)

(14)

, where c depends only on , ? and .
Moreover, if we use a finite number of iterations K at every step, we obtain the following error.
? ?K+1
? K+1
1
1 + ??
?
?
|f (t) f?tK (t)| ? c
kf
k
(1
+
t)
+
kf k2
(15)
2,
?
?
?
1
1
?
?
Proof. Provided in the Appendix.
Theorem 2 is the main result of this paper. It shows that the upper bound of the real-time reconstruction error using the decoding algorithm in Equations 11 and 12, decreases polynomially as a function
of time. This implies that the approximation f?t (t) becomes more and more accurate with the passage
of time, and moreover, we can calculate the exact amount of time we would need to record to have a
given level of accuracy. Given a maximum allowed error ?, these bounds can provide a combination
(t, K) that will ensure |f (t) f?tK (t)| ? ? if f 2 L?
2, , and if the density constraint is met.
We can further show that the L2 norm of the reconstruction remains bounded with a bounded input (BIBO stability), by bounding the L2 norm of the error between the original signal and the
reconstruction.
Corollary 1. Bounded L2 norm: The causal decoder provided in Theorem 2, with the same assumptions and
the case of K ! 1, constructs a signal f?t (t) s.t. the L2 norm of the error
qin
p
R1
kf f?t k2 =
|f (t) f?t (t)|2 dt is bounded: kf f?t k2 ? c/ 2 ? 1 kf k2, where c is the same
0

1

constant as in Theorem 2.

?

Proof.
sZ

1
0

|f (t)

f?t (t)|2 dt

?

v
uZ
u
t

1
0

c
1

?
?

!2

kf k22, (1 + t)

2

dt =

p
c/ 2
1

1
?
?

kf k2,

(16)

Here, the first inequality is due to Theorem 2, and all the constants are as defined in the same.
Remark
q R 1: This result also implies that we have a decay in the root-mean-square (RMS) error, i.e.
T !1
1 T
f?t (t)|2 dt
! 0. For the case of a finite number of iterations K < 1, the RMS
T 0 |f (t)
error converges to a non-zero constant

? K+1 1+
?
1

?
?
?
?

kf k2 .

Remark 2: The methods used
in Corollary 1 also provide a bound on the error in the weighted L2
p
c/
?
norm, i.e. kf f k2, ? 1 ? 1 kf k2, for
2, which may be a more intuitive form to use for a
?
subsequent stability analysis.

4

Numerical Simulations

We simulated signals f (t) of the following form, for t 2 [0, 100], using a stepsize of 10
P50
wk (sinc? (t dk ))
f (t) = i=1 P50
i=1 wk

2

.
(17)

Here, the wk ?s and dk ?s were picked uniformly at random from the interval [0, 1] and [0, 100] respectively. Note that f 2 L2,? . All simulations were performed using MATLAB R2014a. For each
simulation experiment, at every time t we decoded using only the spikes before time t.
We first provide example reconstructions using the Real-Time Decoder for four signals in Figure 3,
using constant thresholds, i.e. qi = q 8i. We compare our results to those obtained using a Linear
Firing Rate (FR) Decoder, i.e. we let the reconstructed signal be a linear function of the number
of spikes in the past seconds, being the window size. We can see that there is a delay in the
reconstruction with this decoding approach. Moreover, the reconstruction is not as accurate as that
using the Real-Time Decoder.
5

0.1

0.08

0.08
Amplitude

Amplitude

0.1

0.06
0.04

0.04

0.02

0.02

0
0

20

40
60
Time (s)

0
0

80

0.1

0.1

0.08

0.08

0.06
0.04
0.02
0
0

20

40
60
Time (s)

0.04

0
0

80

20

40
60
Time (s)

80

(d) ? = 0.3?; Linear FR Decoder
0.1

0.08

0.08
Amplitude

Amplitude

80

0.02

(c) ? = 0.3?; Real-Time Decoder

0.06

0.04

0.02

0.06

0.04

0.02

20

40
60
Time (s)

0
0

80

(e) ? = 0.4?; Real-Time Decoder
0.07

0.07

0.06

0.06

0.05

0.05

Amplitude

0.08

0.04
0.03

80

0.03
0.02

0.01

0.01
40
60
Time (s)

40
60
Time (s)

0.04

0.02

20

20

(f) ? = 0.4?; Linear FR Decoder

0.08

0
0

40
60
Time (s)

0.06

0.1

0
0

20

(b) ? = 0.2?; Linear FR Decoder

Amplitude

Amplitude

(a) ? = 0.2?; Real-Time Decoder

Amplitude

0.06

0
0

80

(g) ? = 0.5?; Real-Time Decoder

20

40
60
Time (s)

80

(h) ? = 0.5?; Linear FR Decoder

Figure 3: (a,c,e,g) Four example reconstructions using the Real-Time Decoder, with the original
signal f (t) in black solid and the reconstructed signal f?t (t) in red dashed lines. Here, [ , K] =
[2, 500], and qi = 0.01 8i. (b,d,f,h) The same signal was decoded using a Linear Firing Rate (FR)
Decoder. A window size of = 3s was used.

6

?4

?4

x 10

3

2.5

x 10

2

!f ? f?t! 2
!f ! 2,?

!f ? f?t! 2
!f ! 2,?

2

1.5

1

1

0.5

0
0.1pi

0.2pi

?

0.3pi

(a) ? is varied; [ , , K] = [2,

0
0.6

0.4pi

0.8

1

1.2

1.4

1.6

?
(b) is varied; [?, , K] = [0.3?, 2, 500]

?
, 500]
2?

?4

2

x 10

?4

?6

10

!f ? f?t! 2
!f ! 2,?

!f ? f?t! 2
!f ! 2,?

10

1

?8

10

?10

10

2

2.5

3

3.5

4

4.5

0
0

5

?
(c)

is varied; [?, , K] = [0.3?,

100

200

300

400

500

K
1
0.3

(d) K is varied; [?, , ] = [0.3?, 53 , 2]

, 500]

Figure 4: Average error for 20 different signals while varying different parameters.

Next, we show the decay of the real-time error by averaging out the error for 20 different input
signals, while varying certain parameters, namely ?, , and K (Figure 4). The thresholds qi were
chosen to be constant a priori, but were reduced to satisfy the density constraint wherever necessary.
According to Equation 14 (including the effect of the constant c), the error should decrease as ? is
decreased. We see this effect in the simulation study in Figure 4a. For these simulations, we chose
such that ?? < 1, thus was decreasing as ? increased; however, the effect of the increasing ?
dominated in this case.
In Figure 4b we see that increasing while keeping the bandwidth constant does indeed increase the
error, thus the algorithm is sensitive to the density of the spikes. In this figure, all the values of
satisfy the density constraint, i.e. ?? < 1.
Increasing is seen to have a large effect, as seen in Figure 4c: the error decreases polynomially
in (note the log scale on the y-axis). Although increasing in our simulations also increased
the bandwidth of the signal, the faster decay had a larger effect on the error than the change in
bandwidth.
In Figure 4d, the effect of increasing K is apparent; however, this error flattens out for large values
of K, showing convergence of the algorithm.
7

5

Conclusions

We provide a real-time decoder to reconstruct a signal f 2 L?
2, encoded by an IAF encoder. Under
Nyquist-type spike density conditions, we show that the reconstructed signal f?t (t) converges to f (t)
polynomially in time, or with a fixed error that depends on the computation power used to reconstruct
the function. Moreover, we get a lower error as the spike density increases, i.e. we get better results
if we have more spikes. Decreasing the bandwidth or increasing the decay of the signal both lead to
a decrease in the error, corroborated by the numerical simulations. This decoder also outperforms
the linear decoder that acts on the firing rate of the neuron. However, the main utility of this decoder
is that it comes with verifiable bounds on the error of decoding as we record more spikes.
There is a severe need in the BMI community for considering error bounds while decoding signals
from the brain. For example, in the case where the reconstructed signal is driving a prosthetic, we are
usually placing the decoder and machine in an inherent feedback loop (where the feedback is visual
in this case). A stability analysis of this feedback loop includes calculating a bound on the error
incurred by the decoding process, which is the first step for the construction of a device that robustly
tracks agile maneuvers. In this paper, we provide an upper bound on the error incurred by the realtime decoding process, which can be used along with concepts in robust control theory to provide
sufficient conditions on the prosthetic and feedback system in order to ensure stability [15?17].
Acknowledgments
Research supported by the National Science Foundation?s Emerging Frontiers in Research and Innovation Grant (1137237).

References
[1] A. L. Hodgkin and A. F. Huxley, ?A quantitative description of membrane current and its
application to conduction and excitation in nerve,? The Journal of physiology, vol. 117, no. 4,
p. 500, 1952.
[2] W. Gerstner and W. M. Kistler, Spiking neuron models: Single neurons, populations, plasticity.
Cambridge university press, 2002.
[3] A. A. Lazar, ?Population encoding with hodgkin?huxley neurons,? Information Theory, IEEE
Transactions on, vol. 56, no. 2, pp. 821?837, 2010.
[4] J. M. Carmena, M. A. Lebedev, R. E. Crist, J. E. O?Doherty, D. M. Santucci, D. F. Dimitrov,
P. G. Patil, C. S. Henriquez, and M. A. Nicolelis, ?Learning to control a brain?machine interface for reaching and grasping by primates,? PLoS biology, vol. 1, no. 2, p. e42, 2003.
[5] M. D. Serruya, N. G. Hatsopoulos, L. Paninski, M. R. Fellows, and J. P. Donoghue, ?Brainmachine interface: Instant neural control of a movement signal,? Nature, vol. 416, no. 6877,
pp. 141?142, 2002.
[6] W. Wu, J. E. Kulkarni, N. G. Hatsopoulos, and L. Paninski, ?Neural decoding of hand motion using a linear state-space model with hidden states,? Neural Systems and Rehabilitation
Engineering, IEEE Transactions on, vol. 17, no. 4, pp. 370?378, 2009.
[7] E. N. Brown, L. M. Frank, D. Tang, M. C. Quirk, and M. A. Wilson, ?A statistical paradigm for
neural spike train decoding applied to position prediction from ensemble firing patterns of rat
hippocampal place cells,? The Journal of Neuroscience, vol. 18, no. 18, pp. 7411?7425, 1998.
[8] U. T. Eden, L. M. Frank, R. Barbieri, V. Solo, and E. N. Brown, ?Dynamic analysis of neural
encoding by point process adaptive filtering,? Neural Computation, vol. 16, no. 5, pp. 971?998,
2004.
[9] A. A. Lazar, ?Time encoding with an integrate-and-fire neuron with a refractory period,? Neurocomputing, vol. 58, pp. 53?58, 2004.
[10] A. A. Lazar and L. T. T?oth, ?Time encoding and perfect recovery of bandlimited signals,?
Proceedings of the ICASSP, vol. 3, pp. 709?712, 2003.
[11] H. G. Feichtinger and K. Gr?ochenig, ?Theory and practice of irregular sampling,? Wavelets:
mathematics and applications, vol. 1994, pp. 305?363, 1994.
8

[12] H. G. Feichtinger, J. C. Pr??ncipe, J. L. Romero, A. S. Alvarado, and G. A. Velasco, ?Approximate reconstruction of bandlimited functions for the integrate and fire sampler,? Advances in
computational mathematics, vol. 36, no. 1, pp. 67?78, 2012.
[13] A. A. Lazar and L. T. T?oth, ?Perfect recovery and sensitivity analysis of time encoded bandlimited signals,? Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 51, no. 10,
pp. 2060?2073, 2004.
[14] D. Gontier and M. Vetterli, ?Sampling based on timing: Time encoding machines on shiftinvariant subspaces,? Applied and Computational Harmonic Analysis, vol. 36, no. 1, pp. 63?78,
2014.
[15] S. V. Sarma and M. A. Dahleh, ?Remote control over noisy communication channels: A firstorder example,? Automatic Control, IEEE Transactions on, vol. 52, no. 2, pp. 284?289, 2007.
[16] ??, ?Signal reconstruction in the presence of finite-rate measurements: finite-horizon control
applications,? International Journal of Robust and Nonlinear Control, vol. 20, no. 1, pp. 41?58,
2010.
[17] S. Saxena and M. A. Dahleh, ?Analyzing the effect of an integrate and fire encoder and decoder
in feedback,? Proceedings of 53rd IEEE Conference on Decision and Control (CDC), 2014.

9

"
1992,History-Dependent Attractor Neural Networks,,627-history-dependent-attractor-neural-networks.pdf,Abstract Missing,"History-dependent Attractor Neural
Networks
Isaac Meilijson
Eytan Ruppin
School of Mathematical Sciences
Raymond and Beverly Sackler Faculty of Exact Sciences
Tel-A viv University, 69978 Tel-Aviv, Israel.

Abstract
We present a methodological framework enabling a detailed description of the performance of Hopfield-like attractor neural networks (ANN) in the first two iterations. Using the Bayesian approach, we find that performance is improved when a history-based
term is included in the neuron's dynamics. A further enhancement
of the network's performance is achieved by judiciously choosing
the censored neurons (those which become active in a given iteration) on the basis of the magnitude of their post-synaptic potentials. The contribution of biologically plausible, censored, historydependent dynamics is especially marked in conditions of low firing
activity and sparse connectivity, two important characteristics of
the mammalian cortex. In such networks, the performance attained is higher than the performance of two 'independent' iterations, which represents an upper bound on the performance of
history-independent networks.

1

Introduction

Associative Attractor Neural Network (ANN) models provide a theoretical background for the understanding of human memory processes. Considerable effort has
been devoted recently to narrow the gap between the original ANN Hopfield model
(Hopfield 1982) and the realm of the structure and dynamics of the brain (e.g.,
Amit & Tsodyks 1991). In this paper, we contribute to the examination of the
performance of ANNs under cortical-like architectures, where neurons are typically
572

History-dependent Attractor Neural Networks

connected to only a fraction of their neighboring neurons, and have a low firing
activity (Abeles et. al. 1990). We develop a general framework for examining various signalling mechanisms (firing functions) and activation rules (the mechanism
for deciding which neurons are active in some interval of time).
The Hopfield model is based on memoryless dynamics, which identify the notion of
'post-synaptic potential' with the input field received by a neuron from the neurons
active in the current iteration. We follow a Bayesian approach under which the
neuron's signalling and activation decisions are based on the current a-posteriori
probabilities assigned to its two possible true memory states, ?1. As we shall
see, the a-posteriori belief in +1 is the sigmoidal function evaluated at a neuron's
generalized field, a linear combination of present and past input fields. From a
biological perspective, this history-dependent approach is strongly motivated by
the observation that the time span of the different channel conductances in a given
neuron is very broad (see Lytton 1991 for a review). While some channels are active
for only microseconds, some slow-acting channels may remain open for seconds.
Hence, a synaptic input currently impending on the neuron may influence both its
current post-synaptic membrane potential, and its post-synaptic potential at some
future time.

2

The Model

The neural network model presented is characterized as follows. There are m 'random memories' elJ , 1 < iJ ::; m, and one 'true' memorye m +1 = e. The (m + 1)N
entries of these memories are independent and identically distributed, with equally
likely values of +1 or --1. The initial state X has similarityP(Xi ei)
(1+?)/2,
P(Xi = -ed = (1 - ?)/2, independently of everything else. The weight of the
synaptic connection between neurons i and j (i -j:. j) is given by the simple Hebbian
law

= =

m+l

Wij* =

L elJielJ j

(1)

1J=1

Each neuron receives incoming synaptic connections from a random choice of K of
the N neurons in the network in such a way that if a synapse exists, the synapse
in the opposite direction exists with probability r, the reflexivity parameter. In the
first iteration, a random sample of Ll neurons become active (i.e., 'fire'), thus on
the average nl = LlK/N neurons update the state of each neuron. The field //1)
of neuron i in the first iteration is
N

j I.(l) -- ~ ~
~

w.IJ..* [..I?(l)X?
IJ 1
J'

(2)

nl j=1

where Iij denotes the indicator function of the event 'neuron i receives a synaptic
t ) denotes the indicator function of the event
connection from neuron j', and
'neuron j is active in the t'th iteration'. Under the Bayesian approach we adopt,
neuron i assigns an a-priori probability ,\/0) = p(ei = +1\Xi)
(1 + {Xi)/2 to
having +1 as the correct memory state and evaluates the corresponding a-posteriori
probability ,\/1) = p(ei = +1\Xi , fi(I), which turns out to be expressible as the

I/

=

573

574

Meilijson and Ruppin

sigmoidal function 1/( 1 + exp( -2x)) evaluated at some linear combination of Xi
and fi(I).
In the second iteration the belief A/I) of a neuron determines the probability that
the neuron is active. We illustrate two extreme modes for determining the active
updating neurons, or activation: the random case where L2 active neurons a.re
randomly chosen, independently of the strength of their fields, and the censored
case, which consists of selecting the L2 neurons whose belief belongs to some set.
The most appealing censoring rule from the biological point of view is tail-censoring,
where the active neurons are those with the strongest beliefs. Performance, however,
is improved under interval-censoring, where the active neurons are those with midrange beliefs, and even further by combining tail and interval censoring into a hybrid
rule.
Let n2 = L2 f{ / N . The activation rule is given by a function C : [~, 1] -+ [0, 1] .
Neuron j, with belief A/I) in +1, becomes active with probability C(maxp/I), 1Aj (1?)), independently of everything else. For example, the random case corresponds
to C
and the tail-censored case corresponds to C(A) = 1 or 0 depending on
whether max(A, 1 - A) exceeds some threshold. The output of an active neuron j
is a signal function S(A/ I ?) of its current belief. The field f/ 2 ) of neuron i in the
second iteration is

=In

N

f ,?(2) -- ~ """"
L...J w.I}.. * /-I }?I}?(2)S(A } ?(1?)
n2

?

(3)

j=1

Neuron i now evaluates its a-posteriori belief
Ai(2) = p(ei = +1IXi,Ii(l),f/I\I/2?). As we shall see, Ai(2) is, again, the
sigmoidal function evaluated at some linear combination of the neuron 's history
Xi, XJi(I), f/ I ) and 1/ 2). In contrast to the common history-independent Hopfield
dynamics where the signal emitted by neuron j in the t'th iteration is a function
of /jet-I) only, Bayesian history-dependent dynamics involve signals and activation
rules which depend on the neuron's generalized field, obtained by adaptively incorporating /j(t-I) to its previous generalized field. The final state X/ 2 ) of neuron i
is taken as -lor +1, depending on which of 1 - A/ 2 ) and A/ 2 ) exceeds 1/2.
For nI/N, ndN, m/N, K/N constant, and N large, we develop explicit expressions
for the performance of the network, for any signal function (e.g., SI(A) = Sgn(A1/2) or S2(A) = 2A - 1) and activation rule. Performance is measured by the final
overlap e"" = ~ L eiX/2) (or equivalently by the final similarity (1+e"")/2). Various
possible combina.tions of activation modes and signal functions described above are
then examined under varying degrees of connectivity and neuronal activity.

3

Single-iteration optimization: the Bayesian approach

Consider the following well known basic fact in Bayesian Hypothesis Testing,
Lemma 1
Express the prior probability as
1
= 1) = ---:-1 + e- 2x

pee

(4)

History-dependent Attractor Neural Networks

and assume an observable Y which, given

e, is distributed according to

Yle"", N(lle, (12)

(5)

~or some constants Il E (-00,00) and (12 E (0,00). Then the posterior probability
IS

P(CI,

1

= llY = y) = 1 + e-2?
) ).
x+ 1-'/0'2 y

Applying this Lemma to Y =

~i(1)

fi(I),

with Il = e and (12

= .ill.
nl

(6)

=al, we see that

= p(ei = 11Xi , fi(I)) = __
---:-_1_--:-:---:1 + e- 2f (""Y(f)X,+f,(1)/0:'1)

,

if

(7)

where i( f) = log ~:!:~. Hence, pee = 11Xi , f/ 1 )) > 1/2 if and only if 1/1)
an( f)Xi > O. The single-iteration performance is then given by the similarity

+

(8)

1;f~ (;."" +1(f)fo1) + 1; '~ (;."" -1(f)fo1)
= Q(e, at)
where <I> is the standard normal distribution function. The Hopfield dynamics, modified by redefining W ii as mi(e) (in the Neural Network terminology) is equivalent
(in the Bayesia.n jargon) to the obvious optimal policy, under which a neuron sets
for itself the sign with posterior probability above 1/2 of being correct.

4

Two-iterations optiInization

For mathematical convenience, we will relate signals and activation rules to normalized generalized fields rather than to beliefs . We let

h ( x ) -- S(1

1

+ e- 2CX ) , p( x ) --

C

(( + 1
max

1

1))

e- 2cx ' 1 - 1 + e- 2cx

(9)

for C = f/ -Jal. The signal function h is assumed to be odd, and the activation
function p, even .
In order to evaluate the belief ~/2), we need the conditional distribution of li(2)
given Xi, 1/ 1 ) and 1/1), for ei = -1 or ei = + 1. We adopt the working a.ssumption
that the pair of random variables (Ii (1), h(2)) has a bivariate normal distribution
1/ 1 ) and Xi, with
1/ 1 ) and Xi affecting means but not va.riances or
given
correlations. Under this working assumption, fi(2) is conditionally normal given
(ei,1/ 1 ),Xi,I/ 1 )), with constant variance and a mean which we will identify. This
working assumption allows us to model performance via the following well known
regression model.

ei,

ei,

575

576

Meilijson and Ruppin

Lemma 2
If two random variables U and V with finite variances are such that E(VIU) is a
linear function of U and Var(VIU) is constant, then
E(VIU) = E(V)

+ Cov(U, V) (U Var(U)

E(U?

(10)

and

Letting U = fi(l) and V = f/ 2), we obtain

A/ 2 )

= pee; = llXi, h(l>, li(l), 1/2 ? =

(12)

1

1 + exp{ -2 [E (// 1)la1 + {(E)X i ) +

f*;;af

(fi(2) - bXa/ 1) - a fi (1?)]}

-

1

1 + exp{ -2 [

(q(

E) -

b(f*T;af) I/ 1

?) Xi + (afl - a(f:;af?) li(l) + f* ;;af 1/ 2 )] }

which is the sigmoidal function evaluated at some generalized field. Expression (12)
shows that the correct definition of a final state Xi (2), as the most likely value among
+1 or -1, is
_ b(E"" - af)I.Cl?) . (~_ a(E"" - aE?)
X ,.(2) -_ Sgn [( E{ ()
f
2
I
X, +
2
al

T

T

J."".(1) + E"" - 2 aE f .(2)j

h

I

T

(13)
and the performance is given by

p(X/2) =

eilei) =

1; (R +
E4>

{(E)N)

1 (R -{(E)~)

+ ~ E4>

=
(14)

Q( E, a"")
where the one-iteration performance function Q is defined by (8), and
m
a "" =m
n""

(15)

We see that the performance is conveniently expressed as the single-iteration optimal
performance, had this iteration involved n"" rather than nl sampled neurons. This
formula yields a numerical and analytical tool to assess the network 's performance
with different signal functions, activation rules and architectures. Due to space
restrictions, the identification of the various parameters used in the above formulas
is not presented. However, it can be shown that in the sparse limit arrived at
by fixing al and a2 and letting both J{ 1m and N IJ{ go to infinity, it is always
better to replace an iteration by two smaller ones. This suggests that Bayesian

History-dependent Attractor Neural Networks

updating dynamics should be essentially asynchronous.
two-iterations performance Q

(c, -L+-L(~)2)
CIt

1

cw2

We also show that the

is superior to the performance

~

Q (2Q( c, al) - 1, (2) of two independent optimal single iterations.

Heuristics on activation and signalling

5

U

1

-1

-4

Figure 1: A typical plot of R(x) = ?l(X)/?O(x). Network parameters are N
K
500, n1
n2
50 and m 10.

=

= =

=

= 500,

By (14) and (15), performance is mostly determined by the magnitude of (col< - ac)2.
It can be shown that
(16)
and

~a =

1

00

(17)

p(x)?o(x)dx

where ?l and ?o are some specific linear combinations of Gaussian densities and
their derivatives , and ~ a = n2/ K is the activity level. High performance is achieved
by maximizing over p and possibly over h the absolute value of expression (16)
~eeping (17) fixed. In complete analogy to Hypothesis ~esting in Statistics, where
Wa takes the role of level of significance and (c? - ac)wa the role of power, p(x)
should be 1 or a (activate the neuron or don't) depending on whether the field
value x is such that the likelihood ratio h(X)?l(X)/?O(x) is above or below a given
threshold, determined by (17). Omitting details, the ratio R(x)
?l(X)/?O(x)
looks as in figure 1, and converges to -00 as x --+ 00 .

=

We see that there are three reasonable ways to make the ratio h(X)?l(X)/?O(x)
large: we can take a negative threshold such as t1 in figure 1, activate all neurons
with generalized field exceeding /33 (tail-censoring) and signal hex) = -Sgn(x),

577

578

Meilijson and Ruppin

or take a positive threshold such as t2 and activate all neurons with field value
between /31 and /32 (interval-censoring) and signal h(x) = Sgn(x). Better still, we
can consider the hybrid signalling-censoring rule: Activate all neurons with absolute
field value between /31 and /32, or beyond /33' The first group should signal their
preferred sign, while those in the second group should signal the sign opposite to
the one they so strongly believe in !

6

Numerical results
Performance
Random activation
Tail censoring
Intervalj'Hybrid censoring
Hopfield - zero diagonal
Independent ,( () diagonal
Independent zero diagonal

predicted
0.955
0.972
0.975
-

0.96
0.913

Table 1: Sparsely connected, low activity network: N
20,m = 5.
I

?

experimental
0.951
0.973
0.972
0.902,0.973
-

I i '

= 1500, J{ = 50, nl = n2 =
'.-1 :)..~

i

../

. ??f

....:...... i

.~.

,

'

i ..... ~. . . . . . .

0.95

~
','

....
~.

............................ .
.

""'

..

.......?( ........
-'-...

.' .

0.85

.....

- - First Iteration
.... Random Activation
> ? ? ?? Tail-censoring
.... .- .. Interval-censoring
.......: Hybrid censoring-signalling

0.80 L-~_--'-_~_'--~_-'-_-'----.J'--~_--'
20000.0
40000.0
60000.0
80000.0 100000.0
0.0
K

Figure 2: Performance of a large-scale cortical-like 'columnar' ANN, at different
values of connectivity J{, for initial similarity 0.75. N = 10 5 , nl = n2 = 200,
m = 50. The horizontal line denotes the performance of a single iteration.

History-dependent Attractor Neural Networks

Our theoretical performance predictions show good correspondence with simulation results, already at fairly small-scale networks. The superiority of historydependent dynamics is apparent. Table 1 shows the performance achieved in a
sparsely-connected network. The predicted similarity after two iterations is reported, starting from initial similarity 0.75, and compared with experimental results
averaged over 100 trials.
Figure 2 illustrates the theoretical two-iterations performance of large, low-activity
'cortical-like' networks, as a function of connectivity. We see that interval-censoring
can maintain high performance throughout the connectivity range. The performance of tail-censoring is very sensitive to connectivity, almost achieving the performance of interval censoring at a narrow low-connectivity range, and becoming
optimal only at very high connectivity. The superior hybrid rule improves on the
others only under high connectivity. As a cortical neuron should receive the concomitant firing of about 200 - 300 neurons in order to be activated (Treves & Rolls
1991), we have set n = 200. We find that the optimal connectivity per neuron, for
biologically plausible tail-censoring activation, is of the same order of magnitude as
actual cortical connectivity. The actual number nN/ K of neurons firing in every
iteration is about 5000, which is in close correspondence with the evidence suggesting that about 4% of the neurons in a module fire at any given moment (Abeles et.
a!. 1990).

References
[1] J.J. Hopfield. Neural networks and physical systems with emergent collective
abilities. Proc. Nat. Acad. Sci. USA, 79:2554,1982.
[2] D. J. Amit and M. V. Tsodyks. Quantitative study of attractor neural network retrieving at low spike rates: I. substrate-spikes, rates and neuronal gain.
Network, 2:259-273, 1991.
[3] M. Abeles, E. Vaadia, and H. Bergman. Firing patterns of single units in the
prefrontal cortex and neural network models. Network, 1:13-25, 1990.
[4] W. Lytton. Simulations of cortical pyramidal neurons synchronized by inhibitory
interneurons. J. Neurophysiol., 66(3):1059-1079, 1991.
[5] A. Treves and E. T. Rolls. What determines the capacity of autoassociative
memories in the brain? Network, 2:371-397, 1991.

579

"
1994,Reinforcement Learning Methods for Continuous-Time Markov Decision Problems,,889-reinforcement-learning-methods-for-continuous-time-markov-decision-problems.pdf,Abstract Missing,"Reinforcement Learning Methods for
Continuous-Time Markov Decision
Problems

Steven J. Bradtke
Computer Science Department
University of Massachusetts
Amherst, MA 01003
bradtkeGcs.umass.edu

Michael O. Duff
Computer Science Department
University of Massachusetts
Amherst, MA 01003
duffGcs.umass.edu

Abstract
Semi-Markov Decision Problems are continuous time generalizations of discrete time Markov Decision Problems. A number of
reinforcement learning algorithms have been developed recently
for the solution of Markov Decision Problems, based on the ideas
of asynchronous dynamic programming and stochastic approximation. Among these are TD(,x), Q-Iearning, and Real-time Dynamic
Programming. After reviewing semi-Markov Decision Problems
and Bellman's optimality equation in that context, we propose algorithms similar to those named above, adapted to the solution of
semi-Markov Decision Problems. We demonstrate these algorithms
by applying them to the problem of determining the optimal control for a simple queueing system. We conclude with a discussion
of circumstances under which these algorithms may be usefully applied.

1

Introduction

A number of reinforcement learning algorithms based on the ideas of asynchronous
dynamic programming and stochastic approximation have been developed recently
for the solution of Markov Decision Problems. Among these are Sutton's TD(,x)
[10], Watkins' Q-Iearning [12], and Real-time Dynamic Programming (RTDP) [1,

394

Steven Bradtke, Michael O. Duff

3]. These learning alogorithms are widely used, but their domain of application
has been limited to processes modeled by discrete-time Markov Decision Problems
(MDP's).
This paper derives analogous algorithms for semi-Markov Decision Problems
(SMDP's) - extending the domain of applicability to continuous time. This effort was originally motivated by the desire to apply reinforcement learning methods
to problems of adaptive control of queueing systems, and to the problem of adaptive
routing in computer networks in particular. We apply the new algorithms to the
well-known problem of routing to two heterogeneous servers [7]. We conclude with
a discussion of circumstances under which these algorithms may be usefully applied.

2

Semi-Markov Decision Problems

A semi-Markov process is a continuous time dynamic system consisting of a countable state set, X, and a finite action set, A. Suppose that the system is originally
observed to be in state z EX, and that action a E A is applied. A semi-Markov
process [9] then evolves as follows:
? The next state, y, is chosen according to the transition probabilities Pz,(a)
? A reward rate p(z, a) is defined until the next transition occurs
? Conditional on the event that the next state is y, the time until the transition from z to y occurs has probability distribution Fz,(?Ja)
One form of the SMDP is to find a policy the minimizes the expected infinite horizon
discounted cost, the ""value"" for each state:

e {IoOO e-.B t p(z(t), a(t?dt},
where z(t) and aCt) denote, respectively, the state and action at time t.
For a fixed policy 71', the value of a given state z must satisfy

v,..(z)

L P ,(7I'(z? 10(00 10re-.B? p(z, 71'(z?dsdFz,(tJ7I'(z? +
z

,E X

L

Pz,(7I'(Z? fooo e-.B t V,..(y)dFz,(tJ7I'(z?.

(1)

,EX
Defining

R(z, y, a)

= foOO fot e-.B? p(z, 71'(z?dsdFz, (tJ7I'(z?,

the expected reward that will be received on transition from state z to state y on
action a, and

Reinforcement Learning Methods for Continuous-Time Markov Decision Problems

395

the expected discount factor to be applied to the value of state y on transition
from state z on action a, it is clear that equation (1) is nearly identical to the
value-function equation for discrete time Markov reward processes,

Vw(z)

= R(z, 1I""(z? + ""Y I: Pzr (1I""(z?Vw (Y),

(2)

rEX
where R(z, a) = :ErEx Pzr(a)R(z, y, a). If transition times are identically one for
an SMDP, then a standard discrete-time MDP results.
Similarly, while the value function associated with an optimal policy for an MDP
satisfies the Bellman optimality equation

I:

Ve(z) = max {R(Z' a) + ""Y
pzr(a)v*(y)} ,
ilEA
X
rE

(3)

the optimal value function for an SMDP satisfies the following version of the Bellman
optimality equation:

V*(z)

=

max {
ilEA

I:X Pzr(a) 1
re-fJa p(z, a)dsdFzr(tJa)
0 10
00

+

rE

I: Pzr(a) loo e-fJtv*(y)dFzr(tJa)} .

(4)

rEX

3

Temporal Difference learning for SMDP's

Sutton's TD(O) [10] is a stochastic approximation method for finding solutions to
the system of equations (2). Having observed a transition from state z to state y
with sample reward r(z, y, 1I""(z?, TD(O) updates the value function estimate V(A:)(z)
in the direction of the sample value r(z, y, 1I""(z?+""YV(A:)(y). The TD(O) update rule
for MDP's is
V(A:+l)(Z) = V(A:)(z) + QA:[r(z, y, 1I""(z? + ""YV(A:)(y) - V(A:)(z)],
(5)
where QA: is the learning rate. The sequence of value-function estimates generated
by the TD(O) proceedure will converge to the true solution, Vw , with probability
one [5,8, 11] under the appropriate conditions on the QA: and on the definition of the
MDP.
The TD(O) learning rule for SMDP's, intended to solve the system of equations (1)
given a sequence of sampled state transitions, is:
1
-fJT
]
V(A:+1)(z) = V(A:)(z) + QA: [ - ;
r(z, y, 1I""(z? + e-fJTV(A:)(y) - V(A:)(z) , (6)

where the sampled transition time from state z to state y was T time units,
r(z, y, 1I""(z? is the sample reward received in T time units, and e- fJT is the
sample discount on the value of the next state given a transition time of T time
units. The TD(>.) learning rule for SMDP's is straightforward to define from here.

I_p-tl.

Steven Bradtke. Michael 0. Duff

396

4

Q-Iearning for SMDP's

Denardo [6] and Watkins [12] define Q.f) the Q-function corresponding to the policy
as
(7)
Q'II""(z, a) = R(z, a) + 'Y
PzJ(a)V'II""(Y)

71"",

2:

YEX

Notice that a can be any action. It is not necesarily the action 7I""(z) that would be
chosen by policy 71"". The function Q. corresponds to the optimal policy. Q'II""(z, a)
represents the total discounted return that can be expected if any action is taken
from state z, and policy 71"" is followed thereafter. Equation (7) can be rewritten as
Q'II""(z, a) = R(z, a) + 'Y

2: PZJ(a)Q'II""(Y' 7I""(Y?,

(8)

yEX

and Q. satisfies the Bellman-style optimality equation

Q?(z, a)

= R(z, a) + 'Y 2: Pzy(a) max Q.(y, a'),
JEX

(9)

A'EA

Q-Iearning, first described by Watkins [12], uses stochastic approximation to iteratively refine an estimate for the function Q ?. The Q-Iearning rule is very similar to
TD(O). Upon a sampled transition from state z to state y upon selection of a, with
sampled reward r(z, y, a), the Q-function estimate is updated according to
Q(A:+l)(Z, a) = Q(J:)(z, a) + etJ: [r(z, y, a) + 'Y ~~ Q(J:)(y, a') - Q(J:)(z, a)].

(10)

Q-functions may also be defined for SMDP's. The optimal Q-function for an SMDP
satisfies the equation

Q?(z, a)

2: PZJ(a) roo t

10 10

'V

JE""-

e- tJ ? p(z, a)dsdFzJ(tla) +

2: Pz1I (a) roo e- tJt max Q.(y, a')dFzJ(tla).
10

'V

JE""-

(11)

A'EA

This leads to the following Q-Iearning rule for SMDP's:
Q(A:+l)(Z, a) = Q(J:)(z, a)+etJ:

[1 -

;-tJ'r' r(z, y, a) + e-tJ'r'

~~ Q(J:)(y, a') _ Q(J:)(z, a)]
(12)

5

RTDP and Adaptive RTDP for SMDP's

The TD(O) and Q-Iearning algorithms are model-free, and rely upon stochastic
approximation for asymptotic convergence to the desired function (V'll"" and Q., respectively). Convergence is typically rather slow. Real-Time Dynamic Programming (RTDP) and Adaptive RTDP [1,3] use a system model to speed convergence.

Reinforcement Learning Methods for Continltolts-Time Markov Decision Problems

397

RTDP assumes that a system model is known a priori; Adaptive RTDP builds a
model as it interacts with the system. As discussed by Barto et al. [1], these asynchronous DP algorithms can have computational advantages over traditional DP
algorithms even when a system model is given.
Inspecting equation (4), we see that the model needed by RTDP in the SMDP
domain consists of three parts:
1. the state transition probabilities

Pzy(a),

2. the expected reward on transition from state z to state y using action a,
R(z, y, a), and
3. the expected discount factor to be applied to the value of the next state on
transition from state z to state y using action a, 'Y(z, y, a).
If the process dynamics are governed by a continuous time Markov chain, then the
model needed by RTDP can be analytically derived through uniJormization [2]. In
general, however, the model can be very difficult to analytically derive. In these
cases Adaptive RTD P can be used to incrementally build a system model through
direct interaction with the system. One version of the Adaptive RTDP algorithm
for SMDP's is described in Figure 1.
1
2
3
4

Set k = 0, and set Zo to some start state.
Initialize P, R, and ~.
repeat forever {
For all actions a, compute

Q(Ie)(ZIe,a) =

L P..""v(a) [ R(zIe,y,a) +~(zIe,y,a)V(Ie)(y) ]
veX

Perform the update V(le+l)(ZIe) = minoeA Q(Ie)(zIe,a)
Select an action, ale.
Perform ale and observe the transition to ZIe+l after T time units. Update
P. Use the sample reward 1__;;11'"" r(ZIe,Zle+l,ale) and the sample discount

5
6
7

factor e- f3T to update
k=k+l

8

9

R and ~.

}

Figure 1: Adaptive RTDP for SMDP's.
by Adaptive RTDP of P, R, and 'Y.

P, il, and .y are the estimates maintained

Notice that the action selection procedure (line 6) is left unspecified. Unlike RTDP,
Adaptive RTDP can not always choose the greedy action. This is because it only has
an e8timate of the system model on which to base its decisions, and the estimate
could initially be quite inaccurate. Adaptive RTDP needs to explore, to choose
actions that do not currently appear to be optimal, in order to ensure that the
estimated model converges to the true model over time.

398

6

Steven Bradtke, Michael O. Duff

Experiment: Routing to two heterogeneous servers

Consider the queueing system shown in Figure 2. Arrivals are assumed to be Poisson
with rate ).. Upon arrival, a customer must be routed to one of the two queues,
whose servers have service times that are exponentially distributed with parameters
J.l.1 and J.l.2 respectively. The goal is compute a policy that minimizes the objective
function:

e {foOO e-tJ t [c1n1(t) + C2n2(t)]dt},

where C1 and C2 are scalar cost factors, and n1(t) and n2(t) denote the number of
customers in the respective queues at time t. The pair (n1(t), n2(t)) is the state of
the system at time t; the state space for this problem is countably infinite. There
are two actions available at every state: if an arrival occurs, route it to queue 1 or
route it to queue 2.

-.<-~
___

-.J~

Figure 2: Routing to two queueing systems.

It is known for this problem (and many like it [7]), that the optimal policy is a
threshold policy; i.e., the set of states Sl for which it is optimal to route to the
first queue is characterized by a monotonically nondecreasing threshold function F
via Sl
{(nl,n2)ln1 $ F(n2)}' For the case where C1
C2
1 and J.l.1
J.l.2,
the policy is simply to join the shortest queue, and the theshold function is a line
slicing diagnonally through the state space.

=

= =

=

We applied the SMDP version of Q-Iearning to this problem in an attempt to find
the optimal policy for some subset of the state space. The system parameters were
set to ).
J.l.1 J.l.2 1, /3 0.1, and C1 C2 1. We used a feedforward neural
network trained using backpropagation as a function approximator.

= = =

=

= =

Q-Iearning must take exploratory actions in order to adequately sample all of the
available state transitions. At each decision time k, we selected the action aA: to be
applied to state ZA: via the Boltzmann distribution

where TA: is the ""computational temperature."" The temperature is initialized to a
relatively high value, resulting in a uniform distribution for prospective actions. TA:
is gradually lowered as computation proceeds, raising the probability of selecting
actions with lower (and for this application, better) Q-values. In the limit, the action
that is greedy with respect to the Q-function estimate is selected. The temperature
and the learning rate erA: are decreased over time using a ""search then converge""
method [4].

Reinforcement Learning Methods for Continuous-Time Markov Decision Problems

399

Figure 3 shows the results obtained by Q-Iearning for this problem. Each square
denotes a state visited, with nl(t) running along the z-axis, and n2(t) along the yaxis. The color of each square represents the probability of choosing action 1 (route
arrivals to queue 1). Black represents probability 1, white represents probability o.
An optimal policy would be black above the diagonal, white below the diagonal,
and could have arbitrary colors along the diagonal.

==
==
==
==
== =
=

;;=
==
==
== =
=

==
;;=
==
==
== =
=

? .
??

II II

?? ...

!m il lUll
it ?

w?

?

@@ ~d r2

E M m@ mi?1@

moo mllw

? ? ? ? ? ? ,.m @ll

A

;;;;;;

.

..
?

~2

III

l1li 0

@oo

@ %@Ii

Ell =11111
m??
? ??
IIIIJI III
??
????????
@w liliiii w????????
'

@ m ? ? ? ? ? ? ? ? ? '.

B

II

??

mm

oow mw m]lw
mill lUll lM]lm
??
? ? ? ? ? ? ? ? lIlIg!WI'm
=
wji
n i?????????? II

...'.'........'.
c

Figure 3: Results of the Q-Iearning experiment. Panel A represents the policy after
50,000 total updates, Panel B represents the policy after 100,000 total updates, and
Panel C represents the policy after 150,000 total updates.
One unsatisfactory feature of the algorithm's performance is that convergence is
rather slow, though the schedules governing the decrease of Boltzmann temperature
TA: and learning rate 0A: involve design parameters whose tweakings may result in
faster convergence. If it is known that the optimal policies are of theshold type,
or that some other structural property holds, then it may be of extreme practical
utility to make use of this fact by constraining the value-functions in some way or
perhaps by representing them as a combination of appropriate basis vectors that
implicity realize or enforce the given structural property.

7

Discussion

In this paper we have proposed extending the applicability of well-known reinforcement learning methods developed for discrete-time MDP's to the continuous time
domain. We derived semi-Markov versions of TD(O), Q-Iearning, RTDP, and Adaptive RTDP in a straightforward way from their discrete-time analogues. While we
have not given any convergence proofs for these new algorithms, such proofs should
not be difficult to obtain if we limit ourselves to problems with finite state spaces.
(Proof of convergence for these new algorithms is complicated by the fact that, in
general, the state spaces involved are infinite; convergence proofs for traditional
reinforcement learning methods assume the state space is finite.) Ongoing work
is directed toward applying these techniques to more complicated systems, examining distributed control issues, and investigating methods for incorporating prior

400

Steven Bradtke, Michael 0. Duff

knowledge (such as structured function approximators).
Acknowledgements
Thanks to Professor Andrew Barto, Bob Crites, and to the members of the Adaptive
Networks Laboratory. This work was supported by the National Science Foundation
under Grant ECS-9214866 to Professor Barto.

References
[1] A. G. Barto, S. J. Bradtke, and S. P. Singh. Learning to act using real-time
dynamic programming. Artificial Intelligence. Accepted.
[2] D. P. Bertsekas. Dynamic Programming: Deterministic and Stochastic Models.
Prentice Hall, Englewood Cliffs, NJ, 1987.
[3] S. J. Bradtke. Incremental Dynamic Programming for On-line Adaptive Optimal Control. PhD thesis, University of Massachusetts, 1994.
[4] C. Darken, J. Chang, and J. Moody. Learning rate schedules for faster stochastic gradient search. In Neural Networks for Signal Processing ~ - Proceedings
of the 199~ IEEE Workshop. IEEE Press, 1992.
[5] P. Dayan and T. J. Sejnowski. Td(A): Convergence with probability 1. Machine
Learning, 1994.
[6] E. V. Denardo. Contraction mappings in the theory underlying dynamic programming. SIAM Review, 9(2):165-177, April 1967.
[7] B. Hajek. Optimal control of two interacting service stations.
29:491-499, 1984.

IEEE-TAC,

[8] T. Jaakkola, M. I. Jordan, and S. P. Singh. On the convergence of stochastic
iterative dynamic programming algorithms. Neural Computation, 1994.
[9] S. M. Ross. Applied Probability Models with Optimization Applications. HoldenDay, San Francisco, 1970.
[10] R. S. Sutton. Learning to predict by the method of temporal differences.
Machine Learning, 3:9-44, 1988.
[11] J. N. Tsitsiklis. Asynchronous stochastic approximation and Q-Iearning. Technical Report LIDS-P-2172, Laboratory for Information and Decision Systems,
MIT, Cambridge, MA, 1993.
[12] C. J. C. H. Watkins. Learning from Delayed Rewards. PhD thesis, Cambridge
University, Cambridge, England, 1989.

"
2013,Sign Cauchy Projections and Chi-Square Kernel,Poster,5075-sign-cauchy-projections-and-chi-square-kernel.pdf,"The method of Cauchy random projections is popular  for computing the $l_1$ distance in high dimension. In this paper, we propose to use only the signs of the projected data and show that the  probability of collision (i.e., when the two signs differ) can be accurately approximated as a function of the chi-square ($\chi^2$) similarity, which is a popular  measure for nonnegative data (e.g., when features are generated from histograms as common in text and vision applications). Our experiments   confirm that this method of sign Cauchy random projections is promising for large-scale  learning applications. Furthermore, we extend the idea to sign $\alpha$-stable random projections and derive a bound of the collision probability.","Sign Cauchy Projections and Chi-Square Kernel
Ping Li
Dept of Statistics & Biostat.
Dept of Computer Science
Rutgers University
pingli@stat.rutgers.edu

Gennady Samorodnitsky
ORIE and Dept of Stat. Science
Cornell University
Ithaca, NY 14853
gs18@cornell.edu

John Hopcroft
Dept of Computer Science
Cornell University
Ithaca, NY 14853
jeh@cs.cornell.edu

Abstract

The method of stable random projections is useful for efficiently approximating
the l? distance (0 < ? ? 2) in high dimension and it is naturally suitable for data
streams. In this paper, we propose to use only the signs of the projected data and
we analyze the probability of collision (i.e., when the two signs differ). Interestingly, when ? = 1 (i.e., Cauchy random projections), we show that the probability
of collision can be accurately approximated as functions of the chi-square (?2 )
similarity. In text and vision applications, the ?2 similarity is a popular measure
when the features are generated from histograms (which are a typical example of
data streams). Experiments confirm that the proposed method is promising for
large-scale learning applications. The full paper is available at arXiv:1308.1009.
There are many future research problems. For example, when ? ? 0, the collision
probability is a function of the resemblance (of the binary-quantized data). This
provides an effective mechanism for resemblance estimation in data streams.

1

Introduction

High-dimensional representations have become very popular in modern applications of machine
learning, computer vision, and information retrieval. For example, Winner of 2009 PASCAL image
classification challenge used millions of features [29]. [1, 30] described applications with billion or
trillion features. The use of high-dimensional data often achieves good accuracies at the cost of a
significant increase in computations, storage, and energy consumptions.
Consider two data vectors (e.g., two images) u, v ? RD . A basic task is to compute their distance
or similarity. For example, the correlation (?2 ) and l? distance (d? ) are commonly used:
?D
D
?
ui vi
?2 (u, v) = ?? i=1 ?
,
d? (u, v) =
|ui ? vi |?
(1)
D
D
2
2
i=1
u
v
i=1 i
i=1 i
In this study, we are particularly interested in the ?2 similarity, denoted by ??2 :
??2 =

D
?
2ui vi
,
u
+ vi
i=1 i

where ui ? 0, vi ? 0,

D
?

ui =

i=1

D
?

vi = 1

(2)

i=1

The chi-square similarity is closely related to the chi-square distance d?2 :
d?2 =

D
?
(ui ? vi )2
i=1

ui + vi

=

D
D
?
?
4ui vi
(ui + vi ) ?
= 2 ? 2??2
u
+ vi
i=1
i=1 i

(3)

The chi-square similarity is an instance of the Hilbertian metrics, which are defined over probability
space [10] and suitable for data generated from histograms. Histogram-based features (e.g., bagof-word or bag-of-visual-word models) are extremely popular in computer vision, natural language
processing (NLP), and information retrieval. Empirical studies have demonstrated the superiority of
the ?2 distance over l2 or l1 distances for image and text classification tasks [4, 10, 13, 2, 28, 27, 26].
The method of normal random projections (i.e., ?-stable projections with ? = 2) has become
popular in machine learning (e.g., [7]) for reducing the data dimensions and data sizes, to facilitate
1

efficient computations of the l2 distances and correlations. More generally, the method of stable
random projections [11, 17] provides an efficient algorithm to compute the l? distances (0 < ? ? 2).
In this paper, we propose to use only the signs of the projected data after applying stable projections.
1.1 Stable Random Projections and Sign (1-Bit) Stable Random Projections
Consider two high-dimensional data vectors u, v ? RD . The basic idea of stable random projections
is to multiply u and v by a random matrix R ? RD?k : x = uR ? Rk , y = vR ? Rk , where entries
of R are i.i.d. samples from a symmetric ?-stable distribution with unit scale. By properties of
stable distributions, xj ? yj follows a symmetric ?-stable distribution with scale d? . Hence, the
task of computing d? boils down to estimating the scale d? from k i.i.d. samples. In this paper, we
propose to store only the signs of projected data and we study the probability of collision:
P? = Pr (sign(xj ) ?= sign(yj ))

(4)

Using only the signs (i.e., 1 bit) has significant advantages for applications in search and learning.
When ? = 2, this probability can be analytically evaluated [9] (or via a simple geometric argument):
1
P2 = Pr (sign(xj ) ?= sign(yj )) = cos?1 ?2
(5)
?
which is an important result known as sim-hash [5]. For ? < 2, the collision probability is an
open problem. When the data are nonnegative, this paper (Theorem 1) will prove a bound of P?
for general 0 < ? ? 2. The bound is exact at ? = 2 and becomes less sharp as ? moves away
from 2. Furthermore, for ? = 1 and nonnegative data, we have the interesting observation that the
probability P1 can be well approximated as functions of the ?2 similarity ??2 .
1.2 The Advantages of Sign Stable Random Projections
1. There is a significant saving in storage space by using only 1 bit instead of (e.g.,) 64 bits.
2. This scheme leads to an efficient linear algorithm (e.g., linear SVM). For example, a negative sign can be coded as ?01? and a positive sign as ?10? (i.e., a vector of length 2). With
k projections, we concatenate k short vectors to form a vector of length 2k. This idea is
inspired by b-bit minwise hashing [20], which was designed for binary sparse data.
3. This scheme also leads to an efficient near neighbor search algorithm [8, 12]. We can code
a negative sign by ?0? and positive sign by ?1? and concatenate k such bits to form a hash
table of 2k buckets. In the query phase, one only searches for similar vectors in one bucket.
1.3 Data Stream Computations
Stable random projections are naturally suitable for data streams. In modern applications, massive
datasets are often generated in a streaming fashion, which are difficult to transmit and store [22], as
the processing is done on the fly in one-pass of the data. In the standard turnstile model [22], a data
stream can be viewed as high-dimensional vector with the entry values changing over time.
(t)

Here, we denote a stream at time t by ui , i = 1 to D. At time t, a stream element (it , It )
(t)
(t?1)
arrives and updates the it -th coordinate as uit = uit
+ It . Clearly, the turnstile data stream
model is particularly suitable for describing histograms and it is also a standard model for network
traffic summarization and monitoring [31]. Because this stream model is linear, methods based on
linear projections (i.e., matrix-vector multiplications) can naturally handle streaming data of this
sort. Basically, entries of the projection matrix R ? RD?k are (re)generated as needed using
pseudo-random number techniques [23]. As (it , It ) arrives, only the entries in the it -th row, i.e.,
(t)
(t?1)
rit ,j , j = 1 to k, are (re)generated and the projected data are updated as xj = xj
+ It ? rit j .
Recall that, in the definition of ?2 similarity, the data are assumed to be normalized (summing to
1). For nonnegative streams, the sum can be computed error-free by using merely one counter:
?D (t)
?t
= s=1 Is . Thus we can still use, without loss of generality, the sum-to-one assumpi=1 ui
tion, even in the streaming environment. This fact was recently exploited by another data stream
algorithm named Compressed Counting (CC) [18] for estimating the Shannon entropy of streams.
Because the use of the ?2 similarity is popular in (e.g.,) computer vision, recently there are other
proposals for estimating the ?2 similarity. For example, [15] proposed a nice technique to approximate ??2 by first expanding the data from D dimensions to (e.g.,) 5 ? 10 ? D dimensions through
a nonlinear transformation and then applying normal random projections on the expanded data. The
nonlinear transformation makes their method not applicable to data streams, unlike our proposal.
2

For notational simplicity, we will drop the superscript (t) for the rest of the paper.

2

An Experimental Study of Chi-Square Kernels

We provide an experimental study to validate the use of ?2 similarity. Here, the ??2 -kernel? is
defined as K(u, v) = ??2 and the ?acos-?2 -kernel? as K(u, v) = 1 ? ?1 cos?1 ??2 . With a slight
abuse of terminology, we call both ??2 kernel? when it is clear in the context.
We use the ?precomputed kernel? functionality in LIBSVM on two datasets: (i) UCI-PEMS, with
267 training examples and 173 testing examples in 138,672 dimensions; (ii) MNIST-small, a subset
of the popular MNIST dataset, with 10,000 training examples and 10,000 testing examples.
The results are shown in Figure 1. To compare these two types of ?2 kernels with ?linear? kernel,
we also test the same data using LIBLINEAR [6] after normalizing the data to have unit Euclidian
norm, i.e., we basically use ?2 . For both LIBSVM and LIBLINEAR, we use l2 -regularization with
a regularization parameter C and we report the test errors for a wide range of C values.
100
PEMS

Classification Acc (%)

Classification Acc (%)

100
80
60
40

linear

20

?2

0 ?2
10

2

acos ?
?1

10

0

1

10

10

2

10

MNIST?Small
90
80
linear
acos ?2
60 ?2
10

3

10

C

?2

70

?1

10

0

10
C

1

10

2

10

Figure 1: Classification accuracies. C is the l2 -regularization parameter. We use LIBLINEAR
for ?linear? (i.e., ?2 ) kernel and LIBSVM ?precomputed kernel? for two types of ?2 kernels (??2 kernel? and ?acos-?2 -kernel?). For UCI-PEMS, the ?2 -kernel has better performance than the linear
kernel and acos-?2 -kernel. For MNIST-Small, both ?2 kernels noticeably outperform linear kernel.
Note that MNIST-small used the original MNIST test set and merely 1/6 of the original training set.
Here, we should state that it is not the intention of this paper to use these two small examples
to conclude the advantage of ?2 kernels over linear kernel. We simply use them to validate our
proposed method, which is general-purpose and is not limited to data generated from histograms.

3

Sign Stable Random Projections and the Collision Probability Bound

?D
?D
We apply stable random projections on two vectors u, v ? RD : x = i=1 ui ri , y = i=1 vi ri ,
ri ? S(?, 1), i.i.d. Here Z ? S(?, ?)
denotes) a symmetric ?-stable distribution with scale ?,
( ?
?
whose characteristic function [24] is E e ?1Zt = e??|t| . By properties of stable distributions,
( ?
)
D
we know x?y ? S ?, i=1 |ui ? vi |? . Applications including linear learning and near neighbor
search will benefit from sign ?-stable random projections. When ? = 2 (i.e. normal), the collision
probability Pr (sign(x) ?= sign(y)) is known [5, 9]. For ? < 2, it is a difficult probability problem.
This section provides a bound of Pr (sign(x) ?= sign(y)), which is fairly accurate for ? close to 2.
3.1 Collision Probability Bound
In this paper, we focus on nonnegative data (as common in practice). We present our first theorem.
Theorem 1 When the data are nonnegative, i.e., ui ? 0, vi ? 0, we have
?
?2/?
?D ?/2 ?/2
u
v
1
?
Pr (sign(x) ?= sign(y)) ? cos?1 ?? , where ?? = ? ?? i=1 i ? i

?
D
D
?
?
u
v
i=1 i
i=1 i

(6)

For ? = 2, this bound is exact [5, 9]. In fact the result for ? = 2 leads to the following Lemma:
Lemma 1 The kernel defined as K(u, v) = 1 ?

1
?

cos?1 ?2 is positive definite (PD).

Proof: The indicator function 1 {sign(x) = sign(y)} can be written as an inner product (hence PD)
and Pr (sign(x) = sign(y)) = E (1 {sign(x) = sign(y)}) = 1 ? ?1 cos?1 ?2 .

3

3.2 A Simulation Study to Verify the Bound of the Collision Probability
We generate the original data u and v by sampling from a bivariate t-distribution, which has two
parameters: the correlation and the number of degrees of freedom (which is taken to be 1 in our
experiments). We use a full range of the correlation parameter from 0 to 1 (spaced at 0.01). To
generate positive data, we simply take the absolute values of the generated data. Then we fix the
data as our original data (like u and v), apply sign stable random projections, and report the empirical
collision probabilities (after 105 repetitions).
Figure 2 presents the simulated collision probability Pr (sign(x) ?= sign(y)) for D = 100 and ? ?
{1.5, 1.2, 1.0, 0.5}. In each panel, the dashed curve is the theoretical upper bound ?1 cos?1 ?? , and
the solid curve is the simulated collision probability. Note that it is expected that the simulated data
can not cover the entire range of ?? values, especially as ? ? 0.

0.3
0.2
0.1
0
0.2

? = 1.5, D = 100
0.4

0.6
??

0.8

0.5

0.4
0.3
0.2
0.1
0
0.4

1

? = 1.2, D = 100
0.6
??

0.8

0.4
0.3
0.2
0.1
0
0.4

1

0.5
Collision probability

0.4

Collision probability

0.5
Collision probability

Collision probability

0.5

? = 1, D = 100
0.6
0.8
??

0.4
0.3
0.2
0.1
0
0.7

1

? = 0.5, D = 100
0.8
0.9
??

1

Figure 2: Dense Data and D = 100. Simulated collision probability Pr (sign(x) ?= sign(y)) for
sign stable random projections. In each panel, the dashed curve is the upper bound ?1 cos?1 ?? .
Figure 2 verifies the theoretical upper bound ?1 cos?1 ?? . When ? ? 1.5, this upper bound is fairly
sharp. However, when ? ? 1, the bound is not tight, especially for small ?. Also, the curves of the
empirical collision probabilities are not smooth (in terms of ?? ).
Real-world high-dimensional datasets are often sparse. To verify the theoretical upper bound of
the collision probability on sparse data, we also simulate sparse data by randomly making 50% of
the generated data as used in Figure 2 be zero. With sparse data, it is even more obvious that the
theoretical upper bound ?1 cos?1 ?? is not sharp when ? ? 1, as shown in Figure 3.

0.3
0.2
0.1
0
0

? = 1.5, D = 100, Sparse
0.2

0.4

??

0.6

0.8

1

0.5

0.4
0.3
0.2
0.1
0
0

? = 1.2, D = 100, Sparse
0.2

0.4

??

0.6

0.8

1

0.5
Collision probability

0.4

Collision probability

0.5
Collision probability

Collision probability

0.5

0.4
0.3
0.2
0.1
0
0

? = 1, D = 100, Sparse
0.2

0.4

??

0.6

0.8

1

0.4
0.3
0.2
0.1
0
0

? = 0.5, D = 100, Sparse
0.1

0.2
??

0.3

0.4

Figure 3: Sparse Data and D = 100. Simulated collision probability Pr (sign(x) ?= sign(y)) for
sign stable random projection. The upper bound is not tight especially when ? ? 1.
In summary, the collision probability bound: Pr (sign(x) ?= sign(y)) ? ?1 cos?1 ?? is fairly sharp
when ? is close to 2 (e.g., ? ? 1.5). However, for ? ? 1, a better approximation is needed.

4 ? = 1 and Chi-Square (?2 ) Similarity
In this section, we focus on nonnegative data (ui ? 0, vi ? 0) and ? = 1. This case is important in
practice. For example, we can view the data (ui , vi ) as empirical probabilities, which are common
when data are generated from histograms (as popular in NLP and vision) [4, 10, 13, 2, 28, 27, 26].
?D
?D
In this context, we always normalize the data, i.e., i=1 ui = i=1 vi = 1. Theorem 1 implies
(D
)2
? 1/2 1/2
1
?1
Pr (sign(x) ?= sign(y)) ? cos ?1 , where ?1 =
ui vi
(7)
?
i=1
While the bound is not tight, interestingly, the collision probability can be related to the ?2 similarity.
2
?D
i)
Recall the definitions of the chi-square distance d?2 = i=1 (uuii?v
+vi and the chi-square similarity
?D
i vi
. In this context, we should view 00 = 0.
??2 = 1 ? 12 d?2 = i=1 u2u
i +vi
4

Lemma 2 Assume ui ? 0, vi ? 0,
??2 =

?D
i=1

ui = 1,

D
?
2ui vi
u
+ vi
i=1 i

?D

vi = 1. Then
(D
)2
? 1/2 1/2
? ?1 =
u i vi

i=1

(8)

i=1

It is known that the ?2 -kernel is PD [10]. Consequently, we know the acos-?2 -kernel is also PD.
Lemma 3 The kernel defined as K(u, v) = 1 ?

1
?

cos?1 ??2 is positive definite (PD).



The remaining question is how to connect Cauchy random projections with the ?2 similarity.

5 Two Approximations of Collision Probability for Sign Cauchy Projections
It is a difficult problem to derive the collision probability of sign Cauchy projections if we would
like to express the probability only in terms of certain summary statistics (e.g., some distance). Our
first observation is that the collision probability can be well approximated using the ?2 similarity:
( )
1
Pr (sign(x) ?= sign(y)) ? P?2 (1) = cos?1 ??2
(9)
?
Figure 4 shows this approximation
is better than ?1 cos?1 (?1 ). Particularly, in sparse data, the
( )
1
?1
??2 is very accurate (except when ??2 is close to 1), while the bound
approximation ? cos
1
?1
cos
(?
)
is
not
sharp
(and the curve is not smooth in ?1 ).
1
?
0.5
Collision probability

Collision probability

0.5
0.4
0.3

2

?

1

0.2
0.1
0
0.4

? = 1, D = 100
0.6

??2, ?1

0.8

0.4
0.3
0.2
0.1
? = 1, D = 100, Sparse
0
0

1

0.2

0.4
0.6
??2, ?1

1

0.8

2

?

1

Figure 4: The dashed curve is ?1 cos?1 (?), where ? can be ?1 or ??2 depending on the context. In
each panel, the two solid curves are the empirical collision probabilities in terms of ?1 (labeled by
?1?) or ??2 (labeled by ??2 ). It is clear that the proposed approximation ?1 cos?1 ??2 in (9) is more
tight than the upper bound ?1 cos?1 ?1 , especially so in sparse data.
Our second (and less obvious) approximation is the following integral:
)
(
? ?/2
??2
1
2
?1
tan t dt
Pr (sign(x) ?= sign(y)) ? P?2 (2) = ? 2
tan
2 ? 0
2 ? 2??2

(10)

Figure 5 illustrates that, for dense data, the second approximation (10) is more accurate than the
first (9). The second approximation (10) is also accurate for sparse data. Both approximations,
P?2 (1) and P?2 (2) , are monotone functions of ??2 . In practice, we often do not need the ??2 values
explicitly because it often suffices if the collision probability is a monotone function of the similarity.
5.1 Binary Data
Interestingly, when the data are binary (before normalization), we can compute the collision probability exactly, which allows us to analytically assess the accuracy of the approximations. In fact,
this case inspired us to propose the second approximation (10), which is otherwise not intuitive.
For convenience, we define a = |Ia |, b = |Ib |, c = |Ic |, where
Ia = {i|ui > 0, vi = 0},
Ib = {i|vi > 0, ui = 0},
Ic = {i|ui > 0, vi > 0},

(11)

Assume binary data (before normalization, i.e., sum to one). That is,
1
1
1
1
=
, ?i ? Ia ? Ic ,
vi =
=
, ?i ? Ib ? Ic (12)
ui =
|Ia | + |Ic |
a+c
|Ib | + |Ic |
b+c
?D
??2
2c
c
i vi
The chi-square similarity ??2 becomes ??2 = i=1 u2u
= a+b+2c
and hence 2?2?
= a+b
.
2
i +vi
?

5

0.5

? = 1, D = 100

Collision probability

Collision probability

0.5
0.4
0.3
0.2
0.1

?2 (1)
2

? (2)
Empirical

0
0.4

0.6

0.8

??2

? = 1, D = 100, Sparse

0.4
0.3
0.2

?2 (1)

0.1

? (2)
Empirical

2

0
0

1

0.2

0.4

??2

0.6

0.8

1

Figure 5: Comparison of two approximations: ?2 (1) based on (9) and ?2 (2) based on (10). The
solid curves (empirical probabilities expressed in terms of ??2 ) are the same solid curves labeled
??2 ? in Figure 4. The left panel shows that the second approximation (10) is more accurate in dense
data. The right panel illustrate that both approximations are accurate in sparse data. (9) is slightly
more accurate at small ??2 and (10) is more accurate at ??2 close to 1.
Theorem 2 Assume binary data. When ? = 1, the exact collision probability is
( c )}
(c )
2 {
1
|R| tan?1 |R|
Pr (sign(x) ?= sign(y)) = ? 2 E tan?1
2 ?
a
b

(13)


where R is a standard Cauchy random variable.

{
(
)}
(
(
{
)
)}
c
When a = 0 or b = 0, we have E tan?1 ac |R| tan?1 cb |R| = ?2 E tan?1 a+b
|R| . This
observation inspires us to propose the approximation (10):
{
(
)}
)
(
? ?/2
1
1
c
1
2
c
?1
?1
P?2 (2) = ? E tan
|R|
= ? 2
tan t dt
tan
2 ?
a+b
2 ? 0
a+b
To validate this approximation for binary data, we study the difference between (13) and (10), i.e.,
Z(a/c, b/c) = Err = Pr (sign(x) ?= sign(y)) ? P?2 (2)
{
(
)
(
)}
{
(
)}
1
1
1
1
2
|R| tan?1
|R|
+ E tan?1
|R|
= ? 2 E tan?1
?
a/c
b/c
?
a/c + b/c

(14)

(14) can be easily computed by simulations. Figure 6 confirms that the errors are larger than zero
and very small . The maximum error is smaller than 0.0192, as proved in Lemma 4.
2

10

0.02

1

10

0.015

0

Z(t)

b/c

0.019

10

0.01

0.01
?1

10

0.005
0.001

?2

10 ?2
10

?1

10

0

10
a/c

1

10

0 ?2
10

2

10

?1

10

0

1

10

10

2

10

3

10

t

Figure 6: Left panel: contour plot for the error Z(a/c, b/c) in (14). The maximum error (which is
< 0.0192) occurs along the diagonal line. Right panel: the diagonal curve of Z(a/c, b/c).
Lemma 4 The error defined in (14) ranges between 0 and Z(t? ):
? ?{
( r ))2 1
( r )} 2 1
2 (
+ tan?1
dr (15)
0 ? Z(a/c, b/c) ? Z(t? ) =
? 2 tan?1 ?
?
t
?
2t?
? 1 + r2
0
where t? = 2.77935 is the solution to

1
t2 ?1

log

2t
1+t

6

=

log(2t)
(2t)2 ?1 .

Numerically, Z(t? ) = 0.01919.



0.5

0.03

0.4

0.02

?2(1)

0.2

0
?0.01

2

? (2)

0.1
0
0

?2(2)

0.01

0.3

Error

Collision probability

5.2 An Experiment Based on 3.6 Million English Word Pairs
To further validate the two ?2 approximations (in non-binary data), we experiment with a word
occurrences dataset (which is an example of histogram data) from a chunk of D = 216 web crawl
documents. There are in total 2,702 words, i.e., 2,702 vectors and 3,649,051 word pairs. The entries
of a vector are the occurrences of the word. This is a typical sparse, non-binary dataset. Interestingly,
the errors of the collision probabilities based on two ?2 approximations are still very small. To report
the results, we apply sign Cauchy random projections 107 times to evaluate the approximation errors
of (9) and (10). The results, as presented in Figure 7, again confirm that the upper bound ?1 cos?1 ?1
is not tight and both ?2 approximations, P?2 (1) and P?2 (2) , are accurate.

?2(1)

?0.02
0.2

0.4
0.6
??2 or ?1

0.8

?0.03
0

1

0.2

0.4

??2

0.6

0.8

1

Figure 7: Empirical collision probabilities for 3.6 million English word pairs. In the left panel,
we plot the empirical collision probabilities against ?1 (lower, green if color is available) and ??2
(higher, red). The curves confirm that the bound ?1 cos?1 ?1 is not tight (and the curve is not smooth).
We plot the two ?2 approximations as dashed curves which largely match the empirical probabilities
plotted against ??2 , confirming that the ?2 approximations are good. For smaller ??2 values, the
first approximation P?2 (1) is slightly more accurate. For larger ??2 values, the second approximation
P?2 (2) is more accurate. In the right panel, we plot the errors for both P?2 (1) and P?2 (2) .

6

Sign Cauchy Random Projections for Classification

Our method provides an effective strategy for classification. For each (high-dimensional) data vector, using k sign Cauchy projections, we encode a negative sign as ?01? and a positive as ?10? (i.e.,
a vector of length 2) and concatenate k short vectors to form a new feature vector of length 2k. We
then feed the new data into a linear classifier (e.g., LIBLINEAR). Interestingly, this linear classifier
approximates a nonlinear kernel classifier based on acos-?2 -kernel: K(u, v) = 1? ?1 cos?1 ??2 . See
Figure 8 for the experiments on the same two datasets in Figure 1: UCI-PEMS and MNIST-Small.
100

80

k = 2048,4096,8192

Classification Acc (%)

Classification Acc (%)

100
1024
512

60

k = 256
k = 128

40

k = 64
k = 32

20
0 ?2
10

PEMS: SVM
?1

10

0

1

10

10

2

10

10

C

1024

2048

k = 128

80

k = 64

70
MNIST?Small: SVM
60 ?2
10

3

k = 4096, 8192
k = 512
k = 256

90

?1

10

0

10
C

1

10

2

10

Figure 8: The two dashed (red if color is available) curves are the classification results obtained
using ?acos-?2 -kernel? via the ?precomputed kernel? functionality in LIBSVM. The solid (black)
curves are the accuracies using k sign Cauchy projections and LIBLINEAR. The results confirm
that the linear kernel from sign Cauchy projections can approximate the nonlinear acos-?2 -kernel.
Figure 1 has already shown that, for the UCI-PEMS dataset, the ?2 -kernel (??2 ) can produce noticeably better classification results than the acos-?2 -kernel (1 ? ?1 cos?1 ??2 ). Although our method
does not directly approximate ??2 , we can still estimate ??2 by assuming the collision probability
is exactly Pr (sign(x) ?= sign(y)) = ?1 cos?1 ??2 and then we can feed the estimated ??2 values
into LIBSVM ?precomputed kernel? for classification. Figure 9 verifies that this method can also
approximate the ?2 kernel with enough projections.
7

PEMS: ?2 kernel SVM

80
60
40
20
0 ?2
10

?1

10

0

1

10

10

100
Classification Acc (%)

Classification Acc (%)

100

k = 8192
k = 4096
k = 2048
k = 1024
k = 512
k = 256
k = 128
k = 64
k = 32

2

10

MNIST?Small: ?2 kernel SVM

k = 256

80
70
60 ?2
10

3

10

C

k = 1024 2048 4096 8192

90

k = 512

k = 128

k = 64

?1

10

0

10
C

1

10

2

10

Figure 9: Nonlinear kernels. The dashed curves are the classification results obtained using ?2 kernel and LIBSVM ?precomputed kernel? functionality. We apply k sign Cauchy projections and
estimate ??2 assuming the collision probability is exactly ?1 cos?1 ??2 and then feed the estimated
??2 into LIBSVM again using the ?precomputed kernel? functionality.

7

Conclusion

The use of ?2 similarity is widespread in machine learning, especially when features are generated
from histograms, as common in natural language processing and computer vision. Many prior studies [4, 10, 13, 2, 28, 27, 26] have shown the advantage of using ?2 similarity compared to other
measures such as l2 distance. However, for large-scale applications with ultra-high-dimensional
datasets, using ?2 similarity becomes challenging for practical reasons. Simply storing (and maneuvering) all the high-dimensional features would be difficult if there are a large number of observations. Computing all pairwise ?2 similarities can be time-consuming and in fact we usually can not
materialize an all-pairwise similarity matrix even if there are merely 106 data points. Furthermore,
the ?2 similarity is nonlinear, making it difficult to take advantage of modern linear algorithms
which are known to be very efficient, e.g., [14, 25, 6, 3]. When data are generated in a streaming
fashion, computing ?2 similarities without storing the original data will be even more challenging.
The method of ?-stable random projections (0 < ? ? 2) [11, 17] is popular for efficiently computing the l? distances in massive (streaming) data. We propose sign stable random projections by
storing only the signs (i.e., 1-bit) of the projected data. Obviously, the saving in storage would be
a significant advantage. Also, these bits offer the indexing capability which allows efficient search.
For example, we can build hash tables using the bits to achieve sublinear time near neighbor search
(although this paper does not focus on near neighbor search). We can also build efficient linear
classifiers using these bits, for large-scale high-dimensional machine learning applications.
A crucial task in analyzing sign stable random projections is to study the probability of collision (i.e.,
when the two signs differ). We derive a theoretical bound of the collision probability which is exact
when ? = 2. The bound is fairly sharp for ? close to 2. For ? = 1 (i.e., Cauchy random projections), we find the ?2 approximation is significantly more accurate. In addition, for binary data, we
analytically show that the errors from using the ?2 approximation are less than 0.0192. Experiments
on real and simulated data confirm that our proposed ?2 approximations are very accurate.
We are enthusiastic about the practicality of sign stable projections in learning and search applications. The previous idea of using the signs from normal random projections has been widely adopted
in practice, for approximating correlations. Given the widespread use of the ?2 similarity and the
simplicity of our method, we expect the proposed method will be adopted by practitioners.
Future research Many interesting future research topics can be studied. (i) The processing cost
of conducting stable random projections can be dramatically reduced by very sparse stable random
projections [16]. This will make our proposed method even more practical. (ii) We can try to utilize
more than just 1-bit of the projected data, i.e., we can study the general coding problem [19]. (iii)
Another interesting research would be to study the use of sign stable projections for sparse signal
recovery (Compressed Sensing) with stable distributions [21]. (iv) When ? ? 0, the collision
probability becomes Pr (sign(x) ?= sign(y)) = 12 ? 12 Resemblance, which provides an elegant
mechanism for computing resemblance (of the binary-quantized data) in sparse data streams.
Acknowledgement The work of Ping Li is supported by NSF-III-1360971, NSF-Bigdata1419210, ONR-N00014-13-1-0764, and AFOSR-FA9550-13-1-0137. The work of Gennady
Samorodnitsky is supported by ARO-W911NF-12-10385.

8

References
[1] http://googleresearch.blogspot.com/2010/04/ lessons-learned-developing-practical.html.
[2] Bogdan Alexe, Thomas Deselaers, and Vittorio Ferrari. What is an object? In CVPR, pages 73?80, 2010.
[3] Leon Bottou. http://leon.bottou.org/projects/sgd.
[4] Olivier Chapelle, Patrick Haffner, and Vladimir N. Vapnik. Support vector machines for histogram-based
image classification. IEEE Trans. Neural Networks, 10(5):1055?1064, 1999.
[5] Moses S. Charikar. Similarity estimation techniques from rounding algorithms. In STOC, 2002.
[6] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. Liblinear: A library
for large linear classification. Journal of Machine Learning Research, 9:1871?1874, 2008.
[7] Yoav Freund, Sanjoy Dasgupta, Mayank Kabra, and Nakul Verma. Learning the structure of manifolds
using random projections. In NIPS, Vancouver, BC, Canada, 2008.
[8] Jerome H. Friedman, F. Baskett, and L. Shustek. An algorithm for finding nearest neighbors. IEEE
Transactions on Computers, 24:1000?1006, 1975.
[9] Michel X. Goemans and David P. Williamson. Improved approximation algorithms for maximum cut and
satisfiability problems using semidefinite programming. Journal of ACM, 42(6):1115?1145, 1995.
[10] Matthias Hein and Olivier Bousquet. Hilbertian metrics and positive definite kernels on probability measures. In AISTATS, pages 136?143, Barbados, 2005.
[11] Piotr Indyk. Stable distributions, pseudorandom generators, embeddings, and data stream computation.
Journal of ACM, 53(3):307?323, 2006.
[12] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: Towards removing the curse of dimensionality. In STOC, pages 604?613, Dallas, TX, 1998.
[13] Yugang Jiang, Chongwah Ngo, and Jun Yang. Towards optimal bag-of-features for object categorization
and semantic video retrieval. In CIVR, pages 494?501, Amsterdam, Netherlands, 2007.
[14] Thorsten Joachims. Training linear svms in linear time. In KDD, pages 217?226, Pittsburgh, PA, 2006.
[15] Fuxin Li, Guy Lebanon, and Cristian Sminchisescu. A linear approximation to the ?2 kernel with geometric convergence. Technical report, arXiv:1206.4074, 2013.
[16] Ping Li. Very sparse stable random projections for dimension reduction in l? (0 < ? ? 2) norm. In
KDD, San Jose, CA, 2007.
[17] Ping Li. Estimators and tail bounds for dimension reduction in l? (0 < ? ? 2) using stable random
projections. In SODA, pages 10 ? 19, San Francisco, CA, 2008.
[18] Ping Li. Improving compressed counting. In UAI, Montreal, CA, 2009.
[19] Ping Li, Michael Mitzenmacher, and Anshumali Shrivastava. Coding for random projections. 2013.
[20] Ping Li, Art B Owen, and Cun-Hui Zhang. One permutation hashing. In NIPS, Lake Tahoe, NV, 2012.
[21] Ping Li, Cun-Hui Zhang, and Tong Zhang. Compressed counting meets compressed sensing. 2013.
[22] S. Muthukrishnan. Data streams: Algorithms and applications. Foundations and Trends in Theoretical
Computer Science, 1:117?236, 2 2005.
[23] Noam Nisan. Pseudorandom generators for space-bounded computations. In STOC, 1990.
[24] Gennady Samorodnitsky and Murad S. Taqqu. Stable Non-Gaussian Random Processes. Chapman &
Hall, New York, 1994.
[25] Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: Primal estimated sub-gradient solver
for svm. In ICML, pages 807?814, Corvalis, Oregon, 2007.
[26] Andrea Vedaldi and Andrew Zisserman. Efficient additive kernels via explicit feature maps. IEEE Trans.
Pattern Anal. Mach. Intell., 34(3):480?492, 2012.
[27] Sreekanth Vempati, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Generalized rbf feature maps
for efficient detection. In BMVC, pages 1?11, Aberystwyth, UK, 2010.
[28] Gang Wang, Derek Hoiem, and David A. Forsyth. Building text features for object image classification.
In CVPR, pages 1367?1374, Miami, Florida, 2009.
[29] Jinjun Wang, Jianchao Yang, Kai Yu, Fengjun Lv, Thomas S. Huang, and Yihong Gong. Localityconstrained linear coding for image classification. In CVPR, pages 3360?3367, San Francisco, CA, 2010.
[30] Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. Feature hashing
for large scale multitask learning. In ICML, pages 1113?1120, 2009.
[31] Haiquan (Chuck) Zhao, Nan Hua, Ashwin Lall, Ping Li, Jia Wang, and Jun Xu. Towards a universal sketch
for origin-destination network measurements. In Network and Parallel Computing, pages 201?213, 2011.

9

"
2004,Hierarchical Distributed Representations for Statistical Language Modeling,,2691-hierarchical-distributed-representations-for-statistical-language-modeling.pdf,Abstract Missing,"Hierarchical Distributed Representations for
Statistical Language Modeling

John Blitzer, Kilian Q. Weinberger, Lawrence K. Saul, and Fernando C. N. Pereira
Department of Computer and Information Science, University of Pennsylvania
Levine Hall, 3330 Walnut Street, Philadelphia, PA 19104
{blitzer,kilianw,lsaul,pereira}@cis.upenn.edu

Abstract
Statistical language models estimate the probability of a word occurring
in a given context. The most common language models rely on a discrete
enumeration of predictive contexts (e.g., n-grams) and consequently fail
to capture and exploit statistical regularities across these contexts. In this
paper, we show how to learn hierarchical, distributed representations of
word contexts that maximize the predictive value of a statistical language
model. The representations are initialized by unsupervised algorithms for
linear and nonlinear dimensionality reduction [14], then fed as input into
a hierarchical mixture of experts, where each expert is a multinomial distribution over predicted words [12]. While the distributed representations
in our model are inspired by the neural probabilistic language model of
Bengio et al. [2, 3], our particular architecture enables us to work with
significantly larger vocabularies and training corpora. For example, on a
large-scale bigram modeling task involving a sixty thousand word vocabulary and a training corpus of three million sentences, we demonstrate
consistent improvement over class-based bigram models [10, 13]. We
also discuss extensions of our approach to longer multiword contexts.

1

Introduction

Statistical language models are essential components of natural language systems for
human-computer interaction. They play a central role in automatic speech recognition [11],
machine translation [5], statistical parsing [8], and information retrieval [15]. These models estimate the probability that a word will occur in a given context, where in general
a context specifies a relationship to one or more words that have already been observed.
The simplest, most studied case is that of n-gram language modeling, where each word is
predicted from the preceding n?1 words. The main problem in building these models is
that the vast majority of word combinations occur very infrequently, making it difficult to
estimate accurate probabilities of words in most contexts.
Researchers in statistical language modeling have developed a variety of smoothing techniques to alleviate this problem of data sparseness. Most smoothing methods are based on
simple back-off formulas or interpolation schemes that discount the probability of observed
events and assign the ?leftover? probability mass to events unseen in training [7]. Unfortunately, these methods do not typically represent or take advantage of statistical regularities

among contexts. One expects the probabilities of rare or unseen events in one context to be
related to their probabilities in statistically similar contexts. Thus, it should be possible to
estimate more accurate probabilities by exploiting these regularities.
Several approaches have been suggested for sharing statistical information across contexts.
The aggregate Markov model (AMM) of Saul and Pereira [13] (also discussed by Hofmann
and Puzicha [10] as a special case of the aspect model) factors the conditional probability
table of a word given its context by a latent variable representing context ?classes?. However, this latent variable approach is difficult to generalize to multiword contexts, as the
size of the conditional probability table for class given context grows exponentially with
the context length.
The neural probabilistic language model (NPLM) of Bengio et al. [2, 3] achieved significant improvements over state-of-the-art smoothed n-gram models [6]. The NPLM encodes
contexts as low-dimensional continuous vectors. These are fed to a multilayer neural network that outputs a probability distribution over words. The low-dimensional vectors and
the parameters of the network are trained simultaneously to minimize the perplexity of the
language model. This model has no difficulty encoding multiword contexts, but its training
and application are very costly because of the need to compute a separate normalization for
the conditional probabilities associated to each context.
In this paper, we introduce and evaluate a statistical language model that combines the
advantages of the AMM and NPLM. Like the NPLM, it can be used for multiword contexts, and like the AMM it avoids per-context normalization. In our model, contexts are
represented as low-dimensional real vectors initialized by unsupervised algorithms for dimensionality reduction [14]. The probabilities of words given contexts are represented by
a hierarchical mixture of experts (HME) [12], where each expert is a multinomial distribution over predicted words. This tree-structured mixture model allows a rich dependency
on context without expensive per-context normalization. Proper initialization of the distributed representations is crucial; in particular, we find that initializations from the results
of linear and nonlinear dimensionality reduction algorithms lead to better models (with
significantly lower test perplexities) than random initialization.
In practice our model is several orders of magnitude faster to train and apply than the
NPLM, enabling us to work with larger vocabularies and training corpora. We present results on a large-scale bigram modeling task, showing that our model also leads to significant
improvements over comparable AMMs.

2

Distributed representations of words

Natural language has complex, multidimensional semantics. As a trivial example, consider
the following four sentences:
The vase broke.
The window broke.

The vase contains water.
The window contains water.

The bottom right sentence is syntactically valid but semantically meaningless. As shown by
the table, a two-bit distributed representation of the words ?vase? and ?window? suffices to
express that a vase is both a container and breakable, while a window is breakable but cannot be a container. More generally, we expect low dimensional continuous representations
of words to be even more effective at capturing semantic regularities.
Distributed representations of words can be derived in several ways. In a given corpus of
text, for example, consider the matrix of bigram counts whose element C
Pij records the
number of times that word wj follows word wi . Further, let pij = Cij / k Cik denote
the conditional frequencies derived from these counts, and let p~i denote the V -dimensional

frequency vector with elements pij , where V is the vocabulary size. Note that the vectors p~i
themselves provide a distributed representation of the words wi in the corpus. For large
vocabularies and training corpora, however, this is an extremely unwieldy representation,
tantamount to storing the full matrix of bigram counts. Thus, it is natural to seek a lower
dimensional representation that captures the same information. To this end, we need to map
each vector p~i to some d-dimensional vector ~xi , with d  V . We consider two methods in
dimensionality reduction for this problem. The results from these methods are then used to
initialize the HME architecture in the next section.
2.1

Linear dimensionality reduction

The simplest form of dimensionality reduction is principal component analysis (PCA).
PCA computes a linear projection of the frequency vectors p~i into the low dimensional
subspace that maximizes their variance. The variance-maximizing subspace of dimensionality d is spanned by the top d eigenvectors of the frequency vector covariance matrix. The
eigenvalues of the covariance matrix measure the variance captured by each axis of the
subspace. The effect of PCA can also be understood as a translation and rotation of the
frequency vectors p~i , followed by a truncation that preserves only their first d elements.
2.2

Nonlinear dimensionality reduction

Intuitively, we would like to map the vectors p~i into a low dimensional space where semantically similar words remain close together and semantically dissimilar words are far
apart. Can we find a nonlinear mapping that does this better than PCA? Weinberger et al.
recently proposed a new solution to this problem based on semidefinite programming [14].
Let ~xi denote the image of p~i under this mapping. The mapping is discovered by first
2
learning the V ?V matrix of Euclidean squared distances [1] given by Dij = |~xi ? ~xj | .
This is done by balancing two competing goals: (i) to co-locate semantically similar words,
and (ii) to separate semantically dissimilar words. The first goal is achieved by fixing the
distances between words with similar frequency vectors to their original values. In particular, if p~j and p~k lie within some small neighborhood of each other, then the corresponding
2
element Djk in the distance matrix is fixed to the value |~
pj ? p~k | . The second goal is
achieved by maximizing the sum of pairwise squared distances ?ij Dij . Thus, we push the
words in the vocabulary as far apart as possible subject to the constraint that the distances
between semantically similar words do not change.
The only freedom in this optimization is the criterion for judging that two words are semantically similar. In practice, we adopt a simple criterion such as k-nearest neighbors in
the space of frequency vectors p~i and choose k as small as possible so that the resulting
neighborhood graph is connected [14].
The optimization is performed over the space of Euclidean squared distance matrices [1].
Necessary and sufficient conditions for the matrix D to be interpretable as a Euclidean
squared distance matrix are that D is symmetric and that the Gram matrix1 derived from
G = ? 12 HDH T is semipositive definite, where H = I ? V1 11T . The optimization can
thus be formulated as the semidefinite programming problem:
Maximize ?ij Dij subject to: (i) DT = D, (ii) ? 12 HDH  0, and
2
(iii) Dij = |~
pi ? p~j | for all neighboring vectors p~i and p~j .
1
Assuming without loss of generality that the vectors ~
xi are centered on the origin, the dot products Gij = ~
xi ? ~
xj are related to the pairwise squared distances Dij = |~
xi ? ~
xj |2 as stated above.

PCA
SDE
0.0

0.2

0.4

0.6

0.8

1.0

Figure 1: Eigenvalues from principal component analysis (PCA) and semide?nite embedding (SDE), applied to bigram distributions of the 2000 most frequently occuring words in
the corpus. The eigenvalues, shown normalized by their sum, measure the relative variance
captured by individual dimensions.
The optimization is convex, and its global maximum can be computed in polynomial
time [4]. The optimization here differs slightly from the one used by Weinberger et al. [14]
in that here we only preserve local distances, as opposed to local distances and angles.
After computing the matrix Dij by semide?nite programming, a low dimensional embedding ~xi is obtained by metric multidimensional scaling [1, 9, 14]. The top eigenvalues of
the Gram matrix measure the variance captured by the leading dimensions of this embedding. Thus, one can compare the eigenvalue spectra from this method and PCA to ascertain
if the variance of the nonlinear embedding is concentrated in fewer dimensions. We refer
to this method of nonlinear dimensionality reduction as semide?nite embedding (SDE).
Fig. 1 compares the eigenvalue spectra of PCA and SDE applied to the 2000 most frequent
words2 in the corpus described in section 4. The ?gure shows that the nonlinear embedding
by SDE concentrates its variance in many fewer dimensions than the linear embedding by
PCA. Indeed, Fig. 2 shows that even the ?rst two dimensions of the nonlinear embedding
preserve the neighboring relationships of many words that are semantically similar. By
contrast, the analogous plot generated by PCA (not shown) reveals no such structure.
MONDAY
TUESDAY
WEDNESDAY
THURSDAY
FRIDAY
SATURDAY
SUNDAY

MAY, WOULD, COULD, SHOULD,
MIGHT, MUST, CAN, CANNOT,
COULDN'T, WON'T, WILL

ONE, TWO, THREE,
FOUR, FIVE, SIX,
SEVEN, EIGHT, NINE,
TEN, ELEVEN,
TWELVE, THIRTEEN,
FOURTEEN, FIFTEEN,
SIXTEEN,
SEVENTEEN,
EIGHTEEN

MILLION
BILLION

ZERO

JANUARY
FEBRUARY
MARCH
APRIL
JUNE
JULY
AUGUST
SEPTEMBER
OCTOBER
NOVEMBER
DECEMBER

Figure 2: Projection of the normalized bigram counts of the 2000 most frequent words
onto the ?rst two dimensions of the nonlinear embedding obtained by semide?nite programming. Note that semantically meaningful neighborhoods are preserved, despite the
massive dimensionality reduction from V = 60000 to d = 2.

2

Though convex, the optimization over distance matrices for SDE is prohibitively expensive for
large matrices. For the results in this paper?on the corpus described in section 4?we solved the
semide?nite program in this section to embed the 2000 most frequent words in the corpus, then used
a greedy incremental solver to embed the remaining 58000 words in the vocabulary. Details of this
incremental solver will be given elsewhere. Though not the main point of this paper, the nonlinear
embedding of V = 60000 words is to our knowledge one of the largest applications of recently
developed spectral methods for nonlinear dimensionality reduction [9, 14].

3

Hierarchical mixture of experts

The model we use to compute the probability that word w0 follows word w is known as a
hierarchical mixture of experts (HME) [12]. HMEs are fully probabilistic models, making
them ideally suited to the task of statistical language modeling. Furthermore, like multilayer neural networks they can parameterize complex, nonlinear functions of their input.
Figure 3 depicts a simple, two-layer HME. HMEs are tree-structured mixture models in
which the mixture components are ?experts? that lie at the leaves of the tree. The interior
nodes of the tree perform binary logistic regressions on the input vector to the HME, and
the mixing weight for a leaf is computed by multiplying the probabilities of each branch
(left or right) along the path to that leaf. In our model, the input vector ~x is a function of
the context word w, and the expert at each leaf specifies a multinomial distribution over
the predicted word w0 . Letting ? denote a path through the tree from root to leaf, the HME
computes the probability of a word w0 conditioned on a context word w as
X
Pr(w0 |w) =
Pr(?|~x(w)) ? Pr(w0 |?).
(1)
?

We can compute the maximum likelihood parameters for the HME using an ExpectationMaximization (EM) algorithm [12]. The E-step involves computing the posterior probability over paths Pr(?|w, w0 ) for each observed bigram in the training corpus. This can be
done by a recursive pass through the tree. In the M-step, we must maximize the EM auxiliary function with respect to the parameters of the logistic regressions and multinomial
leaves as well as the input vectors ~x(w). The logistic regressions in the tree decouple and
can be optimized separately by Newton?s method, while the multinomial leaves have a simple closed-form update. Though the input vectors are shared across all logistic regressions
in the tree, we can compute their gradients and hessians in one recursive pass and update
them by Newton?s method as well.
The EM algorithm for HMEs converges to a local maximum of the log-likelihood, or equivalently, a local minimum of the training perplexity
?
?? C1
?Y
?
Ptrain =
Pr(wj |wi )Cij
,
(2)
?
?
ij

P

where C = ij Cij is the total number of observed bigrams in the training corpus. The
algorithm is sensitive to the choice of initialization; in particular, as we show in the next

word
initialized by
PCA or SDE

input
vector

logistic
regression
logistic
regression

logistic
regression
multinomial
distribution

multinomial
distribution

multinomial
distribution

multinomial
distribution

Figure 3: Two-layer HME for bigram modeling. Words are mapped to input vectors; probabilities of next words are computed by summing over paths through the tree. The mapping
from words to input vectors is initialized by dimensionality reduction of bigram counts.

Ptest
init
random
PCA
SDE

Ptest
m
8
16
32
64

d
4
468
406
385

8
407
364
361

12
378
362
360

16
373
351
355

Table 1: Test perplexities of HMEs
with different input dimensionalities
and initializations.

4
435
385
350
336

d
8
429
361
328
308

12
426
360
320
298

16
428
355
317
294

Table 2: Test perplexities of HMEs
with different input dimensionalities
and numbers of leaves.

section, initialization of the input vectors by PCA or SDE leads to significantly better models than random initialization. We initialized the logistic regressions in the HME to split
the input vectors recursively along their dimensions of greatest variance. The multinomial
distributions at leaf nodes were initialized by uniform distributions.
For an HME with m multinomial leaves and d-dimensional input vectors, the number of
parameters scales as O(V d + V m + dm). The resulting model can be therefore be much
more compact than a full bigram model over V words.

4

Results

We evaluated our models on the ARPA North American Business News (NAB) corpus.
Our training set contained 78 million words from a 60,000 word vocabulary. In the interest
of speed, we truncated the lowest-count bigrams from our training set. This left us with
a training set consisting of 1.7 million unique bigrams. The test set, untruncated, had 13
million words resulting in 2.1 million unique bigrams.
4.1

Empirical evaluation

Table 1 reports the test perplexities of several HMEs whose input vectors were initialized
in different ways. The number of mixture components (i.e., leaves of the HME) was fixed
at m = 16. In all cases, the inputs initialized by PCA and SDE significantly outperformed
random initialization. PCA and SDE initialization performed equally well for all but the
lowest-dimensional inputs. Here SDE outperformed PCA, most likely because the first few
eigenvectors of SDE capture more variance in the bigram counts than those of PCA (see
Figure 1).
Table 2 reports the test perplexities of several HMEs initialized by SDE, but with varying
input dimensionality (d) and numbers of leaves (m). Perplexity decreases with increasing
tree depth and input dimensionality, but increasing the dimensionality beyond d = 8 does
not appear to give much gain.
4.2

Comparison to a class-based bigram model

w

z

w'

Figure 4: Belief network for AMM.

We obtained baseline results from an AMM [13]
trained on the same corpus. The model (Figure 4)
has the form
X
Pr(w0 |w) =
Pr(z|w) ? Pr(w0 |z). (3)
z

The number of estimated parameters in AMMs
scales as 2?|Z|?V , where |Z| is the size of the latent variable (i.e., number of classes)
and V is the number of words in the vocabulary.

parameters (*1000)
960
1440
2400
4320

Ptest (AMM)
456
414
353
310

Ptest (HME)
429
361
328
308

improvement
6%
13%
7%
1%

Table 3: Test perplexities of HMEs and AMMs with roughly equal parameter counts.
Table 3 compares the test perplexities of several HMEs and AMMs with similar numbers
of parameters. All these HMEs had d = 8 inputs initialized by SDE. In all cases, the HMEs
match or outperform the AMMs. The performance is nearly equal for the larger models,
which may be explained by the fact that most of the parameters of the larger HMEs come
from the multinomial leaves, not from the distributed inputs.
4.3

Comparison to NPLM

The most successful large-scale application of distributed representations to language modeling is the NPLM of Bengio et al. [2, 3], which in part inspired our work. We now compare
the main aspects of the two models.
?
m
8
16
32
64

d
4
1
2
4
9

8
1
2
4
10

12
1
2
4
10

16
1
2
4
10

Table 4: Training times ? in hours
for HMEs with m leaves.

The NPLM uses softmax to compute the probability of a word w0 given its context, thus requiring a
separate normalization for each context. Estimating the parameters of this softmax requires O(V )
computation per observed context and accounts
for almost all of the computational resources required by the model. Because of this, the NPLM
vocabulary size was restricted to 18000 words,
and even then it required more than 3 weeks using 40 CPUs to finish 5 epochs of training [2].

By contrast, our HMEs require O(md) computation per observed bigram. As Table 4 shows, actual training times are rather insensitive to input dimensionality. This allowed us to use
a 3.5? larger vocabulary and a larger training corpus than were used for the NPLM, and
still complete training our largest models in a matter of hours. Note that the numbers in
Table 4 do not include the time to compute the initial distributed representations by PCA
(30 minutes) or SDE (3 days), but these computations do not need to be repeated for each
trained model.
The second difference between our model and the NPLM is the choice of initialization.
Bengio et al. [3] report negligible improvement from initializing the NPLM input vectors
by singular value decomposition. By contrast, we found that initialization by PCA or SDE
was essential for optimal performance of our models (Table 1).
Finally, the NPLM was applied to multiword contexts. We have not done these experiments yet, but our model extends naturally to multiword contexts, as we explain in the next
section.

5

Discussion

In this paper, we have presented a statistical language model that exploits hierarchical
distributed representations of word contexts. The model shares the advantages of the
NPLM [2], but differs in its use of dimensionality reduction for effective parameter ini-

tialization and in the significant speedup provided by the HME architecture. We can consequently scale our models to larger training corpora and vocabularies. We have also demonstrated that our models consistently match or outperform a baseline class-based bigram
model.
The class-based bigram model is nearly as effective as the HME, but it has the major drawback that there is no straightforward way to extend it to multiword contexts without exploding its parameter count. Like the NPLM, however, the HME can be easily extended.
We can form an input vector for a multiword history (w1 , w2 ) simply by concatenating the
vectors ~x(w1 ) and ~x(w2 ). The parameters of the corresponding HME can be learned by
an EM algorithm similar to the one in this paper. Initialization from dimensionality reduction is also straightforward: we can compute the low dimensional representation for each
word separately. We are actively pursuing these ideas to train models with hierarchical
distributed representations of multiword contexts.

References
[1] A. Y. Alfakih, A. Khandani, and H. Wolkowicz. Solving Euclidean distance matrix completion problems via semidefinite programming. Computational Optimization Applications, 12(13):13?30, 1999.
[2] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model.
Journal of Machine Learning Research, 3:1137?1155, 2003.
[3] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language model. In
T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing
Systems, volume 13, Cambridge, MA, 2001. MIT Press.
[4] D. B. Borchers. CSDP, a C library for semidefinite programming. Optimization Methods and
Software, 11(1):613?623, 1999.
[5] P. Brown, S. D. Pietra, V. D. Pietra, and R. Mercer. The mathematics of statistical machine
translation: parameter estimation. Computational Linguistics, 19(2):263?311, 1991.
[6] P. F. Brown, V. J. D. Pietra, P. V. deSouza, J. C. Lai, and R. L. Mercer. Class-based n-gram
models of natural language. Computational Linguistics, 18(4):467?479, 1992.
[7] S. Chen and J. Goodman. An empirical study of smoothing techniques for language modeling.
In Proceedings of the 34th Annual Meeting of the ACL, pages 310?318, 1996.
[8] M. Collins. Three generative, lexicalised models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Computational Linguistics, 1997.
[9] J. Ham, D. D. Lee, S. Mika, and B. Sch?olkopf. A kernel view of the dimensionality reduction of
manifolds. In Proceedings of the Twenty First International Conference on Machine Learning
(ICML-04), Banff, Canada, 2004.
[10] T. Hofmann and J. Puzicha. Statistical models for co-occurrence and histogram data. In Proceedings of the International Conference Pattern Recognition, pages 192?194, 1998.
[11] F. Jelinek. Statistical Methods for Speech Recognition. MIT Press, 1997.
[12] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural
Computation, 6:181?214, 1994.
[13] L. K. Saul and F. C. N. Pereira. Aggregate and mixed-order Markov models for statistical
language processing. In C. Cardie and R. Weischedel, editors, Proceedings of the Second Conference on Empirical Methods in Natural Language Processing (EMNLP-97), pages 81?89,
New Providence, RI, 1997.
[14] K. Q. Weinberger, F. Sha, and L. K. Saul. Learning a kernel matrix for nonlinear dimensionality
reduction. In Proceedings of the Twenty First International Confernence on Machine Learning
(ICML-04), Banff, Canada, 2004.
[15] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Transactions on Information Systems, 22(2):179?214, 2004.

"
2017,Fisher GAN,Poster,6845-fisher-gan.pdf,"Generative Adversarial Networks (GANs) are powerful models for learning complex distributions. Stable training of GANs has been addressed in many recent works which explore different metrics between distributions. In this paper we introduce Fisher GAN that fits within the Integral Probability Metrics (IPM) framework for  training GANs. Fisher GAN defines a data dependent constraint on the second order moments of the critic. We show in this paper that Fisher GAN allows for stable and time efficient  training that does not compromise the capacity of the critic, and does not need data independent constraints such as weight clipping. We analyze our Fisher IPM theoretically and provide an algorithm based on Augmented Lagrangian for Fisher GAN. We validate our claims on both image sample generation and semi-supervised classification using Fisher GAN.","Fisher GAN
Youssef Mroueh? , Tom Sercu?
mroueh@us.ibm.com, tom.sercu1@ibm.com
? Equal Contribution
AI Foundations, IBM Research AI
IBM T.J Watson Research Center

Abstract
Generative Adversarial Networks (GANs) are powerful models for learning complex distributions. Stable training of GANs has been addressed in many recent
works which explore different metrics between distributions. In this paper we
introduce Fisher GAN which fits within the Integral Probability Metrics (IPM)
framework for training GANs. Fisher GAN defines a critic with a data dependent
constraint on its second order moments. We show in this paper that Fisher GAN
allows for stable and time efficient training that does not compromise the capacity
of the critic, and does not need data independent constraints such as weight clipping. We analyze our Fisher IPM theoretically and provide an algorithm based on
Augmented Lagrangian for Fisher GAN. We validate our claims on both image
sample generation and semi-supervised classification using Fisher GAN.

1

Introduction

Generative Adversarial Networks (GANs) [1] have recently become a prominent method to learn
high-dimensional probability distributions. The basic framework consists of a generator neural
network which learns to generate samples which approximate the distribution, while the discriminator
measures the distance between the real data distribution, and this learned distribution that is referred
to as fake distribution. The generator uses the gradients from the discriminator to minimize the
distance with the real data distribution. The distance between these distributions was the object of
study in [2], and highlighted the impact of the distance choice on the stability of the optimization. The
original GAN formulation optimizes the Jensen-Shannon divergence, while later work generalized
this to optimize f-divergences [3], KL [4], the Least Squares objective [5]. Closely related to our
work, Wasserstein GAN (WGAN) [6] uses the earth mover distance, for which the discriminator
function class needs to be constrained to be Lipschitz. To impose this Lipschitz constraint, WGAN
proposes to use weight clipping, i.e. a data independent constraint, but this comes at the cost of
reducing the capacity of the critic and high sensitivity to the choice of the clipping hyper-parameter.
A recent development Improved Wasserstein GAN (WGAN-GP) [7] introduced a data dependent
constraint namely a gradient penalty to enforce the Lipschitz constraint on the critic, which does not
compromise the capacity of the critic but comes at a high computational cost.
We build in this work on the Integral probability Metrics (IPM) framework for learning GAN of [8].
Intuitively the IPM defines a critic function f , that maximally discriminates between the real and
fake distributions. We propose a theoretically sound and time efficient data dependent constraint on
the critic of Wasserstein GAN, that allows a stable training of GAN and does not compromise the
capacity of the critic. Where WGAN-GP uses a penalty on the gradients of the critic, Fisher GAN
imposes a constraint on the second order moments of the critic. This extension to the IPM framework
is inspired by the Fisher Discriminant Analysis method.
The main contributions of our paper are:
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

1. We introduce in Section 2 the Fisher IPM, a scaling invariant distance between distributions.
Fisher IPM introduces a data dependent constraint on the second order moments of the critic that
discriminates between the two distributions. Such a constraint ensures the boundedness of the metric
and the critic. We show in Section 2.2 that Fisher IPM when approximated with neural networks,
corresponds to a discrepancy between whitened mean feature embeddings of the distributions. In
other words a mean feature discrepancy that is measured with a Mahalanobis distance in the space
computed by the neural network.
2. We show in Section 3 that Fisher IPM corresponds to the Chi-squared distance ( 2 ) when the
critic has unlimited capacity (the critic belongs to a universal hypothesis function class). Moreover
we prove in Theorem 2 that even when the critic is parametrized by a neural network, it approximates
the 2 distance with a factor which is a inner product between optimal and neural network critic. We
finally derive generalization bounds of the learned critic from samples from the two distributions,
assessing the statistical error and its convergence to the Chi-squared distance from finite sample size.
3. We use Fisher IPM as a GAN objective 1 and formulate an algorithm that combines desirable
properties (Table 1): a stable and meaningful loss between distributions for GAN as in Wasserstein
GAN [6], at a low computational cost similar to simple weight clipping, while not compromising the
capacity of the critic via a data dependent constraint but at a much lower computational cost than [7].
Fisher GAN achieves strong semi-supervised learning results without need of batch normalization in
the critic.
Table 1: Comparison between Fisher GAN and recent related approaches.
Stability Unconstrained Efficient
Representation
capacity
Computation power (SSL)
Standard GAN [1, 9]
7
3
3
3
WGAN, McGan [6, 8]
3
7
3
7
WGAN-GP [7]
3
3
7
?
Fisher Gan (Ours)
3
3
3
3

2

Learning GANs with Fisher IPM

2.1

Fisher IPM in an arbitrary function space: General framework

Integral Probability Metric (IPM). Intuitively an IPM defines a critic function f belonging to a
function class F , that maximally discriminates between two distributions. The function class F
defines how f is bounded, which is crucial to define the metric. More formally, consider a compact
space X in Rd . Let F be a set of measurable, symmetric and bounded real valued functions on
X . Let P(X ) be the set of measurable probability distributions on X . Given two probability
distributions P, Q 2 P(X ), the IPM indexed by
n a symmetric functionospace F is defined as follows
[10]:
dF (P, Q) = sup E f (x)
E f (x) .
(1)
f 2F

x?P

x?Q

It is easy to see that dF defines a pseudo-metric over P(X ). Note specifically that if F is not
bounded, supf will scale f to be arbitrarily large. By choosing F appropriately [11], various
distances between probability measures can be defined.

First formulation: Rayleigh Quotient. In order to define an IPM in the GAN context, [6, 8] impose
the boundedness of the function space via a data independent constraint. This was achieved via
restricting the norms of the weights parametrizing the function space to a `p ball. Imposing such a
data independent constraint makes the training highly dependent on the constraint hyper-parameters
and restricts the capacity of the learned network, limiting the usability of the learned critic in a semisupervised learning task. Here we take a different angle and design the IPM to be scaling invariant
as a Rayleigh quotient. Instead of measuring the discrepancy between means as in Equation (1), we
measure a standardized discrepancy, so that the distance is bounded by construction. Standardizing
this discrepancy introduces as we will see a data dependent constraint, that controls the growth of the
weights of the critic f and ensures the stability of the training while maintaining the capacity of the
critic. Given two distributions P, Q 2 P(X ) the Fisher IPM for a function space F is defined as
follows:
E [f (x)]
E [f (x)]
x?P
x?Q
dF (P, Q) = sup p
.
(2)
2
2
1/2E
1
f 2F
x?P f (x) + /2Ex?Q f (x)
1

Code is available at https://github.com/tomsercu/FisherGAN

2

Real

P

!

v
x
Q

! (x)

2 Rm

Fake
Figure 1: Illustration of Fisher IPM with Neural Networks. ! is a convolutional neural network
which defines the embedding space. v is the direction in this embedding space with maximal mean
separation hv, ?! (P) ?! (Q)i, constrained by the hyperellipsoid v > ?! (P; Q) v = 1.
While a standard IPM (Equation (1)) maximizes the discrepancy between the means of a function
under two different distributions, Fisher IPM looks for critic f that achieves a tradeoff between
maximizing the discrepancy between the means under the two distributions (between class variance),
and reducing the pooled second order moment (an upper bound on the intra-class variance).
Standardized discrepancies have a long history in statistics and the so-called two-samples hypothesis
testing. For example the classic two samples Student?s t test defines the student statistics as the
ratio between means discrepancy and the sum of standard deviations. It is now well established that
learning generative models has its roots in the two-samples hypothesis testing problem [12]. Non
parametric two samples testing and model criticism from the kernel literature lead to the so called
maximum kernel mean discrepancy (MMD) [13]. The MMD cost function and the mean matching
IPM for a general function space has been recently used for training GAN [14, 15, 8].
Interestingly Harchaoui et al [16] proposed Kernel Fisher Discriminant Analysis for the two samples
hypothesis testing problem, and showed its statistical consistency. The Standard Fisher discrepancy
used in Linear Discriminant
Analysis (LDA)
or Kernel Fisher Discriminant Analysis (KFDA) can
?
?
E [f (x)]

E [f (x)]

2

be written: supf 2F Varx?P (f(x))+Varx?Q (f(x)) , where Varx?P (f(x)) = Ex?P f 2 (x) (Ex?P (f(x)))2 .
Note that in LDA F is restricted to linear functions, in KFDA F is restricted to a Reproducing
Kernel Hilbert Space (RKHS). Our Fisher IPM (Eq (2)) deviates from the standard Fisher discrepancy
since the numerator is not squared, and we use in the denominator the second order moments instead
of the variances. Moreover in our definition of Fisher IPM, F can be any symmetric function class.
x?P

x?Q

Second formulation: Constrained form. Since the distance is scaling invariant, dF can be written
equivalently in the following constrained form:
dF (P, Q) =

sup

f 2F , 12 Ex?P f 2 (x)+ 12 Ex?Q f 2 (x)=1

E (f ) := E [f (x)]
x?P

E [f (x)].

x?Q

(3)

Specifying P, Q: Learning GAN with Fisher IPM. We turn now to the problem of learning GAN
with Fisher IPM. Given a distribution Pr 2 P(X ), we learn a function g? : Z ? Rnz ! X , such
that for z ? pz , the distribution of g? (z) is close to the real data distribution Pr , where pz is a fixed
distribution on Z (for instance z ? N (0, Inz )). Let P? be the distribution of g? (z), z ? pz . Using
Fisher IPM (Equation (3)) indexed by a parametric function class Fp , the generator minimizes the
IPM: ming? dFp (Pr , P? ). Given samples {xi , 1 . . . N } from Pr and samples {zi , 1 . . . M } from pz
we shall solve the following empirical problem:
N
1 X
fp (xi )
min sup E?(fp , g? ) :=
g? f 2F
N i=1
p
p

? p , g? ) =
where ?(f

1
2N

PN

i=1

fp2 (xi ) +

1
2M

PM

M
1 X
? p , g? ) = 1,
fp (g? (zj )) Subject to ?(f
M j=1

j=1

3

(4)

fp2 (g? (zj )). For simplicity we will have M = N .

2.2

Fisher IPM with Neural Networks

We will specifically study the case where F is a finite dimensional Hilbert space induced by a
neural network ! (see Figure 1 for an illustration). In this case, an IPM with data-independent
constraint will be equivalent to mean matching [8]. We will now show that Fisher IPM will give rise
to a whitened mean matching interpretation, or equivalently to mean matching with a Mahalanobis
distance.
Rayleigh Quotient. Consider the function space Fv,! , defined as follows
Fv,! = {f (x) = hv, ! (x)i |v 2 Rm , ! : X ! Rm },
! is typically parametrized with a multi-layer neural network. We define the mean and covariance
(Gramian) feature embedding of a distribution as in McGan [8]:
?! (P) = E (
x?P

! (x))

and

?! (P) = E

x?P

! (x)

! (x)

>

,

Fisher IPM as defined in Equation (2) on Fv,! can be written as follows:
hv, ?! (P) ?! (Q)i
dFv,! (P, Q) = max max q
,
!
v
v > ( 12 ?! (P) + 12 ?! (Q) + Im )v

(5)

where we added a regularization term ( > 0) to avoid singularity of the covariances. Note that if !
was implemented with homogeneous non linearities such as RELU, if we swap (v, !) with (cv, c0 !)
for any constants c, c0 > 0, the distance dFv,! remains unchanged, hence the scaling invariance.
Constrained Form. Since the Rayleigh Quotient is not amenable to optimization, we will consider
Fisher IPM as a constrained optimization problem. By virtue of the scaling invariance and the
constrained form of the Fisher IPM given in Equation (3), dFv,! can be written equivalently as:
dFv,! (P, Q) =

max

!,v,v > ( 12 ?! (P)+ 12 ?! (Q)+ Im )v=1

hv, ?! (P)

?! (Q)i

(6)

Define the pooled covariance: ?! (P; Q) = 12 ?! (P) + 12 ?! (Q) + Im . Doing a simple change of
1
variable u = (?! (P; Q)) 2 v we see that:
D
E
1
dFu,! (P, Q) = max max u, (?! (P; Q)) 2 (?! (P) ?! (Q))
!

=

u,kuk=1

max (?! (P; Q))
!

1
2

(?! (P)

?! (Q)) ,

(7)

hence we see that fisher IPM corresponds to the worst case distance between whitened means.
Since the means are white, we don?t need to impose further constraints on ! as in [6, 8]. Another
interpretation of the Fisher IPM stems from the fact that:
q
dFv,! (P, Q) = max (?! (P) ?! (Q))> ?! 1 (P; Q)(?! (P) ?! (Q)),
!

from which we see that Fisher IPM is a Mahalanobis distance between the mean feature embeddings
of the distributions. The Mahalanobis distance is defined by the positive definite matrix ?w (P; Q).
We show in Appendix A that the gradient penalty in Improved Wasserstein [7] gives rise to a similar
Mahalanobis mean matching interpretation.
Learning GAN with Fisher IPM. Hence we see that learning GAN with Fisher IPM:
min max
max
hv, ?w (Pr ) ?! (P? )i
1
1
g?

!

v,v > ( 2 ?! (Pr )+ 2 ?! (P? )+ Im )v=1

corresponds to a min-max game between a feature space and a generator. The feature space tries
to maximize the Mahalanobis distance between the feature means embeddings of real and fake
distributions. The generator tries to minimize the mean embedding distance.

3

Theory

We will start first by studying the Fisher IPM defined in Equation (2) when the function space has full
R
capacity i.e when the critic belongs to L2 (X , 12 (P + Q)) meaning that X f 2 (x) (P(x)+Q(x))
dx < 1.
2
Theorem 1 shows that under this condition, the Fisher IPM corresponds to the Chi-squared distance
between distributions, and gives a closed form expression of the optimal critic function f (See
Appendix B for its relation with the Pearson Divergence). Proofs are given in Appendix D.
4

(b)

2
1
0
1
2

1.0

0.5
Exact
MLP, N=M=10k

2

3

1.5

0.0

4
2

0

2

4

0

1

2

3

distance and MLP estimate

3

(c)
2.0

2.0

2

distance and MLP estimate

(a) 2D Gaussians, contour plot

1.5

1.0

0.5
MLP, shift=3
MLP, shift=1
MLP, shift=0.5

0.0
101

4

Mean shift

102

103

N=M=num training samples

Figure 2: Example on 2D synthetic data, where both P and Q are fixed normal distributions with the
same covariance and shifted means along the x-axis, see (a). Fig (b, c) show the exact 2 distance
from numerically integrating Eq (8), together with the estimate obtained from training a 5-layer MLP
with layer size = 16 and LeakyReLU nonlinearity on different training sample sizes. The MLP is
trained using Algorithm 1, where sampling from the generator is replaced by sampling from Q, and
the 2 MLP estimate is computed with Equation (2) on a large number of samples (i.e. out of sample
estimate). We see in (b) that for large enough sample size, the MLP estimate is extremely good. In (c)
we see that for smaller sample sizes, the MLP approximation bounds the ground truth 2 from below
(see Theorem 2) and converges to the ground truth roughly as O( p1N ) (Theorem 3). We notice that
when the distributions have small 2 distance, a larger training size is needed to get a better estimate again this is in line with Theorem 3.
Theorem 1 (Chi-squared distance at full capacity). Consider the Fisher IPM for F being the space
of all measurable functions endowed by 12 (P + Q), i.e. F := L2 (X , P+Q
2 ). Define the Chi-squared
distance between two distributions:
sZ
(P(x) Q(x))2
dx
(8)
2 (P, Q) =
P(x)+Q(x)
X

2

The following holds true for any P, Q, P 6= Q:
1) The Fisher IPM for F = L2 (X , P+Q
2 ) is equal to the Chi-squared distance defined above:
dF (P, Q) = 2 (P, Q).
2) The optimal critic of the Fisher IPM on L2 (X , P+Q
2 ) is :
f (x) =

P(x)

1
2 (P, Q)

Q(x)

P(x)+Q(x)
2

.

We note here that LSGAN [5] at full capacity corresponds to a Chi-Squared divergence, with the
main difference that LSGAN has different objectives for the generator and the discriminator (bilevel
optimizaton), and hence does not optimize a single objective that is a distance between distributions.
The Chi-squared divergence can also be achieved in the f -gan framework from [3]. We discuss the
advantages of the Fisher formulation in Appendix C.
Optimizing over L2 (X , P+Q
2 ) is not tractable, hence we have to restrict our function class, to a
hypothesis class H , that enables tractable computations. Here are some typical choices of the space
H : Linear functions in the input features, RKHS, a non linear multilayer neural network with a
linear last layer (Fv,! ). In this Section we don?t make any assumptions about the function space and
show in Theorem 2 how the Chi-squared distance is approximated in H , and how this depends on
the approximation error of the optimal critic f in H .
Theorem 2 (Approximating Chi-squared distance in an arbitrary function space H ). Let H
be an arbitrary symmetric function space. We define the inner product hf, f iL2 (X , P+Q ) =
2
R
P(x)+Q(x)
f
(x)f
(x)
dx,
which
induces
the
Lebesgue
norm.
Let
S
P+Q
be
the
unit
sphere
L2 (X ,
)
2
X
2

in L2 (X , P+Q
= {f : X ! R, kf kL2 (X , P+Q ) = 1}. The fisher IPM defined on an
2 ): SL2 (X , P+Q
2 )
2
arbitrary function space H dH (P, Q), approximates the Chi-squared distance. The approximation
5

quality depends on the cosine of the approximation of the optimal critic f in H . Since H is
symmetric this cosine is always positive (otherwise the same equality holds with an absolute value)
dH (P, Q) =

2 (P, Q)

sup

f 2H \ SL

2 (X ,

P+Q )
2

hf, f iL2 (X , P+Q ) ,
2

Equivalently we have following relative approximation error:
2 (P, Q)

dH (P, Q)
1
=
inf
kf
2 f 2H \ SL2 (X , P+Q )
2 (P, Q)
2

2

f kL2 (X , P+Q ) .
2

From Theorem 2, we know that we have always dH (P, Q) ? 2 (P, Q). Moreover if the space
H was rich enough to provide a good approximation of the optimal critic f , then dH is a good
approximation of the Chi-squared distance 2 .
Generalization bounds for the sample quality of the estimated Fisher IPM from samples from P and
Q can be done akin to [11], with the main difficulty that for Fisher IPM we have to bound the excess
risk of a cost function with data dependent constraints on the function class. We give generalization
bounds for learning the Fisher IPM in the supplementary material (Theorem 3, Appendix E). In a
nutshell the generalization error of the critic learned in a hypothesis class H from samples of P and
Q, decomposes to the approximation error from Theorem 2 and a statisticalperror that is bounded
using data dependent local Rademacher complexities [17] and scales like O( 1/n), n = M N/M +N .
We illustrate in Figure 2 our main theoretical claims on a toy problem.

4

Fisher GAN Algorithm using ALM

For any choice of the parametric function class Fp (for example Fv,! ), note the constraint in Equation
? p , g? ) = 1 PN f 2 (xi ) + 1 PN f 2 (g? (zj )). Define the Augmented Lagrangian
(4) by ?(f
i=1 p
j=1 p
2N
2N
[18] corresponding to Fisher GAN objective and constraint given in Equation (4):

? ?
(?(fp , g? ) 1)2
(9)
2
where is the Lagrange multiplier and ? > 0 is the quadratic penalty weight. We alternate between
optimizing the critic and the generator. Similarly to [7] we impose the constraint when training the
critic only. Given ?, for training the critic we solve maxp min LF (p, ?, ). Then given the critic
parameters p we optimize the generator weights ? to minimize the objective min? E?(fp , g? ). We
give in Algorithm 1, an algorithm for Fisher GAN, note that we use ADAM [19] for optimizing the
parameters of the critic and the generator. We use SGD for the Lagrange multiplier with learning rate
? following practices in Augmented Lagrangian [18].

LF (p, ?, ) = E?(fp , g? ) + (1

? p , g? ))
?(f

Algorithm 1 Fisher GAN
Input: ? penalty weight, ? Learning rate, nc number of iterations for training the critic, N batch
size
Initialize p, ?, = 0
repeat
for j = 1 to nc do
Sample a minibatch xi , i = 1 . . . N, xi ? Pr
Sample a minibatch zi , i = 1 . . . N, zi ? pz
(gp , g )
(rp LF , r LF )(p, ?, )
p
p + ? ADAM (p, gp )
?g {SGD rule on with learning rate ?}
end for
Sample zi , i = 1 . . . N, zi ? pz
PN
d?
r? E?(fp , g? ) = r? N1 i=1 fp (g? (zi ))
?
? ? ADAM (?, d? )
until ? converges

6

(c) CIFAR-10

0
4

4

E? train
E? val

E? train

4

2

Mean difference E?

(b) CelebA
Mean difference E?

Mean difference E?

(a) LSUN
4

2

2

0

0

3

4
?
?

3

?
?

2

2

1

1

1

0

0

0.5

1.0

g? iterations

1.5

0
0

?10

5

?
?

3

2

0.0

E? train
E? val

4

1

2

g? iterations

3

4
?10

5

0.0

0.5

1.0

g? iterations

1.5
?105

?
Figure 3: Samples and plots of the loss E?(.), lagrange multiplier , and constraint ?(.)
on 3
benchmark datasets. We see that during training as grows slowly, the constraint becomes tight.

Figure 4: No Batch Norm: Training results from a critic f without batch normalization. Fisher GAN
(left) produces decent samples, while WGAN with weight clipping (right) does not. We hypothesize
that this is due to the implicit whitening that Fisher GAN provides. (Note that WGAN-GP does also
succesfully converge without BN [7]). For both models the learning rate was appropriately reduced.

5

Experiments

We experimentally validate the proposed Fisher GAN. We claim three main results: (1) stable training
with a meaningful and stable loss going down as training progresses and correlating with sample
quality, similar to [6, 7]. (2) very fast convergence to good sample quality as measured by inception
score. (3) competitive semi-supervised learning performance, on par with literature baselines, without
requiring normalization of the critic.
We report results on three benchmark datasets: CIFAR-10 [20], LSUN [21] and CelebA [22]. We
parametrize the generator g? and critic f with convolutional neural networks following the model
design from DCGAN [23]. For 64 ? 64 images (LSUN, CelebA) we use the model architecture in
Appendix F.2, for CIFAR-10 we train at a 32 ? 32 resolution using architecture in F.3 for experiments
regarding sample quality (inception score), while for semi-supervised learning we use a better
regularized discriminator similar to the Openai [9] and ALI [24] architectures, as given in F.4.We
used Adam [19] as optimizer for all our experiments, hyper-parameters given in Appendix F.
Qualitative: Loss stability and sample quality. Figure 3 shows samples and plots during training.
For LSUN we use a higher number of D updates (nc = 5) , since we see similarly to WGAN that
the loss shows large fluctuations with lower nc values. For CIFAR-10 and CelebA we use reduced
nc = 2 with no negative impact on loss stability. CIFAR-10 here was trained without any label
information. We show both train and validation loss on LSUN and CIFAR-10 showing, as can be
expected, no overfitting on the large LSUN dataset and some overfitting on the small CIFAR-10
dataset. To back up our claim that Fisher GAN provides stable training, we trained both a Fisher Gan
and WGAN where the batch normalization in the critic f was removed (Figure 4).
Quantitative analysis: Inception Score and Speed. It is agreed upon that evaluating generative
models is hard [25]. We follow the literature in using ?inception score? [9] as a metric for the quality
7

8
7

Inception score

6
5
4
3

(a) Fisher GAN: CE, Conditional
(b) Fisher GAN: CE, G Not Cond.

2

(c) Fisher GAN: No Lab
WGAN-GP
WGAN
DCGAN

1
0
0.00

0.25

0.50

0.75

1.00

1.25

1.50

1.75

g? iterations

2.00
?105

0.0

0.5

1.0

1.5

2.0

2.5

3.0

Wallclock time (seconds)

3.5

4.0
?104

Figure 5: CIFAR-10 inception scores under 3 training conditions. Corresponding samples are given
in rows from top to bottom (a,b,c). The inception score plots are mirroring Figure 3 from [7].
Note All inception scores are computed from the same tensorflow codebase, using the architecture
described in appendix F.3, and with weight initialization from a normal distribution with stdev=0.02.
In Appendix F.1 we show that these choices are also benefiting our WGAN-GP baseline.
of CIFAR-10 samples. Figure 5 shows the inception score as a function of number of g? updates
and wallclock time. All timings are obtained by running on a single K40 GPU on the same cluster.
We see from Figure 5, that Fisher GAN both produces better inception scores, and has a clear speed
advantage over WGAN-GP.
Quantitative analysis: SSL. One of the main premises of unsupervised learning, is to learn features
on a large corpus of unlabeled data in an unsupervised fashion, which are then transferable to other
tasks. This provides a proper framework to measure the performance of our algorithm. This leads
us to quantify the performance of Fisher GAN by semi-supervised learning (SSL) experiments on
CIFAR-10. We do joint supervised and unsupervised training on CIFAR-10, by adding a cross-entropy
term to the IPM objective, in conditional and unconditional generation.
Table 2: CIFAR-10 inception scores using resnet architecture and codebase from [7]. We used
Layer Normalization [26] which outperformed unnormalized resnets. Apart from this, no additional
hyperparameter tuning was done to get stable training of the resnets.
Method
Score
Method
Score
ALI [24]
BEGAN [27]
DCGAN [23] (in [28])
Improved GAN (-L+HA) [9]
EGAN-Ent-VI [29]
DFM [30]
WGAN-GP ResNet [7]
Fisher GAN ResNet (ours)

SteinGan [31]
DCGAN (with labels, in [31])
Improved GAN [9]
Fisher GAN ResNet (ours)
AC-GAN [32]
SGAN-no-joint [28]
WGAN-GP ResNet [7]
SGAN [28]

5.34 ? .05
5.62
6.16 ? .07
6.86 ? .06
7.07 ? .10
7.72 ? .13
7.86 ? .07
7.90 ? .05

Unsupervised

6.35
6.58
8.09 ? .07
8.16 ? .12
8.25 ? .07
8.37 ? .08
8.42 ? .10
8.59 ? .12

Supervised

Unconditional Generation with CE Regularization. We parametrize the critic f as in Fv,! .
While training the critic using the Fisher GAN objective LF given in Equation (9), we train a linear
classifier on the feature space ! of the critic, whenever labels are available (K labels). The linear
classifier isPtrained with Cross-Entropy (CE) minimization. Then the critic loss becomes LD =
LF
log [Softmax(hS, ! (x)i)y ],
D
(x,y)2lab CE(x, y; S, ! ), where CE(x, y; S, ! ) =
K?m
K
where S 2 R
is the linear classifier and hS, ! i 2 R with slight abuse of notation. D is the
regularization hyper-parameter. We now sample three minibatches for each critic update: one labeled
batch from the small labeled dataset for the CE term, and an unlabeled batch + generated batch for
the IPM.
Conditional Generation with CE Regularization. We also trained conditional generator models,
conditioning the generator on y by concatenating the input noise with a 1-of-K embedding of the
label: we now have g? (z, y). We parametrize the critic in Fv,! and modify the critic objective
as above. We also add a cross-entropy term for the generator to minimize during its training step:
P
LG = E? + G z?p(z),y?p(y) CE(g? (z, y), y; S, ! ). For generator updates we still need to sample
only a single minibatch since we use the minibatch of samples from g? (z, y) to compute both the
8

IPM loss E? and CE. The labels are sampled according to the prior y ? p(y), which defaults to the
discrete uniform prior when there is no class imbalance. We found D = G = 0.1 to be optimal.
New Parametrization of the Critic: ?K + 1 SSL?. One specific successful formulation of SSL in
the standard GAN framework was provided in [9], where the discriminator classifies samples into
K + 1 categories: the K correct clases, and K + 1 for fake samples. Intuitively this puts the real
classes in competition with the fake class. In order to implement this idea in the Fisher framework,
we define a new function class of the critic that puts in competition the K class directions of the
classifier Sy , and another ?K+1? direction v that indicates fake samples. Hence we propose the
PK
following parametrization for the critic: f (x) = y=1 p(y|x) hSy , ! (x)i hv, ! (x)i, where
p(y|x) = Softmax(hS, ! (x)i)y which is also optimized with Cross-Entropy. Note that this critic
does not fall under the interpretation with whitened means from Section 2.2, but does fall under
the general Fisher IPM framework from Section 2.1. We can use this critic with both conditional
and unconditional generation in the same way as described above. In this setting we found D =
1.5, G = 0.1 to be optimal.
Layerwise normalization on the critic. For most GAN formulations following DCGAN design
principles, batch normalization (BN) [33] in the critic is an essential ingredient. From our semisupervised learning experiments however, it appears that batch normalization gives substantially
worse performance than layer normalization (LN) [26] or even no layerwise normalization. We
attribute this to the implicit whitening Fisher GAN provides.
Table 3 shows the SSL results on CIFAR-10. We show that Fisher GAN has competitive results, on
par with state of the art literature baselines. When comparing to WGAN with weight clipping, it
becomes clear that we recover the lost SSL performance. Results with the K + 1 critic are better
across the board, proving consistently the advantage of our proposed K + 1 formulation. Conditional
generation does not provide gains in the setting with layer normalization or without normalization.

Number of labeled examples
Model

Table 3: CIFAR-10 SSL results.
1000
2000
4000
Misclassification rate

CatGAN [34]
Improved GAN (FM) [9]
ALI [24]

8000

21.83 ? 2.01
19.98 ? 0.89

19.61 ? 2.09
19.09 ? 0.44

19.58
18.63 ? 2.32
17.99 ? 1.62
40.85
42.00

30.56
30.91

Fisher GAN BN Cond
Fisher GAN BN Uncond
Fisher GAN BN K+1 Cond
Fisher GAN BN K+1 Uncond

36.37
36.42
34.94
33.49

32.03
33.49
28.04
28.60

27.42
27.36
23.85
24.19

22.85
22.82
20.75
21.59

Fisher GAN LN Cond
Fisher GAN LN Uncond
Fisher GAN LN K+1 Cond
Fisher GAN LN K+1, Uncond

26.78 ? 1.04
24.39 ? 1.22
20.99 ? 0.66
19.74 ? 0.21

23.30 ? 0.39
22.69 ? 1.27
19.01 ? 0.21
17.87 ? 0.38

20.56 ? 0.64
19.53 ? 0.34
17.41 ? 0.38
16.13 ? 0.53

18.26 ? 0.25
17.84 ? 0.15
15.50 ? 0.41
14.81 ? 0.16

WGAN (weight clipping) Uncond
WGAN (weight clipping) Cond

Fisher GAN No Norm K+1, Uncond

6

69.01
68.11

21.15 ? 0.54

56.48
58.59

18.21 ? 0.30

16.74 ? 0.19

17.72 ? 1.82
17.05 ? 1.49

14.80 ? 0.15

Conclusion

We have defined Fisher GAN, which provide a stable and fast way of training GANs. The Fisher
GAN is based on a scale invariant IPM, by constraining the second order moments of the critic. We
provide an interpretation as whitened (Mahalanobis) mean feature matching and 2 distance. We
show graceful theoretical and empirical advantages of our proposed Fisher GAN.
Acknowledgments. The authors thank Steven J. Rennie for many helpful discussions and Martin
Arjovsky for helpful clarifications and pointers.
9

References
[1] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
[2] Martin Arjovsky and L?on Bottou. Towards principled methods for training generative adversarial networks. In ICLR, 2017.
[3] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural
samplers using variational divergence minimization. In NIPS, 2016.
[4] Casper Kaae S?nderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Husz?r. Amortised
map inference for image super-resolution. ICLR, 2017.
[5] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, and Zhen Wang. Least squares
generative adversarial networks. arXiv:1611.04076, 2016.
[6] Martin Arjovsky, Soumith Chintala, and L?on Bottou. Wasserstein gan. ICML, 2017.
[7] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville.
Improved training of wasserstein gans. arXiv:1704.00028, 2017.
[8] Youssef Mroueh, Tom Sercu, and Vaibhava Goel. Mcgan: Mean and covariance feature
matching gan. arXiv:1702.08398 ICML, 2017.
[9] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. NIPS, 2016.
[10] Alfred M?ller. Integral probability metrics and their generating classes of functions. Advances
in Applied Probability, 1997.
[11] Bharath K. Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Sch?lkopf, and Gert
R. G. Lanckriet. On the empirical estimation of integral probability metrics. Electronic Journal
of Statistics, 2012.
[12] Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models.
arXiv:1610.03483, 2016.
[13] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch?lkopf, and Alexander
Smola. A kernel two-sample test. JMLR, 2012.
[14] Yujia Li, Kevin Swersky, and Richard Zemel. Generative moment matching networks. In ICML,
2015.
[15] Gintare Karolina Dziugaite, Daniel M Roy, and Zoubin Ghahramani. Training generative neural
networks via maximum mean discrepancy optimization. UAI, 2015.
[16] Za?d Harchaoui, Francis R Bach, and Eric Moulines. Testing for homogeneity with kernel fisher
discriminant analysis. In NIPS, 2008.
[17] Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities.
Ann. Statist., 2005.
[18] J. Nocedal and S. J. Wright. Numerical Optimization. Springer, 2nd edition, 2006.
[19] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
[20] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master?s
thesis, 2009.
[21] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao.
Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop.
arXiv:1506.03365, 2015.
[22] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the
wild. In ICCV, 2015.
10

[23] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with
deep convolutional generative adversarial networks. arXiv:1511.06434, 2015.
[24] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro, and Aaron Courville. Adversarially learned inference. ICLR, 2017.
[25] Lucas Theis, A?ron van den Oord, and Matthias Bethge. A note on the evaluation of generative
models. ICLR, 2016.
[26] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
[27] David Berthelot, Tom Schumm, and Luke Metz. Began: Boundary equilibrium generative
adversarial networks. arXiv preprint arXiv:1703.10717, 2017.
[28] Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, and Serge Belongie. Stacked generative
adversarial networks. arXiv preprint arXiv:1612.04357, 2016.
[29] Zihang Dai, Amjad Almahairi, Philip Bachman, Eduard Hovy, and Aaron Courville. Calibrating
energy-based generative adversarial networks. arXiv preprint arXiv:1702.01691, 2017.
[30] D Warde-Farley and Y Bengio. Improving generative adversarial networks with denoising
feature matching. ICLR submissions, 8, 2017.
[31] Dilin Wang and Qiang Liu. Learning to draw samples: With application to amortized mle for
generative adversarial learning. arXiv preprint arXiv:1611.01722, 2016.
[32] Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with
auxiliary classifier gans. arXiv preprint arXiv:1610.09585, 2016.
[33] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. Proc. ICML, 2015.
[34] Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks. arXiv:1511.06390, 2015.
[35] Alessandra Tosi, S?ren Hauberg, Alfredo Vellido, and Neil D. Lawrence. Metrics for probabilistic geometries. 2014.
[36] Bharath K. Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Scholkopf, and Gert
R. G. Lanckriet. On integral probability metrics, -divergences and binary classification. 2009.
[37] I. Ekeland and T. Turnbull. Infinite-dimensional Optimization and Convexity. The University of
Chicago Press, 1983.
[38] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward
neural networks. In International conference on artificial intelligence and statistics, pages
249?256, 2010.
[39] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. arXiv preprint arXiv:1502.01852,
2015.

11

"
1998,Viewing Classifier Systems as Model Free Learning in POMDPs,,1492-viewing-classifier-systems-as-model-free-learning-in-pomdps.pdf,Abstract Missing,"Viewing Classifier Systems
as Model Free Learning in POMDPs

Akira Hayashi and Nobuo Suematsu
Faculty of Information Sciences
Hiroshima City University
3-4-1 Ozuka-higashi, Asaminami-ku, Hiroshima, 731-3194 Japan
{akira,suematsu }@im.hiroshima-cu.ac.jp

Abstract
Classifier systems are now viewed disappointing because of their problems such as the rule strength vs rule set performance problem and the
credit assignment problem. In order to solve the problems, we have developed a hybrid classifier system: GLS (Generalization Learning System). In designing GLS, we view CSs as model free learning in POMDPs
and take a hybrid approach to finding the best generalization, given the
total number of rules. GLS uses the policy improvement procedure by
Jaakkola et al. for an locally optimal stochastic policy when a set of
rule conditions is given. GLS uses GA to search for the best set of rule
conditions.

1

INTRODUCTION

Classifier systems (CSs) (Holland 1986) have been among the most used in reinforcement
learning. Some of the advantages of CSs are (1) they have a built-in feature (the use of
don't care symbols ""#"") for input generalization, and (2) the complexity of pOlicies can
be controlled by restricting the number of rules. In spite of these attractive features, CSs
are now viewed somewhat disappointing because of their problems (Wilson and Goldberg
1989; Westerdale 1997). Among them are the rule strength vs rule set performance problem, the definition of the rule strength parameter, and the credit assignment (BBA vs PSP)
problem.
In order to solve the problems, we have developed a hybrid classifier system: GLS (Generalization Learning System). GLS is based on the recent progress ofRL research in partially
observable Markov decision processes (POMDPs). In POMDPs, the environments are really Markovian, but the agent cannot identify the state from the current observation. It may
be due to noisy sensing or perceptual aliasing. Perceptual aliasing occurs when the sensor
returns the same observation in multiple states. Note that even for a completely observable

A. Hayashi and N. Suematsu

990

MDP, the use of don't care symbols for input generalization will make the process as if it
were partially observable.
In designing GLS, we view CSs as RL in POMDPs and take a hybrid approach to finding
the best generalization, given the total number of rules. GLS uses the policy improvement
procedure in Jaakkola et a!. (1994) for an locally optimal stochastic policy when a set of
rule conditions is given. GLS uses GA to search for the best set of rule conditions.
The paper is organized as follows. Since CS problems are easier to understand from GLS
perspective, we introduce Jaakkola et a!. (1994), propose GLS, and then discuss CS problems.

2

LEARNING IN POMDPS

Jaakkola et a1. (1994) consider POMDPs with perceptual aliasing and memoryless stochastic policies. Following the authors, let us call the observations messages. Therefore, a
policy is a mapping from messages to probability distributions (PDs) over the actions.
Given a policy 7r, the value of a state s, V7!' (s), is defined for POMDPs just as for MDPs.
Then, the value of a message m under policy 7r, V7!' (m ), can be defined as follows:

V7!'(m) = LP7!'(slm)V7!'(s)

(1)

sES

where P7!' (slm) is the probability that the state is s when the message is m under the policy
7r.

Then, the following holds.
N

lim'""' E{R(st, at) -R

N-+(X)~

t=l
E{V(s) Is

--t

lSI

= s}

m}

(2)
(3)

where St and at refer to the state and the action taken at the tth step respectively, R( St, at)
is the immediate reward at the tth step, R is the (unknown) gain (Le. the average reward
per step). s --t m refers to all the instances where m is observed in sand E{? I s --t m} is
a Monte-Carlo expectation.
In order to compute E{V(s)

Is

--t

m}, Jaakkola et a1. showed a Monte-Carlo procedure:

1
vt(m) = 'k{

Rtl

+rl,IRtl+l + rl,2Rtl+2 + ... + rl ,t-tIRt

+

Rt2

+r2,IRt2 +l + r2,2Rt2+2 + ... + r2,t-t2Rt

(4)

+ Rtk +rk ,IRtdl + ... + rk ,t-tkRtl
where tk denotes. the time step corresponding to the kth occurrence of the message m,
R t = R(st, at) - R for every t, rk,T indicates the discounting at the Tth step in the kth
sequence. By estimating R and by suitably setting rk ,T, Vt(m) converges to V7!'(m).
Q7!' (m, a), Q-value of the message m for the action a under the policy 7r, is also defined
and computed in the same way.
Jaakkola et a1. have developed a policy improvement method:
Step 1 Evaluate the current policy 7r by computing V7!' (m) and Q7!' (m, a) for each m and
a.

Viewing Classifier Systems as Model Free Learning in POMDPs

991

Step 2 Test for any m whether max a Q1r (m, a) > V 1r (m) holds. If 110t, then return 7r.
Step 3 For each m and a, define 7r 1 (alm) as follows:
7r 1 (aim) = 1.0 when a = argmaxaQ1r(m, a),
7r 1 (aim) = 0.0 otherwise.
f
f
Then, define 7r as 7r (aim) = (1 - ? )7r( aim) + ?7r 1 (aim)
Step 4 Set the new policy as 7r = 7r f , and goto Stepl.

3

GLS

Each rule in GLS consists of a condition part, an action part, and an evaluation
part: Rule = (Condit'i on, Action, Evaluation). The condition part is a string c
over the alphabet {O, 1, #}, and is compared with a binary sensor message. # is a
don't care symbol, and matches 0 and 1. When the condition c matches the message, the action is randomly selected using the PD in the action part: Action =
(p(allc),p(a21c), ... ,p(a IA!lc)), I:j'!\ p(ajlc) = 1.0 where IAI is the total number ofactions. The evaluation part records the value of the condition V (c) and the Q-values of the
condition action pairs Q(c, a): Evaluation = (V(c), Q(c, ad , Q(c, a2), ... ,Q(c, a lAI))'
Each rule set consists of N rules, {Rulel, Rule2,"""" RuleN}. N, the total number of
rules in a rule set, is a design parameter to control the complexity of policies. All the rules
except the last one are called standard rules. The last rule Rule N is a special rule which is
called the default rule. The condition part of the default rule is a string of # 's and matches
any message.
Learning in GLS proceeds as follows: (1 )Initialization: randomly generate an initial population of M rule sets, (2)Policy Evaluation and Improvement: for each rule set, repeat a
policy evaluation and improvement cycle for a suboptimal policy, then, record the gain of
the policy for each rule set, (3)Genetic Algorithm: use the gain of each rule set as its fitness measure and produce a new generation of rule sets, (4) Repeat: repeat from the policy
evaluation and improvement step with the new generation of rule sets.
In (2)Policy Evaluation and Improvement, GLS repeats the following cycle for each rule
set.

Step 1 Set ? sufficiently small. Set t max sufficiently large.
Step 2 Repeat for 1 :::; t :::; t max ?
1. Make an observation of the environment and receive a message mt from the
sensor.
2. From all the rules whose condition matches the message mt, find the rule
whose condition is the most specific l . Let us call the rule the active rule.
3. Select the next action at randomly according to the PD in the action part of
the active rule, execute the action, and receive the reward R( St, at) from the
environment. (The state St is not observable.)
4. Update the current estimate of the gain R from its previous estimate and
R( St, ad . Let R t = R( St , ad - R. For each rule, consider its condition Ci
as (a generalization of) a message, and update its evaluation part V (Ci ) and
Q(c;, aHa E A) using Eq.(4).
Step 3 Check whether the following holds. If not, exit.
3i (1 :::; i :::; N), max a Q (Ci , a) > V (cd
Step 4 Improve the current policy according to the method in the previous section, and
update the action part of the corresponding rules and goto Step 2.
IThe most specific rule has the least number of #'s. This is intended only for saving the number
of rules.

A. Hayashi and N. Suematsu

992

GLS extracts the condition parts of all the rules in a rule set and concatenates them to
form a string. The string will be an individual to be manipulated by the genetic algorithm
(GA). The genetic algorithm used in GLS is a fairly standard one. GLS combines the SGA
(the simple genetic algorithm) (Goldberg 1989) with the elitist keeping strategy. The SGA
is composed of three genetic operators: selection, crossover, and mutation. The fitness
proportional selection and the single-point crossover are used. The three operators are
applied to an entire population at each generation. Since the original SGA does not consider
#'s in the rule conditions, we modified SGA as follows. When GLS randomly generates
an initial population of rule sets, it generates # at each allele position in rule conditions
according to the probability P#.

4

CS PROBLEMS AND GLS

In the history of classifier systems, there were two quite different approaches: the Michigan
approach (Holland and Reitman 1978), and the Pittsburgh (Pitt) approach (Dejong 1988).
In the Michigan approach, each rule is considered as an individual and the rule set as the
population in GA. Each rule has its strength parameter, which is based on its future payoff
and is used as the fitness measure in GA. These aspects of the approach cause many problems. One is the rule strength vs rule set performance problem. Can we collect only strong
rules and get the best rule set performance? Not necessarily. A strong rule may cooperate
with weak rules to increase its payoff. Then, how can we define and compute the strength
parameter for the best rule set performance? In spite of its problems, this approach is now
so much more popular than the other, that when people simply say classifier systems, they
refer to Michigan type classifier systems. In the Pitt approach, the problems of the Michigan approach are avoided by requiring GA to evaluate a whole rule set. In the approach, a
rule set is considered as an individual and multiple rule sets are kept as the population. The
problem of the Pitt approach is its computational difficulties.

GLS can be considered as a combination of the Michigan and Pitt approaches. GA in GLS
works as that in the Pitt approach. It evaluates a total rule set, and completely avoids the
rule strength vs rule set performance problem in the Michigan approach. As the Michigan
type CSs, GLS evaluates each rule to improve the policy. This alleviates the computational
burden in the Pitt approach. Moreover, GLS evaluates each rule in a more formal and sound
way than the Michigan approach. The values, V(c), and Q(c, a), are defined on the basis
of POMDPs, and the policy improvement procedure using the values is guaranteed to find
a local maximum.
Westerdale (1997) has recently made an excellent analysis of problematic behaviors of
Michigan type CSs. Two popular methods for credit assignment in CSs are the bucket
brigade algorithm (BBA) (Holland 1986) and the profit sharing plan (PSP) (Grefenstette
1988). Westerdale shows that BBA does not work in POMDPs. He insists that PSP with
infinite time span is necessary for the right credit assignment, although he does not show
how to carry out the computation. GLS does not use BBA or PSP. GLS uses the Monte
Carlo procedure, Eq.(4), to compute the value of each condition action pair. The series
in Eq.(4) is slow to converge. But, this is the cost we have to pay for the right credit
assignment in POMDPs. Westerdale points out another CS problem. He claims that a
distinction must be made between the availability and the payoff of rules. We agree with
him. As he says, if the expected payoff of Rule 1 is twice as much as Rule 2, then we
want to a/ways choose Rule 1. GLS makes the distinction. The probability of a stochastic
policy 71'(alc) in GLS corresponds to the availability, and the value of a condition action
pair Q ( c, a) corresponds to the payoff.
Samuel System (Grefenstette et a1. 1990) can also be considered as a combination of the
Michigan and Pitt approaches. Samuel is a highly sophisticated system which has lots of
features. We conjecture, however, that Samuel is not free from the CS problems which

Viewing Classifier Systems as Model Free Learning in POMDPs

993

Westerdale has analyzed. This is because Samuel uses PSP for credit assignment, and
Samuel uses the payoff of each rule for action selection, and does not make a distinction
between the availability and the payoff of rules.
xes (Wilson 1995) seems to be an exceptionally reliable Michigan?type es. In xes, each
rule's fitness is based not on its future payoff but on the prediction accuracy of its future
payoff (XeS uses BBA for credit assignment). Wilson reports that xes's population tends
to form a complete and accurate mapping from sensor messages and actions to payoff
predictions. We conjecture that xes tries to build the most general Markovian model of
the environment. Therefore, it will be difficult to apply xes when the environment is not
Markovian, or when we cannot afford the number of rules enough to build a Markovian
model of the environment, even if the environment itself is Markovian. As we will see in
the next section, GLS is intended exactly for these situations.
Kaelbling et a1. (19%) surveys methods for input generalization when reward is delayed.
The methods use a function approximator to represent the value function by mapping a state
description to a value . Since they use value iteration or Q?leaming anyway, it is difficult to
apply the methods when the generalization violates the Markov assumption and induces a
POMDP.

5

EXPERIMENTS

We have tested GLS with some of the representative problems in es literature. Fig. 1 shows
Grefl world (Grefenstette 1987). In Grefl world, we used GLS to find the smallest rule set
which is necessary for the optimal performance. Since this is not a POMDP but an MDP, the
optimal policy can easily be learned when we have a corresponding rule for each of the 16
states. However, when the total number of rules is less than that of states, the environment
looks like a POMDP to the learning agent, even if the environment itself is an MDP. The
graph shows how the gain of the best rule set in the population changes with the generation.
We can see from the figure that four rules are enough for the optimal performance. Also
note that the saving of the rules is achieved by selecting the most specific matching rule
as an active rule. The rule set with this rule selection is called the defallit hierarchy in es
literature.
payoff

150
200

'i

~~~--------~

ISO ....................................... .. ..........~ ................. .
100
N: . l N=3 N=2

50

M ? ?? ? ? _ _

O L-~~__~~~~
~L~~
?I ~
.. -.~
. ~.-~
-

o

10

15

10

15

30

35

40

g.!ner.a tioruJ

Figure 1: LEFT: GREF1 World. States {O, 1,2, 3} are the start states and states {12.13, 14, 15 }
are the end states. In each state, the agent gets the state number (4 bits) as a message, and chooses
an action a,b,c, or d. When the agent reaches the end states, he receives reward 1000 in state 13, but
reward 0 in other states. Then the agent is put in one of the start states with equal probability. We
added 10% action errors to make the process ergodic. When an action error occurs, the agent moves
to one of the 16 states with equal probability.
RIGHT: Gain of the best rule set. Parameters: tma ;r =: 10000. ? =: 0.10. M =: 10. N =:
2,3 , 4, P# =: 0.33. For N =: 4, the best rule set at the 40 th generation was { if 0101 (State 5)
then a 1.0, if 1010 (State 10) then c 1.0, if ##11 (States 3,7,11,15) then d 1.0, if #### (Default
Rule) then b 1.0}.

A. Hayashi and N. Suematsu

994

oo~~~~~~~~~--~

80
70

60

BlillaD

a

II
II II

a

a

30
N06NoS-

20

10 L -......~~~oo4---'---'!'Pz.:tim:=o?.:...1-_""""....
-.-J.
o W 20 30 ~ ~ 60 m 80 00 ~
aenerations

Figure 2: LEFf: McCallum's Maze. We show the state numbers in the left, and the messages in the
right. States 8 and 9 are the start states, and state G is the goal state. In each state, the agent receives
a sensor message which is 4 bit long, Each bit in the message tells whether a wall exists in each of
the four directions. From each state, the agent moves to one of the adjacent states. When the agent
reaches the goal state, he receives reward 1000. The agent is then put in one of the start states with
equal probability.
RIGHT: Gain of the best rule set. Parameters: t mBX = 50000, ~ = 0.10, M = 10, N = 5,6, P# =
0.33.

Fig. 2 is a POMDP known as as McCallum's Maze (McCallum 1993). Thanks to the use
of stochastic policies, GLS achieves near optimal gain for memoryless poliCies. Note that
no memoryless deterministic policy can take the agent to the goal for this problem.
We have seen GLS's generalization capability for an MDP in Grefl World, the advantage
of stochastic policies for a POMDP in McCallum's maze. In Woods7 (Wilson 1994), we
attempt to test GLS's generalization capability for a POMDP. See Fig. 3. Since each sensor
message is 16 bit long, and the conditions of GLS rules can have either O,l,or # for each of
the 16 bits, there are 3 16 possible conditions in total. When we notice that there are only
92 different actual sensor messages in the environment, it seems quite difficult to discover
them only by using GA. In fact, when we ran GLS for the first time, the standard rules
very rarely matched the messages and the default rule took over most of the time. In order
to avoid the no matching rule problem, we made the number of rules in a rule set large
(N = 100), increased P# from 0.33 in the previous problems to 0.70.
The problem was independently attacked by other methods. Wilson applied his ZCS, zeroth
level classifier system, to Woods7 (Wilson 1994). The gain was 0.20. ZCS has a special
covering procedure to tum around the no matching rule problem. The covering procedure
generates a rule which matches a message when none of the current rules matches the
message. We expect further improvement on the gain, if we equip GLS with some covering
procedure.

6

SUMMARY

In order to solve the CS problems such as the rule strength vs rule set performance problem
and the credit assignment problem, we have developed a hybrid classifier system: GLS.
We notice that generalization often leads to state aliasing. Therefore, in designing GLS,
we view CSs as model free learning in POMDPs and take a hybrid approach to finding
the best generalization, given the total number of rules. GLS uses the policy improvement
procedure by Jaakkola et a1. for an locally optimal stochastic policy when a set of rule
conditions is given. GLS uses GA to search for the best set of rule conditions.

995

Viewing Classifier Systems as Model Free Learning in POMDPs
0.24
????? , ????? 0 . ? ??????...? .. ???????.??? 00 ? ? , ????? ,0 ?? . , . ""

??

. or o . . . . . F . ???? .. . F .? ? .???. ,D . ????? . F ?? , ? . ? . ? . r o., ..... .
.. . . ?? 0 . ? . ?? . . . 00 . . . ???. F . ? ?.?.? ?. ????? ..
? . . . .? . ? ?? . ?? ?? ? . .???? .? ??? 0 . . . . . . 0 . . . . . . . .. F ?????? 0 ?? ???

. . . r ' "" .. oro ..... .. . oro .. _.. . .... F . ?.???.?? 00 ???? . ? r ... .
? . ? 00 .. . . ?? . .???. . . ? ? ? . . ? ? ? ? . .. .??. 0 . . . ?..?? . ???? ? ??. . 0 ?? .

:~r~:.:: : :~~::::: : :~r: ::: : ::~::: :::: :: : ~t~ : :: :::: :~:: ::::: 'i

? ???????? ? ???.??.????????????.? ? ?.? . ?? ? ? . .??.????. 0 . ?.?? . .
. . ? 00 . . .. ?. , 0 . . . . . .

. . . F . . ..... r ......

. .... . . . . ~

. . . . . _ ? . 0 .?? . . ..? 00 ? . .. ? ?. 0 . ""

. ? ?? .

. o.. . . .. . . Fo . .. . . ... F... ... .. or . . . . oro.

.. .

~

.. .. . . . ... . . . . ... ... .. . .. .. . . . .

? . 0 , , , ?? . ? . . . . . ? ? . .??.?? . 0 ? . ..? . ??.??? 0 ???? . ? 0 ?????? . 0 ????

. . r . .. . ... F . ?? . ? .. ?? . . ?.. ? F . . ? ????. . Fo . ? . ... . r .? ?. ?. oF . . .
. . 0 , .. _ . . ? 00 ?. . ? .. .? . . . . ? ?? 0 .?? ??.? , .

... ... .. . . . . . . . . . . .. . 0 ? .? ?? ? ? . ?.? ""

.. . F .. .. . ? ? ? oro ...

.. . r . . . .. . . ""

. . . 00 . ? . ? . . ??.?? ?. ?? . 0 . . . . . .

.??? ? ? ? ? 0 ? ?.? .??? ..

....... 0 . . ... . ....... .

F . . ? .?.?? r .

... . 0 F .. ????.?

. . . 00 . . . . ??? 0 ? .? ?? . 0 ..? ?.?? .

.----~~~~~~~.....,_.__,_""

0.23

02 2
021
0.2
0.19

0.18
0.17
0.16
0.15
0.14

L.-~~~~~~~_---.J

o

W W

~

~

~

~

~

~

~

~

geoentionJ

Figure 3: LEFT: Woods7.Each cell is either empty""."", contains a stone ""0"", or contains food ""F'.
The cells which contain a stone are not passable, and the cells which contain food are goals. In each
cell, the agent receives a 2 * 8 = 16 bit long sensor message, which tells the contents of the eight
adjacent cells. From each cell, the agent can move to one of the eight adjacent cells. When the agent
reaches a cell which contains food, he receives reward 1. The agent is then put in one of the empty
cells with equal probability.
RlGHT:Gain of the best rule set. Parameters: t ma x = 10000, to = 0.10, M = 10, N = 100, P#
0.70.

=

References
Dejong, K. A. (1988). Learning with genetic algorithms: An overview. Machine Learning, 3:121-138.
Goldberg, D. E. (1989). Genetic Algorithms in Search, Optimization, and Machine
Learning. Addison-Wesley.
Grefenstette, J. J. (1987). Multilevel credit assignment in a genetic learning system. In
Proc. Second Int. Con! on Genetic Algorithms, pp. 202-209.
Grefenstette, J. J. (1988). Credit assignment in rule discovery systems based on genetic
algorithms. Machine Learning, 3:225-245.
Grefenstette, J. J., C. L. Ramsey, and A. C. Schultz (1990). Learning sequential decision
rules using simulation and competition. Machine Learning, 5:355-381.
Holland, J. H. (1986). Escaping brittleness: the possibilities of general purpose learning
algorithms applied to parallel rule-based systems. In Machine Learning II, pp. 593623. Morgan Kaufmann.
Holland, J. H. and J. S. Reitman (1978). Cognitive systems based on adaptive algorithms. In D. A. Waterman and F. Hayes-Roth (Eds.), Pattern-directed inference
systems. Academic Press.
Jaakkola, T., S. P. Singh, and M. I. Jordan (1994). Reinforcement learning algorithm for
partially observable markov decision problems. In Advances of Neural Information
Processing Systems 7, pp. 345-352.
Kaelbling, L. P., M. L. Littman, and A. W. Moore (1996). Reinforcement learning: A
survey. Journal of Artificial Intelligence Research, 4:237-285.
McCallum, R. A. (1993). Overcoming incomplete perception with utile distinction
memory. In Proc . the Tenth Int. Con! on Machine Learning, pp. 190-196.
Westerdale, T. H. (1997). Classifier systems - no wonder they don't work. In Proc. Second Annual Genetic Programming Conference, pp. 529-537.
Wilson, S. W. (1994). Zcs: A zeroth order classifier system. Evolutionary Computation ,
2(1): 1-18.
Wilson, S. W. (1995). Classifier fitness based on accuracy. Evolutionary Computation ,
3(2): 149-175.
Wilson, S. W. and D. E. Goldberg (1989). A critical review of classifier systems. In Proc .
Third Int . Con! on Genetic Algorithms, pp. 244-255.

"
1987,A Novel Net that Learns Sequential Decision Process,,74-a-novel-net-that-learns-sequential-decision-process.pdf,Abstract Missing,"760

A NOVEL NET THAT LEARNS
SEQUENTIAL DECISION PROCESS
G.Z. SUN, Y.C. LEE and H.H. CHEN
Department of PhYJicJ and AJtronomy
and
InJtitute for Advanced Computer StudieJ

UNIVERSITY OF MARYLAND,COLLEGE PARK,MD 20742

ABSTRACT
We propose a new scheme to construct neural networks to classify patterns. The new scheme has several novel features :
1. We focus attention on the important attributes of patterns in ranking
order. Extract the most important ones first and the less important
ones later.
2. In training we use the information as a measure instead of the error
function.
3. A multi-percept ron-like architecture is formed auomatically. Decision
is made according to the tree structure of learned attributes.
This new scheme is expected to self-organize and perform well in large scale
problems.

? American Institute of Physics 1988

761

1

INTRODUCTION

It is well known that two-layered percept ron with binary connections but no
hidden units is unsuitable as a classifier due to its limited power [1]. It cannot
solve even the simple exclusive-or problem. Two extensions have been prop'osed to remedy this problem. The first is to use higher order connections
l2]. It has been demonstrated that high order connections could in many
cases solve the problem with speed and high accuracy [3], [4]. The representations in general are more local than distributive. The main drawback
is however the combinatorial explosion of the number of high-order terms.
Some kind of heuristic judgement has to be made in the choice of these terms
to be represented in the network.
A second proposal is the multi-layered binary network with hidden units
r5]. These hidden units function as features extracted from the bottom input
layer to facilitate the classification of patterns by the output units. In order
to train the weights, learning algorithms have been proposed that backpropagate the errors from the visible output layer to the hidden layers for
eventual adaptation to the desired values. The multi-layered networks enjoy
great popularity in their flexibility.
However, there are also problems in implementing the multi-layered nets.
Firstly, there is the problem of allocating the resources. Namely, how many
hidden units would be optimal for a particular problem. If we allocate too
many, it is not only wasteful but also could negatively affect the performance
of the network. Since too many hidden units implies too many free parameters to fit specifically the training patterns. Their ability to generalize to
noval test patterns would be adversely affected. On the other hand, if too
few hidden units were allocated then the network would not have the power
even to represent the trainig set. How could one judge beforehand how many
are needed in solving a problem? This is similar to the problem encountered
in the high order net in its choice of high order terms to be represented.
Secondly, there is also the problem of scaling up the network. Since the
network represents a parallel or coorperative process of the whole system,
each added unit would interact with every other units. This would become
a serious problem when the size of our patterns becomes large.
Thirdly, there is no sequential communication among the patterns in the
conventional network. To accomplish a cognitive function we would need
the patterns to interact and communicate with each other as the human
reasoning does. It is difficult to envision such an interacton in current systems
which are basically input-output mappings.

2

THE NEW SCHEME

In this paper, we would like to propose a scheme that constructs a network
taking advantages of both the parallel and the sequential processes.
We note that in order to classify patterns, one has to extract the intrinsic
features, which we call attributes. For a complex pattern set, there may
be a large number of attributes. But differnt attributes may have different

762

ranking of importance. Instead of ext racing them all simultaneously it may
be wiser to extract them sequentially in order of its importance [6], [7]. Here
the importance of an attribute is determined by its ability to partition the
pattern set into sub-categories. A measure of this ability of a processing unit
should be based on the extracted information. For simplicity, let us assume
that there are only two categories so that the units have only binary output
values 1 and
but the input patterns may have analog representations). We
call these units, including their connection weights to the input layer, nodes.
For given connection weights, the patterns that are classified by a node as
in category 1 may have their true classifications either 1 or 0. Similarly, the
patterns that are classified by a node as in category 0 may also have their
true classifications either 1 or o. As a result, four groups of patterns are
formed: (1,1), (0,0), (1,0), (0,1). We then need to judge on the efficiency of
the node by its ability to split these patterns optimally. To do this we shall
construct the impurity fuctions for the node. Before splitting, the impurity
of the input patterns reaching the node is given by

?(

(1)

where pt = Nf / N is the probability of being truely classified as in category
1, and P~ = N~/N is the probability of being truely classified as in category
o. After splitting, the patterns are channelled into two branches, the impurity
becomes

1(1 = -Pt

L
j=O,1

P(j, 1) logP(j, 1) - P;

L

P(j, O)logP(j, 0)

(2)

j=O,1

where Pi = Ni / N is the probability of being classified by the node as in
category 1, P; = N8/N is the probability of being classified by the node as
in category 0, and P(j, i) is the probability of a pattern, which should be in
category j, but is classified by the node as in category i. The difference

(3)
represents the decrease of the impurity at the node after splitting. It is the
quantity that we seek to optimize at each node. The logarithm in the impurity function come from the information entropy of Shannon and Weaver.
For all practical purI?ose, we found the. optimization of (3) the same as maximizing the entropy l6]

where Ni is the number of training patterns classified by the node as in
category i, N ij is the number of training patterns with true classification in
category i but classified by the node as in category j. Later we shall call the
terms in the first bracket SI and the second S2. Obviously, we have
i = 0,1

763

After we trained the first unit, the training patterns were split into two
branches by the unit. If the classificaton in either one of these two branches
is pure enough, or equivalently either one of Sl and S2 is fairly close to 1,
then we would terminate that branch ( or branches) as a leaf of the decision
tree, and classify the patterns as such. On the other hand, if either branch is
not pure enough, we add additional node to split the pattern set further. The
subsequent unit is trained with only those patterns channeled through this
branch. These operations are repeated until all the branches are terminated
as leaves.

3

LEARNING ALGORITHM

We used the stochastic gradient descent method to learn the weights of each
node. The training set for each node are those patterns being channeled to
this node. As stated in the previous section, we seek to maximize the entropy
function S. The learning of the weights is therefore conducted through

oS
1:::. Wj = 11 ow-

(5)

J

Where 11 is the learning rate. The gradient of S can be calculated from the
following equation

oS = ~ [(1 _ 2NJ1) oNn
oWj
N
Nl oW;

(1 _ 2NJo) ONIO
NJ oWj

+ (1 _ 2 Nil ) oNOl +
Nl oWj

+ (1 _ 2N'fO) ONoo]
NJ oWj

(6)

Using analog units

or =

1 + exp( -

we have

oor =

ow-

1

Lj WjII)

orC1 _ or)!';
J

J

Furthermore, let Ar
then

N;;

=

t.

= 1 or 0 being the

[iA'

+ (1 -

(7)

(8)

true answer for the input pattern r ,

i)(1 - A')

1[i O' + (1 - j)(1 - 0') 1

(9)

Substituting these into equation (5), we get

1:::.Wj = 2T} :L[2Ar(NU - NlO)
r
Nl
No

+ Ni~ -

Ni;]or(l - or)IJ
(10)
No
Nl
In applying the formula (10),instead of calculating the whole summation at
once, we update the weights for each pattern individually. Meanwhile we
update N ij in accord with equation (9).

764

Figure 1: The given classification tree, where 01 , O'l and 03 are chosen to be
all zeros in the numerical example.

4

AN EXAMPLE

To illustrate our method, we construct an example which is itself a decision
tree. Assuming there are three hidden variables ai, a'l, a3, a pattern is given
by a ten-dimensional vector II, I'l, ... , 110 , constructed from the three hidden
variables as follows

+ a3

II

-

al

1'l

-

2al - a'l

16 17 -

a3 - 2a'l

18

-

2al

19

-

4a3 - 3a l

13
I""
Is

+ 2a'l + 3a3

-

al

-

5al - 4a""

110

2a3

a3 - al

2al

+ 3a3
+ 2a'l + 2 a 3?

A given pattern is classified as either 1 (yes) or 0 (no) according to the
corresponding values of the hidden variables ai, a'l, a3. The actual decision
is derived from the decision tree in Fig.1.
In order to learn this classification tree, we construct a training set of 5000
patterns generated by randomly chosen values ai, a'l, a3 in the interval -1 to
+1. We randomly choose the initial weights for each node, and terminate

765

5=0.79

G

51 =0.60/

~=0.87

""

G
G

51 =0.65/

VI

51 = 0.85/
(SS/S)W i

(fIg (2519/35)

,S2= 0.88

~OCE:S] (16171114)

~= 0.73

5. =0.90/
(92/S)rul

52=0.96
ffQ](548/12)

Figure 2: The learned classification tree structure
a branch as a leaf whenever the branch entropy is greater than 0.80. The
entropy is started at S = 0.65, and terminated at its maximum value S =
0.79 for the first node. The two branches of this node have the entropy
fuction valued at SI = 0.61, S2 = 0.87 respectively. This corrosponds to
2446 patterns channeled to the first branch and 2554 to the second. Since
S2 > 0.80 we terminate the second branch. Among 2554 patterns channeled
to the second branch there are 2519 patterns with true classification as no and
35 yes which are considered as errors. After completing the whole training
process, there are totally four nodes automatically introduced. The final
result is shown in a tree structure in Fig.2.
The total errors classified by the learned tree are 3.4 % of the 5000 trainig
patterns. After trainig we have tested the result using 10000 novel patterns,
the error among which is 3.2 %.

5

SUMMARY

We propose here a new scheme to construct neural network that can automatically learn the attributes sequentially to facilitate the classification
of patterns according to the ranking importance of each attribute. This
scheme uses information as a measure of the performance of each unit. It is

766

self-organized into a presumably optimal structure for a specific task. The
sequential learning procedure focuses attention of the network to the most
important attribute first and then branches out' to the less important attributes. This strategy of searching for attributes would alleviate the scale
up problem forced by the overall parallel back-propagation scheme. It also
avoids the problem of resource allocation encountered in the high-order net
and the multi-layered net. In the example we showed the performance of the
new method is satisfactory. We expect much better performance in problems
that demand large size of units.

6

acknowledgement

This work is partially supported by AFOSR under the grant 87-0388.

References
[1] M. Minsky and S. Papert, Perceptron, MIT Press Cambridge, Ma(1969).
[2] Y.C. Lee, G. Doolen, H.H. Chen, G.Z. Sun, T. Maxwell, H.Y. Lee and
C.L. Giles, Machine Learning Using A High Order Connection Netweork, Physica D22,776-306 (1986).
[3] H.H. Chen, Y.C. Lee, G.Z. Sun, H.Y. Lee, T. Maxwell and C.L. Giles,
High Order Connection Model For Associate Memory, AlP Proceedings
Vol.151,p.86, Ed. John Denker (1986).
[4] T. Maxwell, C.L. Giles, Y.C. Lee and H.H. Chen, Nonlinear Dynamics
of Artificial Neural System, AlP Proceedings Vol.151,p.299, Ed. John
Denker(1986).
[5] D. Rummenlhart and J. McClelland, Parallel Distributit'e Processing,
MIT Press(1986).
[6] L. Breiman, J. Friedman, R. Olshen, C.J. Stone, Classification and Regression Trees,Wadsworth Belmont, California(1984).
[7] J.R. Quinlan, Machine Learning, Vol.1 No.1(1986).

"
2008,Privacy-preserving logistic regression,,3486-privacy-preserving-logistic-regression.pdf,"This paper addresses the important tradeoff between privacy and learnability, when designing algorithms for learning from private databases. First we apply an idea of Dwork et al. to design a specific privacy-preserving machine learning algorithm, logistic regression. This involves bounding the sensitivity of logistic regression, and perturbing the learned classifier with noise proportional to the sensitivity. Noting that the approach of Dwork et al. has limitations when applied to other machine learning algorithms, we then present another privacy-preserving logistic regression algorithm. The algorithm is based on solving a perturbed objective, and does not depend on the sensitivity. We prove that our algorithm preserves privacy in the model due to Dwork et al., and we provide a learning performance guarantee. Our work also reveals an interesting connection between regularization and privacy.","Privacy-preserving logistic regression

Kamalika Chaudhuri
Information Theory and Applications
University of California, San Diego
kamalika@soe.ucsd.edu

Claire Monteleoni?
Center for Computational Learning Systems
Columbia University
cmontel@ccls.columbia.edu

Abstract
This paper addresses the important tradeoff between privacy and learnability,
when designing algorithms for learning from private databases. We focus on
privacy-preserving logistic regression. First we apply an idea of Dwork et al. [6]
to design a privacy-preserving logistic regression algorithm. This involves bounding the sensitivity of regularized logistic regression, and perturbing the learned
classifier with noise proportional to the sensitivity.
We then provide a privacy-preserving regularized logistic regression algorithm
based on a new privacy-preserving technique: solving a perturbed optimization
problem. We prove that our algorithm preserves privacy in the model due to [6].
We provide learning guarantees for both algorithms, which are tighter for our new
algorithm, in cases in which one would typically apply logistic regression. Experiments demonstrate improved learning performance of our method, versus the
sensitivity method. Our privacy-preserving technique does not depend on the sensitivity of the function, and extends easily to a class of convex loss functions. Our
work also reveals an interesting connection between regularization and privacy.

1

Introduction

Privacy-preserving machine learning is an emerging problem, due in part to the increased reliance on
the internet for day-to-day tasks such as banking, shopping, and social networking. Moreover, private data such as medical and financial records are increasingly being digitized, stored, and managed
by independent companies. In the literature on cryptography and information security, data privacy
definitions have been proposed, however designing machine learning algorithms that adhere to them
has not been well-explored. On the other hand, data-mining algorithms have been introduced that
aim to respect other notions of privacy that may be less formally justified.
Our goal is to bridge the gap between approaches in the cryptography and information security community, and those in the data-mining community. This is necessary, as there is a tradeoff between
the privacy of a protocol, and the learnability of functions that respect the protocol. In addition to
the specific contributions of our paper, we hope to encourage the machine learning community to
embrace the goals of privacy-preserving machine learning, as it is still a fledgling endeavor.
In this work, we provide algorithms for learning in a privacy model introduced by Dwork et al. [6].
The -differential privacy model limits how much information an adversary can gain about a particular private value, by observing a function learned from a database containing that value, even if
she knows every other value in the database. An initial positive result [6] in this setting depends on
the sensitivity of the function to be learned, which is the maximum amount the function value can
change due to an arbitrary change in one input. Using this method requires bounding the sensitivity
of the function class to be learned, and then adding noise proportional to the sensitivity. This might
be difficult for some functions that are important for machine learning.
?

The majority of this work was done while at UC San Diego.

1

The contributions of this paper are as follows. First we apply the sensitivity-based method of designing privacy-preserving algorithms [6] to a specific machine learning algorithm, logistic regression.
Then we present a second privacy-preserving logistic regression algorithm. The second algorithm is
based on solving a perturbed objective function, and does not depend on the sensitivity. We prove
that the new method is private in the -differential privacy model. We provide learning performance
guarantees for both algorithms, which are tighter for our new algorithm, in cases in which one would
typically apply logistic regression. Finally, we provide experiments demonstrating superior learning
performance of our new method, with respect to the algorithm based on [6]. Our technique may have
broader applications, and we show that it can be applied to certain classes of optimization problems.
1.1

Overview and related work

At the first glance, it may seem that anonymizing a data-set ? namely, stripping it of identifying
information about individuals, such as names, addresses, etc ? is sufficient to preserve privacy.
However, this is problematic, because an adversary may have some auxiliary information, which
may even be publicly available, and which can be used to breach privacy. For more details on such
attacks, see [12].
To formally address this issue, we need a definition of privacy which works in the presence of
auxiliary knowledge by the adversary. The definition we use is due to Dwork et al. [6], and has been
used in several applications [4, 11, 2]. We explain this definition and privacy model in more detail
in Section 2.
Privacy and learning. The work most related to ours is [8] and [3]. [8] shows how to find classifiers
that preserve -differential privacy; however, their algorithm takes time exponential in d for inputs
in Rd . [3] provides a general method for publishing data-sets while preserving -differential privacy
such that the answers to all queries of a certain type with low VC-dimension are approximately
correct. However, their algorithm can also be computationally inefficient.
Additional related work. There has been a substantial amount of work on privacy in the literature,
spanning several communities. Much work on privacy has been done in the data-mining community
[1, 7], [14, 10], however the privacy definitions used in these papers are different, and some are susceptible to attacks when the adversary has some prior information. In contrast, the privacy definition
we use avoids these attacks, and is very strong.

2

Sensitivity and the -differential privacy model

Before we define the privacy model that we study, we will note a few preliminary points. Both in
that model, and for our algorithm and analyses, we assume that each value in the database is a real
vector with norm at most one. That is, a database contains values x1 , . . . , xn , where xi ? Rd ,
and kxi k ? 1 for all i ? {1, . . . , n}. This assumption is used in the privacy model. In addition,
we assume that when learning linear separators, the best separator passes through the origin. Note
that this is not an assumption that the data is separable, but instead an assumption that a vector?s
classification is based on its angle, regardless of its norm.
In both privacy-preserving logistic regression algorithms that we state, the output is a parameter
vector, w, which makes prediction SGN(w ? x), on a point x. For a vector x, we use ||x|| to denote
its Euclidean norm. For a function G(x) defined on Rd , we use ?G to denote its gradient and ?2 G
to denote its Hessian.
Privacy Definition. The privacy definition we use is due to Dwork et al. [6, 5]. In this model, users
have access to private data about individuals through a sanitization mechanism, usually denoted by
M . The interaction between the sanitization mechanism and an adversary is modelled as a sequence
of queries, made by the adversary, and responses, made by the sanitizer. The sanitizer, which is
typically a randomized algorithm, is said to preserve -differential privacy, if the private value of
any one individual in the data set does not affect the likelihood of a specific answer by the sanitizer
by more than .
More formally, -differential privacy can be defined as follows.
2

Definition 1 A randomized mechanism M provides -differential privacy, if, for all databases D1
and D2 which differ by at most one element, and for any t,
Pr[M (D1 ) = t]
? e
Pr[M (D2 ) = t]
It was shown in [6] that if a mechanism satisfies -differential privacy, then an adversary who knows
the private value of all the individuals in the data-set, except for one single individual, cannot figure
out the private value of the unknown individual, with sufficient confidence, from the responses of
the sanitizer. -differential privacy is therefore a very strong notion of privacy.
[6] also provides a general method for computing an approximation to any function f while preserving -differential privacy. Before we can describe their method, we need a definition.
Definition 2 For any function f with n inputs, we define the sensitivity S(f ) as the maximum, over
all inputs, of the difference in the value of f when one input of f is changed. That is,
S(f ) = max
|f (x1 , . . . , xn?1 , xn = a) ? f (x1 , . . . , xn?1 , xn = a0 )|
0
(a,a )

[6] shows that for any input x1 , . . . , xn , releasing f (x1 , . . . , xn ) + ?, where ? is a random variable
)
drawn from a Laplace distribution with mean 0 and standard deviation S(f
 preserves -differential
privacy.
In [13], Nissim et al. showed that given any input x to a function, and a function f , it is sufficient
)
to draw ? from a Laplace distribution with standard deviation SS(f
 , where SS(f ) is the smoothedsensitivity of f around x. Although this method sometimes requires adding a smaller amount of
noise to preserve privacy, in general, smoothed sensitivity of a function can be hard to compute.

3

A Simple Algorithm

Based on [6], one can come up with a simple algorithm for privacy-preserving logistic regression,
which adds noise to the classifier obtained by logistic regression, proportional to its sensitivity. From
2
Corollary 2, the sensitivity of logistic regression is at most n?
. This leads to Algorithm 1, which
obeys the privacy guarantees in Theorem 1.
Algorithm 1:
1. Compute w? , the classifier obtained by regularized logistic regression on the labelled examples (x1 , y1 ), . . . , (xn , yn ).
n?

2. Pick a noise vector ? according to the following density function: h(?) ? e? 2 ||?|| .
2
To pick such a vector, we choose the norm of ? from the ?(d, n?
) distribution, and the
direction of ? uniformly at random.
3. Output w? + ?.
Theorem 1 Let (x1 , y1 ), . . . , (xn , yn ) be a set of labelled points over Rd such that ||xi || ? 1 for
all i. Then, Algorithm 1 preserves -differential privacy.
P ROOF : The proof follows by a combination of [6], and Corollary 2, which states that the sensitivity
2
of logistic regression is at most n?
.
Lemma 1 Let G(w) and g(w) be two convex functions, which are continuous and differentiable at
g1
all points. If w1 = argminw G(w) and w2 = argminw G(w) + g(w), then, ||w1 ? w2 || ? G
. Here,
2
T 2
g1 = maxw ||?g(w)|| and G2 = minv minw v ? G(w)v, for any unit vector v.
The main idea of the proof is to examine the gradient and the Hessian of the functions G and g
around w1 and w2 . Due to lack of space, the full proof appears in the full version of our paper.
Corollary 2 Given a set of n examples x1 , . . . , xn in Rd , with labels y1 , . . . , yn , such that for all i,
2
.
||xi || ? 1, the sensitivity of logistic regression with regularization parameter ? is at most n?
3

P ROOF : We use a triangle inequality and the fact that G2 ? ? and g1 ?

1
n.



Learning Performance. In order to assess the performance of Algorithm 1, we first try to bound
the performance of Algorithm 1 on the training data. To do this, we need to define some notation.
For a classifier w, we use L(w) to denote the expected loss of w over the data distribution, and
?
?
L(w)
to denote the empirical average loss of w over the training data. In other words, L(w)
=
Pn
1
?yi wT xi
), where, (xi , yi ), i = 1, . . . , n are the training examples.
i=1 log(1 + e
n
Further, for a classifier w, we use the notation f? (w) to denote the quantity 12 ?||w||2 + L(w) and
?
f?? (w) to denote the quantity 21 ?||w||2 + L(w).
Our guarantees on this algorithm can be summarized
by the following lemma.
Lemma 3 Given a logistic regression problem with regularization parameter ?, let w1 be the classifier that minimizes f?? , and w2 be the classifier output by Algorithm 1 respectively. Then, with prob2
log2 (d/?)
.
ability 1 ? ? over the randomness in the privacy mechanism, f?? (w2 ) ? f?? (w1 ) + 2d (1+?)
?2 n2 2
Due to lack of space, the proof is deferred to the full version.
From Lemma 3, we see that performance of Algorithm 1 degrades with decreasing ?, and is poor in
particular when ? is very small. One question is, can we get a privacy-preserving approximation to
logistic regression, which has better performance bounds for small ?? To explore this, in the next
section, we look at a different algorithm.

4

A New Algorithm

In this section, we provide a new privacy-preserving algorithm for logistic regression. The input to
our algorithm is a set of examples x1 , . . . , xn over Rd such that ||xi || ? 1 for all i, a set of labels
y1 , . . . , yn for the examples, a regularization constant ? and a privacy parameter , and the output is
a vector w? in Rd . Our algorithm works as follows.
Algorithm 2:


1. We pick a random vector b from the density function h(b) ? e? 2 ||b|| . To implement this,
we pick the norm of b from the ?(d, 2 ) distribution, and the direction of b uniformly at
random.
2. Given examples x1 , . . . , xn , with labels y1 , . . . , yn and a regularization constant ?, we
Pn
T
T
compute w? = argminw 21 ?wT w + b nw + n1 i=1 log(1 + e?yi w xi ). Output w? .
We observe that our method solves a convex programming problem very similar to the logistic
regression convex program, and therefore it has running time similar to that of logistic regression.
In the sequel, we show that the output of Algorithm 2 is privacy preserving.
Theorem 2 Given a set of n examples x1 , . . . , xn over Rd , with labels y1 , . . . , yn , where for each
i, ||xi || ? 1, the output of Algorithm 2 preserves -differential privacy.
P ROOF : Let a and a0 be any two vectors over Rd with norm at most 1, and y, y 0 ?
{?1, 1}. For any such (a, y), (a0 , y 0 ), consider the inputs (x1 , y1 ), . . . , (xn?1 , yn?1 ), (a, y) and
(x1 , y1 ) . . . , (xn?1 , yn?1 ), (a0 , y 0 ). Then, for any w? output by our algorithm, there is a unique
value of b that maps the input to the output. This uniqueness holds, because both the regularization
function and the loss functions are differentiable everywhere.
Let the values of b for the first and second cases respectively, be b1 and b2 .
Since w? is the value that minimizes both the optimization problems, the derivative of both optimization functions at w? is 0.
This implies that for every b1 in the first case, there exists a b2 in the second case such that: b1 ?
0 0
ya
1
1
= b2 ? 1+eyy0 wa ?T a0 . Since ||a|| ? 1, ||a0 || ? 1, and 1+eyw
?1
?T a ? 1, and
1+eyw?T a
1+ey0 w?T a0
4

for any w? , ||b1 ? b2 || ? 2. Using the triangle inequality, ||b1 || ? 2 ? ||b2 || ? ||b1 || + 2. Therefore,
for any pair (a, y), (a0 , y 0 ),

h(b1 )
Pr[w? |x1 , . . . , xn?1 , y1 , . . . , yn?1 , xn = a, yn = y]
=
= e? 2 (||b1 ||?||b2 ||)
?
0
0
Pr[w |x1 , . . . , xn?1 , y1 , . . . , yn?1 , xn = a , yn = y ]
h(b2 )

where h(bi ) for i = 1, 2 is the density of bi . Since ?2 ? ||b1 || ? ||b2 || ? 2, this ratio is at most e .
theorem follows. 
We notice that the privacy guarantee for our algorithm does not depend on ?; in other words, for any
value of ?, our algorithm is private. On the other hand, as we show in Section 5, the performance of
our algorithm does degrade with decreasing ? in the worst case, although the degradation is better
than that of Algorithm 1 for ? < 1.
Other Applications. Our algorithm for privacy-preserving logistic regression can be generalized to
provide privacy-preserving outputs for more general convex optimization problems, so long as the
problems satisfy certain conditions. These conditions can be formalized in the theorem below.
Theorem 3 Let X = {x1 , . . . , xn } be a database containing private data of individuals.
Pn Suppose
we would like to compute a vector w that minimizes the function F (w) = G(w) + i=1 l(w, xi ),
over w ? Rd for some d, such that all of the following hold:
1. G(w) and l(w, xi ) are differentiable everywhere, and have continuous derivatives
2. G(w) is strongly convex and l(w, xi ) are convex for all i
3. ||?w l(w, x)|| ? ?, for any x.
?
Let b = B ? ?b, where B is drawn from ?(d, 2?
the surface of a d ), and b is drawn uniformly from
Pn
?
dimensional unit sphere. Then, computing w , where w? = argminw G(w) + i=1 l(w, xi ) + bT w,
provides -differential privacy.

5

Learning Guarantees

In this section, we show theoretical bounds on the number of samples required by the algorithms to
learn a linear classifier. For the rest of the section, we use the same notation used in Section 3.
First we show that, for Algorithm 2, the values of f?? (w2 ) and f?? (w1 ) are close.
Lemma 4 Given a logistic regression problem with regularization parameter ?, let w1 be the classifier that minimizes f?? , and w2 be the classifier output by Algorithm 2 respectively. Then, with
2
log2 (d/?)
.
probability 1 ? ? over the randomness in the privacy mechanism, f?? (w2 ) ? f?? (w1 ) + 8d ?n
2 2
The proof is in the full version of our paper. As desired, for ? < 1, we have attained a tighter bound
using Algorithm 2, than Lemma 3 for Algorithm 1.
Now we give a performance guarantee for Algorithm 2.
Theorem 4 Let w0 be a classifier with expected loss L over the data distribution. If the training ex2 d log( d )||w ||
0
?
amples are drawn independently from the data distribution, and if n > C max( ||w02|| ,
),
g 
g
for some constant C, then, with probability 1 ? ?, the classifier output by Algorithm 2 has loss at
most L + g over the data distribution.
P ROOF : Let w? be the classifier that minimizes f? (w) over the data distribution, and let w1 and w2
T
be the classifiers that minimize f?? (w) and f?? (w) + b nw over the data distribution respectively. We
can use an analysis as in [15] to write that:
L(w2 ) = L(w0 ) + (f? (w2 ) ? f? (w? )) + (f? (w? ) ? f? (w0 )) +
5

?
?
||w0 ||2 ? ||w2 ||2
2
2

(1)

8d2 log2 (d/?)
. Using this and [16], we can bound
?n2 2
16d2 log2 (d/?)
1
?
the second quantity in equation 1 as f? (w2 ) ? f? (w ) ?
+ O( ?n
). By definition of
?n2 2
g
?
w , the third quantity in Equation 1 is non-positive. If ? is set to be ||w0 ||2 , then, the fourth quantity
2


1
in Equation 1 is at most 2g . Now, if n > C ? ||w02|| for a suitable constant C, ?n
? 4g . In addition,
g
16d2 log2 ( d
||w ||d log( d )

?)
if n > C ? 0 g ? , then,
? 4g . In either case, the total loss of the classifier w2
?n2 2

Notice that from Lemma 4, f?? (w2 ) ? f?? (w1 ) ?

output by Algorithm 2 is at most L(w0 ) + g .

The same technique can be used to analyze the sensitivity-based algorithm, using Lemma 3, which
yields the following.
Theorem 5 Let w0 be a classifier with expected loss L over the data distribution.
If
the training examples are drawn independently from the data distribution, and if n >
2 d log( d )||w || d log( d )||w ||2
0
0
?
?
,
), for some constant C, then, with probability 1 ? ?, the
C max( ||w02|| ,
3/2
g 
g

g



classifier output by Algorithm 2 has loss at most L + g over the data distribution.
It is clear that this bound is never lower than the bound for Algorithm 2. Note that for problems in
which (non-private) logistic regression performs well, kw0 k ? 1 if w0 has low loss, since otherwise
one can show that the loss of w0 would be lower bounded by log(1 + 1e ). Thus the performance
guarantee for Algorithm 2 is significantly stronger than for Algorithm 1, for problems in which one
would typically apply logistic regression.

6

Results in Simulation
Sensitivity method
New method
Standard LR

Uniform, margin=0.03
0.2962?0.0617
0.1426?0.1284
0?0.0016

Unseparable (uniform with noise 0.2 in margin 0.1)
0.3257?0.0536
0.1903?0.1105
0.0530?0.1105

Figure 1: Test error: mean ? standard deviation over five folds. N=17,500.
We include some simulations that compare the two privacy-preserving methods, and demonstrate
that using our privacy-preserving approach to logistic regression does not degrade learning performance terribly, from that of standard logistic regression. Performance degradation is inevitable
however, as in both cases, in order to address privacy concerns, we are adding noise, either to the
learned classifier, or to the objective.
In order to obtain a clean comparison between the various logistic regression variants, we first experimented with artificial data that is separable through the origin. Because the classification of a
vector by a linear separator through the origin depends only its angle, not its norm, we sampled the
data from the unit hypersphere. We used a uniform distribution on the hypersphere in 10 dimensions
with zero mass within a small margin (0.03) from the generating linear separator. Then we experimented on uniform data that is not linearly separable. We sampled data from the surface of the unit
ball in 10 dimensions, and labeled it with a classifier through the origin. In the band of margin ? 0.1
with respect to the perfect classifier, we performed random label flipping with probability 0.2. For
our experiments, we used convex optimization software provided by [9].
Figure 1 gives mean and standard deviation of test error over five-fold cross-validation, on 17,500
points. In both simulations, our new method is superior to the sensitivity method, although incurs
more error than standard (non-private) logistic regression. For both problems, we tuned the logistic
regression parameter, ?, to minimize the test error of standard logistic regression, using five-fold
cross-validation on a holdout set of 10,000 points (the tuned values are: ? = 0.01 in both cases).
For each test error computation, the performance of each of the privacy-preserving algorithms was
evaluated by averaging over 200 random restarts, since they are both randomized algorithms.
In Figure 2a)-b) we provide learning curves. We graph the test error after each increment of 1000
points, averaged over five-fold cross validation. The learning curves reveal that, not only does the
6

0.55

Avg test error over 5?fold cross?valid. 200 random restarts.

Avg test error over 5?fold cross?valid. 200 random restarts.

0.7

Our method
Standard LR
Sensitivity method

0.6

0.5

0.4

0.3

0.2

0.1

0

2

4

6

8

10

12

14

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0.6

Our method
Sensitivity method

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0

0.02

0.04

0.06

0.08

0.1

0.12

0.14

0.16

0.18

2

4

6

8

10

12

14

N/1000. Learning curve for unseparable data. d=10, epsilon=0.1, lambda=0.01

Avg test error over 5?fold cross?valid. 200 random restarts.

Avg test error over 5!fold cross!valid. 200 random restarts.

N/1000. Learning curve for uniform data. d=10, epsilon=0.1, margin=0.03, lambda=0.01

0.55

Our method
Standard LR
Sensitivity method

0.5

0.2

0.55

Our method
Sensitivity method

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0

Epsilon. Uniform data, d=10, n=10,000, margin=0.03, lambda=0.01

0.02

0.04

0.06

0.08

0.1

0.12

0.14

0.16

0.18

0.2

Epsilon. Unseparable data, d=10, n=10,000, lambda=0.01

Figure 2: Learning curves: a) Uniform distribution, margin=0.03, b) Unseparable data.
Epsilon curves: c) Uniform distribution, margin=0.03, d) Unseparable data.

new method reach a lower final error than the sensitivity method, but it also has better performance
at most smaller training set sizes.
In order to observe the effect of the level of privacy on the learning performance of the privacypreserving learning algorithms, in Figure 2c)-d) we vary , the privacy parameter to the two algorithms, on both the uniform, low margin data, and the unseparable data. As per the definition of
-differential privacy in Section 2, strengthening the privacy guarantee corresponds to reducing .
Both algorithms? learning performance degrades in this direction. For the majority of values of 
that we tested, the new method is superior in managing the tradeoff between privacy and learning
performance. For very small , corresponding to extremely stringent privacy requirements, the sensitivity method performs better but also has a predication accuracy close to chance, which is not
useful for machine learning purposes.

7

Conclusion

In conclusion, we show two ways to construct a privacy-preserving linear classifier through logistic
regression. The first one uses the methods of [6], and the second one is a new algorithm. Using the -differential privacy definition of Dwork et al. [6], we prove that our new algorithm is
privacy-preserving. We provide learning performance guarantees for the two algorithms, which are
tighter for our new algorithm, in cases in which one would typically apply logistic regression. In
simulations, our new algorithm outperforms the method based on [6].
Our work reveals an interesting connection between regularization and privacy: the larger the regularization constant, the less sensitive the logistic regression function is to any one individual example, and as a result, the less noise one needs to add to make it privacy-preserving. Therefore,
regularization not only prevents overfitting, but also helps with privacy, by making the classifier less
7

sensitive. An interesting future direction would be to explore whether other methods that prevent
overfitting also have such properties.
Other future directions would be to apply our techniques to other commonly used machine-learning
algorithms, and to explore whether our techniques can be applied to more general optimization
problems. Theorem 3 shows that our method can be applied to a class of optimization problems
with certain restrictions. An open question would be to remove some of these restrictions.
Acknowledgements. We thank Sanjoy Dasgupta and Daniel Hsu for several pointers.

References
[1] R. Agrawal and R. Srikant. Privacy-preserving data mining. SIGMOD Rec., 29(2):439?450, 2000.
[2] B. Barak, K. Chaudhuri, C. Dwork, S. Kale, F. McSherry, and K. Talwar. Privacy, accuracy, and consistency too: a holistic solution to contingency table release. In PODS, pages 273?282, 2007.
[3] A. Blum, K. Ligett, and A. Roth. A learning theory approach to non-interactive database privacy. In R. E.
Ladner and C. Dwork, editors, STOC, pages 609?618. ACM, 2008.
[4] K. Chaudhuri and N. Mishra. When random sampling preserves privacy. In C. Dwork, editor, CRYPTO,
volume 4117 of Lecture Notes in Computer Science, pages 198?213. Springer, 2006.
[5] C. Dwork. Differential privacy. In M. Bugliesi, B. Preneel, V. Sassone, and I. Wegener, editors, ICALP
(2), volume 4052 of Lecture Notes in Computer Science, pages 1?12. Springer, 2006.
[6] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis.
In Theory of Cryptography Conference, pages 265?284, 2006.
[7] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting privacy breaches in privacy preserving data mining.
In PODS, pages 211?222, 2003.
[8] S. P. Kasiviswanathan, H. K. Lee, K. Nissim, S. Raskhodnikova, and A. Smith. What can we learn
privately? In Proc. of Foundations of Computer Science, 2008.
[9] C. T. Kelley. Iterative Methods for Optimization. SIAM, 1999.
[10] A. Machanavajjhala, J. Gehrke, D. Kifer, and M. Venkitasubramaniam. l-diversity: Privacy beyond kanonymity. In ICDE, page 24, 2006.
[11] F. McSherry and K. Talwar. Mechanism design via differential privacy. In FOCS, pages 94?103, 2007.
[12] A. Narayanan and V. Shmatikov. Robust de-anonymization of large sparse datasets. In IEEE Symposium
on Security and Privacy, pages 111?125. IEEE Computer Society, 2008.
[13] K. Nissim, S. Raskhodnikova, and A. Smith. Smooth sensitivity and sampling in private data analysis. In
D. S. Johnson and U. Feige, editors, STOC, pages 75?84. ACM, 2007.
[14] P. Samarati and L. Sweeney. Protecting privacy when disclosing information: k-anonymity and its enforcement through generalization and suppression. In Proc. of the IEEE Symposium on Research in
Security and Privacy, 1998.
[15] S. Shalev-Shwartz and N. Srebro. Svm optimization: Inverse dependence on training set size. In International Conference on Machine Learning(ICML), 2008.
[16] K. Sridharan, N. Srebro, and S. Shalev-Schwartz. Fast rates for regularized objectives. In Neural Information Processing Systems, 2008.

8

"
1992,Weight Space Probability Densities in Stochastic Learning: I. Dynamics and Equilibria,,634-weight-space-probability-densities-in-stochastic-learning-i-dynamics-and-equilibria.pdf,Abstract Missing,"Weight Space Probability Densities
in Stochastic Learning:
I. Dynamics and Equilibria

Todd K. Leen and John E. Moody
Department of Computer Science and Engineering
Oregon Graduate Institute of Science & Technology
19600 N.W. von Neumann Dr.
Beaverton, OR 97006-1999

Abstract
The ensemble dynamics of stochastic learning algorithms can be
studied using theoretical techniques from statistical physics. We
develop the equations of motion for the weight space probability
densities for stochastic learning algorithms. We discuss equilibria
in the diffusion approximation and provide expressions for special
cases of the LMS algorithm. The equilibrium densities are not in
general thermal (Gibbs) distributions in the objective function being minimized, but rather depend upon an effective potential that
includes diffusion effects. Finally we present an exact analytical
expression for the time evolution of the density for a learning algorithm with weight updates proportional to the sign of the gradient.

1

Introduction: Theoretical Framework

Stochastic learning algorithms involve weight updates of the form

w(n+1) = w(n)

+

/-l(n)H[w(n),x(n)]

(1)

where w E 7?m is the vector of m weights, /-l is the learning rate, H[.] E 7?m is the
update function, and x(n) is the exemplar (input or input/target pair) presented

451

452

Leen and Moody
to the network at the nth iteration of the learning rule. Often the update function
is based on the gradient of a cost function H(w,x) = -a?{w,x) law. We assume
that the exemplars are Li.d. with underlying probability density p{x).
We are interested in studying the time evolution and steady state behavior of
the weight space probability density P(w, n) for ensembles of networks trained by
stochastic learning. Stochastic process theory and classical statistical mechanics
provide tools for doing this. As we shall see, the ensemble behavior of stochastic learning algorithms is similar to that of diffusion processes in physical systems,
although significant differences do exist.

1.1

Dynamics of the Weight Space Probability Density

Equation (1) defines a Markov process on the weight space. Given the particular
input x, the single time-step transition probability density for this process is a Dirac
delta function whose arguments satisfy the weight update (1):

W ( w'

~

w I x) = 8 ( w - w' - J-t H[ w' , x]) .

(2)

From this conditional transition probability, we calculate the total single time-step
transition probability (Leen and Orr 1992, Ritter and Schulten 1988)

W(w ' ~ w) = ( 8( w - w' - J-tH[w',x]) }z

(3)

where ( ... }z denotes integration over the measure on the random variable x.
The time evolution of the density is given by the Kolmogorov equation

P(w, n + 1) =

J

dw' P(w' , n) W(w '

~ w) ,

(4)

which forms the basis for our dynamical description of the weight space probability
density 1.
Stationary, or equilibrium, probability distributions are eigenfunctions of the transition probability

Ps(w)

=

J

dw' Ps(w') W(w'

~ w).

(5)

It is particularly interesting to note that for problems in which there exists an
optimal weight w,. such that

H(w,.,x) = 0, ""Ix,
one stationary solution is a delta function at w = w,.. An important class of such
examples are noise-free mapping problems for which weight values exist that realize
the desired mapping over all possible input/target pairs. For such problems, the
ensemble can settle into a sharp distribution at the optimal weights (for examples
see Leen and Orr 1992, Orr and Leen 1993).
Although the Kolmogorov equation can be integrated numerically, we would like
to make further analytic progress. Towards this end we convert the Kolmogorov
1 An alternative is to base the time evolution on a suitable master equation.
approaches give the same results.

Both

Weight Space Probability Densities in Stochastic Learning: I. Dynamics and Equilibria
equ~,tion into a differential? difference equation by expanding (3) as a power series
in J.l. Since the transition probability is defined in the sense of generalized functions
(i.e. distributions), the proper way to proceed is to smear (4) with a smooth test
function of compact support f(w) to obtain

J

dw f{w) P(w, n + 1) =

J

dw dw' f(w) P(w', n) W(w' -t w).

(6)

Next we use the transition probability (3) to perform the integration over wand
expand the resulting expression as a power series in J.l. Finally, we integrate by
part5 to take derivatives off f, dropping the surface terms. This results in a discrete
time version of the classic Kramers?Moyal expansion (llisken 1989)
P(w,n+1) - P(w,n)

=

where Hja denotes the ja th component of the m-component vector H.
In section 3, we present an algorithm for which the Kramers-Moyal expansion can
be explicitly summed. In general the full expansion is not analytically tractable,
and to make further analytic progress we will truncate it at second order to obtain
the Fokker-Planck equation.

1.2

The Fokker-Planck (Diffusion) Approximation

For small enough 1J.l HI, the Kramers-Moyal expansion (7) can be truncated to
second order to obtain a Fokker-Planck equation: 2

P(w, n + 1) - P(w, n) =
{)

-J.l {)Wi [ Ai(W) P(w, n)

]

(8)

In (8), and throughout the remainder of the paper, repeated indices are summed
over. In the Fokker-Planck approximation, only two coefficients appear: Ai (w) =
(Hi)z, called the drift vector, and Bij(W) = (Hi Hj)z' called the diffusion matrix.
The drift vector is simply the average update applied at w. Since the diffusion
coefficients can be strongly dependent on the position in weight space, the equilibrium densities will, in general, not be thermal (Gibbs) distributions in the potential
corresponding to (H( w, x) ) z' This is exemplified in our discussion of equilibrium
densities for the LMS algorithm in section 2.1 below 3 ?
2Radons et al. (1990) independently derived a Fokker-Planck equation for backpropagation. Earlier, Ritter and Schulten (1988) derived a Fokker-Planck equation (for Kohonen's
self-ordering feature map) that is valid in the neighborhood of a local optimum.
3See (Leen and Orr 1992, Orr and Leen 1993) for further examples.

453

454

Leen and Moody

2

Equilibrium Densities in the Fokker-Planck
Approximation

= P(w, n)

=Ps(w),

~2 a: j [Bij(W) P8(W)] )

(9)

In equilibrium the probability density is stationary, P(w, n+1)
so the Fokker-Planck equation (8) becomes
0= - a:i Ji(w) == - a:i

(11. Ai(W) P8(W)

-

Here, we have implicitly defined the probability density current J(w). In equilibrium, its divergence is zero.
If the drift and diffusion coefficients satisfy potential conditions, then the equilibrium
current itself is zero and detailed balance is obtained. The potential conditions are
(Gardiner, 1990)

OZk
OZ,
OWl - OWk

=

0,

Zk(W)

where

0
= Bk/(w) [J-l2"" ow;
Bi;(W) -

Ai(W)

1

(10)

Under these conditions the solution to (9) for the equilibrium density is:

Ps(w)
where

J(

= !...
e-2:F(w)/~,
J(

F(w)

=1dWk Zk(W)
w

(11)

is a normalization constant and F( w) is called the effective potential.

In general, the potential conditions are not satisfied for stochastic learning algorithms in multiple dimensions. 4 In this respect, stochastic learning differs from
most physical diffusion processes. However for LMS with inputs whose correlation
matrix is isotropic, the conditions are satisfied and the equilibrium density can be
reduced to the quadrature in (11).
2.1

Equilibrium Density for the LMS Algorithm

The best known on-line learning system is the LMS adaptive filter. For the LMS
algorithm, the training examples consist of input/target pairs x(n) = {s(n),t(n)},
the model output is u(n) = W? s(n), and the cost function is the squared error:

?(w,x(n))

1

1

= 2 [t(n)-u(n)]2 = 2 [t(n)-w?s(n)]2

(12)

The resulting update equations (for constant learning rate J-l) are

w(n+l)

= w(n) + J-l[t(n)-w.s(n)]s(n).

(13)

We assume that the training data are generated according to a ""signal plus noise""
model:

t(n)

= w? . s(n) + ?(n)

,

(14)

where w. is the ""true"" weight vector and ?( n) is LLd. noise with mean zero and
variance (12. We denote the correlation matrix of the inputs s( n) by R and the
4For one-dimensional algorithms, the potential conditions are trivially satisfied.

Weight Space Probability Densities in Stochastic Learning: I. Dynamics and Equilibria

fourth order correlation tensor of the inputs by S. It is convenient to shift the
origin of coordinates in weight space and define the weight error vector

=w -

v

w?.

In terms of v, the weight update is

v(n+l)

= v(n)

- JJ[s(n).v(n)]s(n)

+

JJf(n)s(n).

The drift vector and diffusion matrix are given by
Ai=-(SiSj}s Vj

and
Bij

= (Si Sj Sle SI

Vie VI

+

=

f2 Sj Sj ) s ,(

-RijVj

(15)

= Sijlel Vie VI + (72 Rij

(16)

respectively. Notice that the diffusion matrix is quadratic in v. Thus as we move
away from the global minimum at v = 0, diffusive spreading of the probability
density is enhanced. Notice also that, in general, both terms of the diffusion matrix
contribute an anisotropy.
We further assume that the inputs are drawn from a zero-mean Gaussian process.
This assumption allows us to appeal to the Gaussian moment factoring theorem
(Haykin, 1991, p318) to express the fourth-order correlation S in terms of R
Sijlel

= Rij Rlcl +

Rile Rjl

+

Ril Rjle

.

The diffusion matrix reduces to
(17)
To compute the effective potential (10 and 11) the diffusion matrix is inverted
using the Sherman-Morrison formula (Press, 1987, p67). As a final simplification,
we assume that the input distribution is spherically symmetric. Thus

R = rI ,
where I denotes the identity matrix.
Together these assumptions insure detailed balance, and we can integrate (11) in
closed form. In figure 1, we compare the effective potential F(v) (for 1-D LMS)
with the potential corresponding to the quadratic cost function.

v
Fig.l: Effective potential (dashed curve) and cost function (solid curve) for I-D LMS.

The spatial dependence of the the diffusion coefficient forces the effective potential
to soften relative to the cost function for large Ivl. This accentuates the tails of the
distribution relative to a gaussian.

455

456

Leen and Moody

The equilibrium density is

1 [

Ps{v) = K

1+

3r

u21vl2

] -( ~+m

),

(18)

where, as before, m and J( denote the dimension of the weight vector and the
normalization constant for the density respectively. For a l-D filter, the equilibrium
density can be found in closed form without assuming Gaussian input data. We
find
(19)
With gaussian inputs (for which S

= 3r2 ) (19) properly reduces to (18) with m = 1.

The equilibrium densities (18) and (19) are clearly not gaussian, however in the limit
of very small J.lr they reduce to gaussian distributions with variance J.lu 2 /2. Figure
2 shows a comparison between the theoretical result and a histogram of 200,000
values of v generated by simulation with J.l = 0.005, and u 2 = 1.0. The input data
were drawn from a zero-mean Gaussian distribution with r = 4.0.

I

I

-0.2 -0.1

i

I

I

0.0

0.1

0.2

v
Fig.2: Equilibrium density for 1-D LMS

3

An Exactly Summable Model

As in the case of LMS learning above, stochastic gradient descent algorithms update
weights based on an instantaneous estimate of the gradient of some average cost
function ?(w) {?(w, x) }z. That is, the update is given by

=

o

Hi(W,X) = --0 ?(w,x).
Wi
An alternative is to increment or decrement each weight by a fixed amount depending only on the sign of O?/OWi. We formulated this alternative update rule because
it avoids a common problem for sigmoidal networks, getting stuck on ""flat spots"" or
""plateaus"". The standard gradient descent update rule yields very slow movement
on plateaus, while second order methods such as gauss-newton can be unstable.
The sign-of-gradient update rule suffers from neither of these problems. s
5The use of the sign of the gradient has been suggested previously in the stochastic
approximation literature by Fabian (1960) and in the neural network literature by Derthick
(1984).

Weight Space Probability Densities in Stochastic Learning: I. Dynamics and Equilibria

If at each iteration one chooses a weight at random for updating, then the KramersMoyal expansion can be exactly summed. Thus at each iteration we 1) choose a
weight Wi and an exemplar x at random, and 2) update Wi with

H I.( w,x ) -_

.

-Sign

(8?(w,x(n)))
8

(20)

Wi

With this update rule, Hj = ?1 or 0 and Hi Hj = lSij (or 0). All of the coefficients
(HiHj Hk ... ) z in the Kramers-Moyal expansion (7) vanish unless i = j = k = ....
The remaining series can be summed by breaking it into odd and even parts. This
leav\!s
P(w,n+l) - P(w,n)

1
2m

+

1
2m

=

m

L

{ P(w

+ Ilj,n) Aj(w + Ilj) -

P(w -Ilh n) Aj(w -Ilj) }

j=1
m

L

{ P(w + Ilj, n) Bjj(w + Ilj) - 2P(w, n) Bjj(w)

j=1

+ P(w -Ilj, n) Bjj(w -Ilj)

=

}

(21)

where /-tj denotes a displacement along Wj a distance /-t, Aj(w) (Hj(w, x) )z' and
Bjj(w)
(H;(w,x)z' Note that Bjj(w) = 1 unless H(w,x) = 0, for all x, in
which case Bjj(w)
O. Although exact, (21) curiously has the form of a second
order finite difference approximation to the Fokker-Planck equation with diagonal
diffusion matrix. This form is understandable, since the dynamics (20) restrict the
weight values W to a hypercubic lattice with cell length /-t and generate only nearest
neighbor interactions.

=

=

L:

-0.5

!J!: k!~
0.5

v

1

1.5

2

2.5

-0.5

n=5oo

-0.5

0.5

v

1

1.5

2

2.5

2

2.5

n =5000

-0.5

0.5

v

1

1.5

Fig.3: Sequence of densities for the XOR problem

As an example, figure 3 shows the cost function evaluated along a 1-D slice through
the weight space for the XOR problem. Along this line are local and global minima
at v = 1 and v = 0 respectively. Also shown is the probability density (vertical
lines). The sequence shows the spreading of the density from its initialization at
the local minimum, and its eventual collection at the global minimum.

457

458

Leen and Moody

4

Discussion

A theoretical approach that focuses on the dynamics of the weight space probability
density, as we do here, provides powerful tools to extend understanding of stochastic
search. Both transient and equilibrium behavior can be studied using these tools.
We expect that knowledge of equilibrium weight space distributions can be used in
conjunction with theories of generalization (e.g. Moody, 1992) to assess the influence
of stochastic search on prediction error. Characterization of transient phenomena
should facilitate the design and evaluation of search strategies such as data batching
and adaptive learning rate schedules. Transient phenomena are treated in greater
depth in the companion paper in this volume (Orr and Leen, 1993).
Acknowledgements
T. Leen was supported under grants N00014-91-J-1482 and N00014-90-J-1349 from
ONR. J. Moody was supported under grants 89-0478 from AFOSR, ECS-9114333
from NSF, and N00014-89-J-1228 and N00014-92-J-4062 from ONR.
References
Todd K. Leen and Genevieve B. Orr (1992), Weight-space probability densities and convergence times for stochastic learning. In International Joint Conference on Neural Networks,
pages IV 158-164. IEEE, June.
H. Ritter and K. Schulten (1988), Convergence properties of Kohonen's topology conserving maps: Fluctuations, stability and dimension selection, Bioi. Cybern., 60, 59-71.
Genevieve B. Orr and Todd K. Leen (1993), Probability densities in stochastic learning;
II. Transients and Basin Hopping Times. In Giles, C.L., Hanson, S.J., and Cowan, J.D.
(eds.), Advances in Neural Information Processing Systems 5. San Mateo, CA: Morgan
Kaufmann Publishers.
H. Risken (1989), The Fokker-Planck Equation Springer-Verlag, Berlin.
G. Radons, H.G. Schuster and D. Werner (1990), Fokker-Planck description oflearning in
backpropagation networks, International Neural Network Conference - INNC 90, Paris, II
993-996, Kluwer Academic Publishers.
C.W. Gardiner (1990), Handbook of Stochastic Methods, 2nd Ed. Springer-Verlag, Berlin.
Simon Haykin (1991), Adaptive Filter Theory, 2nd edition. Prentice Hall, Englewood
Cliffs, N.J.
W.H. Press, B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling (1987) Numerical Recipes
- the Art of Scientific Computing. Cambridge University Press, Cambridge I New York.
V. Fabian (1960), Stochastic approximation methods. Czechoslovak Math J., 10, 123-159.
Mark Derthick (1984), Variations on the Boltzmann machine learning algorithm. Technical
Report CMU-CS-84-120, Department of Computer Science, Carnegie-Mellon University,
Pittsburgh, PA, August.
John E. Moody (1992), The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems. In J .E. Moody, S.J. Hanson, and
R.P. Lipmann, editors, Advances in Neural Information Processing Systems 4. Morgan
Kaufmann Publishers, San Mateo, CA.

"
2012,Convex Multi-view Subspace Learning,,4632-convex-multi-view-subspace-learning.pdf,"Subspace learning seeks a low dimensional representation of data that enables accurate reconstruction.  However, in many applications, data is obtained from multiple sources rather than a single source (e.g. an object might be viewed by cameras at different angles, or a document might consist of text and images).  The conditional independence of separate sources imposes constraints on their shared latent representation, which, if respected, can improve the quality of the learned low dimensional representation.  In this paper, we present a convex formulation of multi-view subspace learning that enforces conditional independence while reducing dimensionality.  For this formulation, we develop an efficient algorithm that recovers an optimal data reconstruction by exploiting an implicit convex regularizer, then recovers the corresponding latent representation and reconstruction model, jointly and optimally.  Experiments illustrate that the proposed method produces high quality results.","Convex Multi-view Subspace Learning

Martha White, Yaoliang Yu, Xinhua Zhang? and Dale Schuurmans
Department of Computing Science, University of Alberta, Edmonton AB T6G 2E8, Canada
{whitem,yaoliang,xinhua2,dale}@cs.ualberta.ca

Abstract
Subspace learning seeks a low dimensional representation of data that enables
accurate reconstruction. However, in many applications, data is obtained from
multiple sources rather than a single source (e.g. an object might be viewed by
cameras at different angles, or a document might consist of text and images). The
conditional independence of separate sources imposes constraints on their shared
latent representation, which, if respected, can improve the quality of a learned
low dimensional representation. In this paper, we present a convex formulation
of multi-view subspace learning that enforces conditional independence while reducing dimensionality. For this formulation, we develop an efficient algorithm
that recovers an optimal data reconstruction by exploiting an implicit convex regularizer, then recovers the corresponding latent representation and reconstruction
model, jointly and optimally. Experiments illustrate that the proposed method
produces high quality results.

1

Introduction

Dimensionality reduction is one of the most important forms of unsupervised learning, with roots
dating to the origins of data analysis. Re-expressing high dimensional data in a low dimensional
representation has been used to discover important latent information about individual data items,
visualize entire data sets to uncover their global organization, and even improve subsequent clustering or supervised learning [1]. Modern data is increasingly complex, however, with descriptions
of increasing size and heterogeneity. For example, multimedia data analysis considers data objects
(e.g. documents or webpages) described by related text, image, video, and audio components. Multiview learning focuses on the analysis of such multi-modal data by exploiting its implicit conditional
independence structure. For example, given multiple camera views of a single object, the particular idiosyncrasies of each camera are generally independent, hence the images they capture will
be conditionally independent given the scene. Similarly, the idiosyncrasies of text and images are
generally conditionally independent given a topic. The goal of multi-view learning, therefore, is to
use known conditional independence structure to improve the quality of learning results.
In this paper we focus on the problem of multi-view subspace learning: reducing dimensionality
when data consists of multiple, conditionally independent sources. Classically, multi-view subspace
learning has been achieved by an application of canonical correlation analysis (CCA) [2, 3]. In
particular, many successes have been achieved in using CCA to recover meaningful latent representations in a multi-view setting [4?6]. Such work has been extended to probabilistic [7] and sparse
formulations [8]. However, a key limitation of CCA-based approaches is that they only admit efficient global solutions when using the squared-error loss (i.e. Gaussian models), while extensions to
robust models have had to settle for approximate solutions [9].
By contrast, in the single-view setting, recent work has developed new generalizations of subspace
learning that can accommodate arbitrary convex losses [10?12]. These papers replace the hard bound
on the dimension of the latent representation with a structured convex regularizer that still reduces
rank, but in a relaxed manner that admits greater flexibility while retaining tractable formulations.
?

Xinhua Zhang is now at the National ICT Australia (NICTA), Machine Learning Group.

1

Subspace learning can be achieved in this case by first recovering an optimal reduced rank response
matrix and then extracting the latent representation and reconstruction model. Such formulations
have recently been extended to the multi-view case [13, 14]. Unfortunately, the multi-view formulation of subspace learning does not have an obvious convex form, and current work has resorted
to local training methods based on alternating descent minimization (or approximating intractable
integrals). Consequently, there is no guarantee of recovering a globally optimal subspace.
In this paper we provide a formulation of multi-view subspace learning that can be solved optimally
and efficiently. We achieve this by adapting the new single-view training methods of [11, 12] to the
multi-view case. After deriving a new formulation of multi-view subspace learning that allows a
global solution, we also derive efficient new algorithms. The outcome is an efficient approach to
multi-view subspace discovery that can produce high quality repeatable results.
0
Notation: We use Ik for the
A, k ? k2 for the
p k ?k identity matrix, A for the transpose of matrix
P
0
Euclidean norm, kXkF = tr(X X) for the Frobenius norm and kXktr = i ?i (X) for the trace
norm, where ?i (X) is the ith singular value of X.

2

Background

nh io
x
Assume one is given t paired observations yjj
consisting of two views: an x-view and a y-view,
of lengths m and n respectively. The goal of multi-view subspace learning is to infer, for each pair,
a shared latent representation, hj , of dimension k < min(n, m), such that the original data can be
accurately modeled. We first consider a linear formulation. Given paired observations the goal is to
infer a set of latent representations, hj , and reconstruction models, A and B, such that Ahj ? xj
and Bhj ? yj for all j. hLetiX denote the n ? t matrix of x observations, Y the m ? t matrix of
X
Y

the concatenated (n + m) ? t data matrix. The problem can then be
h i
A , and a k ? t matrix
expressed as recovering a (n + m) ? k matrix of model parameters, C = B
of latent representations, H, such that Z ? CH.
y observations, and Z =

The key assumption of multi-view learning is that each of the two views, xj and yj , is conditionally independent given the shared latent representation, hj . Although multi-view data can always
be concatenated and treated as a single view, if the conditional independence assumption holds, explicitly representing multiple views enables more accurate identification of the latent representation
(as we will see). The classical formulation of multi-view subspace learning is given by canonical
correlation analysis (CCA), which is typically expressed as the problem of projecting two views so
that the correlation between them is maximized [2]. Assuming the data is centered (i.e. X1 = 0 and
Y 1 = 0), the sample covariance of X and Y is given by XX 0 /t and Y Y 0 /t respectively. CCA can
then be expressed as an optimization over matrix variables
max tr(U 0 XY 0 V ) s.t. U 0 XX 0 U = V 0 Y Y 0 V = I
U,V

(1)

for U ? Rn?k , V ? Rm?k [3]. Although this classical formulation (1) does not make the shared
latent representation explicit, CCA can be expressed by a generative model: given a latent representation, hj , the observations xj = Ahj +j and yj = Bhj +?j are generated by a linear mapping plus
independent zero mean Gaussian noise,  ? N (0, ?x ), ? ? N (0, ?y ) [7]. In fact, one can show that
the classical CCA problem (1) is equivalent to the following multi-view subspace learning problem.


(XX 0 )?1/2 X
Proposition 1. Fix k, let Z? =
and
(Y Y 0 )?1/2 Y
(C, H) = arg min kZ? ? CHk2F ,
where C =

h i
A
B

C,H

? 21

. Then U = (XX 0 )

0

A and V = (Y Y 0 )

? 21

(2)

B provide an optimal solution to (1),

0

implying that A A = B B = I is satisfied in the solution to (2).
(The proof is given in Appendix A.) From Proposition 1, one can see how formulation (2) respects
the conditional independence of the separate views: given a latent representation hj , the reconstruction losses on the two views, xj and yj , cannot influence each other, since the reconstruction models
A and B are individually constrained. By contrast, in single-view subspace learning (i.e. principal
2

components analysis) A and B are concatenated in the larger variable C, where C as a whole is constrained but A and B are not. A and B must then compete against each other to acquire magnitude
to explain their respective ?views? given hj (i.e. conditional independence is not enforced). Such
sharing can be detrimental if the two views really are conditionally independent given hj .
Despite its elegance, a key limitation of CCA is its restriction to squared loss under a particular
normalization. Recently, subspace learning algorithms have been greatly generalized in the single
view case by relaxing the rank(H) = k constraint while imposing a structured regularizer that is
a convex relaxation of rank [10?12]. Such a relaxation allows one to incorporate arbitrary convex
losses, including robust losses [10], while maintaining tractability.
As mentioned, these relaxations of single-view subspace learning have only recently been proposed
for the multi-view setting [13, 14]. An extension of these proposals can be achieved by reformulating
(2) to first incorporate an arbitrary loss function L that is convex in its first argument (for examples,
see [15]), then relaxing the rank constraint by replacing it with a rank-reducing regularizer on H. In
particular, we consider the following training problem that extends [14]:





A
A:,i
min L
H; Z + ?kHk2,1 , s.t.
? C for all i,
B
B:,i
A,B,H





a
A
where C :=
: kak2 ? ?, kbk2 ? ? , C =
,
(3)
b
B
P
and kHk2,1 = i kHi,: k2 is a matrix block norm. The significance of using the (2, 1)-block norm
as a regularizer is that it encourages rows of H to become sparse, hence reducing the dimensionality
of the learned representation [16]. C must be constrained however, otherwise kHk2,1 can be pushed
arbitrarily close to zero simply by re-scaling H/s and Cs (s > 0) while preserving the same loss.
Unfortunately, (3) is not jointly convex in A, B and H. Thus, the algorithmic approaches proposed
by [13, 14] have been restricted to alternating block coordinate descent between components A, B
and H, which cannot guarantee a global solution. Our main result is to show that (3) can in fact be
solved globally and efficiently for A, B and H, improving on the previous local solutions [13, 14].

3

Reformulation

Our first main contribution is to derive an equivalent but tractable reformulation of (3), followed
by an efficient optimization algorithm. Note that (3) can in principle be tackled by a boosting
strategy; however, one would have to formulate a difficult weak learning oracle that considers both
views simultaneously [17]. Instead, we find that a direct matrix factorization approach of the form
developed in [11, 12] is more effective.
To derive our tractable reformulation, we first introduce the change of variable Z? = CH which
allows us to rewrite (3) equivalently as
n
o
? Z) + ? min
min L(Z;
min kHk2,1 .
(4)
?
{C:C:,i ?C} {H:CH=Z}

?
Z

A key step in the derivation is the following characterization of the inner minimization in (4).
? whose dual norm is
Proposition 2.
min
min kHk2,1 defines a norm ||| ? |||? (on Z)
?
{C:C:,i ?C} {H:CH=Z}

|||?||| :=

max

c?C,khk2 ?1

c0 ?h.

? i,: where
Proof. Let ?i = kHi,: k2 be the Euclidean norm of the i-th row of H. Then Hi,: = ?i H
?
?
Hi,: has unit length (if ?i = 0, then take Hi,: to be any unit vector). Therefore
min

min

?
{C:C:,i ?C} {H:CH=Z}

kHk2,1 =

min

? P ?i C:,i H
? i,: }
{C,?i :C:,i ?C,?i ?0, Z=
i

P

i

?i

=

min

?
{t?0:Z?tK}

t, (5)

where K is the convex hull of the set G := {ch0 : c ? C, khk2 = 1}. In other words, we seek a
? using only elements from G. Since the set K is convex and symmetric,
rank-one decomposition of Z,
3

(5) is known as a gauge function and defines a norm on Z? (see e.g. [18, Proposition V.3.2.1]). This
norm has a dual given by
|||?||| := max tr(?0 Z) =
Z?K

max

c?C,khk2 ?1

c0 ?h,

(6)

where the last equality follows because maximizing any linear function over the convex hull K of a
set G achieves the same value as maximizing over the set G itself.

Applying Proposition 2 to problem (3) leads to a simpler formulation of the optimization problem.
p

2
2
0
? Z)+? max kD??1 Zk
? tr , where D? = ? +? ? In p
.
Lemma 3. (3) = min L(Z;
??0
?
0
? 2 +? 2 /? Im
Z
Proof. The lemma is proved by first deriving an explicit form of the norm ||| ? ||| in (6), then deriving
its dual norm. The details are given in Appendix B.

Unfortunately the inner maximization problem in Lemma 3 is not concave in ?. However, it is
possible to re-parameterize D? to achieve a tractable formulation as follows. First, define a matrix


?
?/ ? In
? 0
E? := D ?2 (1??) =
, such that D? = E ?2 .
0
?/ 1 ? ? Im
? 2 ?+? 2
?2 ?
? with ? ? 0 corresponding to 0 ? ? ? 1. The
? = max0???1 kE??1 Zk,
Note that max??0 kD??1 Zk
following lemma proves that this re-parameterization yields an efficient computational approach.
? tr is concave in ? over [0, 1].
Lemma 4. h(?) := kE ?1 Zk
?


? q
?

? ?X 
q

Z
2


?
? ?X 0 ?X
1?? ? Y 0 ? Y
?
? ?
Proof. Expand h(?) into 
, where tr( ?)
? 2 (Z ) Z + ? 2 (Z ) Z

 q 1?? ? Y 
 = tr


?2 Z
tr
means summing the square root of the eigenvalues (i.e. a spectral function). By [19], if a spectral
function f is concave on [0, ?), then tr(f (M )) must be concave on positive semidefinite matrices.
? Y 0 ? Y is positive semi-definite for ? ? [0, 1] and
The result follows since ??2 (Z? X )0 Z? X + 1??
? 2 (Z ) Z
?
f = ? is concave on [0, ?).

From Lemmas 3 and 4 we achieve the first main result.
Theorem 5.

? Z) + ? max kE??1 Zk
? tr = max min L(Z;
? Z) + ?kE??1 Zk
? tr . (7)
(3) = min L(Z;
?
Z

0???1

0???1

?
Z

Hence (3) is equivalent to a concave-convex maxi-min problem with no local maxima nor minima.
Thus we have achieved a new formulation for multi-view subspace learning that respects conditional
independence of the separate views (see discussion in Section 2) while allowing a globally solvable
formulation. To the best of our knowledge, this has not previously been achieved in the literature.

4

Efficient Training Procedure

This new formulation for multi-view subspace learning also allows for an efficient algorithmic approach. Before conducting an experimental comparison to other methods, we first develop an ef? = E??1 Z? in (7), which
ficient implementation. To do so we introduce a further transformation Q
leads to an equivalent but computationally more convenient formulation of (3):
? Z) + ?kQk
? tr .
(3) = max min L(E? Q;
0???1 Q
?

(8)

? Z) + ?kQk
? tr . The transformation does not affect the concavity of
Denote g(?) := minQ? L(E? Q;
the problem with respect to ? established in Lemma 4; therefore, (8) remains tractable. The training
? which allows Z? = E? Q
? to
procedure then consists of two stages: first, solve (8) to recover ? and Q,
?
be computed; then, recover the optimal factors H and C (i.e. A and B) from Z.
4

? The key to efficiently recovering Z? is to observe that (8) has a conveRecovering an optimal Z:
nient form. The concave outer maximization is defined over a scalar variable ?, hence simple line
search can be used to solve the problem, normally requiring at most a dozen evaluations to achieve
? is a standard trace-norm-regularized loss
a small tolerance. Crucially, the inner minimization in Q
minimization problem, which has been extensively studied in the matrix completion literature [20?
22]. By exploiting these algorithms, g(?) and its subgradient can both be computed efficiently.
? Once Z? is obtained, we need to recover a C and H that satisfy
Recovering C and H from Z:
?
? ? , and C:,i ? C for all i.
CH = Z,
kHk2,1 = |||Z|||
(9)
We exploit recent sparse approximation methods [23, 24] to solve this problem. First, note from
P
?
? ? = min
(5) that |||Z|||
? P ?i C:,i H
? i,: }
i ?i , where kHi,: k2 ? 1. Since we already
{C,?i :C:i ?C,?i ?0, Z=
i
? ? = kE ?1 Zk
? tr from the first stage, we can rescale the problem so that |||Z|||
? ? = 1 without
have |||Z|||
?
P
P
? i,: where ? ? 0 and
loss of generality. In such a case, Z? = i ?i C:,i H
i ?i = 1 (we restore the
?
?
proper scale to H afterward). So now, Z lies in the convex hull of the set G := {ch0 : c ? C, khk2 ?
1} and any expansion of Z? as a convex combination of the elements in G is a valid recovery. From
this connection, we can now exploit the recent greedy algorithms developed in [23, 24] to solve the
recovery problem. In particular, the recovery just needs to solve
min f (K), where f (K) := kZ? ? Kk2F .
(10)
K?convG

where conv denotes the convex hull. Note that the optimal value of (10) is 0. The greedy (boosting)
algorithm provided by [23, 24] produces a factorization of Z? into C and H and proceeds as follows:
1. Weak learning step: greedily pick Gt = ct h0t ? argminG?G h?f (Kt?1 ), Gi . Note that this step
can be computed efficiently with a form of power method iteration (see Appendix C.2).
P

t
t
P
(t)
?i Gi , then Kt =
f
?i Gi .
2. ?Totally corrective? step: ?(t) = argmin
P
??0,

i

i=1

?i =1

i=1

This procedure will find a Kt satisfying kZ? ? Kt k2F <  within O(1/) iterations [23, 24].
Acceleration: In practice, this procedure can be considerably accelerated via more refined analysis.
? it is not hard to recover its dual
Recall Z? is penalized by the dual of the norm in (6). Given Z,
? Then given ?, the
variable ? by exploiting the dual norm relationship: ?= argmax?:|||?|||?1 tr(?0 Z).
following theorem guarantees many bases in C can be eliminated from the recovery problem (9).
0
Theorem 6. (C, H) satisfying Z? = CH is optimal iff k?0 C:,i k = 1 and Hi,: = kHi,: k2 C:,i
?, ?i.
Theorem 6 prunes many elements from G and the weak learning step only needs to consider a proper
subset. Interestingly this constrained search can be solved with no increase in the computational
complexity. The accelerated boosting generates ct in the weak learning step, giving the recovery
C = [c1 , . . . , ck ] and H = diag(?)C 0 ?. The rank, k, is implicitly determined by termination of the
boosting algorithm. The detailed algorithm and proof of Theorem 6 are given in Appendix C.

5

Comparisons

Below we compare the proposed global learning method, Multi-view Subspace Learning (MSL),
against a few benchmark competitors.
Local Multi-view Subspace Learning (LSL) An obvious competitor is to solve (3) by alternating
descent over the variables: optimize H with A and B fixed, optimize A with B and H fixed, etc.
This is the computational strategy employed by [13, 14]. Since A and B are both constrained and H
is regularized by the (2,1)-block norm which is not smooth, we optimized them using the proximal
gradient method [25].
Single-view Subspace Learning
h i (SSL) Single view learning can be cast as a relaxation of (3),
A are normalized as a whole, rather than individually for A and B:
where the columns of C = B
? Z) + ?(? 2 +? 2 )? 21 kHk
? 2,1 (11)
min ?
L(CH; Z) + ?kHk2,1 =
min
L(C? H;
{H,C:kC:,i k2 ?

? 2+? 2 }

? C:k
? C
?:,i k2 ?1}
{H,

? Z) + ?(? 2 + ? 2 )? 12 kZk
? tr .
= min L(Z;
?
Z

5

(12)

Equation
given in [10]. The equality in (11) is by change of variable
p (12) matches the formulation
p
? = ? 2 + ? 2 H. Equation (12) can be established from the basic results
C = ? 2 + ? 2 C? and H
of [11, 12] (or specializing Proposition 2 to the case where C is the unit Euclidean ball). To solve
(12), we used a variant of the boosting algorithm [22] when ? is large, due to its effectiveness
when the solution has low rank. When ? is small, we switch to the alternating direction augmented
Lagrangian method (ADAL) [26] which does not enforce low-rank at all iterations. This hybrid
? in (8) for MSL. Once an optimal Z? is
choice of solver is also applied to the optimization of Q
achieved, the corresponding C and H can be recovered by an SVD: for Z? = U ?V 0 , set C =
1
1
? tr , and so is an
(? 2 +? 2 ) 2 U and H = (? 2 +? 2 )? 2 ?V 0 which satisfies CH = Z? and kHk2,1 = kZk
optimal solution to (11).

6

Experimental results

Datasets We provide experimental results on two datasets: a synthetic dataset and a face-image
dataset. The synthetic dataset is generated as follows. First, we randomly generate a k-by-ttr matrix
Htr for training, a k-by-tte matrix Hte for testing, and two basis matrices, A (n-by-k) and B (mby-k), by (iid) sampling from a zero-mean unit-variance Gaussian distribution. The columns of A
and B are then normalized to ensure that the Euclidean norm of each is 1. Then we set
Xtr = AHtr , Ytr = BHtr , Xte = AHte , Yte = BHte .
? tr , Y?tr , X
? te , Y?te . Following [10], we use sparse
Next, we add noise to these matrices, to obtain X
non-Gaussian noise: 5% of the matrix entries were selected randomly and replaced with a value
drawn uniformly from [?M, M ], where M is 5 times the maximal absolute entry of the matrices.
The second dataset is based on the Extended Yale Face Database B [27]. It contains grey level
face images of 28 human subjects, each with 9 poses and 64 lighting conditions. To construct the
dataset, we set the x-view to a fixed lighting (+000E+00) and the y-view to a different fixed lighting
(+000E+20). We obtain a pair of views by randomly drawing a subject and a pose (under the two
fixed lightings). The underlying assumption is that each lighting has its own set of bases (A and B)
and each (person, pose) pair has the same latent representation for the two lighting conditions. All
images are down-sampled to 100-by-100, meaning n = m = 104 . We kept one view (x-view) clean
and added pixel errors to the second view (y-view). We randomly set 5% of the pixel values to 1,
mimicking the noise in practice, e.g. occlusions and loss of pixel information from image transfer.
The goal is to enable appropriate reconstruction of a noisy image using other views.
Model specification Due to the sparse noise model, we used L1,1 loss for L:
 

A
L
H, Z = kAH ? Xk1,1 + kBH ? Y k1,1 .
B
{z
} |
{z
}
|
:=L1 (AH,X)

(13)

:=L2 (BH,Y )

For computational reasons, we worked on a smoothed version of the L1,1 loss [26].
6.1

Comparing optimization quality

We first compare the optimization performance of MSL (global solver) versus LSL (local solver).
Figure 1(a) indicates that MSL consistently obtains a lower objective value, sometimes by a large
margin: more than two times lower for ? = 10?4 and 10?3 . Interestingly, as ? increases, the
difference shrinks. This result suggests that more local minima occur in the higher rank case (a large
? increases regularization and decreases the rank of the solution). In Section 6.2, we will see that
the lower optimization quality of LSL and the fact that SSL optimizes a less constrained objective
both lead to significantly worse denoising performance.
Second, we compare the runtimes of the three algorithms. Figure 1(b) presents runtimes for LSL
and MSL for an increasing number of samples. Again, the runtime of LSL is significantly worse
for smaller ?, as much as 4000x slower; as ? increases, the runtimes become similar. This result
is likely due to the fact that for small ?, the MSL inner optimization is much faster via the ADAL
solver (the slowest part of the optimization), whereas LSL still has to slowly iterate over the three
variables. They both appear to scale similarly with respect to the number of samples.
6

Objective LSL:MSL For Varying ?
?=1e-4

?=1e-3
1.5

?=1e-2

1

0.5
0

?=1e-1
?=1
200

400

600

800

Number of Samples

1000

3000

?=1e-3

2000
1000

?=1e-2

0

?=1e-1

?1000
0

1200

Runtime on Synthetic Data

?=1e-4

10

4000

(a) Objectives for LSL:MSL

Runtime (seconds)

2

Training Runtime LSL:MSL For Varying ?
5000

Runtime Ratio LSL:MSL

Objective Value Ratio LSL:MSL

2.5

8
6

SSL
MSL
MSL?R

4
2

?=1
200

400

600

800

Number of Samples

1000

1200

(b) Runtimes for LSL:MSL

0
0

200

400

600

800

Number of Samples/Features

1000

(c) Runtimes for SSL and MSL

Figure 1: Comparison between LSL and MSL on synthetic datasets with changing ?, n = m = 20 and 10

10
8

6
4

6
4

2
0
0

MSL
SSL
LSL

2

10
20
Run number

30

0
0

(a) tL = 100

4

4

10

10

MSL
SSL
LSL

Error of SSL

12

8

Error of LSL

10

SNR

SNR

repeats. (a) LSL often gets stuck in local minima, with a significantly higher objective than MSL. (b) For small
?, LSL is significantly slower than MSL. They scale similarly with the number of samples (c) Runtimes of SSL
and MSL for training and recovery with ? = 10?3 . For growing sample size, n = m = 20. MSL-R stands for
the recovery algorithm. The recovery time for SSL is almost 0, so it is not included.

2

10

0

10

0

10
20
Run number

30

2

(a) MSL vs LSL

(b) tL = 300

4

10
10
10
Error of MSL

2

10

0

10

0

2

4

10
10
10
Error of MSL

(b) MSL vs SSL

Figure 2: Signal-to-noise ratio of denoising algorithms on Figure 3: MSL versus SSL error in synthesynthetic data using recovered models on hold-out views. n =
m = 10. In (a), we used tL = 100 pairs of views for training
A and B and tested on 100 hold-out pairs, with 30 repeated
random draws of training and test data. In (b) we used tL =
300. Parameters were set to optimize respective methods.

sizing y-view, over 30 random runs. We set
n=m=200, tL =20, and ttest =80. In (a), LSL error is generally above the diagonal line, indicating higher error than MSL. In (b), SSL error is
considerably higher than MSL.

For SSL versus MSL, we expect SSL to be faster than MSL because it is a more straightforward
? (with a fixed ?) has the same form
optimization: in MSL, each inner optimization of (8) over Q
as the SSL objective. Figure 1(c), however, illustrates that this difference is not substantial for
increasing sample size. Interestingly, the recovery runtime seems independent of dataset size, and
is instead likely proportional to the rank of the data. The trend is similar for increasing features: for
tL = 200, at n = 200, MSL requires ? 20 seconds, and at n = 1000, it requires ? 60 seconds.
6.2

Comparing denoising quality

Next we compare the denoising capabilities of the algorithms on synthetic and face image datasets.
There are two denoising approaches. The simplest is to run the algorithm on the noisy Y?te , giving
the reconstructed Y?te as the denoised image. Another approach is to recover the models, A and B, in
s
s
? te = {?
? te and
a training phase. Given a new set of instances, X
xi }i=1 and Y?te = {?
yi }i=1 , noise in X
Y?te can be removed using A and B, without re-training. This approach requires first recovering the
? te = (h1 , . . . , hs ), for X
? te and Y?te . We use a batch approach for inference:
latent representation, H
?
?
Hte = argmin L1 (AH, Xte )+L2 (BH, Y?te )+?kHk2,1 .
(14)
H

? te = AH
? te and Y?te = B H
? te . We compared
The x-views and y-views are then reconstructed using X
these reconstructions with the clean data, Xte and Yte , in terms of the signal-to-noise ratio:

.

? te , Y?te ) = kXte k2 + kYte k2
? te k2F + kYte ? Y?te k2F .
SNR(X
kX
?
X
(15)
te
F
F
We present the recovery approach on synthetic data and the direct reconstruction approach on the
face dataset. We cross-validated over ? ? {10?4 , 10?3 , 10?2 , 10?1 , 0.5, 1} according to the highest
signal-to-noise ratio on the training data. We set ? = ? = 1 because the data is in the [0, 1] interval.
7

Clean

Noisy : 5%

SSL

LSL

MSL

20

20

20

20

20

40

40

40

40

40

60

60

60

60

60

80

80

80

80

80

100

20

40

60

80 100

100

20

40

60

80 100

100

20

Noisy : 10%

40

60

80 100

100

20

SSL

40

60

80 100

100

LSL

20

20

20

40

40

40

40

60

60

60

60

80

80

80

80

100

100

100

100

40

60

80 100

20

40

60

80 100

20

40

60

40

60

80 100

MSL

20

20

20

80 100

20

40

60

80 100

Figure 4: Reconstruction of a noisy image with 5% or 10% noise. LSL performs only slightly worse than
MSL for larger noise values: a larger regularization parameter is needed for more noise, resulting in fewer local
minima (as discussed in Figure 1). Conversely, SSL performs slightly worse than MSL for 5% noise, but as the
noise increases, the advantages of the MSL objective are apparent.

6.2.1

Using Recovered Models for Denoising

Figure 2 presents the signal-to-noise ratio for recovery on synthetic data. MSL produced the highest
value of signal-to-noise ratio. The performance of LSL is inferior to MSL, but still better than SSL,
corroborating the importance of modelling the data as two views.
6.2.2

Image Denoising

In Figure 4, we can see that MSL outperforms both SSL and LSL on the face image dataset for
two noise levels: 5% and 10%. Interestingly, in addition to having on average a 10x higher SNR
than SSL for these results, MSL also had significantly different objective values. SSL had larger
reconstruction error on the clean x-view (10x higher), lower reconstruction error on the noisy yview (3x lower) and a higher representation norm (3x higher). Likely, the noisy y-view skewed the
representation, due to the joint rather than separate constraint as in the MSL objective.
6.3

Comparing synthesis of views

In image synthesis, the latent representation is computed from only one view:
? te ) + ?kHk2,1 . The y-view is then synthesized: Y?te = B H
? te .
argminH L1 (AH, X

? te
H

=

Figure 3 shows the synthesis error, ||Y?te ?Yte ||2F , of MSL, LSL, and SSL over 30 random runs: MSL
generally incurs less error than LSL, and SSL incurs much higher error because it is not modelling
the conditional independence between views.

7

Conclusion

We provided a convex reformulation of multi-view subspace learning that enables global learning, as
opposed to previous local formulations. We also developed a new training procedure which reconstructs the data optimally and discovers the latent representations efficiently. Experimental results
on synthetic data and image data confirm the effectiveness of our method, which consistently outperformed other approaches in denoising quality. For future work, we are investigating extensions
to semi-supervised settings, such as global methods for co-training and co-regularization. It should
also be possible to extend our approach to more than two views and incorporate kernels.
Acknowledgements
We thank the reviewers for their helpful comments, in particular, an anonymous reviewer whose
suggestions greatly improved the presentation. Research supported by AICML and NSERC.

8

References
[1] J. Lee and M. Verleysen. Nonlinear Dimensionality Reduction. Springer, 2010.
[2] D. Hardoon, S. Szedmak, and J. Shawe-Taylor. Canonical correlation analysis: An overview
with application to learning methods. Neural Computation, 16:2639?2664, 2004.
[3] T. De Bie, N. Cristianini, and R. Rosipal. Eigenproblems in pattern recognition. In Handbook
of Geometric Computing, pages 129?170, 2005.
[4] P. Dhillon, D. Foster, and L. Ungar. Multi-view learning of word embeddings via CCA. In
NIPS, 2011.
[5] C. Lampert and O. Kr?omer. Weakly-paired maximum covariance analysis for multimodal
dimensionality reduction and transfer learning. In ECCV, 2010.
[6] L. Sigal, R. Memisevic, and D. Fleet. Shared kernel information embedding for discriminative
inference. In CVPR, 2009.
[7] F. Bach and M. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical Report 688, Department of Statistics, University of California, Berkeley, 2006.
[8] C. Archambeau and F. Bach. Sparse probabilistic projections. In NIPS, 2008.
[9] J. Viinikanoja, A. Klami, and S. Kaski. Variational Bayesian mixture of robust CCA. In ECML,
2010.
[10] E. Cand`es, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? J.ACM, 58(1):
1?37, 2011.
[11] X. Zhang, Y. Yu, M. White, R. Huang, and D. Schuurmans. Convex sparse coding, subspace
learning, and semi-supervised extensions. In AAAI, 2011.
[12] F. Bach, J. Mairal, and J. Ponce. Convex sparse matrix factorizations. arXiv:0812.1869v1,
2008.
[13] N. Quadrinto and C. Lampert. Learning multi-view neighborhood preserving projections. In
ICML, 2011.
[14] Y. Jia, M. Salzmann, and T. Darrell. Factorized latent spaces with structured sparsity. In NIPS,
pages 982?990, 2010.
[15] M. White and D. Schuurmans. Generalized optimal reverse prediction. In AISTATS, 2012.
[16] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning, 73(3):243?272, 2008.
[17] D. Bradley and A. Bagnell. Convex coding. In UAI, 2009.
[18] J-B Hiriart-Urruty and C. Lemar?echal. Convex Analysis and Minimization Algorithms, I and
II, volume 305 and 306. Springer-Verlag, 1993.
[19] D. Petz. A survey of trace inequalities. In Functional Analysis and Operator Theory, pages
287?298. Banach Center, 2004.
[20] S. Ma, D. Goldfarb, and L. Chen. Fixed point and Bregman iterative methods for matrix rank
minimization. Mathematical Programming, 128:321?353, 2011.
[21] J. Cai, E. Cand`es, and Z. Shen. A singular value thresholding algorithm for matrix completion.
SIAM Journal on Optimization, 20(4):1956?1982, 2010.
[22] X. Zhang, Y. Yu, and D. Schuurmans. Accelerated training for matrix-norm regularization: A
boosting approach. In NIPS, 2012.
[23] A. Tewari, P. Ravikumar, and I. S. Dhillon. Greedy algorithms for structurally constrained high
dimensional problems. In NIPS, 2011.
[24] X. Yuan and S. Yan. Forward basis selection for sparse approximation over dictionary. In
AISTATS, 2012.
[25] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM Journal on Imaging Sciences, 2(1):183?202, 2009.
[26] D. Goldfarb, S. Ma, and K. Scheinberg. Fast alternating linearization methods for minimizing
the sum of two convex functions. Mathematical Programming, to appear.
[27] A. Georghiades, P. Belhumeur, and D. Kriegman. From few to many: Illumination cone models
for face recognition under variable lighting and pose. IEEE TPAMI, 23:643?660, 2001.
9

Supplementary Material
A

Proof of Proposition 1

To show that (1) and (2) have equivalent solutions we exploit some developments from [28]. Let
1
1
N = (XX 0 )? 2 and M = (Y Y 0 )? 2 , hence


I
N XY 0 M
0
?
?
ZZ =
.
M Y X 0N
I
First consider (1). Its solution can be characterized by the maximal solutions to the generalized
eigenvalue problem [3]:






0
XY 0
u
XX 0
0
u
,
=?
Y X0
0
v
0
YY0
v
which, under the change of variables u = N a and v = M b and then shifting the eigenvalues by 1, is
equivalent to



 ?1


0
XY 0 M
a
N
0
a
?
=
?
Y X 0N
0
b
b
0
M ?1








0
0
N XY M
a
I 0
a
?
=?
M Y X 0N
0
b
0 I
b




a
a
?
Z? Z? 0
= (? + 1)
b
b
h i
A to the top k eigenvectors of Z
? Z? 0 one can show that U = N A and V = M B provides
By setting B
an optimal solution to (1) [3].
? where C ? denotes pseudo-inverse.
By comparison, for (2), an optimal H is given by H = C ? Z,
Hence
? 2F
min kZ? ? CHk2F = min k(I ? CC ? )Zk
C,H

C

= tr(Z? Z? 0 ) ?

max

{C:C 0 C=I}

tr(C 0 Z? Z? 0 C).

Here again the solution is given by the top k eigenvectors of Z? Z? 0 [29].1

B

Proof for Lemma 3

First, observe that
(3) =

min

? Z) + ?
min L(CH; Z) + ?kHk2,1 = min L(Z;

{C:C:,i ?C} H

?
Z

min

min

?
{C:C:,i ?C} {H:CH=Z}

kHk2,1

? Z) + ?|||Z|||
? ?,
= min L(Z;
?
Z

where the last step follows from Proposition 2.
? ? = max??0 kD??1 Zk
? tr , which was established in [11]. We reproduce
It only remains to show |||Z|||
the proof in [11] for the convenience of the reader.
We will use two diagonal matrices, I X = diag([1n ; 0m ]) and I Y = diag([0n ; 1m ]) such that I X +
I Y = Im+n . Similarly, for c ? Rm+n , we use cX (respectively cY ) to denote c1:m (respectively
cm+1:m+n ).
The first stage is to prove that the dual norm is characterized by
|||?||| = min kD? ?ksp .
??0

1

[30] gave a similar but not equivalent formulation to (2), due to the lack of normalization.

10

(16)

where the spectral norm kXksp = ?max (X) is the dual of the trace norm, kXktr . To this end, recall
that
|||?||| =

max

c?C,khk2 ?1

c0 ?h = max kc0 ?k2 =

max

c?C

{c:kcX k2 =?, kcY k2 =?}

kc0 ?k2

giving
|||?|||2 =

max

{c:kcX k2 =?, kcY k2 =?}

c0 ??0 c =

max

{?:?0, tr(?I X )?? 2 , tr(?I Y )?? 2 }

tr(???0 ),

(17)

using the fact that when maximizing a convex function, one of the extreme points in the constraint
set {? : ?0, tr(?In )?? 2, tr(?Im )?? 2 } must be optimal. Furthermore, since the extreme points
have rank at most one in this case [31], the rank constraint rank(?) = 1 can be dropped.
Next, form the Lagrangian L(?; ?, ?, ?) = tr(???0 ) + tr(??) + ?(? 2 ? tr(?I X )) + ?(? 2 ?
tr(?I Y )) where ? ? 0, ? ? 0 and ?  0. Note that the primal variable ? can be eliminated
by formulating the equilibrium condition ?L/?? = ??0 + ? ? ?I X ? ?I Y = 0, which implies
??0 ? ?I X ? ?I Y  0. Therefore, we achieve the equivalent dual formulation
(17) =
min
? 2 ? + ? 2 ?.
(18)
{?,?:??0, ??0, ?I X +?I Y ??0 }

Now observe that for ? ? 0 and ? ? 0, the relation ??0  ?I X + ?I Y holds if and only if
D?/? ??0 D?/?  D?/? (?I X +?I Y )D?/? = (? 2 ?+? 2 ?)In+m , hence
(18) =
min 2
? 2 ?+? 2 ?
(19)
2
2
{?,?:??0, ??0, kD?/? ?ksp ?? ?+? ?}

The third constraint must be met with equality at the optimum due to continuity, for otherwise we
would be able to further decrease the objective, a contradiction to optimality. Note that a standard
compactness argument would establish the existence of minimizers. So
(19) = min kD?/? ?k2sp = min kD? ?k2sp .
??0

??0,??0

Finally, for the second stage, we characterize the target norm by observing that
? ?
|||Z|||

=

?
max tr(?0 Z)
?:|||?|||?1

=

max

max

=

max

=

? tr .
max kD??1 Zk

??0 ?:kD? ?ksp ?1

max

? ?k
? sp ?1
??0 ?:k

?
tr(?0 Z)

(20)

? 0 D??1 Z)
?
tr(?
(21)

??0

where (20) uses (16), and (21) exploits the conjugacy of the spectral and trace norms. The lemma
follows.

C

Proof for Theorem 6 and Details of Recovery

Once an optimal reconstruction Z? is obtained, we need to recover the optimal factors C and H that
satisfy
? , kHk2,1 = |||Z|||
? ? , and C:,i ? C for all i.
CH = Z,
(22)
Note that by Proposition 2 and Lemma 3, the recovery problem (22) can be re-expressed as
min

?
{C,H:C:,i ?C ?i, CH=Z}

kHk2,1

=

max

{?:|||?|||?1}

?
tr(?0 Z).

(23)

? then use ? to recover H
Our strategy will be to first recover the optimal dual solution ? given Z,
and C.
?
First, to recover ? one can simply trace back from (21) to (20). Let U ?V 0 be the SVD of D??1 Z.
0
?1
0
? = U V and ? = D U V automatically satisfies |||?||| = 1 while achieving the optimal
Then ?
?
? 0 D??1 Z)
? = tr(?) = kD??1 Zk
? tr .
trace in (23) because tr(?
11

Given such an optimal ?, we are then able to characterize an optimal solution (C, H). Introduce the
set




a
C(?) := arg max k?0 ck = c =
: kak = ?, kbk = ?, k?0 ck = 1 .
(24)
b
c?C
Theorem 6. For a dual optimal ?, (C, H) solves recovery problem (22) if and only if C:,i ? C(?)
0
?
?, such that CH = Z.
and Hi,: = kHi,: k2 C:,i
Proof. By (23), if Z? = CH, then
? ? = tr(?0 Z)
? = tr(?0 CH) =
|||Z|||

X

Hi,: ?0 C:,i .

(25)

i
0
0
Note that ?C:,i ? C, k?0 C:,i k2 ? 1 since |||?||| ? 1 and Hi,: ?
PC:,i = kHi,: ? C:,i k2 ?
0
kHi,: k2 k? C:,i k2 ? kHi,: k2 . If (C, H) is optimal, then (25) =
i kHi,: k2 , hence implying
0
k?0 C:,i k2 = 1 and Hi,: = kHi,: k2 C:,i
?.
0
? ? = P kHi,: k2 ,
On the other hand, if k?0 C:,i k2 = 1 and Hi,: = kHi,: k2 C:,i
?, then we have |||Z|||
i
implying the optimality of (C, H).


Therefore, given ?, the recovery problem (22) has been reduced to finding a vector ? and matrix C
?
such that ? ? 0, C:,i ? C(?) for all i, and C diag(?)C 0 ? = Z.
Next we demonstrate how to incrementally recover ? and C. Denote the range of C diag(?)C 0 by
the set
P
S := { i ?i ci c0i : ci ? C(?), ? ? 0} .
Note that S is the conic hull of (possibly infinitely many) rank one matrices {cc0 : c ? C(?)}.
However, by Carath?eodory?s theorem [32, ?17], any matrix K ? S can be written as the conic combination of finitely many rank one matrices of the form {cc0 : c ? C(?)}. Therefore, conceptually,
the recovery problem has been reduced to finding a sparse set of non-negative weights, ?, over the
set of feasible basis vectors, c ? C(?).
To find these weights, we use a totally corrective ?boosting? procedure [22] that is guaranteed to
converge to a feasible solution. Consider the following objective function for boosting
? 2F , where K ? S.
f (K) = kK? ? Zk
Note that f is clearly a convex function in K with a Lipschitz continuous gradient. Theorem 6
implies that an optimal solution of (22) corresponds precisely to those K ? S such that f (K) = 0.
The idea behind totally corrective boosting [22] is to find a minimizer of f (hence optimal solution
of (22)) incrementally. After initializing K0 = 0, we iterate between two steps:
1. Weak learning step: find
ct ? argmin h?f (Kt?1 ), cc0 i = argmax c0 Qc,
c?C(?)

(26)

c?C(?)

where Q = ??f (Kt?1 ) = 2(Z? ? Kt?1 ?)?0 .
2. ?Totally corrective? step:
?(t)

=

argmin f

P

?:?i ?0

Kt

=

Pt

i=1

t
i=1


?i ci c0i ,

(27)

(t)

?i ci c0i .

Three key facts can be established about this boosting procedure: (i) each weak learning step can
be solved efficiently; (ii) each totally corrective weight update can be solved efficiently; and (iii)
f (Kt ) & 0, hence a feasible solution can be arbitrarily well approximated. (iii) has been proved in
[22], while (ii) is immediate because (27) is a standard quadratic program. Only (i) deserves some
explanation. We show in the next subsection that C(?), defined in (24), can be much simplified, and
consequently we give in the last subsection an efficient algorithm for the oracle problem (26) (the
idea is similar to the one inherent in the proof of Lemma 3).
12

C.1

Simplification of C(?)

Since C(?) is the set of optimal solutions to
max k?0 ck ,

(28)

c?C

our idea is to first obtain an optimal solution to its dual problem, and then use it to recover the
optimal c via the KKT conditions. In fact, its dual problem has been stated in (18). Once we obtain
the optimal ? in (21) by solving (8), it is straightforward to backtrack and recover the optimal ? and
? in (18). Then by KKT condition [32, ?28], c is an optimal solution to (28) if and only if

 

 X

c 
 = ?, 
cY 
 = ?,
(29)
hR, cc0 i = 0, where R = ?I X + ?I Y ? ??0  0.

(30)

Since (30) holds iff c is in the null space of R, we find an orthonormal basis {n1 , . . . , nk } for this
null space. Assume
 X 
N
c = N ?, where N = [n1 , . . . , nk ] =
, ? = (?1 , . . . , ?k )0 .
(31)
NY
By (29), we have

 
2

 
2

(32)
0 = ? 2 
cX 
 ? ? 2 
cY 
 = ?0 ? 2 (N X )0 N X ? ? 2 (N Y )0 N Y ?.
The idea is to go through some linear transformations for simplification. Perform eigendecomposition U ?U 0 = ? 2 (N X )0 N X ? ? 2 (N Y )0 N Y , where ? = diag(?1 , . . . , ?k ), and U ?
Rk?k is orthonormal. Let v = U 0 ?. Then by (31),
c = N U v,
(33)
and (32) is satisfied if and only if
X
v0 ?v =
?i vi2 = 0.
(34)
i

Finally, (29) implies
2

? 2 + ? 2 = kck = v0 U 0 N 0 N U v = v0 v.
In summary, by (33) we have
C(?) = {N U v : v satisfies (34) and (35)}
n
o
2
= N U v : v0 ?v = 0, kvk = ? 2 + ? 2 .
C.2

(35)

(36)

Solving the weak oracle problem (26)

The weak oracle needs to solve
max c0 Qc,
c?C(?)

where Q = ??f (Kt?1 ) = 2(Z? ? Kt?1 ?)?0 . By (36), this optimization is equivalent to
max 2
v0 T v,
v:v0 ?v=0, kvk =? 2 +? 2

0

0

where T = U N QN U . Using the same technique as in the proof of Lemma 3, we have
max0
v0 T v
0
v:v v=1,v ?v=0

0

(let H = vv ) =
(Lagrange dual) =

max

tr(T H)

H0,tr(H)=1,tr(?H)=0

min

?,?:? ?+?I?T 0

?

= min ?max (T ? ? ?),
? ?R

where ?max stands for the maximum eigenvalue. Since ?max is a convex function over real symmetric matrices, the last line search problem is convex in ? , hence can be solved globally and efficiently.
Given the optimal ? and the optimal objective value ?, the optimal v can be recovered using a similar
? = {?
? s }.
trick as in Appendix C.1. Let the null space of ?I + ? ? ? T be spanned by N
n1 , . . . , n
2
s
2
2
0
?
? ? R such that v := N ?
? satisfies kvk = ? + ? and v ?v = 0.
Then find any ?
13

Auxiliary References
[28] L. Sun, S. Ji, and J. Ye. Canonical correlation analysis for multilabel classification: A leastsquares formulation, extensions, and analysis. IEEE TPAMI, 33(1):194?200, 2011.
[29] M. Overton and R. Womersley. Optimality conditions and duality theory for minimizing sums
of the largest eigenvalues of symmetric matrices. Mathematical Programming, 62:321?357,
1993.
[30] B. Long, P. Yu, and Z. Zhang. A general model for multiple view unsupervised learning. In
ICDM, 2008.
[31] G. Pataki. On the rank of extreme matrices in semidefinite programs and the multiplicity of
optimal eigenvalues. Mathematics of Operations Research, 23(2):339?358, 1998.
[32] R. Rockafellar. Convex Analysis. Princeton U. Press, 1970.

14

"
2004,The power of feature clustering: An application to object detection,,2622-the-power-of-feature-clustering-an-application-to-object-detection.pdf,Abstract Missing,"The power of feature clustering: An application
to object detection

Shai Avidan
Mitsibishi Electric Research Labs
201 Broadway
Cambridge, MA 02139
avidan@merl.com

Moshe Butman
Adyoron Intelligent Systems LTD.
34 Habarzel St.
Tel-Aviv, Israel
mosheb@adyoron.com

Abstract
We give a fast rejection scheme that is based on image segments and
demonstrate it on the canonical example of face detection. However, instead of focusing on the detection step we focus on the rejection step and
show that our method is simple and fast to be learned, thus making it
an excellent pre-processing step to accelerate standard machine learning
classifiers, such as neural-networks, Bayes classifiers or SVM. We decompose a collection of face images into regions of pixels with similar
behavior over the image set. The relationships between the mean and
variance of image segments are used to form a cascade of rejectors that
can reject over 99.8% of image patches, thus only a small fraction of the
image patches must be passed to a full-scale classifier. Moreover, the
training time for our method is much less than an hour, on a standard PC.
The shape of the features (i.e. image segments) we use is data-driven,
they are very cheap to compute and they form a very low dimensional
feature space in which exhaustive search for the best features is tractable.

1

Introduction

This work is motivated by recent advances in object detection algorithms that use a cascade
of rejectors to quickly detect objects in images. Instead of using a full fledged classifier on
every image patch, a sequence of increasingly more complex rejectors is applied. Nonface image patches will be rejected early on in the cascade, while face image patches will
survive the entire cascade and will be marked as a face.
The work of Viola & Jones [15] demonstrated the advantages of such an approach. Other
researchers suggested similar methods [4, 6, 12]. Common to all these methods is the
realization that simple and fast classifiers are enough to reject large portions of the image, leaving more time to use more sophisticated, and time consuming, classifiers on the
remaining regions of the image.
All these ?fast? methods must address three issues. First, is the feature space in which to
work, second is a fast method to calculate the features from the raw image data and third is
the feature selection algorithm to use.
Early attempts assumed the feature space to be the space of pixel values. Elad et al. [4]

suggest the maximum rejection criteria that chooses rejectors that maximize the rejection
rate of each classifier. Keren et al. [6] use anti-face detectors by assuming normal distribution on the background. A different approach was suggested by Romdhani et al. [12],
that constructed the full SVM classifier first and then approximated it with a sequence or
support vector rejectors that were calculated using non-linear optimization. All the above
mentioned method need to ?touch? every pixel in an image patch at least once before they
can reject the image patch.
Viola & Jones [15], on the other hand, construct a huge feature space that consists of
combined box regions that can be quickly computed from the raw pixel data using the
?integral image? and use a sequential feature selection algorithm for feature selection. The
rejectors are combined using a variant of AdaBoost [2]. Li et al [7] replaced the sequential
forward searching algorithm with a float search algorithm (which can backtrack as well).
An important advantage of the huge feature space advocated by Viola & Jones is that now
image patches can be rejected with an extremely small number of operations and there is
no need to ?touch? every pixel in the image patch at least once.
Many of these methods focus on developing fast classifiers that are often constructed in a
greedy manner. This precludes classifiers that might demonstrate excellent classification
results but are slower to compute, such as the methods suggested by Schneiderman et al.
[8], Rowley et al. [13], Sung and Poggio [10] or Heisele et al [5].
Our method offers a way to accelerate ?slow? classification methods by using a preprocessing rejection step. Our rejection scheme is fast to be trained and very effective
in rejecting the vast majority of false patterns. On the canonical face detection example, it
took our method much less than an hour to train and it was able to reject over 99.8% of the
image patches, meaning that we can effectively accelerate standard classifiers by several
orders of magnitude, without changing the classifier at all.
Like other, ?fast?, methods we use a cascade of rejectors, but we use a different type of
filters and a different type of feature selection method. We take our features to be the
approximated mean and variance of image segments, where every image segment consists
of pixels that have similar behavior across the entire image set. As a result, our features
are derived from the data and do not have to be hand crafted for the particular object of
interest. In fact they do not even have to form contiguous regions. We use only a small
number of representative pixels to calculate the approximated mean and variance, which
makes our features very fast to compute during detection (in our experiments we found that
our first rejector rejects almost 50% of all image patches, using just 8 pixels). Finally, the
number of segments we use is quite small which makes it possible to exhaustively calculate
all possible rejectors based on single, pairs and triplets of segments in order to find the best
rejectors in every step of the cascade. This is in contrast to methods that construct a huge
feature bank and use a greedy feature selection algorithm to choose ?good? features from
it. Taken together, our algorithm is fast to train and fast to test. In our experiments we train
on a database that contains several thousands of face images and roughly half-a-million
non-faces in less than an hour on an average PC and our rejection module runs at several
frames per second.

2

Algorithm

At the core of our algorithm is the realization that feature representation is a crucial ingredient in any classification system. For instance, the Viola-Jones box filters are extremely efficient to compute using the ?integral image? but they form a large feature space, thus placing
a heavy computational burden on the feature selection algorithm that follows. Moreover,
empirically they show that the first feature selected by their method correspond to meaningful regions in the face. This suggests that it might be better to focus on features that

correspond to coherent regions in the image. This leads to the idea of image segmentation,
that breaks an ensemble of images into regions of pixels that exhibit similar temporal behavior. Given the image segmentation we take our features to be the mean and variance of
each segment, giving us a very small feature space to work on (we chose to segment the
face image into eight segments). Unfortunately, calculating the mean and variance of an
image segment requires going over all the pixels in the segment, a time consuming process. However, since the segments represent similar-behaving pixels we found that we can
approximate the calculation of the mean and variance of the entire segment using quite a
small number of representative pixels. In our experiments, four pixels were enough to adequately represent segments that contain several tens of pixels. Now that we have a very
small feature space to work with, and a fast way to extract features from raw pixels data
we can exhaustively search for all possible combinations of single, pairs or triplets of features to find the best rejector in every stage. The remaining patterns should be passed to a
standard classifier for final validation.
2.1

Image Segments

Image segments were already presented in the past [1] for the problem of classification of
objects such as faces or vehicles. We briefly repeat the presentation for the paper to be
self-contained. An ensemble of scaled, cropped and aligned images of a given object (say
faces) can be approximated by its leading principal components. This is done by stacking
the images (in vector form) in a design matrix A and taking the leading eigenvectors of the
covariance matrix C = N1 AAT , where N is the number of images. The leading principal
components are the leading eigenvectors of the covariance matrix C and they form a basis
that approximates the space of all the columns of the design matrix A [11, 9]. But instead
of looking at the columns of A look at the rows of A. Each row in A gives the intensity
profile of a particular pixel, i.e., each row represents the intensity values that a particular
pixel takes in the different images in the ensemble. If two pixels come from the same
region of the face they are likely to have the same intensity values and hence have a strong
temporal correlation. We wish to find this correlations and segment the image plane into
regions of pixels that have similar temporal behavior. This approach broadly falls under
the category of Factor Analysis [3] that seeks to find a low-dimensional representation that
captures the correlations between features.
Let Ax be the x-th row of the design matrix A. Then Ax is the intensity profile of pixel x
(We address pixels with a single number because the images are represented in a scan-line
vector form). That is, Ax is an N -dimensional vector (where N is the number of images)
that holds the intensity values of pixel x in each image in the ensemble. Pixels x and y
are temporally correlated if the dot product of rows Ax and Ay is approaching 1 and are
temporally uncorrelated if the dot-product is approaching 0.
Thus, to find temporally correlated pixels all we need to do is run a clustering algorithm
on the rows of the design matrix A. In particular, we used the k-means algorithm on the
rows of the matrix A but any method of Factor Analysis can be used. As a result, the
image-plane is segmented into several (possibly non-continuous) segments of temporally
correlated pixels. Experiments in the past [1] showed good classification results on different
objects such as faces and vehicles.
2.2

Finding Representative Pixels

Our algorithm works by comparing the mean and variance properties of one or more image
segments. Unfortunately this requires touching every pixel in the image segment during
test time, thus slowing the classification process considerably. Therefor, during train time
we find a set of representative pixels that will be used during test time. Specifically, we
approximate every segment in a face image with a small number of representative pixels

Face segments

2

4

6

8

10

12

14

16

18

20
2

4

6

8

10

12

14

16

18

20

(a)
(b)
Figure 1: Face segmentation and representative pixels. (a) Face segmentation and representative pixels. The face segmentation was computed using 1400 faces, each segment is
marked with a different color and the segments need not be contiguous. The crosses overlaid on the segments mark the representative pixels that were automatically selected by
our method. (b) Histogram of the difference between an approximated mean and the exact
mean of a particular segment (the light blue segment on the left). The histogram is peaked
at zero, meaning that the representative pixels give a good approximation.
that approximate the mean and variance of the entire image segment. Define ? i (xj ) to be
the true mean of segment i of face j, and let ?
? i (xj ) be its approximation, defined as
Pk
j=1 xj
?
?i (xj ) =
k
where {xj }kj=1 are a subset of pixels in segment i of pattern j. We use a greedy algorithm
that incrementally searches for the next representative pixel that minimize
n
X

(?
?i (xj )) ? ?i (xj ))2

j=1

and add it to the collection of representative pixels of segment i. In practice we use four
representative pixels per segment. The representative pixels computed this way are used
for computing both the approximated mean and the approximated variance of every test
pattern. Figure 1 show how well this approximation works in practice.
Given the representative pixels, the approximated variance ?
? i (xj ) of segment i of pattern j
is given by:
k
X
?
?i (xj ) =
|xj ? ?
?i (xj )|
j=1

2.3

The rejection cascade

We construct a rejection cascade that can quickly reject image patches, with minimal computational load. Our feature space consist of the approximated mean and variance of the
image segments. In our experiments we have 8 segments, each represented by its mean and
variance, giving rise to a 16D feature space. This feature space is very fast to compute, as
we need only four pixels to calculate the approximate mean and variance of the segment.
Because the feature space is so small we can exhaustively search for all classifiers on single,
pairs and triplets of segments. In addition this feature space gives enough information to
reject texture-less regions without the need to normalize the mean or variance of the entire
image patch. We next describe our rejectors in detail.

2.3.1

Feature rejectors

Now, that we have segmented every image into several segments and approximated every
segment with a small number of representative pixels, we can exhaustively search for the
best combination of segments that will reject the largest number of non-face images. We
repeat this process until the improvement in rejection is negligible.
Given a training set of P positive examples (i.e. faces) and N negative examples we construct the following linear rejectors and adjust the parameter ? so that they will correctly
classify d ? P (we use d = 0.95) of the face images and save r, the number of negative
examples they correctly rejected, as well as the parameter ?.
1. For each segment i, find a bound on its approximated mean. Formally, find ? s.t.
?
?i (x) > ? or ?
?i (x) < ?
2. For each segment i, find a bound on its approximated variance. Formally, find ?
s.t.
?
?i (x) > ? or ?
?i (x) < ?
3. For each pair of segments i, j, find a bound on the difference between their approximated means. Formally, find ? s.t.
?
?i (x) ? ?
?j (x) > ? or ?
?i (x) ? ?
?j (x) < ?
4. For each pair of segments i, j, find a bound on the difference between their approximated variance. Formally, find ? s.t.
?
?i (x) ? ?
?j (x) > ? or ?
?i (x) ? ?
?j (x) < ?
5. For each triplet of segments i, j, k find a bound on the difference of the absolute
difference of their approximated means. Formally, find ? s.t.
|?
?i (x) ? ?
?j (x)| ? |?
?i (x) ? ?
?k (x)| > ?
This process is done only once to form a pool of rejectors. We do not re-train rejectors after
selecting a particular rejector.
2.3.2

Training

We form the cascade of rejectors from a large pattern vs. rejector binary table T, where
each entry T(i, j) is 1 if rejector j rejects pattern i. Because the table is binary we can
store every entry in a single bit and therefor a table of 513, 000 patterns and 664 rejectors
can easily fit in the memory. We then use a greedy algorithm to pick the next rejector with
the highest rejection score r. We repeat this process until r falls below some predefined
threshold.
1. Sum each column and choose column (rejector) j with the highest sum.
2. For each entry T (i, j), in column j, that is equal to one, zero row i.
3. Go to step 1
The entire process is extremely fast and takes only several minutes, including I/O. The idea
of creating a rejector pool in advance was independently suggested by [16] to accelerate
the Viola-Jones training time. We obtain 50 rejectors using this method. Figure 2a shows
the rejection rate of this cascade on a training set of 513, 000 images, as well as the number
of arithmetic operations it takes. Note that roughly 50% of all patterns are rejected by the
first rejector using only 12 operations. During testing we compute the approximated mean
and variance only when they are needed and not before hand.

Comparing different image segmentations

Rejection rate
90

90

85

80

80

70

75
rejection rate

rejection rate

60

70

65

50

40

60
30

55
random segments
vertical segments
horizontal segments
image segments

20

50

45

10

0

50

100
150
number of operations

200

250

0

5

10

15

20

25

number of rejectors

(a)
(b)
Figure 2: (a) Rejection rate on training set. The x-axis counts the number of arithmetic
operations needed for rejection. The y-axis is the rejection rate on a training set of about
half-a-million non-faces and about 1500 faces. Note that almost 50% of the false patterns
are rejected with just 12 operations. Overall rejection rate of the feature rejectors on the
training set is 88%, it drops to about 80% on the CMU+MIT database. (b) Rejection rate
as a function of image segmentation method. We trained our system using four types of
image segmentation and show the rejector. We compare our image segmentation approach
against naive segmentation of the image plane into horizontal blocks, vertical blocks or
random segmentation. In each case we trained a cascade of 21 rejectors and calculated their
accumulative rejection rate on our training set. Clearly working with our image segments
gives the best results.
We wanted to confirm our intuition that indeed only meaningful regions in the image can
produce such results and we therefor performed the following experiment. We segmented
the pixels in the image using four different methods. (1) using our image segments (2)
into 8 horizontal blocks (3) into 8 vertical blocks (4) into 8 randomly generated segments.
Figure 2b show that image segments gives the best results, by far.
The remaining false positive patterns are passed on to the next rejectors, as described next.
2.4

Texture-less region rejection

We found that the feature rejectors defined in the previous section are doing poorly in
rejecting texture-less regions. This is because we do not perform any sort of variance
normalization on the image patch, a step that will slow us down. However, by now we
have computed the approximated mean and variance of all the image segments and we
can construct rejectors based on all of them to reject texture-less regions. In particular we
construct the following two rejectors
1. Reject all image patches where the variance of all 8 approximated means falls
below a threshold. Formally, find ? s.t.
?
? (?
?i (x)) < ? i = 1...8
2. Reject all image patches where the variance of all 8 approximated variances falls
below a threshold. Formally, find ? s.t.
?
? (?
?i (x)) < ? i = 1...8
2.5

Linear classifier

Finally, we construct a cascade of 10 linear rejectors, using all 16 features (i.e. the approximated means and variance of all 8 segments).

(a)
(b)
Figure 3: Examples. We show examples from the CMU+MIT dataset. Our method correctly rejected over 99.8% of the image patches in the image, leaving only a handful of
image patches to be tested by a ?slow?, full scale classifier.
2.6

Multi-detection heuristic

As noted by previous authors [15] face classifiers are insensitive to small changes in position and scale and therefor we adopt the heuristic that only four overlapping detections are
declared a face. This help reduce the number of detected rectangles around and face, as
well as reject some spurious false detections.

3

Experiments

We have tested our rejection scheme on the standard CMU+MIT database [13]. We created
a pyramid at increasing scales of 1.1 and scanned every scale for rectangles of size 20 ? 20
in jumps of two pixels. We calculate the approximated mean and variance only when they
are needed, to save time.
Overall, our rejection scheme rejected over 99.8% of the image patches, while correctly detecting 93% of the faces. On average the feature rejectors rejected roughly 80% of all image
patches, the textureless region rejectors rejected additional 10% of the image patches, the
linear rejectors rejected additional 5% and the multi-detection heuristic rejected the remaining image patterns. The average rejection rate per image is over 99.8%. This is not enough
for face detection, as there are roughly 615, 000 image patches per image in the CMU+MIT
database, and our rejector cascade passes, on average, 870 false positive image patches, per
image. This patterns will have to be passed to a full-scale classifier to be properly rejected.
Figure 3 give some examples of our system. Note that the system correctly detects all the
faces, while allowing a small number of false positives.
We have also experimented with rescaling the features, instead of rescaling the image, but
noted that the number of false positives increased by about 5% for every fixed detection
rate we tried (All the results reported here use image pyramids).

4

Summary and Conclusions

We presented a fast rejection scheme that is based on image segments and demonstrated it
on the canonical example of face detection. Image segments are made of regions of pixels
with similar behavior over the image set. The shape of the features (i.e. image segments)
we use is data-driven and they are very cheap to compute The relationships between the
mean and variance of image segments are used to form a cascade of rejectors that can reject
over 99.8% of the image patches, thus only a small fraction of the image patches must be

passed to a full-scale classifier. The training time for our method is much less than an hour,
on a standard PC. We believe that our method can be used to accelerate standard machine
learning algorithms that are too slow for object detection, by serving as a gate keeper that
rejects most of the false patterns.

References
[1] Shai Avidan. EigenSegments: A spatio-temporal decomposition of an ensemble of
image. In European Conference on Computer Vision (ECCV), May 2002, Copenhagen,
Denmark.
[2] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line
learning and an application to boosting. In Computational Learning Theory: Eurocolt
95, pages 2337. Springer-Verlag, 1995.
[3] R. O. Duda and P. E. Hart. Pattern Classification and Scene Analysis. WileyInterscience publication, 1973.
[4] M. Elad, Y. Hel-Or and R. Keshet. Rejection based classifier for face detection. Pattern
Recognition Letters 23 (2002) 1459-1471.
[5] B. Heisele, T. Serre, S. Mukherjee, and T. Poggio. Feature reduction and hierarchy of
classifiers for fast object detection in video images. In Proc. CVPR, volume 2, pages
1824, 2001.
[6] D. Keren, M. Osadchy, and C. Gotsman. Antifaces: A novel, fast method for image
detection. IEEE Trans. on Pattern Analysis and Machine Intelligence, 23(7):747761,
2001.
[7] S.Z. Li, L. Zhu, Z.Q. Zhang, A. Blake, H.J. Zhang and H. Shum. Statistical Learning of Multi-View Face Detection. In Proceedings of the 7th European Conference on
Computer Vision, Copenhagen, Denmark, May 2002.
[8] Henry Schneiderman and Takeo Kanade. A statistical model for 3d object detection
applied to faces and cars. In IEEE Conference on Computer Vision and Pattern Recognition. IEEE, June 2000.
[9] L. Sirovich and M. Kirby. Low-dimensional procedure for the characterization of human faces. In Journal of the Optical Society of America 4, 510-524.
[10] K.-K. Sung and T. Poggio. Example-based Learning for View-Based Human Face Detection. In IEEE Transactions on Pattern Analysis and Machine Intelligence 20(1):3951, 1998.
[11] M. Turk and A. Pentland. Eigenfaces for recognition. In Journal of Cognitive Neuroscience, vol. 3, no. 1, 1991.
[12] S. Romdhani, P. Torr, B. Schoelkopf, and A. Blake. Computationally efficient face
detection. In Proc. Intl. Conf. Computer Vision, pages 695700, 2001.
[13] H. A. Rowley, S. Baluja, and T. Kanade. Neural network-based face detection. IEEE
Trans. on Pattern Analysis and Machine Intelligence, 20(1):2338, 1998.
[14] V. Vapnik. The Nature of Statistical Learning Theory. Springer, N.Y., 1995.
[15] P. Viola and M. Jones. Rapid Object Detection using a Boosted Cascade of Simple
Features. In IEEE Conference on Computer Vision and Pattern Recognition, Hawaii,
2001.
[16] J. Wu, J. M. Rehg, and M. D. Mullin. Learning a Rare Event Detection Cascade
by Direct Feature Selection. To appear in Advances in Neural Information Processing
Systems 16 (NIPS*2003), MIT Press, 2004.

"
1996,Contour Organisation with the EM Algorithm,,1291-contour-organisation-with-the-em-algorithm.pdf,Abstract Missing,"Contour Organisation with the EM
Algorithm

J. A. F. Leite and E. R. Hancock
Department of Computer Science
University of York, York, Y01 5DD, UK.

Abstract
This paper describes how the early visual process of contour organisation can be realised using the EM algorithm. The underlying
computational representation is based on fine spline coverings. According to our EM approach the adjustment of spline parameters
draws on an iterative weighted least-squares fitting process. The
expectation step of our EM procedure computes the likelihood of
the data using a mixture model defined over the set of spline coverings. These splines are limited in their spatial extent using Gaussian windowing functions. The maximisation of the likelihood leads
to a set of linear equations in the spline parameters which solve the
weighted least squares problem. We evaluate the technique on the
localisation of road structures in aerial infra-red images.

1

Introduction

Dempster, Laird and Rubin's EM (expectation and maximisation) [1] algorithm was
originally introduced as a means of finding maximum likelihood solutions to problems posed in terms of incomplete data. The basic idea underlying the algorithm
is to iterate between the expectation and maximisation modes until convergence is
reached. Expectation involves computing a posteriori model probabilities using a
mixture density specified in terms of a series of model parameters. In the maximisation phase, the model parameters are recomputed to maximise the expected
value of the incomplete data likelihood. In fact, when viewed from this perspective,
the updating of a posteriori probabilities in the expectation phase would appear
to have much in common with the probabilistic relaxation process extensively exploited in low and intermediate level vision [9, 2] . Maximisation of the incomplete

Contour Organisation with the EM Algorithm

881

data likelihood is reminiscent of robust estimation where outlier reject is employed
in the iterative re-computation of model parameters [7].
It is these observations that motivate the study reported in this paper. We are
interested in the organisation of the output of local feature enhancement operators
into meaningful global contour structures [13, 2]. Despite providing one of the classical applications of relaxation labelling in low-level vision [9], successful solutions
to the iterative curve reinforcement problem have proved to be surprisingly elusive
[8, 12, 2]. Recently, two contrasting ideas have offered practical relaxation operators. Zucker et al [13] have sought biologically plausible operators which draw on
the idea of computing a global curve organisation potential and locating consistent
structure using a form of local snake dynamics [11]. In essence this biologically
inspired model delivers a fine arrangement of local splines that minimise the curve
organisation potential. Hancock and Kittler [2], on the other hand, appealed to a
more information theoretic motivation [4]. In an attempt to overcome some of the
well documented limitations of the original Rosenfeld, Hummel and Zucker relaxation operator [9] they have developed a Bayesian framework for relaxation labelling
[4]. Of particular significance for the low-level curve enhancement problem is the underlying statistical framework which makes a clear-cut distinction between the roles
of uncertain image data and prior knowledge of contour structure. This framework
has allowed the output of local image operators to be represented in terms of Gaussian measurement densities, while curve structure is represented by a dictionary of
consistent contour structures [2].

While both the fine-spline coverings of Zucker [13] and the dictionary-based relaxation operator of Hancock and Kittler [2] have delivered practical solutions to
the curve reinforcement problem, they each suffer a number of shortcomings. For
instance, although the fine spline operator can achieve quasi-global curve organisation, it is based on an essentially ad hoc local compatibility model. While being
more information theoretic, the dictionary-based relaxation operator is limited by
virtue of the fact that in most practical applications the dictionary can only realistically be evaluated over at most a 3x3 pixel neighbourhood. Our aim in this paper
is to bridge the methodological divide between the biologically inspired fine-spline
operator and the statistical framework of dictionary-based relaxation. We develop
an iterative spline fitting process using the EM algorithm of Dempster et al [1] .
In doing this we retain the statistical framework for representing filter responses
that has been used to great effect in the initialisation of dictionary-based relaxation. However, we overcome the limited contour representation of the dictionary by
drawing on local cubic splines.

2

Prerequisites

The practical goal in this paper is the detection of line-features which manifest
themselves as intensity ridges of variable width in raw image data. Each pixel
is characterised by a vector of measurements, ?<i where i is the pixel-index. This
measurement vector is computed by applying a battery of line-detection filters of
various widths and orientations to the raw image. Suppose that the image data
is indexed by the pixel-set I. Associated with each image pixel is a cubic spline
parameterisation which represents the best-fit contour that couples it to adjacent
feature pixels. The spline is represented by a vector of parameters denoted by

882

1. A. F. Leite and E. R. Hancock

9.; = (q?, q}, q; , qr) T . Let (Xi, Yi) represent the position co-ordinates of the pixel
indexed i . The spline variable, Si,j = X i - Xj associated with the contour connecting
the pixel indexed j is the horizontal displacement between the pixels indexed i and
j. We can write the cubic spline as an inner product F(Si ,j, g) = g[.Si,j where
Si,j = (1, Si,j , S~,j' S~,j) T . Central to our EM algorithm will be the comparison of
the predicted vertical spline displacement with its measured value Ti, j = Yi - Yj.
In order to initialise the EM algorithm, we require a set of initial spline probabilities
Here we use the multi-channel combination model
which we denote by 7l'(q(O?).
-t
recently reported by Leite and Hancock [5] to compute an initial multi-scale linefeature probability. Accordingly, if ~ is the variance-covariance matrix for the
components of the filter bank, then

7l'(g~O?) =

l-exp

[-~~r~-l~i]

(1)

The remainder of this paper outlines how these initial probabilities are iteratively refined using the EM algorithm. Because space is limited we only provide an algorithm
sketch. Essential algorithm details such as the estimation of spline orientation and
the local receptive gating of the spline probabilities are omitted for clarity. Full
details can be found in a forthcoming journal article [6].

3

Expectation

Our basic model of the spline organisation process is as follows . Associated with
each image pixel is a spline parameterisation. Key to our philosophy of exploiting
a mixture model to describe the global contour structure of the image is the idea
that the pixel indexed i can associate to each of the putative splines residing in a
local Gaussian window N i . We commence by developing a mixture model for the
conditional probability density for the filter response ~i given the current global
Vi E I} is the global spline description at
spline description. If ~(n) = {q(n),
-t
iteration n of the EM process, then we can expand the mixture distribution over a
set of putative splines that may associate with the image pixel indexed i
p(~il~(n?) =

L p(~ilg;n?)7l'(g;n?)

(2)

jENi

The components of the above mixture density are the conditional measurement
densities p(~ilq(n?) and the spline mixing proportions 7l'(q(n?). The conditional
-J
-J
measurement densities represent the likelihood that the datum ~i originates from the
spline centred on pixel j. The mixing proportions, on the other hand, represent the
fractional contribution to the data arising from the jth parameter vector i.e. q(n).
-J
Since we are interested in the maximum likelihood estimation of spline parameters,
we turn our attention to the likelihood of the raw data, i.e.
P(~i' Vi E II~(n?) = IIp(~il~(n))

(3)

iEI

The expectation step of the EM algorithm is aimed at estimating the log-likelihood
using the parameters of the mixture distribution. In other words, we need to average the likelihood over the space of potential pixel-spline assignments. In fact,

883

Contour Organisation with the EM Algorithm

it was Dempster, Laird and Rubin [1] who observed that maximising the weighted
log-likelihood was equivalent to maximising the conditional expectation of the likelihood for a new parameter set given an old parameter set. For our spline fitting
problem, maximisation of the expectation of the conditional likelihood is equivalent
to maximising the weighted log-likelihood function
Q(~(n+l)I~(n?)

=L

L

p(g;n)l~i) lnp(~ilg;n+l?)

(4)

iEI JEN,

The a posteriori probabilities p(q(n)l~i) may be computed from the corresponding
-1

components of the mixture density p(~ilqJn?) using the Bayes formula

p(g;n)l~i) = 2:

P(~i Iq(n) )7r( q(n?)

(11 gk(n?))7r (gk(n?)

(5)

kEN. P ~t

For notational convenience, and to make the weighting role of the a posteriori probabilities explicit we use the shorthand w~~) = p(g;n) I~i). Once updated parameter
become available through the maximisation of this criterion, imestimates q(n)
-t
proved estimates of the mixture components may be obtained by substitution into
required to determine
equation (6). The updated mixing proportions, 7r(q(n+l?),
-t
the new weights w~~) are computed from the newly available density components
using the following estimator
7r( (n+l?)
gi

= """"

p( z ?lq(n?)7r(q(n?)

-)

-i

-i

~ ~
(I (n?) 7r (gk(n?)
JEN. L.JkEIP ~j gk

(6)

In order to proceed with the development of a spline fitting process we require
a model for the mixture components, i.e. p(~ilg;n?). Here we assume that the
required model can be specified in terms of Gaussian distribution functions. In
other words, we confine our attention to Gaussian mixtures. The physical variable
of these distributions is the squared error residual for the position prediction of the
ith datum delivered by the jth spline. Accordingly we write

p(~ilg;n?) =

a

exp [-,8

(ri,j - F(Si,j, g;n?)) 2]

(7)

where ,8 is the inverse variance of the fit residuals. Rather than estimating ,8, we
use it in the spirit of a control variable to regulate the effect of fit residuals.
Equations (5), (6) and (11) therefore specify a recursive procedure that iterates the
weighted residuals to compute a new mixing proportions based on the quality of
the spline fit.

4

Maximisation

The maximisation step aims to optimize the quantity Q(~(n+l)I~(n?) with respect
to the spline parameters. Formally this corresponds to finding the set of spline
parameters which satisfy the condition
(8)

884

1. A. F. Leite and E. R. Hancock

We find a local approximation to this condition by solving the following set of linear
equations
8Q( <)(n+l) I<)(n?)
= 0

8(qf)(n+l)

(9)

for each spline parameter (qf)(n+l) in turn, i.e. for k=O,1,2,3. Recovery of the
splines is most conveniently expressed in terms of the following matrix equation for
the components of the parameter-vector q(n)
-t
(10)

The elements of the vector x(n) are weighted cross-moments between the parallel
and perpendicular spline distances in the Gaussian window, i.e.

x(n)

=

(11)

-t

The elements of the matrix A~n), on the other hand, are weighted moments computed purely in terms of the parallel distance Si ,j. If k and I are the row and column
indices, then the (k, l)th element of the matrix A~n) is
I
[A(n)]k
t
,

= ""L

w(n) sk+I-2
t,)

t,l

(12)

JENi

5

Experiments

We have evaluated our iterative spline fitting algorithm on the detection of linefeatures in aerial infra-red images. Figure 1a shows the original picture. The initial
feature probabilities (i.e. 71"" { q~O?)) assigned according to equation (1) are shown
in Figure lb. Figure 1c shows the final contour-map after the EM algorithm has
converged. Notice that the line contours exhibit good connectivity and that the
junctions are well reconstructed. We have highlighted a subregion of the original
image. There are two features in this subregion to which we would like to draw
attention. The first of these is centred on the junction structure. The second
feature is a neighbouring point on the descending branch of the road.
Figure 2 shows the iterative evolution of the cubic splines at these two locations.
The spline shown in Figure 2a adjusts to fit the upper pair of road segments. Notice
also that although initially displaced, the final spline passes directly through the
junction. In the case of the descending road-branch the spline shown in Figure 2b
recovers from an initially poor orientation estimate to align itself with the underlying
road structure. Figure 2c shows how the spline probabilities (i.e. 7I""(q~n?)) evolve
with iteration number. Initially, the neighbourhood is highly ambiguous. Many
neighbouring splines compete to account for the local image structure. As a result
the detected junction is several pixels wide. However, as the fitting process iterates,
the splines move from the inconsistent initial estimate to give a good local estimate
which is less ambiguous. In other words the two splines illustrated in Figure 2 have
successfully arranged themselfs to account for the junction structure.

Contour Organisation with the EM Algorithm

(a) Original image.

(b) Probability map.

885

(c) Line map.

Figure 1: Infra-red aerial picture with corresponding probability map showing region
containing pixel under study and correspondent line map.

6

Conclusions

We have demonstrated how the process of parallel iterative contour refinement can
be realised using the classical EM algorithm of Dempster, Laird and Rubin [1].
The refinement of curves by relaxation operations has been a preoccupation in the
literature since the seminal work of Rosenfeld, Hummel and Zucker [9]. However,
it is only recently that successful algorithms have been developed by appealing
to more sophisticated modelling methodologies [13, 2]. Our EM approach not
only delivers comparable performance, it does so using a very simple underlying
model. Moreover, it allows the contour re-enforcement process to be understood in
a weighted least-squares optimisation framework which has many features in common with snake dynamics [11] without being sensitive on the initial positioning of
control points. Viewed from the perspective of classical relaxation labelling [9, 4],
the EM framework provides a natural way of evaluating support beyond the immediate object neighbourhood. Moreover, the framework for spline fitting in 2D is
readily extendible to the reconstruction of surface patches in 3D [10].

References
[1] Dempster A., Laird N. and Rubin D., ""Maximum-likelihood from incomplete data
via the EM algorithm"", J. Royal Statistical Soc. Ser. B (methodological) ,39, pp 1-38,
1977.
[2] Hancock E.R and Kittler J., ""Edge Labelling using Dictionary-based Probabilistic
Relaxation"", IEEE PAMI, 12, pp. 161-185, 1990.
[3] Jordan M.1. and Jacobs RA, ""Hierarchical Mixtures of Experts and the EM Algorithm"", Neural Computation, 6, pp. 181-214, 1994.
[4] Kittler J. and Hancock, E.R, ""Combining Evidence in Probabilistic relaxation"",
International Journal of Pattern Recognition and Artificial Intelligence, 3, N1, pp
29-51, 1989.
[5] Leite J.A.F. and Hancock, E.R, "" Statistically Combining and Refining Multichannel
Information"" , Progress in Image Analysis and Processing III: Edited by S Impedovo,
World Scientific, pp . 193-200, 1994.
[6] Leite J.A.F. and Hancock, E .R, ""Iterative curve organisation with the EM algorithm"", to appear in Pattern Recognition Letters, 1997.

886

1. A. F. Leite and E. R. Hancock

~
"",

/'

\~
Iili>

(ll'

x

Iv)

(vi>

I ~''''

~
~~
(ril)

(Ie)

(YIIII

,

x

""""""X

""

(j)

<tii>

(i!)

~
+
+
~ :+ :+ :+

:f\ ~ ~
I,.)

:--I- ~ +
so

hy)

(v)

,

""

(Ix)

(i,)

MI)

""

)

:+ ~
I?

,l)

(a)

(b)

(c)

Figure 2: Evolution of the spline in the fitting process. The image in (a) is the
junction spline while the image in (b) is the branch spline. The first spline is shown
in (i), and the subsequent ones from (ii) to (xi). The evolution of the corresponding
spline probabilities is shown in (c).
[7] Meer P., Mintz D., Rosenfeld A . and Kim D.Y., ""Robust Regression Methods for
Computer Vision - A Review"", International Journal of Computer vision, 6, pp.
59- 70, 1991.
[8] Peleg S. and Rosenfeld A"" ""Determining Compatibility Coefficients for curve enhancement relaxation processes"", IEEE SMC, 8, pp. 548-555, 1978.
[9] Rosenfeld A., Hummel R.A. and Zucker S.W., ""Scene labelling by relaxation operations"", IEEE Transactions SMC, SMC-6, pp400-433, 1976.
[10] Sander P.T. and Zucker S.W ., ""Inferring surface structure and differential structure
from 3D images"" , IEEE PAMI, 12, pp 833-854, 1990.
[11] Terzopoulos D. , ""Regularisation of inverse problems involving discontinuities"" , IEEE
PAMI, 8, pp 129-139, 1986.
[12] Zucker, S.W., Hummel R.A., and Rosenfeld A., ""An application ofrelaxation labelling
to line and curve enhancement"", IEEE TC, C-26, pp. 394-403, 1977.
[13] Zucker S. , David C., Dobbins A. and Iverson L., ""The organisation of curve qetection: coarse tangent fields and fine spline coverings"" , Proceedings of the Second
International Conference on Computer Vision, pp. 577-586, 1988.

"
2015,Consistent Multilabel Classification,Poster,5883-consistent-multilabel-classification.pdf,"Multilabel classification is rapidly developing as an important aspect of modern predictive modeling, motivating study of its theoretical aspects. To this end, we propose a framework for constructing and analyzing multilabel classification metrics which reveals novel results on a parametric form for population optimal classifiers, and additional insight into the role of label correlations. In particular, we show that for multilabel metrics constructed as instance-, micro- and macro-averages, the population optimal classifier can be decomposed into binary classifiers based on the marginal instance-conditional distribution of each label, with a weak association between labels via the threshold. Thus, our analysis extends the state of the art from a few known multilabel classification metrics such as Hamming loss, to a general framework applicable to many of the classification metrics in common use. Based on the population-optimal classifier, we propose a computationally efficient and general-purpose plug-in classification algorithm, and prove its consistency with respect to the metric of interest. Empirical results on synthetic and benchmark datasets are supportive of our theoretical findings.","Consistent Multilabel Classification
Oluwasanmi Koyejo?
Department of Psychology,
Stanford University
sanmi@stanford.edu

Nagarajan Natarajan?
Department of Computer Science,
University of Texas at Austin
naga86@cs.utexas.edu

Pradeep Ravikumar
Department of Computer Science,
University of Texas at Austin
pradeepr@cs.utexas.edu

Inderjit S. Dhillon
Department of Computer Science,
University of Texas at Austin
inderjit@cs.utexas.edu

Abstract
Multilabel classification is rapidly developing as an important aspect of modern
predictive modeling, motivating study of its theoretical aspects. To this end, we
propose a framework for constructing and analyzing multilabel classification metrics which reveals novel results on a parametric form for population optimal classifiers, and additional insight into the role of label correlations. In particular,
we show that for multilabel metrics constructed as instance-, micro- and macroaverages, the population optimal classifier can be decomposed into binary classifiers based on the marginal instance-conditional distribution of each label, with a
weak association between labels via the threshold. Thus, our analysis extends the
state of the art from a few known multilabel classification metrics such as Hamming loss, to a general framework applicable to many of the classification metrics
in common use. Based on the population-optimal classifier, we propose a computationally efficient and general-purpose plug-in classification algorithm, and prove
its consistency with respect to the metric of interest. Empirical results on synthetic
and benchmark datasets are supportive of our theoretical findings.

1

Introduction

Modern classification problems often involve the prediction of multiple labels simultaneously associated with a single instance e.g. image tagging by predicting multiple objects in an image. The
growing importance of multilabel classification has motivated the development of several scalable
algorithms [8, 12, 18] and has led to the recent surge in theoretical analysis [1, 3, 7, 16] which helps
guide and understand practical advances. While recent results have advanced our knowledge of
optimal population classifiers and consistent learning algorithms for particular metrics such as the
Hamming loss and multilabel F -measure [3, 4, 5], a general understanding of learning with respect
to multilabel classification metrics has remained an open problem. This is in contrast to the more
traditional settings of binary and multiclass classification where several recently established results
have led to a rich understanding of optimal and consistent classification [9, 10, 11]. This manuscript
constitutes a step towards establishing results for multilabel classification at the level of generality
currently enjoyed only in these traditional settings.
Towards a generalized analysis, we propose a framework for multilabel sample performance metrics
and their corresponding population extensions. A classification metric is constructed to measure the
utility1 of a classifier, as defined by the practitioner or end-user. The utility may be measured using
?

1

Equal contribution.
Equivalently, we may define the loss as the negative utility.

1

the sample metric given a finite dataset, and further generalized to the population metric with respect
to a given data distribution (i.e. with respect to infinite samples). Two distinct approaches have been
proposed for studying the population performance of classifier in the classical settings of binary
and multiclass classification, described by Ye et al. [17] as decision theoretic analysis (DTA) and
empirical utility maximization (EUM). DTA population utilities measure the expected performance
of a classifier on a fixed-size test set, while EUM population utilities are directly defined as a function
of the population confusion matrix. However, state-of-the-art analysis of multilabel classification
has so-far lacked such a distinction. The proposed framework defines both EUM and DTA multilabel
population utility as generalizations of the aforementioned classic definitions. Using this framework,
we observe that existing work on multilabel classification [1, 3, 7, 16] have exclusively focused on
optimizing the DTA utility of (specific) multilabel metrics.
Averaging of binary classification metrics remains one of the most widely used approaches for defining multilabel metrics. Given a binary label representation, such metrics are constructed via averaging with respect to labels (instance-averaging), with respect to examples separately for each label
(macro-averaging), or with respect to both labels and examples (micro-averaging). We consider a
large sub-family of such metrics where the underlying binary metric can be constructed as a fraction of linear combinations of true positives, false positives, false negatives and true negatives [9].
Examples in this family include the ubiquitous Hamming loss, the averaged precision, the multilabel averaged F -measure, and the averaged Jaccard measure, among others. Our key result is that a
Bayes optimal multilabel classifier for such metrics can be explicitly characterized in a simple form
? the optimal classifier thresholds the label-wise conditional probability marginals, and the label dependence in the underlying distribution is relevant to the optimal classifier only through the threshold
parameter. Further, the threshold is shared by all the labels when the metric is instance-averaged
or micro-averaged. This result is surprising and, to our knowledge, a first result to be shown at this
level of generality for multilabel classification. The result also sheds additional insight into the role
of label correlations in multilabel classification ? answering prior conjectures by Dembczy?nski et al.
[3] and others.
We provide a plug-in estimation based algorithm that is efficient as well as theoretically consistent,
i.e. the true utility of the empirical estimator approaches the optimal (EUM) utility of the Bayes
classifier (Section 4). We also present experimental evaluation on synthetic and real-world benchmark multilabel datasets comparing different estimation algorithms (Section 5) for representative
multilabel performance metrics selected from the studied family. The results observed in practice
are supportive of what the theory predicts.
1.1

Related Work

We briefly highlight closely related theoretical results in the multilabel learning literature. Gao and
Zhou [7] consider the consistency of multilabel learning with respect to DTA utility, with a focus
on two specific losses ? Hamming and rank loss (the corresponding measures are defined in Section
2). Surrogate losses are devised which result in consistent learning with respect to these metrics.
In contrast, we propose a plug-in estimation based algorithm which directly estimates the Bayes
optimal, without going through surrogate losses. Dembczynski et al. [2] analyze the DTA population
optimal classifier for the multilabel rank loss, showing that the Bayes optimal is independent of label
correlations in the unweighted case, and construct certain weighted univariate losses which are DTA
consistent surrogates in the more general weighted case. Perhaps the work most closely related
to ours is by Dembczynski et al. [4] who propose a novel DTA consistent plug-in rule estimation
based algorithm for multilabel F -measure. Cheng et al. [1] consider optimizing popular losses in
multilabel learning such as Hamming, rank and subset 0/1 loss (which is the multilabel analog of
the classical 0-1 loss). They propose a probabilistic version of classifier chains (first introduced by
Read et al. [13]) for estimating the Bayes optimal with respect to subset 0/1 loss, though without
rigorous theoretical justification.

2

A Framework for Multilabel Classification Metrics

Consider multilabel classification with M labels, where each instance is denoted by x 2 X . For
convenience, we will focus on the common binary encoding, where the labels are represented by
a vector y 2 Y = {0, 1}M , so ym = 1 iff the mth label is associated with the instance, and
2

ym = 0 otherwise. The goal is to learn a multilabel classifier f : X 7! Y that optimizes a certain
performance metric with respect to P ? a fixed data generating distribution over the domain X ? Y,
using a training set of instance-label pairs (x(n) , y(n) ), n = 1, 2, . . . , N drawn (typically assumed
iid.) from P. Let X and Y denote the random variables for instances and labels respectively, and let
denote the performance (utility) metric of interest.
Most classification metrics can be represented as functions of the entries of the confusion matrix. In
case of binary classification, the confusion matrix is specified by four numbers, i.e., true positives,
true negatives, false positives and false negatives. Similarly, we construct the following primitives
for multilabel classification:
(n)
(n)
c )m,n = Jfm (x(n) ) = 1, ym
c )m,n = Jfm (x(n) ) = 0, ym
TP(f
= 1K
TN(f
= 0K
(1)
(n)
(n)
(n)
(n)
c
c
FP(f )m,n = Jfm (x ) = 1, ym = 0K
FN(f )m,n = Jfm (x ) = 0, ym = 1K
where JZK denotes the indicator function that is 1 if the predicate Z is true or 0 otherwise. It is clear
that most multilabel classification metrics considered in the literature can be written as a function of
the M N primitives defined in (1).

In the following, we consider a construction which is of sufficient generality to capture all multilabel
c )m,n , FP(f
c )m,n , TN(f
c )m,n , FN(f
c )m,n }M,N
metrics in common use. Let Ak (f ) : {TP(f
m=1,n=1 7! R,
k = 1, 2, . . . , K represent a set of K functions. Consider sample multilabel metrics constructed as
functions: : {Ak (f )}K
k=1 7! [0, 1). We note that the metric need not decompose over individual
instances. Equipped with this definition of a sample performance metric , consider the population
utility of a multilabel classifier f defined as:
U (f ; , P) = ({E [ Ak (f ) ]}K
(2)
k=1 ),
where the expectation is over iid draws from the joint distribution P. Note that this can be seen as
a multilabel generalization of the so-called Empirical Utility Maximization (EUM) style classifiers
studied in binary [9, 10] and multiclass [11] settings.
Our goal is to learn a multilabel classifier that maximizes U (f ; , P) for general performance metrics
. Define the (Bayes) optimal multilabel classifier as:
f ? = argmax U (f ; , P).
(3)
f :X ! {0,1}M

p

Let U (f ; , P) = U . We say that ?f is a consistent estimator of f ? if U (?f ; , P) ! U ? .
?

?

Examples. The averaged accuracy (1 - Hamming loss) used in multilabel classification correPM PN
c )m,n + FN(f
c )m,n and Ham (f ) =
sponds to simply choosing: A1 (f ) = M1N m=1 n=1 FP(f
2
1 A1 (f ). The measure
corresponding
to
rank
loss
can
be
obtained
by choosing Ak (f ) =
?
??
?
PM
PM
1
c
c )m2 ,k , for k = 1, 2, . . . , N and Rank = 1
FN(f
m1 =1
m2 =1 FP(f )m1 ,k
M2
P
N
1
k=1 Ak (f ). Note that the choice of {Ak }, and therefore , is not unique.
N
Remark 1. Existing results on multilabel classification have focused on decision-theoretic analysis
(DTA) style classifiers, where the utility is defined as:
?
?
UDTA (f ; , P) = E
({Ak (f )}K
(4)
k=1 ) ,
and the expectation is over iid samples from P. Furthermore, there are no theoretical results for
consistency with respect to general performance metrics in this setting (See Appendix B.2).
For the remainder of this manuscript, we refer to U (f ; P) as the utility defined in (2). We will also
c ) as TP)
c when it is clear from the context.
drop the argument f (e.g. write TP(f
2.1

A Framework for Averaged Binary Multilabel Classification Metrics

The most popular class of multilabel performance metrics consists of averaged binary performance
metrics, that correspond to particular settings of {Ak (f )} using certain averages as described in the
following. For the remainder of this subsection, the metric : [0, 1]4 ! [0, 1) will refer to a binary
classification metric as is typically applied to a binary confusion matrix.
2
A subtle but important aspect of the definition of rank loss in the existing literature, including [2] and [7],
is that the Bayes optimal is allowed to be a real-valued function and may not correspond to a label decision.

3

Micro-averaging: Micro-averaged multilabel performance metrics micro are defined by averaging over both labels and examples. Let:
N X
M
N X
M
X
X
c )= 1
c )m,n ,
c )= 1
c )m,n ,
TP(f
TP(f
FP(f
FP(f
(5)
M N n=1 m=1
M N n=1 m=1
c ) and FN(f
c ) are defined similarly, then the micro-averaged multilabel performance metrics are
TN(f
given by:
K
c FP,
c TN,
c FN).
c
(TP,
(6)
micro ({Ak (f )}k=1 ) :=
Thus, for micro-averaging, one applies a binary performance metric to the confusion matrix defined
by the averaged quantities described in (5).

Macro-averaging: The metric macro measures average classification performance across labels.
Define the averaged measures:
N
N
X
X
c )m,n ,
c m (f ) = 1
c )m,n ,
c m (f ) = 1
TP
TP(f
FP
FP(f
N n=1
N n=1
c m (f ) and FN
c m (f ) are defined similarly. The macro-averaged performance metric is given by:
TN
K
macro ({Ak (f )}k=1 )

:=

M
1 X
c m , FP
c m , TN
c m , FN
c m ).
(TP
M m=1

(7)

Instance-averaging: The metric instance measures the average classification performance across
examples. Define the averaged measures:
M
M
X
X
c n (f ) = 1
c )m,n ,
c n (f ) = 1
c )m,n ,
TP
TP(f
FP
FP(f
M m=1
M m=1
c n (f ) and FN
c n (f ) are defined similarly. The instance-averaged performance metric is given by:
TN
K
instance ({Ak (f )}k=1 ) :=

3

N
1 X c c c c
(TPn , FPn , TNn , FNn ).
N n=1

(8)

Characterizing the Bayes Optimal Classifier for Multilabel Metrics

We now characterize the optimal multilabel classifier for the large family of metrics outlined in
Section 2.1 ( micro , macro and instance ) with respect to the EUM utility. We begin by observing
that while micro-averaging and instance-averaging seem quite different when viewed as sample
averages, they are in fact equivalent at the population level. Thus, we need only focus on micro to
characterize instance as well.
Proposition 1. For a given binary classification metric , consider the averaged multilabel metrics
micro defined in (6) and
instance defined in (8). For any f , U (f ; micro , P) ? U (f ; instance , P). In
particular, f ? ? ? f ? ? .
micro

instance

We further restrict our study to metrics selected from the linear-fractional metric family, recently
studied in the context of binary classification [9]. Any in this family can be written as:
c
c
c
c
c FP,
c FN,
c TN)
c = a0 + a11 TP + a10 FP + a01 FN + a00 TN ,
(TP,
c + b10 FP
c + b01 FN
c + b00 TN
c
b0 + b11 TP
c FP,
c FN,
c TN
c are defined as in Section 2.1. Many
where a0 , b0 , aij , bij , i, j 2 {0, 1} are fixed, and TP,
popular multilabel metrics can be derived using linear-fractional . Some examples include3 :
c
TP
c
(1 + 2 )TP
Jaccard :
Jacc =
F :
c + FP
c + FN
c
F =
TP
c + 2 FN
c + FP
c
(9)
(1 + 2 )TP
c
TP
c
c
Precision :
Hamming :
Prec =
Ham = TP + TN
c + FP
c
TP
3

Note that Hamming is typically defined as the loss, given by 1

4

Ham .

PM
PM
Define the population
h
i quantities: ? = m=1 P(Ym = 1) and (f ) = m=1 P(fm (x) = 1). Let
c ) , where the expectation is over iid draws from P. From (5), it follows that,
TP(f ) = E TP(f
h
i
c ) = (f ) TP(f ), TN(f ) = 1 ?
FP(f ) := E FP(f
(f ) + TP(f ) and FN(f ) = (f ) TP(f ).
Now, the population utility (2) corresponding to
U (f ;

micro , P)

=

can be written succinctly as:
c0 + c1 TP(f ) + c2 (f )
(TP(f ), FP(f ), FN(f ), TN(f )) =
d0 + d1 TP(f ) + d2 (f )
micro

(10)

with the constants:
c0 = a01 ? + a00
d0 = b01 ? + b00

a00 ? + a0 ,
b00 ? + b0 ,

c1 = a11 a10
d1 = b11 b10

a01 + a00 ,
b01 + b00 ,

c2 = a10
d2 = b10

a00 and
b00 .

We assume that the joint P has a density ? that satisfies dP = ?dx, and define ?m (x) = P(Ym =
1|X = x). Our first main result characterizes the Bayes optimal multilabel classifier f ? micro .
Theorem 2. Given the constants {c1 , c2 , c0 } and {d1 , d2 , d0 }, define:
d2 U ? micro c2
?
=
.
c1 d1 U ? micro

(11)

The optimal Bayes classifier f ? := f ? micro defined in (3) is given by:

?
1. When c1 > d1 U ? micro , f ? takes the form fm
(x) = J?m (x) >
?
2. When c1 < d1 U ? micro , f ? takes the form fm
(x) = J?m (x) <

?
?

K, for m 2 [M ].
K, for m 2 [M ].

The proof is provided in Appendix A.2, and applies equivalently to instance-averaging. Theorem 2
recovers existing results in binary [9] settings (See Appendix B.1 for details), and is sufficiently
general to capture many of the multilabel metrics used in practice. Our proof is closely related to
the binary classification case analyzed in Theorem 2 of [9], but differs in the additional averaging
across labels. A key observation from Theorem 2 is that the optimal multilabel classifier can be
obtained by thresholding the marginal instance-conditional probability for each label P(Ym = 1|x)
and, importantly, that the optimal classifiers for all the labels share the same threshold ? . Thus,
the effect of the joint distribution is only in the threshold parameter. We emphasize that while the
presented results characterize the optimal population classifier, incorporating label correlations into
the prediction algorithm may have other benefits with finite samples, such as statistical efficiency
when there are known structural similarities between the marginal distributions [3]. Further analysis
is left for future work.
The Bayes optimal for the macro-averaged population metric is straightforward to establish. We
observe that the threshold is not shared in this case.
Proposition 3. For a given linear-fractional metric , consider the macro-averaged multilabel
metric macro defined in (7). Let c1 > d1 U ? macro and f ? = f ? ?macro (x). We have, for m = 1, 2, . . . , M :
?
fm
= J?m (x) >

?
m K,

?
where m
2 [0, 1] is a constant that depends on the metric and the label-wise instance-conditional
marginals of P. Analogous results hold for c1 < d1 U ? macro .
Remark 2. It is clear that micro-, macro- and instance- averaging are equivalent at the population
level when the metric is linear. This is a straightforward consequence of the observation that
the corresponding sample utilities are the same. More generally, micro-, macro- and instanceaveraging are equivalent whenever the optimal threshold is a constant independent of P, such as for
linear metrics, where d1 = d2 = 0 so ? = cc12 (cf. Corollary 4 of Koyejo et al. [9]). Thus, our
analysis recovers known results for Hamming loss [3, 7].

4

Consistent Plug-in Estimation Algorithm

Importantly, the Bayes optimal characterization points to a simple plug-in estimation algorithm
that enjoys consistency as follows. First, one obtains an estimate ??m (x) of the marginal instanceconditional probability ?m (x) = P(Ym = 1|x) for each label m (see Reid and Williamson [14])
5

using a training sample. Then, the given metric micro (f ) is maximized on a validation sample. For
the remainder of this manuscript, we assume wlog. that c1 > d1 U ? . Note that in order to maximize
over {f : fm (x) = J?m (x) > K 8m = 1, 2, . . . , M, 2 (0, 1)}, it suffices to optimize:
? = argmax

? ),

(12)

micro (f

2(0,1)

where micro is the micro-averaged sample metric defined in (6) (similarly for instance ). Though the
threshold search is over a continuous space 2 (0, 1) the number of distinct micro (?f ) values given
a training sample of size N is at most N M . Thus (12) can be solved efficiently on a finite sample.
Algorithm 1: Plugin-Estimator for

micro

and

instance

Input: Training examples S = {x
and metric micro (or
for m = 1, 2, . . . , M do
(n)
1. Select the training data for label m: Sm = {x(n) , ym }N
n=1 .
2. Split the training data Sm into two sets Sm1 and Sm2 .
3. Estimate ??m (x) using Sm1 , define f?m (x) = J?
?m (x) > K.
end for
Obtain ? by solving (12) on S2 = [M
m=1 Sm2 .
Return: ?f ?.
(n)

, y(n) }N
n=1

instance ).

Consistency of the proposed algorithm. The following theorem shows that the plug-in procedure
of Algorithm 1 results in a consistent classifier.
p

Theorem 4. Let micro be a linear-fractional metric. If the estimates ??m (x) satisfy ??m ! ?m , 8m,
then the output multilabel classifier ?f ? of Algorithm 1 is consistent.
The proof is provided in Appendix A.4. From Proposition 1, it follows that consistency holds for
instance as well. Additionally, in light of Proposition 3, we may apply the learning algorithms
proposed by [9] for binary classification independently for each label to obtain a consistent estimator
for macro .

5

Experiments

We present two sets of results. The first is an experimental validation on synthetic data with known
ground truth probabilities. The results serve to verify our main result (Theorem 2) characterizing
the Bayes optimal for averaged multilabel metrics. The second is an experimental evaluation of
the plugin estimator algorithms for micro-, instance-, and macro-averaged multilabel metrics on
benchmark datasets.
5.1

Synthetic data: Verification of Bayes optimal

We consider the micro-averaged F1 metric in (9) for multilabel classification with 4 labels. We
sample a set of five 2-dimensional vectors x = {x(1) , x(2) , . . . , x(5) } from the standard Gaussian.
The conditional probability ?m for label m is modeled using a sigmoid function: ?m (x) = P(Ym =
1|x) = 1+exp1 wT x , using a vector wm sampled from the standard Gaussian. The Bayes optimal
m
f ? (x) 2 {0, 1}4 that maximizes the micro-averaged F1 population utility is then obtained by exhaustive search over all possible label vectors for each instance. In Figure 1 (a)-(d), we plot the
?
conditional probabilities (wrt. the sample index) for each label, the corresponding fm
for each x,
?
and the optimal threshold
using (11). We observe that the optimal multilabel classifier indeed
thresholds P(Ym |x) for each label m, and furthermore, that the threshold is same for all the labels,
as stated in Theorem 2.

6

(a)

(b)

(c)

(d)

Figure 1: Bayes optimal classifier for multilabel F1 measure on synthetic data with 4 labels, and
distribution supported on 5 instances. Plots from left to right show the Bayes optimal classifier
prediction for instances, and for labels 1 through 4. Note that the optimal ? at which the label-wise
marginal ?m (x) is thresholded is shared, conforming to Theorem 2 (larger plots are included in
Appendix C).

5.2

Benchmark data: Evaluation of plug-in estimators

We now evaluate the proposed plugin-estimation (Algorithm 1) that is consistent for micro- and
instance-averaged multilabel metrics. We focus on two metrics, F1 and Jaccard, listed in (9). We
compare Algorithm 1, designed to optimize micro-averaged (or instance-averaged) multilabel met?
rics to two related plugin-estimation methods: (i) a separate threshold m
tuned for each label m
individually ? this optimizes the utility corresponding to the macro-averaged metric, but is not consistent for micro-averaged or instance-averaged metrics, and is the most common approach in practice. We refer to this as Macro-Thres, (ii) a constant threshold 1/2 for all the labels ? this is known
to be optimal for averaged accuracy (equiv. Hamming loss), but not for non-decomposable F1 or
Jaccard metrics. We refer to this as Binary Relevance (BR) [15].
We use four benchmark multilabel datasets4 in our experiments: (i) S CENE, an image dataset consisting of 6 labels, with 1211 training and 1196 test instances, (ii) B IRDS, an audio dataset consisting
of 19 labels, with 323 training and 322 test instances, (iii) E MOTIONS, a music dataset consisting
of 6 labels, with 393 training and 202 test instances, and (iv) C AL 500, a music dataset consisting
of 174 labels, with 400 training and 100 test instances5 . We perform logistic regression (with L2
regularization) on a separate subsample to obtain estimates of ??m (x) of P(Ym = 1|x), for each label
m (as described in Section 4). All the methods we evaluate rely on obtaining a good estimator for
the conditional probability. So we exclude labels that are associated with very few instances ? in
particular, we train and evaluate using labels associated with at least 20 instances, in each dataset,
for all the methods.
In Table 1, we report the micro-averaged F1 and Jaccard metrics on the test set for Algorithm 1,
Macro-Thres and Binary Relevance. We observe that estimating a fixed threshold for all the labels
(Algorithm 1) consistently performs better than estimating thresholds for each label (Macro-Thres)
and than using threshold 1/2 for all labels (BR); this conforms to our main result in Theorem 2 and
the consistency analysis of Algorithm 1 in Theorem 4. A similar trend is observed for the instanceaveraged metrics computed on the test set, shown in Table 2. Proposition 1 shows that maximizing
the population utilities of micro-averaged and instance-averaged metrics are equivalent; the result
holds in practice as presented in Table 2. Finally, we report macro-averaged metrics computed on
test set in Table 3. We observe that Macro-Thres is competitive in 3 out of 4 datasets; this conforms
to Proposition 3 which shows that in the case of macro-averaged metrics, it is optimal to tune a
threshold specific to each label independently. Beyond consistency, we note that by using more
samples, joint threshold estimation enjoys additional statistical efficiency, while separate threshold
estimation enjoys greater flexibility. This trade-off may explain why Algorithm 1 achieves the best
performance in three out of four datasets in Table 3, though it is not consistent for macro-averaged
metrics.

4
5

The datasets were obtained from http://mulan.sourceforge.net/datasets-mlc.html.
Original C AL 500 dataset does not provide splits; we split the data randomly into train and test sets.

7

DATASET
S CENE
B IRDS
E MOTIONS
C AL 500

BR
0.6559
0.4040
0.5815
0.3647

Algorithm 1
F1
0.6847 ? 0.0072
0.4088 ? 0.0130
0.6554 ? 0.0069
0.4891 ? 0.0035

Macro-Thres
0.6631 ? 0.0125
0.2871 ? 0.0734
0.6419 ? 0.0174
0.4160 ? 0.0078

BR
0.4878
0.2495
0.3982
0.2229

Algorithm 1
Jaccard
0.5151 ? 0.0084
0.2648 ? 0.0095
0.4908 ? 0.0074
0.3225 ? 0.0024

Macro-Thres
0.5010 ? 0.0122
0.1942 ? 0.0401
0.4790 ? 0.0077
0.2608 ? 0.0056

Table 1: Comparison of plugin-estimator methods on multilabel F1 and Jaccard metrics. Reported
values correspond to micro-averaged metric (F1 and Jaccard) computed on test data (with standard
deviation, over 10 random validation sets for tuning thresholds). Algorithm 1 is consistent for microaveraged metrics, and performs the best consistently across datasets.
DATASET

BR

S CENE
B IRDS
E MOTIONS
C AL 500

0.5695
0.1209
0.4787
0.3632

Algorithm 1
F1
0.6422 ? 0.0206
0.1390 ? 0.0110
0.6241 ? 0.0204
0.4855 ? 0.0035

Macro-Thres
0.6303 ? 0.0167
0.1390 ? 0.0259
0.6156 ? 0.0170
0.4135 ? 0.0079

BR
0.5466
0.1058
0.4078
0.2268

Algorithm 1
Jaccard
0.5976 ? 0.0177
0.1239 ? 0.0077
0.5340 ? 0.0072
0.3252 ? 0.0024

Macro-Thres
0.5902 ? 0.0176
0.1195 ? 0.0096
0.5173 ? 0.0086
0.2623 ? 0.0055

Table 2: Comparison of plugin-estimator methods on multilabel F1 and Jaccard metrics. Reported
values correspond to instance-averaged metric (F1 and Jaccard) computed on test data (with standard deviation, over 10 random validation sets for tuning thresholds). Algorithm 1 is consistent for
instance-averaged metrics, and performs the best consistently across datasets.
DATASET

BR

S CENE
B IRDS
E MOTIONS
C AL 500

0.6601
0.3366
0.5440
0.1293

Algorithm 1
F1
0.6941 ? 0.0205
0.3448 ? 0.0110
0.6450 ? 0.0204
0.2687 ? 0.0035

Macro-Thres
0.6737 ? 0.0137
0.2971 ? 0.0267
0.6440 ? 0.0164
0.3226 ? 0.0068

BR
0.5046
0.2178
0.3982
0.0880

Algorithm 1
Jaccard
0.5373 ? 0.0177
0.2341 ? 0.0077
0.4912 ? 0.0072
0.1834 ? 0.0024

Macro-Thres
0.5260 ? 0.0176
0.2051 ? 0.0215
0.4900 ? 0.0133
0.2146 ? 0.0036

Table 3: Comparison of plugin-estimator methods on multilabel F1 and Jaccard metrics. Reported
values correspond to the macro-averaged metric computed on test data (with standard deviation,
over 10 random validation sets for tuning thresholds). Macro-Thres is consistent for macro-averaged
metrics, and is competitive in three out of four datasets. Though not consistent for macro-averaged
metrics, Algorithm 1 achieves the best performance in three out of four datasets.

6

Conclusions and Future Work

We have proposed a framework for the construction and analysis of multilabel classification metrics
and corresponding population optimal classifiers. Our main result is that for a large family of averaged performance metrics, the EUM optimal multilabel classifier can be explicitly characterized by
thresholding of label-wise marginal instance-conditional probabilities, with weak label dependence
via a shared threshold. We have also proposed efficient and consistent estimators for maximizing
such multilabel performance metrics in practice. Our results are a step forward in the direction of
extending the state-of-the-art understanding of learning with respect to general metrics in binary and
multiclass settings. Our work opens up many interesting research directions, including the potential
for further generalization of our results beyond averaged metrics, and generalized results for DTA
population optimal classification, which is currently only well-understood for the F -measure.
Acknowledgments: We acknowledge the support of NSF via CCF-1117055, CCF-1320746 and IIS1320894, and NIH via R01 GM117594-01 as part of the Joint DMS/NIGMS Initiative to Support
Research at the Interface of the Biological and Mathematical Sciences.

8

References
[1] Weiwei Cheng, Eyke H?ullermeier, and Krzysztof J Dembczynski. Bayes optimal multilabel
classification via probabilistic classifier chains. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 279?286, 2010.
[2] Krzysztof Dembczynski, Wojciech Kotlowski, and Eyke H?ullermeier. Consistent multilabel
ranking through univariate losses. In Proceedings of the 29th International Conference on
Machine Learning (ICML-12), pages 1319?1326, 2012.
[3] Krzysztof Dembczy?nski, Willem Waegeman, Weiwei Cheng, and Eyke H?ullermeier. On label
dependence and loss minimization in multi-label classification. Machine Learning, 88(1-2):
5?45, 2012.
[4] Krzysztof Dembczynski, Arkadiusz Jachnik, Wojciech Kotlowski, Willem Waegeman, and
Eyke H?ullermeier. Optimizing the F-measure in multi-label classification: Plug-in rule approach versus structured loss minimization. In Proceedings of the 30th International Conference on Machine Learning, pages 1130?1138, 2013.
[5] Krzysztof J Dembczynski, Willem Waegeman, Weiwei Cheng, and Eyke H?ullermeier. An
exact algorithm for F-measure maximization. In Advances in Neural Information Processing
Systems, pages 1404?1412, 2011.
[6] Luc Devroye. A probabilistic theory of pattern recognition, volume 31. Springer, 1996.
[7] Wei Gao and Zhi-Hua Zhou. On the consistency of multi-label learning. Artificial Intelligence,
199:22?44, 2013.
[8] Ashish Kapoor, Raajay Viswanathan, and Prateek Jain. Multilabel classification using bayesian
compressed sensing. In Advances in Neural Information Processing Systems, pages 2645?
2653, 2012.
[9] Oluwasanmi O Koyejo, Nagarajan Natarajan, Pradeep K Ravikumar, and Inderjit S Dhillon.
Consistent binary classification with generalized performance metrics. In Advances in Neural
Information Processing Systems, pages 2744?2752, 2014.
[10] Harikrishna Narasimhan, Rohit Vaish, and Shivani Agarwal. On the statistical consistency
of plug-in classifiers for non-decomposable performance measures. In Advances in Neural
Information Processing Systems, pages 1493?1501, 2014.
[11] Harikrishna Narasimhan, Harish Ramaswamy, Aadirupa Saha, and Shivani Agarwal. Consistent multiclass algorithms for complex performance measures. In Proceedings of the 32nd
International Conference on Machine Learning (ICML-15), pages 2398?2407, 2015.
[12] James Petterson and Tib?erio S Caetano. Submodular multi-label learning. In Advances in
Neural Information Processing Systems, pages 1512?1520, 2011.
[13] Jesse Read, Bernhard Pfahringer, Geoff Holmes, and Eibe Frank. Classifier chains for multilabel classification. Machine learning, 85(3):333?359, 2011.
[14] Mark D Reid and Robert C Williamson. Composite binary losses. The Journal of Machine
Learning Research, 9999:2387?2422, 2010.
[15] Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vlahavas. Mining multi-label data. In
Data mining and knowledge discovery handbook, pages 667?685. Springer, 2010.
[16] Willem Waegeman, Krzysztof Dembczynski, Arkadiusz Jachnik, Weiwei Cheng, and Eyke
H?ullermeier. On the bayes-optimality of f-measure maximizers. Journal of Machine Learning
Research, 15:3333?3388, 2014.
[17] Nan Ye, Kian Ming A Chai, Wee Sun Lee, and Hai Leong Chieu. Optimizing F-measures: a
tale of two approaches. In Proceedings of the International Conference on Machine Learning,
2012.
[18] Hsiang-Fu Yu, Prateek Jain, Purushottam Kar, and Inderjit Dhillon. Large-scale multi-label
learning with missing labels. In Proceedings of the 31st International Conference on Machine
Learning, pages 593?601, 2014.

9

"
2005,Silicon growth cones map silicon retina,,2773-silicon-growth-cones-map-silicon-retina.pdf,Abstract Missing,"Silicon Growth Cones Map Silicon Retina

Brian Taba and Kwabena Boahen?
Department of Bioengineering
University of Pennsylvania
Philadelphia, PA 19104
{btaba,boahen}@seas.upenn.edu

Abstract
We demonstrate the first fully hardware implementation of retinotopic
self-organization, from photon transduction to neural map formation.
A silicon retina transduces patterned illumination into correlated spike
trains that drive a population of silicon growth cones to automatically
wire a topographic mapping by migrating toward sources of a diffusible
guidance cue that is released by postsynaptic spikes. We varied the pattern of illumination to steer growth cones projected by different retinal
ganglion cell types to self-organize segregated or coordinated retinotopic
maps.

1

Introduction

Engineers have long admired the brain?s ability to effortlessly adapt to novel situations
without instruction, and sought to endow digital computers with a similar capacity for unsupervised self-organization. One prominent example is Kohonen?s self-organizing map
[1], which achieved popularity by distilling neurophysiological insights into a simple set
of mathematical equations. Although these algorithms are readily simulated in software,
previous hardware implementations have required high precision components that are expensive in chip area (e.g. [2, 3]). By contrast, neurobiological systems can self-organize
components possessing remarkably heterogeneous properties. To pursue this biological robustness against component mismatch, we designed circuits that mimic neurophysiological
function down to the subcellular level. In this paper, we demonstrate topographic refinement of connections between a silicon retina and the first neuromorphic self-organizing
map chip, previously reported in [5], which is based on axon migration in the developing
brain.
During development, neurons wire themselves into their mature circuits by extending axonal and dendritic precursors called neurites. Each neurite is tipped by a motile sensory
structure called a growth cone that guides the elongating neurite based on local chemical cues. Growth cones move by continually sprouting and retracting finger-like extensions called filopodia whose dynamics can be biased by diffusible ligands in an activitydependent manner [4]. Based on these observations, we designed and fabricated the Neurotrope1 chip to implement a population of silicon growth cones [5]. We interfaced Neu?

www.neuroengineering.upenn.edu/boahen

target

x
n(x)

source

c(a)
a

a

b

c

d

Figure 1: Neurotropic axon guidance. a. Active source cells (grey) relay spikes down
their axons to their growth cones, which excite nearby target cells. b. Active target cell
bodies secrete neurotropin. c. Neurotropin spreads laterally, establishing a spatial concentration gradient that is sampled by active growth cones. d. Active growth cones climb the
local neurotropin gradient, translating temporal activity coincidence into spatial position
coincidence. Growth cones move by displacing other growth cones.

rotrope1 directly to a spiking silicon retina to illustrate its applicability to larger neuromorphic systems.
This paper is organized as follows. In Section 2, we present an algorithm for axon migration under the guidance of a diffusible chemical whose release and uptake is gated by
activity. In Section 3, we describe our hardware implementation of this algorithm. In Section 4, we examine the Neurotrope1 system?s performance on a topographic refinement
task when driven by spike trains generated by a silicon retina in response to several types
of illumination stimuli.

2

Neurotropic axon guidance

We model the self-organization of connections between two layers of neurons (Fig. 1).
Cells in the source layer innervate cells in the target layer with excitatory axons that are
tipped by motile growth cones. Growth cones tow their axons within the target layer as
directed by a diffusible guidance factor called neurotropin that they bind from the local
extracellular environment. Neurotropin is released by postsynaptically active target cell
bodies and bound by presynaptically active growth cones, so the retrograde transfer of
neurotropin from a target cell to a source cell measures the temporal coincidence of their
spike activities. Growth cones move to maximize their neurotropic uptake, a Hebbian-like
learning rule that causes cells that fire at the same time to wire to the same place. To
prevent the population of growth cones from attempting to trivially maximize their uptake
by all exciting the same target cell, we impose a synaptic density constraint that requires a
migrating growth cone to displace any other growth cone occupying its path.
To state the model more formally, source cell bodies occupy nodes of a regular twodimensional (2D) lattice embedded in the source layer, while growth cones and target cell
bodies occupy nodes on separate 2D lattices that are interleaved in the target layer. We index nodes by their positions in their respective layers, using Greek letters for source layer
positions (e.g., ? ? Z2 ) and Roman letters for target layer positions (e.g., x, c ? Z2 ).
Each source cell ? fires spikes at a rate aSC (?) and conveys this presynaptic activity down
an axon that elaborates an excitatory arbor in the target layer centered on c(?). In principle,
every branch of this arbor is tipped by its own motile growth cone, but to facilitate efficient

Silicon retina

RAM
0 3

Neurotrope1

1 0
2 4

GC

3 1
4 2
1 0
3 1

GC

N

4 2
0 3
2 4

N

GC

AER
GC

GC
N

N

AER

GC

USB
Computer

?C

a

b

c

Figure 2: a. Neurotrope1 system. Spike communication is by address-events (AER). b.
Neurotrope1 cell mosaic. The extracellular medium (grey) is laid out as a monolithic honeycomb lattice. Growth cones (GC) occupy nodes of this lattice and extend filopodia to the
adjacent nodes. Neurotropin receptors (black) are located at the tip of each filopodium and
at the growth cone body. Target cells (N) occupy nodes of an interleaved triangular lattice.
c. Detail of chip layout.
hardware implementation, we abstract the collection of branch growth cones into a single
central growth cone that tows the arbor?s trunk around the target layer, dragging the rest of
the arbor with it. The arbor overlaps nearby target cells with a branch density A(x ? c(?))
that diminishes with distance kx ? c(?)k from the arbor center. The postsynaptic activity
aTC(x) of target cell x is proportional to the linear sum of its excitation.
X
aSC (?)A(x ? c(?))
(1)
aTC(x) =
?

Postsynaptically active target cell bodies release neurotropin, which spreads laterally until
consumed by constitutive decay processes. The neurotropin n(x0 ) present at target site
x0 is assembled from contributions from all active release sites. The contribution of each
target cell x is proportional to its postsynaptic activity and weighted by a spreading kernel
N (x ? x0) that is a decreasing function of its distance kx ? x0 k from the measurement site
x0.
X
n(x0 ) =
aTC (x)N (x ? x0)
(2)
x

A presynaptically active growth cone located at c(?) computes the direction of the local
neurotropin gradient by identifying the adjacent lattice node c0 (?) ? C(c(?)) with the most
neurotropin, where C(c(?)) includes c(?) and its nearest neighbors.
c0(?) = arg maxx0 ?C(c(?)) n(x0)

(3)

Once the growth cone has identified c0 (?), it swaps positions with the growth cone already located at c0 (?), increasing its own neurotropic uptake while preserving a constant synaptic density. Growth cones compute position updates independently, at a rate
?(?) ? aSC (?) maxy?C(c(?)) n(x0 ). Updates are executed asynchronously, in order of
their arrival.
Software simulation of a similar set of equations generates self-organized feature maps
when driven by appropriately correlated source cell activity [6]. Here, we illustrate topographic map formation in hardware using correlated spike trains generated by a silicon
retina.

Route

3 0 4 1 2
0 1 2 3 4

Update

0 1 2 3 4
1 3 4 0 2

3 0 4 1 2
0 1 2 3 4

0 1 2 3 4
1 3 4 0 2

Route

2 0 4 1 3
0 1 2 3 4

0 1 2 3 4
1 3 0 4 2

a
b
c
Figure 3: Virtual axon remapping. a. Cell bodies tag their spikes with their own source
layer addresses, which the forward lookup table translates into target layer destinations.
b. Axon updates are computed by growth cones, which decode their own target layer
addresses through the reverse lookup table to obtain the source layer addresses of their cell
bodies that identify their entries in the forward lookup table. c. Growth cones move by
modifying their entries in the forward and reverse lookup tables to reroute their spikes to
updated locations.

3

Neurotrope1 system

Our hardware implementation splits the model into three stages: the source layer, the target
layer, and the intervening axons (Fig. 2a). Any population of spiking neurons can act
as a source layer; in this paper we employ the silicon retina of [7]. The target layer is
implemented by a full custom VLSI chip that interleaves a 48 ? 20 array of growth cone
circuits with a 24 ? 20 array of target cell circuits. There is also a spreading network
that represents the intervening medium for propagating neurotropin. The Neurotrope1 chip
was fabricated by MOSIS using the TSMC 0.35?m process and has an area of 11.5 mm2 .
Connections are specified as entries in a pair of lookup tables, stored in an off-chip RAM,
that are updated by a Ubicom ip2022 microcontroller as instructed by the Neurotrope1 chip.
The ip2022 also controls a USB link that allows a computer to write and read the contents
of the RAM. Subsection 3.1 explains how updates are computed by the Neurotrope1 chip
and Subsection 3.2 describes the procedure for executing these updates.
3.1 Axon updates
Axon updates are computed by the Neurotrope1 chip using the transistor circuits described
in [5]. Here, we provide a brief description. The Neurotrope1 chip represents neurotropin
as charge spreading through a monolithic transistor channel laid out as a honeycomb lattice. Each growth cone occupies one node of this lattice and extends filopodia to the three
adjacent nodes, expressing neurotropin receptors at all four locations (Fig. 2b-c). When
a growth cone receives a presynaptic spike, its receptor circuits tap charge from all four
nodes onto separate capacitors. The first capacitor voltage to integrate to a threshold resets
all of the growth cone?s capacitors and transmits a request off-chip to update the growth
cone?s position by swapping locations with the growth cone currently occupying the winning node.
3.2 Address-event remapping
Chips in the Neurotrope1 system exchange spikes encoded in the address-event representation (AER) [8], an asynchronous communication protocol that merges spike trains from
every cell on the same chip onto a single shared data link instead of requiring a dedicated
wire for each connection. Each spike is tagged with the address of its originating cell for
transmission off-chip. Between chips, spikes are routed through a forward lookup table
that translates their original source layer addresses into their destined target layer addresses

Retina color map

n=0

n=85

<FH n L >
25
20
15
10
5
n
25 50 75

a

b

c

d

Figure 4: Retinotopic self-organization of ON-center RGCs. a. Silicon retina color map
of ON-center RGC body positions. A representative RGC body is outlined in white, as
are the RGC neighbors that participate in its topographic order parameter ?(n). b. Target
layer color map of growth cone positions for sample n = 0, colored by the retinal positions
of their cell bodies. Growth cones projected by the representative RGC and its nearest
neighbors are outlined in white. Grey lines denote target layer distances used to compute
?(n) . c. Target layer color map at n = 85. d. Order parameter evolution.

on the receiving chip (Fig. 3a). An axon entry in this forward lookup table is indexed by
the source layer address of its cell body and contains the target layer address of its growth
cone. The virtual axon moves by updating this entry.
Axon updates are computed by growth cone circuits on the Neurotrope1 chip, encoded as
address-events, and sent to the ip2022 for processing. Each update identifies a pair of axon
terminals to be swapped. These growth cone addresses are translated through a reverse
lookup table into the source layer addresses that index the relevant forward lookup table
entries (Fig. 3b). Modification of the affected entries in each lookup table completes the
axon migration (Fig. 3c).

4

Retinotopic self-organization

We programmed the growth cone population to self-organize retinotopic maps by driving
them with correlated spike trains generated by the silicon retina. The silicon retina translates patterned illumination in real-time into spike trains that are fed into the Neurotrope1
chip as presynaptic input from different retinal ganglion cell (RGC) types. An ON-center
RGC is excited by a spot of light in the center of its receptive field and inhibited by light in
the surrounding annulus, while an OFF-center RGC responds analogously to the absence
of light. There is an ON-center and an OFF-center RGC located at every retinal coordinate.
To generate appropriately correlated RGC spike trains, we illuminated the silicon retina
with various mixtures of light and dark spot stimuli. Each spot stimulus was presented
against a uniformly grey background for 100 ms and covered a contiguous cluster of RGCs
centered on a pseudorandomly selected position in the retinal plane, eliciting overlapping
bursts of spikes whose coactivity established a spatially restricted presynaptic correlation
kernel containing enough information to instruct topographic ordering [9]. Strongly driven
RGCs could fire at nearly 1 kHz, which was the highest mean rate at which the silicon retina
could still be tuned to roughly balance ON- and OFF-center RGC excitability. We tracked
the evolution of the growth cone population by reading out the contents of the lookup table
every five minutes, a sampling interval selected to include enough patch stimuli to allow
each of the 48 ? 20 possible patches to be activated on average at least once per sample.
We first induced retinotopic self-organization within a single RGC cell type by illuminating the silicon retina with a sequence of randomly centered spots of light presented against
a grey background, selectively activating only ON-center RGCs. Each of the 960 growth

Rate H kHz L

RGC stimulus

n=0

n=310

ON-center

1

<FH n L >
25
20

x

15

OFF-center

1
10
5
n
100

x

a

b

c

d

200

300

e

Figure 5: Segregation by cell type under separate light and dark spot stimulation. Top:
ON-center; bottom: OFF-center. a. Silicon retina image of representative spot stimulus.
Light or dark intensity denotes relative ON- or OFF-center RGC output rate. b. Spike rates
for ON-center (grey) and OFF-center (black) RGCs in column x of a cross-section of a
representative spot stimulus. c. Target layer color maps of RGC growth cones at sample
n = 0. Black indicates the absence of a growth cone projected by an RGC of this cell type.
Other colors as in Fig. 4. d. Target layer color maps at n = 310. e. Order parameter
evolution for ON-center (grey) and OFF-center (black) RGCs.

cones was randomly assigned to a different ON-center RGC, creating a scrambled map
from retina to target layer (Fig. 4a-b). The ON-center RGC growth cone population visibly
refined the topography of the nonretinotopic initial state (Fig. 4c). We quantify this observation by introducing an order parameter ?(n) whose value measures the instantaneous
retinotopy for an RGC at the nth sample. The definition of retinotopy is that adjacent RGCs
innervate adjacent target cells, so we define ?(n) for a given RGC to be the average target
layer distance separating its growth cone from the growth cones projected by the six adjacent RGCs of the same cell type. The population average h?(n) i converges to a value that
represents the achievable performance on this task (Fig. 4d).
We next induced growth cones projected by each cell type to self-organize disjoint topographic maps by illuminating the silicon retina with a sequence of randomly centered light
or dark spots presented against a grey background (Fig. 5a-b). Half the growth cones
were assigned to ON-center RGCs and the other half were assigned to the corresponding
OFF-center RGCs. We seeded the system with a random projection that evenly distributed
growth cones of both cell types across the entire target layer (Fig. 5c). Since only RGCs of
the same cell type were coactive, growth cones segregated into ON- and OFF-center clusters on opposite sides of the target layer (Fig. 5d). OFF-center RGCs were slightly more
excitable on average than ON-center RGCs, so their growth cones refined their topography
more quickly (Fig. 5e) and clustered in the right half of the target layer, which was also
more excitable due to poor power distribution on the Neurotrope1 chip.
Finally, we induced growth cones of both cell types to self-organize coordinated retinotopic maps by illuminating the retina with center-surround stimuli that oscillate radially
from light to dark or vice versa (Fig. 6). The light-dark oscillation injected enough coactivity between neighboring ON- and OFF-center RGCs to prevent their growth cones from
segregating by cell type into disjoint clusters. Instead, both subpopulations developed and
maintained coarse retinotopic maps that cover the entire target layer and are oriented in
register with one another, properties sufficient to seed more interesting circuits such as
oriented receptive fields [10].
Performance in this hardware implementation is limited mainly by variability in the behav-

Rate H kHz L

RGC stimulus

n=0

n=335

ON-center

1

<FH n L >
25
20

x

15

OFF-center

1
10
5
n
100

x

a

b

c

d

200

300

e

Figure 6: Coordinated retinotopy under center-surround stimulation. Top: ON-center; bottom: OFF-center. a. Silicon retina image of a representative center-surround stimulus.
Light or dark intensity denotes relative ON- or OFF-center RGC output rate. b. Spike rates
for ON-center (grey) and OFF-center (black) RGCs in column x of a cross-section of a
representative center-surround stimulus. c. Target layer color maps of RGC growth cones
for sample n = 0. Colors as in Fig. 5. d. Target layer color maps at n = 335. e. Order
parameter evolution for ON-center (grey) and OFF-center (black) RGCs.
ior of nominally identical circuits on the Neurotrope1 chip and the silicon retina. In the
silicon retina, the wide variance of the RGC output rates [7] limits both the convergence
speed and the final topographic level achieved by the spot-driven growth cone population.
Growth cones move faster when stimulated at higher rates, but elevating the mean output
rate of the RGC population allows more excitable RGCs to fire spontaneously at a sustained rate, swamping growth cone-specific guidance signals with stimulus-independent
postsynaptic activity that globally attracts all growth cones. The mean RGC output rate
must remain low enough to suppress these spontaneous distractors, limiting convergence
speed. Variance in the output rates of neighboring RGCs also distorts the shape of the spot
stimulus, eroding the fidelity of the correlation-encoded instructions received by the growth
cones.
Variability in the Neurotrope1 chip further limits topographic convergence. Migrating
growth cones are directed by the local neurotropin landscape, which forms an image of
recent presynaptic activity correlations as filtered through the postsynaptic activation of
the target cell population. This image is distorted by variations between the properties of
individual target cell and neurotropin circuits that are introduced during fabrication. In
particular, poor power distribution on the Neurotrope1 chip creates a systematic gradient
in target cell excitability that warps a growth cone?s impression of the relative coactivity of
its neighbors, attracting it preferentially toward the more excitable target cells on the right
side of the array.

5

Conclusions

In this paper, we demonstrated a completely neuromorphic implementation of retinotopic
self-organization. This is the first time every stage of the process has been implemented
entirely in hardware, from photon transduction through neural map formation. The only
comparable system was described in [11], which processed silicon retina data offline using
a software model of neurotrophic guidance running on a workstation. Our system computes
results in real time at low power, two prerequisites for autonomous mobile applications.
The novel infrastructure developed to implement virtual axon migration allows silicon
growth cones to directly interface with an existing family of AER-compliant devices,

enabling a host of multimodal neuromorphic self-organizing applications. In particular,
the silicon retina?s ability to translate arbitrary visual stimuli into growth cone-compatible
spike trains in real-time opens the door to more ambitious experiments such as using natural
video correlations to automatically wire more complicated visual feature maps.
Our faithful adherence to cellular level details yields an algorithm that is well suited to
physical implementation. In contrast to all previous self-organizing map chips (e.g. [2, 3]),
which implemented a global winner-take-all function to induce competition, our silicon
growth cones compute their own updates using purely local information about the neurotropin gradient, a cellular approach that scales effortlessly to larger populations. Performance might be improved by supplementing our purely morphogenetic model with additional physiologically-inspired mechanisms to prune outliers and consolidate well-placed
growth cones into permanent synapses.
Acknowledgments
We would like to thank J. Arthur for developing a USB system to facilitate data collection.
This project was funded by the David and Lucille Packard Foundation and the NSF/BITS
program (EIA0130822).

References
[1] T. Kohonen (1982), ?Self-organized formation of topologically correct feature maps,? Biol.
Cybernetics, vol. 43, no. 1, pp. 59-69.
[2] W.-C. Fang, B.J. Sheu, O.T.-C. Chen, and J. Choi (1992), ?A VLSI neural processor for image data compression using self-organization networks,? IEEE Trans. Neural Networks, vol. 3,
no. 3, pp. 506-518.
[3] S. Rovetta and R. Zunino (1999), ?Efficient training of neural gas vector quantizers with analog
circuit implementation,? IEEE Trans. Circ. & Sys. II, vol. 46, no. 6, pp. 688-698.
[4] E.W. Dent and F.B. Gertler (2003), ?Cytoskeletal dynamics and transport in growth cone mobility and axon guidance,? Neuron, vol. 40, pp. 209-227.
[5] B. Taba and K. Boahen (2003), ?Topographic map formation by silicon growth cones,? in:
Advances in Neural Information Processing Systems 15 (MIT Press, Cambridge, eds. S. Becker,
S. Thrun, and K. Obermayer), pp. 1163-1170.
[6] S.Y.M. Lam, B.E. Shi, and K.A. Boahen (2005), ?Self-organized cortical map formation by
guiding connections,? Proc. 2005 IEEE Int. Symp. Circ. & Sys., in press.
[7] K.A. Zaghloul and K. Boahen (2004), ?Optic nerve signals in a neuromorphic chip I: Outer and
inner retina models,? IEEE Trans. Bio-Med. Eng., vol. 51, no. 4, pp. 657-666.
[8] K. Boahen (2000), ?Point-to-point connectivity between neuromorphic chips using addressevents,? IEEE Trans. Circ. & Sys. II, vol. 47, pp. 416-434.
[9] K. Miller (1994), ?A model for the development of simple cell receptive fields and the ordered
arrangement of orientation columns through activity-dependent competition between on- and
off-center inputs,? J. Neurosci., vol. 14, no. 1, pp. 409-441.
[10] D. Ringach (2004), ?Haphazard wiring of simple receptive fields and orientation columns in
visual cortex,? J. Neurophys., vol. 92, no. 1, pp. 468-476.
[11] T. Elliott and J. Kramer (2002), ?Coupling an aVLSI neuromorphic vision chip to a neurotrophic model of synaptic plasticity: the development of topography,? Neural Comp., vol. 14,
no. 10, pp. 2353-2370.

"
2012,Bayesian nonparametric models for bipartite graphs,,4837-bayesian-nonparametric-models-for-bipartite-graphs.pdf,"We develop a novel Bayesian nonparametric model for random bipartite graphs. The model is based on the theory of completely random measures and is able to handle a potentially infinite number of nodes. We show that the model has appealing properties and in particular it may exhibit a power-law behavior. We derive a posterior characterization, an Indian Buffet-like generative process for network growth, and a simple and efficient Gibbs sampler for posterior simulation. Our model is shown to be well fitted to several real-world social networks.","Bayesian nonparametric models for bipartite graphs

Franc?ois Caron
INRIA
IMB - University of Bordeaux
Talence, France
Francois.Caron@inria.fr

Abstract
We develop a novel Bayesian nonparametric model for random bipartite graphs.
The model is based on the theory of completely random measures and is able
to handle a potentially infinite number of nodes. We show that the model has
appealing properties and in particular it may exhibit a power-law behavior. We
derive a posterior characterization, a generative process for network growth, and
a simple Gibbs sampler for posterior simulation. Our model is shown to be well
fitted to several real-world social networks.

1

Introduction

The last few years have seen a tremendous interest in the study, understanding and statistical modeling of complex networks [14, 6]. A network is a set if items, called vertices, with connections
between them, called edges. In this article, we shall focus on bipartite networks, also known as twomode, affiliation or collaboration networks [16, 17]. In bipartite networks, items are divided into two
different types A and B, and only connections between items of different types are allowed. Examples of this kind can be found in movie actors co-starring the same movie, scientists co-authoring a
scientific paper, internet users posting a message on the same forum, people reading the same book
or listening to the same song, members of the boards of company directors sitting on the same board,
etc. Following the readers-books example, we will refer to items of type A as readers and items of
type B as books. An example of bipartite graph is shown on Figure 1(b). An important summarizing quantity of a bipartite graph is the degree distribution of readers (resp. books) [14]. The degree
of a vertex in a network is the number of edges connected to that vertex. Degree distributions of
real-world networks are often strongly non-Poissonian and exhibit a power-law behavior [15].
A bipartite graph can be represented by a set of binary variables (zij ) where zij = 1 if reader i has
read book j, 0 otherwise. In many situations, the number of available books may be very large and
potentially unknown. In this case, a Bayesian nonparametric (BNP) approach can be sensible, by
assuming that the pool of books is infinite. To formalize this framework, it will then be convenient
to represent the bipartite graph by a collection of atomic measures Zi , i = 1, . . . , n with
Zi =

?
X

zij ??j

(1)

j=1

where {?j } is the set of books and typically Zi only has a finite set of non-zero zij corresponding to
books reader i has read. Griffiths and Ghahramani [8, 9] have proposed a BNP model for such binary
random measures. The so-called Indian Buffet Process (IBP) is a simple generative process for the
conditional distribution of Zi given Z1 , . . . , Zi?1 . Such process can be constructed by considering
that the binary measures Zi are i.i.d. from some random measure drawn from a beta process [19, 10].
It has found several applications for inferring hidden causes [20], choices [7] or features [5]. Teh
and Gor?ur [18] proposed a three-parameter extension of the IBP, named stable IBP, that enables to
1

model a power-law behavior for the degree distribution of books. Although more flexible, the stable
IBP still induces a Poissonian distribution for the degree of readers.
In this paper, we propose a novel Bayesian nonparametric model for bipartite graphs that addresses
some of the limitations of the stable IBP, while retaining computational tractability. We assume
that each book j is assigned a positive popularity parameter wj > 0. This parameter measures the
popularity of the book, larger weights indicating larger probability to be read. Similarly, each reader
i is assigned a positive parameter ?i which represents its ability to read books. The higher ?i , the
more books the reader i is willing to read. Given the weights wj and ?i , reader i reads book j with
probability 1 ? exp(??i wj ). We will consider that the weights wj and/or ?i are the points of a
Poisson process with a given L?evy measure. We show that depending on the choice of the L?evy
measure, a power-law behavior can be obtained for the degree distribution of books and/or readers.
Moreover, using a set of suitably chosen latent variables, we can derive a generative process for
network growth, and an efficient Gibbs sampler for approximate inference. We provide illustrations
of the fit of the proposed model on several real-world bipartite social networks. Finally, we discuss
some potentially useful extensions of our work, in particular to latent factor models.

2
2.1

Statistical Model
Completely Random Measures

We first provide a brief overview of completely random measures (CRM) [12, 13] before describing
the BNP model for bipartite graphs in Section 2.2. Let ? be a measurable space. A CRM is a
random measure G such that for any collection of disjoint measurable subsets A1 , . . . , An of ?,
the random masses of the subsets G(A1 ), . . . , G(An ) are independent. CRM can be decomposed
into a sum of three independent parts: a non-random measure, a countable collection of atoms with
fixed locations, and a countable collection of atoms with randoms masses at random locations. In this
paper, weP
will be concerned with models defined by CRMs with random masses at random locations,
?
i.e. G = j=1 wj ??j . The law of G can be characterized in terms of a Poisson process over the point
set {(wj , ?j ), j = 1, . . . , ?} ? R+ ? ?. The mean measure ? of this Poisson process is known as
the L?evy measure. We will assume in the following that the L?evy measure decomposes as a product
of two non-atomic densities,
i.e. that G is a homogeneous CRM ?(dw, d?) = ?(w)h(?)dwd? with
R
h : ? ? [0, +?) and ? h(?)d? = 1. It implies that the locations of the atoms in G are independent
of the masses, and are i.i.d. from h, while the masses are distributed according to a Poisson
P?process
over R+ with mean intensity ?. We will further assume that the total mass G(?) = j=1 wj is
positive and finite with probability one, which is guaranteed if the following conditions are satisfied
Z ?
Z ?
?(w)dw = ? and
(1 ? exp(?w))?(w)dw < ?
(2)
0

0

and note g(x) its probability density function evaluated at x. We will refer to ? as the L?evy intensity
in the following, and to h as the base density of G, and write G ? CRM(?, h). We will also note
Z ?
?? (t) = ? log E [exp(?tG(?))] =
(1 ? exp(?tw))?(w)dw
(3)
0
Z ?
?e? (t, b) =
(1 ? exp(?tw))?(w) exp(?bw)dw
(4)
0
Z ?
?(n, z) =
?(w)wn e?zw dw
(5)
0

As a notable particular example of CRM, we can mention the generalized gamma process (GGP) [1],
whose L?evy intensity is given by
?
?(w) =
w???1 e?w?
?(1 ? ?)
GGP encompasses the gamma process (? = 0), the inverse Gaussian process (? = 0.5) and the
stable process (? = 0) as special cases. Table ?? in supplementary material provides the expressions
of ?, ? and ? for these processes.
2

2.2

A Bayesian nonparametric model for bipartite graphs

Let G ? CRM(?, h) where ? satisfies conditions (2). A draw G takes the form
?
X
G=
wj ??j

(6)

j=1

where {?j } is the set of books and {wj } the set of popularity parameters of books. For i = 1, . . . , n,
let consider the latent exponential process
?
X
Vi =
vij ??j
(7)
j=1

defined for j = 1, . . . , ? by vij |wj ? Exp(wj ?i ) where Exp(a) denotes the exponential distribution of rate a. The higher wj and/or ?i , the lower vij . We then define the binary process Zi
conditionally on Vi by

?
X
zij = 1 if vij < 1
(8)
Zi =
zij ??j with
zij = 0 otherwise
j=1

By integrating out the latent variables vij we clearly have p(zij = 1|wj , ?i ) = 1 ? exp(??i wj ).
Proposition 1 Zi is marginally characterized by a Poisson process over the point set {(?j? ), j =
P?
1, . . . , ?} ? ?, of intensity measure ?? (?i )h(?? ). Hence, the total mass Zi (?) =
j=1 zij ,
which corresponds to the total number of books read by reader i is finite with probability one and
admits a Poisson(?? (?i )) distribution, where ?? (z) is defined in Equation (3), while the locations
?j? are i.i.d. from h.
The proof, which makes use of Campbell?s theorem for point processes [13] is given in supplemen
tary material. As an example, for the gamma process we have Zi (?) ? Poisson ? log 1 + ??i .
It will be useful in the following to introduce a censored version of the latent process Vi , defined by
?
X
Ui =
uij ??j
(9)
j=1

where uij = min(vij , 1), for i = 1, . . . , n and j = 1, . . . , ?. Note that Zi can be obtained
deterministically from Ui .
2.3

Characterization of the conditional distributions

The conditional distribution of G given Z1 , . . . , Zn cannot be obtained in closed form1 . We will
make use of the latent process Ui . In this section, we derive the formula for the conditional laws
P (U1 , . . . , Un |G), P (U1 , . . . , Un ) and P (G|U1 , . . . , Un ) . Based on these results, we derive in Section 2.4 a generative process and in Section 2.5 a Gibbs sampler for our model, that both rely on the
introduction of these latent variables.
P?
Assume that K books {?1 , . . . , ?K } have appeared. We write Ki = Zi (?) = j=1 zij the degree
Pn
Pn
of reader i (number of books read by reader i) and mj = i=1 Zi ({?j }) = i=1 zij the degree of
book j (number of people having read book j). The conditional likelihood of U1 , . . . Un given G is
given by
??
?
?
n ? Y
K
?
Y
z
z
?
P (U1 , . . . Un |G) =
?i ij wj ij exp (??i wj uij )? exp (??i G(?\{?1 , . . . , ?K }))
?
?
i=1
j=1
?
?
! K
!
!
!
n
n
n
Y
Y m
X
X
Ki
j
?
=
?i
wj exp ?wj
?i (uij ? 1) ? exp ?
?i G(?)
(10)
i=1

j=1

i=1

i=1

1
In the case where ?i = ?, it is possible to derive P (Z1 , . . . , Zn ) and P (Zn+1 |Z1 , . . . , Zn ) where the
random measure G and the latent variables U are marginalized out. This particular case is described in supplementary material.

3

Proposition 2 The marginal distribution P (U1 , . . . Un ) is given by
!
""
!# K
!
n
n
n
Y
X
Y
X
P (U1 , . . . Un ) =
?iKi exp ???
?i
h(?j )? mj ,
?i uij
i=1

i=1

j=1

(11)

i=1

where ?? and ? are resp. defined by Eq. (3) and (5).
Proof. The proof, detailed in supplementary material, is obtained by an application of the Palm
formula for CRMs [3, 11], and is the same as that of Theorem 1 in [2].
Proposition 3 The conditional distribution of G given the latent processes U1 , . . . Un can be expressed as
K
X
G = G? +
wj ??j
(12)
j=1

where G? and (wj ) are mutually independent with
?

?

G ? CRM(? , h)

?

? (w) = ?(w) exp ?w

n
X

!
?i

(13)

i=1

and the masses are
P (wj |rest) =

Pn
m
?(wj )wj j exp (?wj i=1 ?i Uij )
Pn
?(mj , i=1 ?i uij )

(14)

Proof. The proof, based on the application of the Palm formula and detailed in supplementary
material, is the same as that of Theorem 2 in [2].
Pn
In the case of the GGP, G? is still a GGP of parameters (?? = ?, ? ? = ?, ? ? = ? + i=1 ?i ), while
the wj ?s are conditionally gamma distributed, i.e.
!
n
X
wj |rest ? Gamma mj ? ?, ? +
?i uij
i=1

Corollary 4 The predictive distribution of Zn+1 given the latent processes U1 , . . . , Un is given by
?
Zn+1 = Zn+1
+

K
X

zn+1,j ??j

j=1
?
where the zn+1,j are independent of Zn+1
with
Pn


?(mj , ? + ?n+1 + i=1 ?i uij )
Pn
zn+1,j |U ? Ber 1 ?
?(mj , ? + i=1 ?i uij )
?
where Ber is the Bernoulli distribution and Zn+1 is a homogeneous Poisson process over ? of
intensity measure ??? (?n+1 ) h(?).

For the GGP, we have

and

?
i
 h
?
? Poisson ? ? + Pn+1 ?i ? (? + Pn ?i )?
i=1
i=1
?
?


Zn+1
(?) ?
?n+1
? Poisson ? log 1 + P
?+ n
i=1 ?i

?mj +? !
?n+1
Pn
.
zn+1,j |U ? Ber 1 ? 1 +
? + i=1 ?i uij

if ? 6= 0
if ? = 0

Finally, we consider the distribution of un+1,j |zn+1,j = 1, u1:n,j . This is given by
n
X
p(un+1,j |zn+1,j = 1, u1:n,j ) ? ?(mj + 1, un+1,j ?n+1 +
?i uij )1un+1,j ?[0,1]

(15)

i=1

In supplementary material, we show how to sample from this distribution by the inverse cdf method
for the GGP.
4

Books

A1

A2

A3

B3

B4

B5

...

Reader 1

18

4

14

Reader 2

12

0

8

13

4

Reader 3

16 10

0

0

14

...
9

6

...

B1

B2

(a)

B6

B7

(b)

Figure 1: Illustration of the generative process described in Section 2.4.
2.4

A generative process

In this section we describe the generative process for Zi given (U1 , . . . , Ui?1 ), G being integrated
out. This reinforcement process, where popular books will be more likely to be picked, is reminiscent of the generative process for the beta-Bernoulli process, popularized under the name of the
Indian buffet process [8]. Let xij = ? log(uij ) ? 0 be latent positive scores.
Consider a set of n readers who successively enter into a library with an infinite number of books.
Each reader i = 1, . . . n, has some interest in reading quantified by a positive parameter ?i > 0.
The first reader picks a number K1 ? Poisson(?? (?1 )) books. Then he assigns a positive score
x1j = ? log(u1j ) > 0 to each of these books, where u1j is drawn from distribution (15).
Now consider that reader i enters into the library, and knows about the books read by previous
readers and their scores. Let K be the total number of books chosen by the previous i ? 1 readers,
and mj the number of times each of the K books has been read. Then for each book j = 1, . . . , K,
reader i will choose this book with probability
Pi?1
?(mj , ? + ?i + k=1 ?k ukj )
1?
Pi?1
?(mj , ? + k=1 ?k ukj )
and then will choose an additional number of Ki+ books where
Ki+

? Poisson ?e?

?i ,

i?1
X

!!
?k

k=1

Reader i will then assign a score xij = ? log uij > 0 to each book j he has read, where uij is drawn
from (15). Otherwise he will set the default score xij = 0. This generative process is illustrated in
Figure 1 together with the underlying bipartite graph . In Figure 2 are represented draws from this
generative process with a GGP with parameters ?i = 2 for all i, ? = 1, and different values for ?
and ?.
2.5

Gibbs sampling

From the results derived in Proposition 3, a Gibbs sampler can be easily derived to approximate
the posterior distribution P (G, U |Z). The sampler successively updates U given (w, G? (?)) then
(w, G? (?)) given U . We present here the conditional distributions in the GGP case. For i =
1, . . . , n, j = 1, . . . , K, set uij = 1 if zij = 0, otherwise sample
uij |zij , wj , ?i ? rExp(?i wj , 1)
where rExp(?, a) is the right-truncated exponential distribution of pdf ? exp(??x)/(1 ?
exp(??a))1x?[0,a] from which we can sample exactly. For j = 1, . . . , K, sample
!
n
X
wj |U, ?i ? Gamma mj ? ?, ? +
?i uij
i=1

Pn
and the total mass G (?) follows a distribution g (w) ? g(w) exp (?w i=1 ?i ) where g(w) is
the distribution of G(?). In the case of the GGP, g ? (w) is an exponentially tilted stable distribution
for which exact samplers exist [4]. P
In the particular case of the gamma process, we have the simple
n
update G? (?) ? Gamma (?, ? + i=1 ?i ) .
?

?

5

5

10

10

10

15

15

15

20

20

20

25

25

25

30

30
20

40
Books

60

30

80

20

(a) ? = 1, ? = 0

40
Books

60

80

20

(b) ? = 5, ? = 0
5

5

10

10
Readers

5

15

15
20

20

25

25

25

30

30
40
Books

60

80

30

80

20

(d) ? = 2, ? = 0.1

60

15

20

20

40
Books

(c) ? = 10, ? = 0

10
Readers

Readers

Readers

5

Readers

Readers

5

40
Books

60

80

(e) ? = 2, ? = 0.5

20

40
Books

60

80

(f) ? = 2, ? = 0.9

Figure 2: Realisations from the generative process of Section 2.4 with a GGP of parameters ? = 2,
? = 1 and various values of ? and ?.

3

Update of ?i and other hyperparameters

We may also consider the weight parameters ?i to be unknown and estimate them from the graph.
We can assign a gamma prior ?i ? Gamma(a? , b? ) with parameters (a? > 0, b? > 0) and update
it conditionally on other variables with
?
?
K
K
X
X
?i |G, U ? Gamma ?a? +
zij , b? +
wj uij + G? (?)?
j=1

j=1

In this case, the marginal distribution of Zi (?), hence the degree distribution of books, follows a
continuous mixture of Poisson distributions, which offers more flexibility in the modelling.
We may also go a step further and consider that there is an infinite number of readers with weights ?i
e
associated
P? to a given CRM ? ? CRM(?? , h? ) and a measurable space of readers ?. We then have
? = i=1 ?i ??ei . This provides a lot of flexibility in the modelling of the distribution of the degree
of readers, allowing in particular to obtain a power-law behavior, as shown in Section 5. We focus
here on the case where ? is drawn from a generalized gamma process
Pn of parameters (?? , ?? , ?? ) for
simplicity. Conditionally on (w, G? (?), U ), we have ? = ?? + i=1 ?i ??ei where for i = 1, . . . , n,
?
?
K
K
X
X
?i |G, U ? Gamma ?
zij ? ?? , ? +
wj uij + G? (?)?
j=1

j=1



P


K
?
and ?? ? CRM(??? , h? ) with ??? (?) = ?? (?) exp ??
w
+
G
(?)
. In this case, the
j=1 j
e is now for j = 1, . . . , K
update for (w, G? ) conditional on (U, ?, ?(?))
!
n
X
? e
wj |U, ? ? Gamma mj ? ?, ? +
?i uij + ? (?)
i=1

P

n
? e
and G ? CRM(? , h) with ? (w) = ?(w) exp ?w
. Note that
i=1 ?i + ? (?)
there is now symmetry in the treatment of books/readers. For the scale parameter ? of
the GGP, we can assign agamma prior ? 
? Gamma(a? , b? ) and update it with ?|? ?
Pn
? e
Gamma a? + K, b? + ??
?
+
?
(
?)
. Other parameters of the GGP can be updated
i=1 i
using a Metropolis-Hastings step.
?

?



?

6

4

Discussion

Power-law behavior. We now discuss some of the properties of the model, in the case of the
GGP. The total number of books read by n readers is O(n? ). Moreover, for ? > 0, the degree
distribution follows a power-law distribution: asymptotically, the proportion of books read by m
readers is O(m?1?? ) (details in supplementary material). These results are similar to those of the
stable IBP [18]. However, in our case, a similar behavior can be obtained for the degree distribution
of readers when assigning a GGP to it, while it will always be Poisson for the stable IBP.
Connection to IBP. The stable beta process [18] is a particular case of our construction, obtained
by setting weights ?i = ? and L?evy measure
?(w) = ?

?(1 + c)
?(1 ? e??w )???1 e??w(c+?)
?(1 ? ?)?(c + ?)

(16)

The proof is obtained by a change of variable from the L?evy measure of the stable beta process.
Extensions to latent factor models. So far, we have assumed that the binary matrix Z was observed.
The proposed model can also be used as a prior for latent factor models, similarly to the IBP. As
an example of the potential usefulness of our model compared to IBP, consider the extraction of
features from time series of different lengths. Longer time series are more likely to exhibit more
features than shorter ones, and it is sensible in this case to assume different weights ?i . In a more
general setting, we may want ?i to depend on a set of metadata associated to reader i. Inference for
latent factor models is described in supplementary material.

5

Illustrations on real-world social networks

We now consider estimating the parameters of our model and evaluating its predictive performance
on six bipartite social networks of various sizes. We first provide a short description of these networks. The dataset ?Boards? contains information about members of the boards of Norwegian companies sitting at the same board in August 20112 . ?Forum? is a forum network about web users
contributing to the same forums3 . ?Books? concerns data collected from the Book-Crossing community about users providing ratings on books4 where we extracted the bipartite network from the
ratings. ?Citations? is the co-authorship network based on preprints posted to Condensed Matter
section of ArXiv between 1995 and 1999 [15]. ?Movielens100k? contains information about users
rating particular movies5 from which we extracted the bipartite network. Finally, ?IMDB? contains
information about actors co-starring a movie6 . The sizes of the different networks are given in
Table 1.
Dataset
Board

n
355

K
5766

Edges
1746

Forum
Books
Citations
Movielens100k
IMDB

899
5064
16726
943
28088

552
36275
22016
1682
178074

7089
49997
58595
100000
341313

S-IBP
9.82
(29.8)
-6.7e3
83.1
-3.7e4
-6.7e4
-1.5e5

SG
8.3
(30.8)
-6.7e3
214
-3.7e4
-6.7e4
-1.5e5

IG
-145.1
(81.9)
-5.5e3
4.6e4
-3.1e4
-5.5e4
-1.1e5

GGP
-68.6
(31.9)
-5.6e3
4.4e4
-3.4e4
-5.5e4
-1.1e5

Table 1: Size of the different datasets and test log-likelihood of four different models.
We evaluate the fit of four different models on these datasets. First, the stable IBP [18] with parameters (?IBP , ?IBP , ?IBP ) (S-IBP). Second, our model where the parameter ? is the same over different readers, and is assigned a flat prior (SG). Third our model where each ?i ? Gamma(a? , b? )
where (a? , b? ) are unknown parameters with flat improper prior (IG). Finally, our model with a
GGP model for ?i , with parameters (?? , ?? , ?? ) (GGP). We divide each dataset between a training
2

Data can be downloaded from http://www.boardsandgender.com/data.php
Data for the forum and citation datasets can be downloaded from http://toreopsahl.com/datasets/
4
http://www.informatik.uni-freiburg.de/ cziegler/BX/
5
The dataset can be downloaded from http://www.grouplens.org
6
The dataset can be downloaded from http://www.cise.ufl.edu/research/sparse/matrices/Pajek/IMDB.html
3

7

3

3

10

3

10

4

10

Model
Data

10

Model
Data

Model
Data

Model
Data

3

10
2

2

10

2

10

10

2

10

1

1

10

1

10

10

1

10

0

0

10
0
10

0

10
0
10

2

10

Degree

(a) S-IBP
4

4

3

4

10

1

1

1

1

10

0

0

10
0
10

Degree

(e) S-IBP

10

10

0

10
0
10

Degree

2

10

10

0

10
0
10

10

2

10

10

3

10

2

Model
Data

4

10

3

10

2

5

10
Model
Data

10

3

10

(d) GGP

5

10
Model
Data

10

2

10

Degree

(c) IG

5

10
Model
Data

10

10
0
10

2

10

Degree

(b) GS

5

10

0

10
0
10

2

10

Degree

10
0
10

Degree

(f) GS

Degree

(g) IG

(h) GGP

Figure 3: Degree distributions for movies (a-d) and actors (e-h) for the IMDB movie-actor dataset
with four different models. Data are represented by red plus and samples from the model by blue
crosses.
3

3

10

3

10

2

1

1

1

(a) S-IBP
4

4

3

4

2

2

2

(e) S-IBP

1

10

0

10
0
10

Degree

2

10

1

10

0

10
0
10

10

10

1

10

0

3

10

10

1

10

0

10
0
10

Degree

10
0
10

Degree

(f) GS

Model
Data

4

10

3

10

10

5

10
Model
Data

10

3

10

(d) GGP

5

10
Model
Data

10

2

10

Degree

(c) IG

5

10
Model
Data

10

0

10
0
10

2

10

Degree

(b) GS

5

1

10

0

10
0
10

2

10

Degree

10

10

10

0

10
0
10

2

10

Degree

Model
Data

2

10

10

0

10
0
10

Model
Data

2

10

10

10

Model
Data

2

10

3

10

Model
Data

(g) IG

Degree

(h) GGP

Figure 4: Degree distributions for readers (a-d) and books (e-h) for the BX books dataset with four
different models. Data are represented by red plus and samples from the model by blue crosses.
set containing 3/4 of the readers and a test set with the remaining. For each model, we approximate
the posterior mean of the unknown parameters (respectively (?IBP , ?IBP , ?IBP ), ?, (a? , b? ) and
(?? , ?? , ?? ) for S-IBP, SG, IG and GGP) given the training network with a Gibbs sampler with
10000 burn-in iterations then 10000 samples; then we evaluate the log-likelihood of the estimated
model on the test data. For GGP, we use ??test = ?
b? /3 to take into account the different sample
sizes. For ?Boards?, we do 10 replications with random permutations given the small sample size
and report standard deviation together with mean value. Table 1 shows the results over the different
networks for the different models. Typically, S-IBP and SG give very similar results. This is not
surprising, as they share the same properties, i.e. Poissonian degree distribution for readers and
power-law degree distribution for books. Both methods perform better solely on the Board dataset,
where the Poisson assumption on the number of people sitting on the same board makes sense. On
all the other datasets, IG and GGP perform better and similarly, with slightly better performances for
IG. These two models are better able to capture the power-law distribution of the degrees of readers.
These properties are shown on Figures 3 and 4 which resp. give the empirical degree distributions
of the test network and a draw from the estimated models, for the IMDB dataset and the Books
dataset. It is clearly seen that the four models are able to capture the power-law behavior of the
degree distribution of actors (Figure 3(e-h)) or books (Figure 4(e-h)). However, only IG and GGP
are able to capture the power-law behavior of the degree distribution of movies (Figure 3(a-d)) or
readers (Figure 4(a-d)).
8

References
[1] A. Brix. Generalized gamma measures and shot-noise Cox processes. Advances in Applied
Probability, 31(4):929?953, 1999.
[2] F. Caron and Y. W. Teh. Bayesian nonparametric models for ranked data. In Neural Information
Processing Systems (NIPS), 2012.
[3] D.J. Daley and D. Vere-Jones. An introduction to the theory of point processes. Springer
Verlag, 2008.
[4] L. Devroye. Random variate generation for exponentially and polynomially tilted stable distributions. ACM Transactions on Modeling and Computer Simulation (TOMACS), 19(4):18,
2009.
[5] E.B. Fox, E.B. Sudderth, M.I. Jordan, and A.S. Willsky. Sharing features among dynamical
systems with beta processes. In Advances in Neural Information Processing Systems, volume 22, pages 549?557, 2009.
[6] A. Goldenberg, A.X. Zheng, S.E. Fienberg, and E.M. Airoldi. A survey of statistical network
models. Foundations and Trends in Machine Learning, 2(2):129?233, 2010.
[7] D. G?or?ur, F. J?akel, and C.E. Rasmussen. A choice model with infinitely many latent features. In
Proceedings of the 23rd international conference on Machine learning, pages 361?368. ACM,
2006.
[8] T Griffiths and Z. Ghahramani. Infinite latent feature models and the Indian buffet process. In
NIPS, 2005.
[9] T. Griffiths and Z. Ghahramani. The Indian buffet process: an introduction and review. Journal
of Machine Learning Research, 12(April):1185?1224, 2011.
[10] N.L. Hjort. Nonparametric bayes estimators based on beta processes in models for life history
data. The Annals of Statistics, 18(3):1259?1294, 1990.
[11] L.F. James, A. Lijoi, and I. Pr?unster. Posterior analysis for normalized random measures with
independent increments. Scandinavian Journal of Statistics, 36(1):76?97, 2009.
[12] J.F.C. Kingman. Completely random measures. Pacific Journal of Mathematics, 21(1):59?78,
1967.
[13] J.F.C. Kingman. Poisson processes, volume 3. Oxford University Press, USA, 1993.
[14] M.E.J. Newman. The structure and function of complex networks. SIAM review, pages 167?
256, 2003.
[15] M.E.J. Newman, S.H. Strogatz, and D.J. Watts. Random graphs with arbitrary degree distributions and their applications. Physical Review E, 64(2):26118, 2001.
[16] M.E.J. Newman, D.J. Watts, and S.H. Strogatz. Random graph models of social networks.
Proceedings of the National Academy of Sciences, 99:2566, 2002.
[17] J.J. Ramasco, S.N. Dorogovtsev, and R. Pastor-Satorras. Self-organization of collaboration
networks. Physical review E, 70(3):036106, 2004.
[18] Y.W. Teh and D. G?or?ur. Indian buffet processes with power-law behavior. In NIPS, 2009.
[19] R. Thibaux and M. Jordan. Hierarchical beta processes and the Indian buffet process. In
International Conference on Artificial Intelligence and Statistics, volume 11, pages 564?571,
2007.
[20] F. Wood, T.L. Griffiths, and Z. Ghahramani. A non-parametric Bayesian method for inferring
hidden causes. In Proceedings of the Conference on Uncertainty in Artificial Intelligence,
volume 22, 2006.

9

"
2011,Orthogonal Matching Pursuit with Replacement,,4462-orthogonal-matching-pursuit-with-replacement.pdf,"In this paper, we consider the problem of compressed sensing where the goal is to recover almost all the sparse vectors using a small number of fixed linear measurements. For this problem, we propose a novel partial hard-thresholding operator leading to a general family of iterative algorithms. While one extreme of the family yields well known hard thresholding algorithms like ITI and HTP, the other end of the spectrum leads to a novel algorithm that we call  Orthogonal Matching Pursuit with Replacement (OMPR). OMPR, like the classic greedy algorithm OMP, adds exactly one coordinate to the support at each iteration, based on the correlation with the current residual. However, unlike OMP, OMPR also removes one coordinate from the support. This simple change allows us to prove the best known guarantees for OMPR in terms of the Restricted Isometry Property (a condition on the measurement matrix). In contrast, OMP is known to have very weak performance guarantees under RIP.  We also extend OMPR using locality sensitive hashing to get OMPR-Hash, the first provably sub-linear (in dimensionality) algorithm for sparse recovery. Our  proof techniques are novel and flexible enough to also permit the tightest known analysis of popular iterative algorithms such as CoSaMP and Subspace Pursuit.  We provide experimental results on large problems providing  recovery for vectors of size up to million dimensions.  We demonstrate that for large-scale problems our proposed methods are more robust and faster than the existing  methods.","Orthogonal Matching Pursuit with Replacement

Prateek Jain
Microsoft Research India
Bangalore, INDIA
prajain@microsoft.com

AmbujTewari
The University of Texas at Austin

Austin, TX
ambuj@cs.utexas.edu

Inderjit S. Dhillon
The University of Texas at Austin
Austin, TX
inderjit@cs.utexas.edu

Abstract
In this paper, we consider the problem of compressed sensing where the goal is to recover all sparse
vectors using a small number offixed linear measurements. For this problem, we propose a novel
partial hard-thresholding operator that leads to a general family of iterative algorithms. While one
extreme of the family yields well known hard thresholding algorithms like ITI and HTP[17, 10], the
other end of the spectrum leads to a novel algorithm that we call Orthogonal Matching Pursnit with
Replacement (OMPR). OMPR, like the classic greedy algorithm OMP, adds exactly one coordinate
to the support at each iteration, based on the correlation with the current residnal. However, unlike
OMP, OMPR also removes one coordinate from the support. This simple change allows us to prove
that OMPR has the best known guarantees for sparse recovery in terms of the Restricted Isometry
Property (a condition on the measurement matrix). In contrast, OMP is known to have very weak
performance guarantees under RIP. Given its simple structore, we are able to extend OMPR using
locality sensitive hashing to get OMPR-Hasb, the first provably sub-linear (in dimensionality) algorithm for sparse recovery. Our proof techniques are novel and flexible enough to also permit the
tightest known analysis of popular iterative algorithms such as CoSaMP and Subspace Pursnit. We
provide experimental results on large problems providing recovery for vectors of size up to million
dimensions. We demonstrste that for large-scale problems our proposed methods are more robust
and faster than existing methods.

1 Introduction
We nowadays routinely face high-dimensional datasets in diverse application areas such as biology, astronomy, and
finance. The associated curse of dimensionality is often alleviated by prior knowledge that the object being estimsted
has some structore. One of the most natorsl and well-stodied structural assumption for vectors is sparsity. Accordingly,
a huge amount of recent work in machine learning, statistics and signal processing has been devoted to finding better
ways to leverage sparse structures. Compressed sensing, a new and active branch of modem signal processing, deals
with the problem of designing measurement matrices and recovery algorithms, such that almost all sparse signals can
be recovered from a smalI number of measurements. It has important applications in imsging, computer vision and
machine learning (see, for example, [9,24, 14]).
In this paper, we focus on the compressed sensing setting [3, 7] where we want to design a measurement matrix
A E R=xn such that a sparse vector x* E R n with Ilx*llo := IBUpp(X*)I ::; k < n can be efficiently recovered from
the measurements b = Ax* E R=. Initial work focused on various random ensembles of matrices A such that, if A
was chosen randomly from that ensemble, one would be able to recover all or almost all sparse vectors x* from Ax*.
Candes and Tao[3] isolated a key property called the restricted Isometry property (RIP) and proved that, as long as the
measurement matrix A satisfies RIP, the true sparse vector can be obtained by solving an i,-optimization problem,

min Ilxll, S.t. Ax

=

b.

The above problem can be easily formulated as a linear program and is hence efficiently solvable. We recall for the
reader that a matrix A is said to satisfY RIP of order k if there is some Ok E 10,1) such that, for all x with Ilxllo ::; k,
we have

I

Several random matrix ensembles are known to satisfY 00> < {} with high probability provided one chooses
m ~ 0 (~ log ~) measurements. It was shown in [2] that i,-minimization recovers all k-sparse vectors provided A
satisfies t.k < 0.414 although the conditioohas been recently intproved to 02k < 0.473 [11]. Note that, in compressed
sensing, the goal is to recover all, or most, k-sparse signals using the same measurement matrix A. Hence, weaker
cooditioos such as restricted coovexity [20] studied in the statistical literature (where the aint is to recover a single
sparse vector from noisy linear measurements) typically do not suffice. In fact, if RIP is not satisfied then multiple
sparse vectors x can lead to the sante observatioo b, hence making recovery of the true sparse vector intpossible.
Based on its RIP guarantees, i,-minimizatioo can guarantee recovery using just O(k log(n/ k?) measurements, but it

has been observed in practice that i,-minimization is too expensive in large scale applications [8], for example, when
the dimensionality is in the millions. This has sparked a huge interest in other iterative methods for sparse recovery.
An early classic iterative method is Orthogooal Matching Pursuit (OMP) [21, 6] that greedily chooses elements to add
to the support. It is a natural, easy-to-intplement and fast method but unfortuoately lacks stroug theoretical guarantees.
Indeed, it is known that, if run for k iterations, OMP cannot uoiformly recover all k-sparse vectors assumiug RIP
cooditioo of the form 02k :'0 IJ [22, 18]. However, Zhang [26] showed that OMP, if run for 30k iterations, recovers the
optimal solution when 03'k :'0 1/3; a significantly more restrictive cooditioo than the ones required by other methods
like i,-minimization.
Several other iterative approaches have been proposed that include Iterative Soft Thresholding (1ST) [17], Iterative
Hard Thresholding (!BT) [I], Compressive Santpling Matching Pursuit (CoSaMP) [19], Subspace Pursuit (SP) [4],
Iterative Thresholding with Inversion (IT!) [16], Hard Thresholding Pursuit (HTP) [10] and many others. In the family
ofiterative hard thresholding algorithms, we can identifY two major subfamilies [17]: one- and two-stage algorithms.
As their nantes suggest, the distiuctioo is based on the number of stages in each iteration of the algorithm. One-stage
algorithms such as IHT, m and HTP, decide on the choice of the next support set and then usually solve a least
squares problem on the updated support. The one-stage methods always set the support set to have size k, where k
is the target sparsity level. On the other hand, two-stage algorithms, notable examples being CoSaMP and SP, first
enlarge the support set, solve a least squares 00 it, and then reduce the support set back again to the desired size. A
secood least squares problem is then solved 00 the reduced support. These algorithms typically enlarge and reduce
the support set by k or 2k elements. An exceptioo is the two-stage algorithm FoBa [25] that adds and removes single
elements from the support. However, it differs from our proposed methods as its analysis requires very restrictive RIP
cooditioos (08k < 0.1 as quoted in [14]) and the connection to locality sensitive hashing (see below) is not made.
Another algorithm with replacentent steps was studied by Shalev-Shwartz et al. [23]. However, the algorithm and the
settiug under which it is analyzed are different from ours.
In this paper, we present and provide a unified analysis for a family of one-stage iterative hard thresholding algorithms.
The family is parameterized by a positive integer I :'0 k. At the extrente value I ~ k, we recover the algorithm ITIIHTP.
At the other extrente k ~ 1, we get a novel algorithm that we call Orthogonal Matching Pursuit with Replacement
(OMPR). OMPR can be thought of as a sintple modification of the classic greedy algorithm OMP: instead of sintply
adding an element to the existiug support, it replaces an existiug support element with a new one. Surprisingly, this
change allows us to prove sparse recovery under the condition 02k < 0.499. This is the best 02k based RIP condition
under which any method, including i, -minimization, is (currently) known to provably perform sparse recovery.
OMPR also lends itself to a faster intplententatioo using locality sensitive hashing (LSH). This allows us to provide
recovery guarantees using an algorithm whose run-time is provably sub-linear in n, the number of dimensions. An
added advantage of OMPR, unlike many iterative methods, is that no careful tuning of the step-size parameter is
required even under noisy settiugs or even when RIP does not hold. The default step-size of 1 is always guaranteed to
converge to at least a local optimum.
Finally, we show that our proof techniques used in the analysis of the OMPR family are useful in tightening the
analysis of two-stage algorithms, such as CoSaMP and SP, as well. As a result, we are able to prove better recovery
guarantees for these algorithms: 04k < 0.35 for CoSaMP, and 03k < 0.35 for SP. We hope that this unified analysis
sheds more light on the interrelationships between the various kinds of iterative hard thresholding algorithms.
In summary, the contributions of this paper are as follows .
? We present a family of iterative hard thresholding algorithms that on one end of the spectrum includes existing methods such as ITIIHTP while on the other end gives OMPR. OMPR is an intproventent over the
classical OMP method as it enjoys better theoretical guarantees and is also better in practice as shown in our
experiments .
? Unlike other intprovements over OMP, such as CoSaMP or SP, OMPR changes ouly ooe elentent of the
support at a tinte. This allows us to use Locality Sensitive Hashing (LSH) to speed it up resultiug in the first
provably sub-linear (in the ambient dimensionality n) time sparse recovery algorithm.

2

Algorithm 2 OMPR (I)
1: Input: matrix A, vector b, sparsity level k
2: Parameter: step size 1/ > 0, replacement budget 1
3: Initialize Xl S.t I supp(xl)1 = k, h = supp(x l )
4: fort = ltoTdo
5:
zHI <- x' + 1/AT(b - Ax')
6:
tOPHI <- indices of top 1 elements of Iz};'""11
7:
J'+1 <- I, U tOPHI

Algorithm 1 OMPR
1: Input: matrix A, vector b, sparsity level k
2: Parameter: s1ep size 1/ > 0
3: Initialize Xl S.t Isupp(xl)1 = k, h = supp(XI)
4: for t = 1 to T do
5:
zHI <- x' + 1/AT(b - Ax')
I HII
.
6:
Jt+l +- argmaxj~It Zj

J'+1

I, U {iHI}

7:
8:

yt+l +- H

9:

It+1

HI
10:
x [Hl
11: end for

<-

<-

k

(zt+l)
+l
Jt

supp(y'+1)
HI
A It+l \b , x it+l

+-

+-

yt+l

9:

IHI <- supp(yHI)

+-

XHI
I t +1
11: end for

0

10:

<-

Hk

(z~~:J

8:

A It+l'
\b x'.+1
1t+l

<-

0

? We provide a general proof for all the algorithms in our partial hard thresholding based family. In particular,
we can guarantee recovery using OMPR, under both noiseless and noisy settings, provided 02' < 0.499.
This is the least restrictive 02. cooditioo under which any efficient sparse recovery method is known to work.
Furthermore, our proof technique can be used to provide a general theorem that provides the least restrictive
known guarantees for all the two-stage algorithms such as CoSaMP and SP (see Appendix D).
All proofs omitted from the main body of the paper can be found in the appendix.

2 Orthogonal Matching PUl""lIuit with Replacement
Orthogonal matching pursuit (OMP), is a classic iterative algorithm for sparse recovery. At every stage, it selecta a
coordinate to include in the current support set by maximizing the inner product between columns of the measurement
matrix A and the current residnal b - Ax'. Doce the new coordinate has been added, it solves a least squares problem
to fully miuimize the error on the current support set As a result, the residnal becomes orthogonal to the columos of
A that correspond to the current support set. Thus, the least squares s1ep is also referred to as orthogonalization by
some authors [5].
Let us briefly explain some of our notation. We use the MATI..AB notation:

A\b:= argmin IIAx - bl1 2

?

z

The hard thresholding operator H.O sorts its argument vector in decreasing order (in absolute value) and retains
ooly the top k entries. It is defined formally in the next sectioo. Also, we use subscripts to denote sub-vectors and
submatrices, e.g. if I <;; Inl is a set of cardinality k and x ERn, XI E R' denotes the sub-vector of X indexed by I.
Similarly, AI for a matrix A E Rmx n denotes a sub-matrix of size m x k with columns indexed by I. The complement
of set I is denoted by I and x I denotes the subvector not indexed by I. The support (indices of non-zero entries) of a
vector x is denoted by supp(x).
Our new algorithm called Orthogooal Matching Pursuit with Replacement (OMPR), shown as Algorithm 1, differs
from OMP in two respects. First, the selection of the coordinate to include is based not just on the magnitude of entries
in AT (b - Ax') but instead on a weighted combination x' + 1/AT (b - Ax') with the s1ep-size 1/ cootrolling the relative
importance of the two addends. Second, the selected coordinate replaces one of the existing elements in the support,
namely the one corresponding to the minimum magnitude entry in the weighted combination mentioned above.
Doce the support IHI of the next iterate has been determined, the actna1 iterate X HI is obtained by solving the least
squares problem:
HI =
X
argmin
IIAx - bli2 .
x: supp(z)=It+l

Note that if the matrix A satisfies RIP of order k or larger, the above problem will be well conditioned and can be
solved quickly and reliably using an iterative least squares solver. We will show that OMPR, uulike OMP, recovers any
k-sparse vector under the RIP based cooditioo 02. :<:; 0.499. This appears to be the least restrictive recovery condition
(i.e., best known coodition) under which any method, be it basis pursuit (ll-minimizatioo) or some iterative algorithm,
is guaranteed to recover all k-sparse vectors.
In the literature on sparse recovery, RIP based cooditioos of a different order other than 2k are often provided. It is
seldom possible to directly compare two conditions, say, one based on 62 ? and the other based on 63 ?? Foucart [10] has

3

given a heuristic to compare such RIP conditions based on the number of samples it takes in the Gaussian ensemble
to satisfy a given RIP condition. This heuristic says that an RIP condition of the form lic' < 9 is less restrictive if the
ratio c/92 is smaller. For the OMPR condition Ii,. < 0.499, this ratio is 2/0.4992 """" 8 which makes it heuristically
the least restrictive RIP condition for sparse recovery. The following summarize our main results on OMPR.
Theorem 1 (Noiseless Case). Suppose the vector x* E IRn is k-sparse and the matrix A satisfies 1i2? < 0.499 and
Ii, < 0.002. Then OMPR converges to an E approximate solution (i.e. 1/211Ax - bl1 2 ~ E) from measurements
b ~ Ax* in O(klog(k/E)) iterations.
Theorem 2 (Noisy Case). Suppose the vector x* E IRn is k-sparse and the matrix A satisfies 1i2 ? < 0.499 and
Ii, < 0.002. Then OMPR converges to a (C,E) approximate solution (i.e. 1/211Ax - bll' ~ ~llell' + E) from
measurements b ~ Ax* + e in O(k log((k + IleI1 2 )/E)) iterations. Here C > 1 is a constant dependent only on 1i2 ?.
The above theorems are special cases of our convergence results for a family of algorithms that contains OMPR as a
special case. We now tum our attention to this family. We note that the condition 1i2 < 0.002 is very mild and will
typically hold for standard random matrix ensembles as soon as the number of rows sampled is larger than a fixed
universal constant

3 A New Family of Iterative Algorithms
In this section we show that OMPR is one particular member of a family of algorithms parameterized by a single
integer 1 E {I, ... , k}. The I-th member of this family, OMPR (I), showo in Algorithm 2, replaces at most 1 elements
of the curreot support with new elements. OMPR corresponds to the choice 1 ~ 1. Hence, OMPR and OMPR (1)
refer to the same algorithm.
Our first result in this section conoects the OMPR family to hard thresholding. Given a set I of cardinality k, define
the partial hard thresholding operator

Hk (z; I, I):~

argmin

(I)

Ily - zll .

hl o:S;k

Isupp(y)\II5:l

As is clear from the definition, the above operator tries to find a vector V close to a given vector z under two constraints:
(i) the vector V should have bounded support (1lvllo ~ k), and (ii) its support should not include more than 1 new
elements outside a given support I.
The name partial hard thresholding operator is justified because of the following reasoning. When 1 ~ k, the constraint
I supp(Y)\I1 ~ 1is trivially implied by IIYllo ~ k and hence the operator becomes independent of!. In fact, itbecomes
identical to the standard hard thresholding operator

H. (z; I, k)

~

H. (z)

:~

argmin Ily - zll .

(2)

11.1109

GJ

Even though the definition of Hk (z) seems to involve searching through
subsets, it can in fact be computed
efficiently by simply sorting the vector z by decreasing absolute value and retaming the top k entries.
The following result shows that even the partial hard thresholding operator is easy to compute. In fact, lines 6-8 in
Algorithm 2 precisely compute H. (zt+1; It, I).
Proposition 3. Let

III ~ k and z be given.

Then Y ~ H. (z;I, I) can be computed using the sequence ofoperations

top ~ indices of top 1 elements oflzll,

J ~ I U top,

V ~ Hk (ZJ) .

The proof of this proposition is straightforward and elementary. However, using it, we can now see that the OMPR (I)
algorithm has a simple conceptoa1 s1ructore. In each iteration (with current iterate x' having support It ~ supp(xt?,
we do the following:

1. (Gradient Descent) Fonn zHI ~ xt - '1AT(Axt - b). Note that AT(Axt - b) is the gradient of the objective
function ~IIAx - bll' at x'.
2. (partial Hard Thresholding) Form VH1 by partially hard thresholding zHI using the operator H. (.; It, I).

3. (Least Squares) Form the next iterate X HI by solving a least squares problem on the support IHI ofyHI.
A nice property enjoyed by the entire OMPR family is guaranteed sparse recovery under RIP based conditions. Note
from below that the condition under which OMPR (I) recovers sparse vectors becomes more restrictive as I increases.
This could be an artifact of our analysis, as in experiments, we do not see any degradation in recovery ability as I is

increased.
4

Theorem 4 (Noiseless Case). Suppose the vector x' E IRn is k-sporse. Then OMPR (I) converges to an < approximation solution (i.e. 1/211Ax - bl1 2 :5 <)from measurements b = Ax* in O( ~ log(k/<? iterations provided we choose a
step size 1'/ that satisfies 1'/(1 + 02.) < 1 and 1'/(1 - 02.) > 1/2.
Theorem S (Noisy Case). Suppose the vector x' E IRn is k-sparse. Then OMPR (I) converges to a (C, <) approximate
solution (i.e., 1/211Ax - bl1 2 :5 IIell 2 + <) from measurements b = Ax' + e in O( log?k + IleI1 2)1<) iterations
provided we choose a step size 1'/ that satisfies 1'/(1 + 02,) < 1 and 1'/(1 - 02.) > 1/2. Here C > 1 is a constant
dependent only on 02., 02 ?.
Proof Here we provide a rough sketch of the proof of Theorem 4; the complete proof is giveo in Appeodix A.

t

t

Our proof uses the following crucial observatioo regarding the structure of the vector zH1 = x' - 1'/AT (Ax' - b) .
Due to the least squares step of the previous iteration, the curreot residual Ax' - b is orthogoual to columns of AI,.
This meaos that
ZH1
- x'It'
It -

z~+1
= -nA'!'
(Ax'
It
"" It

- b) .

(3)

As the algorithm proceeds, elemeots come in and move out of the curreot set I,. Let us give names to the set offound
and lost elements as we move from I, to 1'+1:
(found): F, = IH1 \I""
Heoce, using (3) and updates for YH1: Y~;' = Z~;' = -1'/A~,A(x' - x'), and Z~;' = xL. Now let J(x) =
1/211Ax - b11 2, theo using upper RIP and the fact that I supp(yH1 - x')1 = IF, U L,I :5 21, we can sbow that (details
are in the Appeodix A):

J( y H 1) - J(x'):5

C~02' D

IIyWII 2 + 1 ~02'llxUI2.

-

(4)

Furthermore, since yH1 is choseo based on the k largest eotries in z~;:"" we have: IIY~;'112 = Ilz~;'112 ~ Ilz~;'112 =
IlxL 112 . Plugging this into (4), we get:

J(yH1) - J(x'):5 (1 +O2 '-~) M;'112.

(5)

Since J(x H1 ) :5 J(yH1) :5 J(x'), the above expression shows that if 1'/ < 1':."" then our method moootonically
decreases the objective function and converges to a local optimum even if RIP is not satisfied (note that upper RIP
bound is indepeodeot oflower RIP bound, and can always be satisfied by nurma1izing the matrix appropriately).
However, to prove convergeoce to the global optimum, we need to show that at least ooe new elemeot is added at each
step, i.e., IF,I ~ 1. Furthermore, we need to show sufficieot decrease, i.e, IIY~;'112 ~ elJ(x'). We show both these
conditions for global coovergeoce in Lemma 6, whose proof is giveo in Appeodix A.
Lemma 6. Let 02k < 1 - 2~ and 1/2 < 1'/ < 1. Then assuming J(x') > 0, at least one new element is found i.e.
F, '""

0. Furthermore,

IIY~;'11 > teJ(x'), where e = min(41'/(1 - 1'/),,2(21'/- 1-~""? > 0 is a constant.

Assunling Lemma 6, (5) shows that at each iteration OMPR (I) reduces the objective functioo value by at least a
constant fractioo. Furthermore, if XO is choseo to have eotries bounded by 1, theo J(XO) :5 (1 + 02k)k. Heoce, afier
O(k/llog(k/<? iteratioos, the optimal solution x* would be obtained within < error.
D
Speeial Cases: We have already observed that the OMPR algorithm of the previous sectioo is simply OMPR (1).
Also note that Theorem I immediately follows from Theorem 4.
The algorithm at the other extreme of 1 = k has appeared at least three times in the receot literature: as Iterative (hard)
Thresholding with Inversioo (IT!) in [16], as SVP-Newton (in its matrix avatar) in [15], and as Hard Thresholding
Pursuit (HTP) in [10]). Let us call it IHT-Newton as the least squares step can be viewed as a Newton step for the
quadrstic objective. The above geoera1 result for the OMPR family immediately implies that it recovers sparse vectors
as soon as the measuremeot matrix A satisfies 02, < 1/3.
CoroUary 7. Suppose the vector x' E an is k-sparse and the matrix A satisfies 02k < 1/3. Then IlIT-Newton
recovers x* from measurements b = Ax' in O(1og(k? iterations.
5

4 Tighter Analysis of Two Stage Hard Thresholding Algorithms
Recently, Maleki and Donoho [17] proposed a novel family of algorithms, namely two-stage hard thresholding algorithms. Doring each iteration, these algorithms add a fixed nwnber (say l) of elements to the current iterate's support
set. A least squares problem is solved over the larger support set and then I elements with smallest magnitude are
dropped to form next iterate's support set. Next iterate is then obtained by agaiu solviug the least squares over next
iterate's support set. See Appendix D for a more detailed description of the algorithm.
Usiug proof techniques developed for our proof of Theorem 4, we can obtain a simple proof for the entire spectrum of
algorithms iu the two-stage hard thresholding family.
Theorem 8. Suppose the vector x* E {-I, 0, l}n is k-sparse. Then the 7Wo-stage Hard Thresholding algorithm with
replacement size I recovers x* from measurements b = Ax* in O(k) iterations provided: 6. H1 :::; .35.
Note that CoSaMP [19] and Subspace Pursuit(SP) [4] are popular special cases of the two-stage family. Usiug our
general analysis, we are able to provide significantly less restrictive RIP conditions for recovery.
CoroUary 9. CoSaMP[l9] recovers k-sparse x* E {-1,0, l}n from measurements b = Ax* provided 64k :::; 0.35.
CoroUary 10. Subspace Pursuit[4] recovers k-sparse x* E {-I, 0, I}n from measurements b = Ax* provided
63k :::; 0.35.
Note that CoSaMP's analysis given by [19] requires 64k :::; 0.1 while Subspace Pursuit's analysis given by [4] requires
63k :::; 0.205. See Appendix Diu the supplementary material for proofs of the ahove theorem and coroUaries.

5 Fast Implementation Using Hashing
In this section, we discuss a fast implementation of the OMPR method usiug locality-sensitive hashiug. The
mall iutuition behind our approach is that the OMPR method selects at most one element at each step (given by
argmax, IAT(Ax' - b) I); hence, selection of the top most element is equivalent to finding the column Ai that is most
""similar"" (iu magnitude) to r, = Ax' - b, i.e., this may be viewed as the similarity search task for queries of the form
r, and -r, from a database of N vectors IAI""'"" ANI.
To this end, we use locality sensitive hashiug (LSH) [12], a well known data-structore for approximate nearestneighbor retrieval. Note that while LSH is designed for nearest neighbor search (iu terms of Euclidean distances) and
iu general might not have any guarantees for the similar neighbor search task, we are still able to apply it to our task
because we can lower-hound the similarity of the most similar neighbor.
We first briefly describe the LSH scheme that we use. LSH generates hash bits for a vector usiug randoruized hash
functions that have the property that the probability of collision between two vectors is proportional to the similarity
between them. For our problem, we use the following hash function: h,.(a) = sign(uT a), where u ~ N(O, J) is a
random hyper-plane generated from the standard multivariate Gaussian distribution. It can be shown that [13]

Pr[h u (al)

= hu (
a.) ] =

af a2 )
1-;;:I cos - I ( Iladlla211'

created by randoruly sampling hash functions h,., i.e., g( a)
Ui is sampled randoruly from the standard multivariate Gaussian
distribution. Next, q hash tables are constructed doring the pre-processiug stage usiug iudependently constructed hash
key functions gl, 92, ... , gq' Doring the query stage, a query is iudexed iuto each hash table usiug hash-key functions
91, 92, ... ,9q and then the nearest neighbors are retrieved by doing an exhaustive search over the indexed elements.
Now,

an

.-bit hash key is

[hu,(a),hu,(a), ... ,hu.(a)], where each

Below we state the following theorem from [12] that guarantees sub-liuear time nearest neighbor retrieval for LSH.
Theorem 11. Let. = O(logn) and q = O(log 1/6)nr1<, then with probability 1 - 6, LSH recovers (I + f)-nearest
neighbors, i.e., Ila' - rl12 :::; (1 + f)lla' - rll?, where a' is the nearest neighbor to r and a' is a point retrieved by
LSH.

However, we cannot directly use the above theorem to guarantee convergence of our hashing based OMPR algorithm
as our algorithm requires finding the most similar poiut iu terms of magnitude of the iuner product. Below, we provide
appropriate settings of the LSH parameters to guarantee sub-liuear time convergence of our method under a slightly
weaker condition on the RIP constant. A detailed proof of the theorem below can be found iu Appendix B.
Theorem 12. Let 62? < 1/4 -"")' and 'f/ = I -"")"" where"")' > 0 is a small constant, then with probability I - 6, OMPR
with hashing converges to the optimal solution in O(kmnl /(1+0(I/k)) log k/6) computational steps.
The above theorem shows that the time complexity is sub-liuear iu n. However, currently our guarantees are not
particularly strung as for large k the exponent of n will be close to 1. We believe that the exponent can be improved
by more careful analysis and our empirical results iudicate that LSH does speed up the OMPR method significantly.

6

(a)OMPR

(b)OMP

(c) nIT-Newton

Figure 1: Phase Transition Diagrams for different methods. Red represents high probability of success while blue
represents low probability of success. Clearly, OMPR recovers correct solution for a much larger region of the plot
than OMP and is comparable to nIT-Newton. (Best viewed in color)

6 Experimental Results
In this section we present empirical results to demonstrate accurate and fast recovery by our OMPR method. In the first
set of experiments, we present a phase transition diagram for OMPR and compare it to the phase transition diagrams
of OMP and nIT-Newton with step size 1. For the second set of experiments, we demonstrate robostoess of OMPR
compared to many existiog methods when measurements are noisy or smaller in number than what is required for exact
recovery. For the third set of experiments, we demonstrate efficiency of our LSH based implementation by comparing
recovery error and time required for our method with OMP and nIT-Newtoo (with step-size 1 and 1/2). We do not
present results for the i,ibasis pursuit methods, as it has a1readybeen shown in several recent papers [10, 17] that the
i, relaxation based methods are relatively inefficient for very large scale recovery problems.
In all the experiments we generate the measurement matrix by sampling each entry independently from the standard
normal distribotion N (0, 1) and then normalize each column to have uuit norm. The underlying k-sparse vectors are
generated by randomly selecting a support set of size k and then each entry in the support set is sampled uuiformiy from
{ +1, -I}. We use our own optimized implementation of OMP and nIT-Newtoo. All the methods are implemented in
MATLAB and our hashing routioe uses mex files.

6.1

Phase Transition Diagrams

We first compare different methods using phase transition diagrams which are commouly used in compressed sensing
literatore to compare different methods [17]. We first fix the number of measurements to be m = 400 and generate
different problem sizes by varying p = kim and 6 = min. For each problem size (m, n, k), we generate random
m x n Gaussian measurement matrices and k-sparse random vectors. We then estimate the probability of success of
each of the method by applying the method to 100 randomly generated instances. A method is considered successful
for a particular instance if it recovers the underlying k-sparse vector with at most 1%relative error.
In Figure 1, we show the phase transition diagram of our OMPR method as well as that ofOMP and nIT-Newtoo (with
step size 1). The plots shows probability of successful recovery as a function of p = min and 6 = kim. Figure 1 (a)
shows color coding of different success probabilities; red represents high probability of success while blue represents
low probability of success. Note that for Gaussian measurement matrices, the RIP constant 62 ? is less than a fixed
constant if and ouly ifm = Ck log(nlk), where C is a uuiversal constant This implies that = Clog p and hence a
method that recovers for high 62 ? will have a large fraction in the phase transition diagram wbere successful recovery
probability is high. We observe this phenomenon for both OMPR and nIT-Newton method which is consistent with
their respective theoretical goarantees (see Theorem 4). On the other hand, as expected, the phase transition diagram
of OMP has a negligible fraction of the plot that shows high recovery probability.

*

6_2 Performance for Noisy or Under-sampled Observations
Next, we empirically compare performance of OMPR to various existing compressed sensing methods. As shown
in the phase transition diagrams in Figure 1, OMPR provides comparable recovery to the nIT-Newton method for
noiseless cases. Here, we show that OMPR is fairly robust under the noisy settiog as well as in the case of undersampled observations, where the number of observations is much smaller than what is required for exact recovery.

For this experiment, we generate random Gaussian measurement matrix of size m = 200, n = 3000. We then generate
random binary vector x of sparsity k aod add Gaussian noise to it Figure 2 (a) shows recovery error (1iAx - bll)
incurred by various methods for increasing k and noise level of 10%. Clearly, our method outperforms the existing
methods, perhaps a consequence of goaranteed convergence to a local minimum for fixed step size 1/ = 1. Similarly,
Figure 2 (b) shows recovery error incurred by various methods for fixed k = 50 and varying noise level. Here again,
our method outperforms existiog methods and is more robust to noise. Fina11y, in Figure 2 (c) we show difference in

7

Enurvsk(Noi-=10%)

Error w NaIM k=SO
"" _ OMPR
+OMPR(1rI2
:U . IHT-N
~
_ CoSAMP
:ii' + SP

~

3

~'.'~~/
10

20

30

""

Sp.lI'IiIy{k)

(a)

50

0
0.00(0.0)
0.00(0.0)
O.OO(u.O)
0.03(0.0)
U.1""(U.1)
0.31(0.1)
0.37(0.1)

NOise! 1<

""~:----';o,',-'0.""
-'0.""-'0'.-----,10.'
Hoi_LewI

0.00
0.05
0.0
0.20
U.3U
0.40
0.50

(b)

,0
-0.21(0.6)
0.13(0.3)
0.2""(0.3)
0.62(0.2)
U.92(0.3)
1.19(0.3)
1.48(0.3)

5u
0.25(0.3)
0.37(0.3)
O. 3 0.4)
0.58(0.5)
O.92(O.b)

0.84(0.5)
1.24(0.6)

(c)

Figure 2: Error in recovery <lIAx - bll) of n = 3000 dimensiooal vectors from m = 200 measurements. (a): Error
incurred by various methods as the sparsity level k increases. Note that OMPR incurs the least error as it provably
converges to at least a local minimum forfixed step size 1/ = 1. (b): Error incurred by various methods as the noise
level increases. Here again OMPR performs significaotly better than the existing methods. (c): Differeoce in error
incurred by IHT-Newton aod OMPR . Numbers in bracket dooote confideoce interval at 95% significaoce level.
EmrVII n (mIn=O 001 Idm"" 1)

OMPR Huh
+ 00'""-

I:-

0.""

+IHT-NMlanC1 ?

,.

.

."".

.,""

....~

0.03

o.

TlII'MI wn (rMFO.oD1, k/m=,1

:.-

,015

lIo

,.,

'""

..,

O.

00 ?

ncIrOOO)

""'""

(b)

(a)

..

n (x1fOOOO)

000

(c)

Figure 3: (a): Error (11Ax - bll) incurred by various methods as k increases. The measuremoots b = Ax are computing
by gooerating x with support size milO. (b),(c): Error incurred aod time required by various methods to recover
vectors of support size 0.1 mas n increases. IlIT-Newton(1/2) refers to the IHT-Newton method with step size 1/ = 1/2.
error incurred along with confideoce interval (at 95% signficaoce level) by IHT-Newton aod OMPR for varying levels
of noises aod k. Our method is better thao !HT-Newton (at 95% signficaoce level) in terms of recovery error in arouod
30 cells of the table, aod is not worse in aoy of the cells but one.
6.3 Performance of LSD based implementation
Next, we empirically study recovery properties of OMPR-Hasb in the following real-time setop: gooerate a raodom
measuremoot matrix from the Gaussiao ensemble aod construct bash tables ollline using hash functioos specified in
Section 5. During the reconstruction stage, measurements arrive one at a time and the goal is to recover the underlying
sigoal accurately in real-time.For our experimoots, we gooerate measuremoots using raodom sparse vectors aod thoo
report recovery error IIAx - bll aod computatiooal time required by each method averaged over 20 runs.
In our first set of experimoots, we eropirically study the performaoce of different methods as k increases. Here, we fix
m = 500, n = 500, 000 aod gooerate measuremoots using n-dimoosional raodom vectors of support set size milO.
We thoo run differeot methods to estimate vectors x of support size k that minimize IIAx - bll. For our OMPR-Hash
method, we use 8 = 20 bits bash-keys aod gooerate q = ..;n bash-tables. Figure 3 (a) shows the error incurred by
OMPR, OMPR-Hash, aod IHT-Newton for differeot k (recall that k is ao input to both OMPR aod IlIT-Newton).
Note that although OMPR-Hash performs ao approximation at each step, it is still able to achieve error similar to
OMPR aod !HT-Newton. Also, note that since the number of measuremoots are not ooough for exact recovery by the
IHT-Newton method, it typically diverges after a few steps. As a result, we use IHT-Newton with step size 1/ = 1/2
which is always goaraoteed to monotonically converge to at least a local minimum (see Theorem 4). In cootrast, in
OMPR aod OMPR-Hasb cao always set step size 1/ aggressively to be 1.

Next, we evaluate OMPR-Hash as dimoosiooality of the data n increases. For OMPR-Hasb, we use 8 = log2(n)
bash-keys aod q = ..;n hash-tables. Figures 3(b) aod (c) compare error incurred aod time required by OMPR-Hash
with OMPR aod IHT-Newton. Here again we use step size 1/ = 1/2 for !HT-Newton as it does not converge for 1/ = 1.
Note that OMPR-Hash is ao order of magnitude faster thao OMPR while incurring slightly higher error. OMPR-Hash
is also nearly 2 times faster thao IHT-Newton.
Acknowledgement

ISD acknowledges support from the Moncrief Graod Challooge Award.

8

References
[I] T. Blumensath and M. E. Davies. Iterative hard thresholding for compressed sensing. Applied and Computational
Harmonic Analysis, 27(3):265-274, 2009.
[2] E. J. Candes. The restricted isometry property and its implications for compressed sensing. Comptes Rendus
Mathematique, 346(9-10):589-592, 2008.
[3] E. J. Candes and T. Tao. Decoding by lioear programming. IEEE Transactions on Information Theory,
51(12):4203-4215,2005.
[4] W. Dai and O. Milenkovic. Subspace pursuit for compressive seosing signal reconstruction. IEEE Transactions
on Information Theory, 55(5):2230--2249, 2009.
[5] M. A. Davenport and M. B. Wakin. Analysis of orthogonal matching pursuit using the restricted isometry
property. IEEE Transactions on Information Theory, 56(9):4395-4401, 2010.
[6] G. Davis, S. Mallat, and M. Avellaneda. Greedy adaptive approximation. Constr. Approx, 13:57--98, 1997.
[7] D. Donoho. Compressed sensing. IEEE Trans. on Information Theory, 52(4):1289-1306, 2006.
[8] D. Donoho, A. Maleki, and A. Montanari. Message passing algorithms for compressed sensing. Proceedings of
the National Academy ofSciences USA, 106(45):18914-18919,2009.
[9] M. F. Duarte, M. A. Davenport, D. Takhar, 1. N. Laska, T. Sun, K. F. Kelly, and R. G. Baranuik. Single-pixel
imaging via compressive sarnpliog. IEEE Signal Processing Magazine, 25(2):83-91, March 2008.
[10] S. Foucart. Hard thresholding pursuit: an algorithm for compressive sensing, 2010. preprint.
[II] S. Foucart. A note on guaranteed sparse recovery via i,-minimi'ation. Applied and Computational Harmonic
Analysis, 29(1):97-103, 2010.
[12] A. Giouis, P. Indyk, and R Motwani. Similarity search in high dimensions using hashing. In Proceedings of
25th International Conference on Very Large Data Bases, 1999.
[13] M. X. Goemans and D. P. WIlliamson.. 879-approximation algorithms for MAX CUT and MAX 2SAT. In STOC,
pages 422-431,1994.
[14] D. Hsu, S. M. Kakade, J. Langford, and T. Zhang. Multi-label prediction via compressed sensing. In Advances
in Neural Information Processing Systems, 2009.
[15] P. Jain, R. Meka, and I. S. Dhillon. Gusranteed rank minimiUltion via singular value projection. In Advances in
Neural Information Processing Systems, 2010.
[16] A. Maleki. Convergence analysis of iterative thresholding algorithms. InAllerton Conference on Communication,
Control and Computing, 2009.
[17] A. Maleki and D. Donoho. Optimally tuned iterative reconstruction algorithms for compressed sensing. IEEE
Journal ofSelected Topics in Signal Processing, 4(2):330--341, 2010.
[18] Q. Mo and Y. Shen. Remarks on the restricted isometry property in orthogonal matching pursuit algorithm, 2011.
preprint arXiv: 1101.4458.
[19] D. Needell and J. A. Tropp. Cosamp: Iterative signal recovery from incomplete and inaccurate samples. Applied
and Computational Harmonic Analysis, 26(3):301- 321, 2009.
[20] S. Negshban, P. Ravikumar, M. 1. Wainwright, and B. Yu. A unified framework for high-dimensional analysis of
M -estimators with decomposable regularizers. In Advances in Neural Information Processing Systems, 2009.
[21] Y. C. Pati, R Rezaiifar, and P. S. Krishnaprasad. Orthogonal matching pursuit: Recursive function approximation
with applications to wavelet decomposition. In 27th Annu. Asilomar Con! Signals, Systems, and Computers,
volume I, pages 40-44, 1993.
[22] H. Rauhut. On the impossibility of uniform sparse reconstruction using greedy methods. Sampling Theory in
Signal and Image Processing, 7(2):197-215, 2008.
[23] S. Shalev-Shwartz, N. Srebro, and T. Zhang. Trading accuracy for sparsity in optimiUltion problems with sparsity
constraints. SIAM Journal on Optimization, 20:2807-2832, 2010.
[24] J. Wright, Y. Ma, J. Mairal, G. Sapiro, T. S. Huang, and S. Yan. Sparse representation for computer vision and
pattern recognition. Proceedings ofthe IEEE, 98(6):1031-1044, 2010.
[25] T. Zhang. Adaptive forward-backward greedy algorithm for sparse learning with lioear models. In Advances in
Neural Information Processing Systems, 2008.
[26] T. Zhang. Sparse recovery with orthogonal matching pursuit under RIP, 2010. preprint arXiv:lOO5.2249.

9

"
2004,The Cerebellum Chip: an Analog VLSI Implementation of a Cerebellar Model of Classical Conditioning,,2703-the-cerebellum-chip-an-analog-vlsi-implementation-of-a-cerebellar-model-of-classical-conditioning.pdf,Abstract Missing,"The cerebellum chip:
an analog VLSI implementation of a
cerebellar model of classical conditioning
Constanze Hofst?tter, Manuel Gil, Kynan Eng,
Giacomo Indiveri, Matti Mintz, J?rg Kramer* and Paul F. M. J. Verschure
Institute of Neuroinformatics
University/ETH Zurich
CH-8057 Zurich, Switzerland
pfmjv@ini.phys.ethz.ch

Abstract
We present a biophysically constrained cerebellar model of
classical conditioning, implemented using a neuromorphic analog
VLSI (aVLSI) chip. Like its biological counterpart, our cerebellar
model is able to control adaptive behavior by predicting the
precise timing of events. Here we describe the functionality of the
chip and present its learning performance, as evaluated in
simulated conditioning experiments at the circuit level and in
behavioral experiments using a mobile robot. We show that this
aVLSI model supports the acquisition and extinction of adaptively
timed conditioned responses under real-world conditions with
ultra-low power consumption.

I n tro d u cti o n

1

The association of two correlated stimuli, an initially neutral conditioned stimulus
(CS) which predicts a meaningful unconditioned stimulus (US), leading to the
acquisition of an adaptive conditioned response (CR), is one of the most essential
forms of learning. Pavlov introduced the classical conditioning paradigm in the
early 20th century to study associative learning (Pavlov 1927). In classical
conditioning training an animal is repeatedly exposed to a CS followed by a US
after a certain inter-stimulus interval (ISI). The animal learns to elicit a CR
matched to the ISI, reflecting its knowledge about an association between the CS,
US, and their temporal relationship. Our earlier software implementation of a
*

J?rg Kramer designed the cerebellum chip that was first tested at the 2002 Telluride
Neuromorphic Engineering Workshop. Tragically, he died soon afterwards while hiking
on Telescope Peak on 24 July, 2002.

biophysically constrained model of the cerebellar circuit underlying classical
conditioning (Verschure and Mintz 2001; Hofst?tter et al. 2002) provided an
explanation of this phenomenon by assuming a negative feedback loop between the
cerebellar cortex, deep nucleus and inferior olive. It could acquire and extinguish
correctly timed CRs over a range of ISIs in simulated classical conditioning
experiments, as well as in associative obstacle avoidance tasks using a mobile
robot. In this paper we present the analog VLSI (aVLSI) implementation of this
cerebellum model ? the cerebellum chip ? and the results of chip-level and
behavioral robot experiments.

2

T h e mo d el ci r cu i t a n d a VL S I i mp l eme n ta ti o n

Figure 1: Anatomy of the cerebellar model circuit (left) and the block diagram of
the corresponding chip (right).
The model (Figure 1) is based on the identified cerebellar pathways of CS, US and
CR (Kim and Thompson 1997) and includes four key hypotheses which were
implemented in the earlier software model (Hofst?tter et al. 2002):
1.

CS related parallel fiber (pf) and US related climbing fiber (cf) signals
converge at Purkinje cells (PU) in the cerebellum (Steinmetz et al. 1989). The
direction of the synaptic changes at the pf-PU-synapse depends on the temporal
coincidence of pf and cf activity. Long-term depression (LTD) is induced by pf
activity followed by cf activity within a certain time interval, while pf activity
alone induces long-term potentiation (LTP) (Hansel et al. 2001).

2.

A prolonged second messenger response to pf stimulation in the dendrites of
PU constitutes an eligibility trace from the CS pathway (Sutton and Barto
1990) that bridges the ISI (Fiala et al. 1996).

3.

A microcircuit (Ito 1984) comprising PU, deep nucleus (DN) and inferior olive
(IO) forms a negative feedback loop. Shunting inhibition of IO by DN blocks
the reinforcement pathway (Thompson et al. 1998), thus controlling the
induction of LTD and LTP at the pf-PU-synapse.

4.

DN activity triggers behavioral CRs (McCormick and Thompson 1984). The
inhibitory PU controls DN activity by a mechanism called rebound excitation
(Hesslow 1994): When DN cells are disinhibited from PU input, their

membrane potential slowly repolarises and spikes are emitted if a certain
threshold is reached. Thereby, the correct timing of CRs results from the
adaptation of a pause in PU spiking following the CS.
In summary, in the model the expression of a CR is triggered by DN rebound
excitation upon release from PU inhibition. The precise timing of a CR is
dependent on the duration of an acquired pause in PU spiking following a CS. The
PU response is regulated by LTD and LTP at the pf-PU-synapse under the control
of a negative feedback loop comprising DN, PU and IO.
We implemented an analog VLSI version of the cerebellar model using a standard
1.6?m CMOS technology, and occupying an area of approximately 0.25 mm2. A
block diagram of the hardware model is shown in Figure 1. The CS block receives
the conditioned stimulus and generates two signals: an analog long-lasting, slowly
decaying trace (cs_out) and an equally long binary pulse (cs_wind). Similarly, the
US block receives an unconditioned stimulus and generates a fast pulse (us_out).
The two pulses cs_wind and us_out are sent to the LT-ISI block that is responsible
for perfoming LTP and LTD, upregulating or downregulating the synaptic weight
signal w. This signal determines the gain by which the cs_out trace is multiplied in
the MU block. The output of the multiplier MU is sent on to the PU block, together
with the us_out signal. It is a linear integrate-and-fire neuron (the axon-hillock
circuit) connected to a constant current source that produces regular spontaneous
activity. The current source is gated by the digital cf_wind signal, such that the
spontaneous activity is shut off for the duration of the cs_out trace.
The chip allowed one of three learning rules to be connected. Experiments showed
that an ISI-dependent learning rule with short ISIs resulting in the strongest LTD
was the most useful (Kramer and Hofst?tter 2002). Two elements were added to
adapt the model circuit for real-world robot experiments. Firstly, to prevent the
expression of a CR after a US had already been triggered, an inhibitory connection
from IO to CRpathway was added. Secondly, the transduction delay (TD) from the
aVLSI circuit to any effectors (e.g. motor controls of a robot) had to be taken into
account, which was done by adding a delay from DN to IO of 500ms.
The chip?s power consumption is conservatively estimated at around 100 W
(excluding off-chip interfacing), based on measurements from similar integrateand-fire neuron circuits (Indiveri 2003). This figure is an order of magnitude lower
than what could be achieved using conventional microcontrollers (typically 1-10
mW), and could be improved further by optimising the circuit design.

3

S i mu l a ted co n d i ti o n i n g ex p eri men ts

The aim of the ?in vitro? simulated conditioning experiments was to understand the
learning performance of the chip. To obtain a meaningful evaluation of the
performance of the learning system for both the simulated conditioning
experiments and the robot experiments, the measure of effective CRs was used. In
acquisition experiments CS-US pairs are presented with a fixed ISI. Whenever a
CR occurs that precedes the US, the US signal is not propagated to PU due to the
inhibitory connection from DN to IO. Thus in the context of acquisition
experiments a CR is defined as effective if it prevents the occurrence of a US spike

at PU. In contrast, in robot experiments an effective CR is defined at the
behavioral level, including only CRs that prevent the US from occurring.

Figure 2: Learning related response changes in the cerebellar aVLSI chip. The most
relevant neural responses to a CS-US pair (ISI of 3s, ITI of 12s) are presented for a
trial before (naive) significant learning occurred and when a correctly timed CR is
expressed (trained). US-related pf and CS/CR-related cf signals are indicated by
vertical lines passing through the subplots. A CS-related pf-signal evokes a
prolonged response in the pf-PU-synapse, the CS-trace (Trace subplot). While an
active CS-trace is present, an inhibitory element (I) is active which inactivates an
element representing the spontaneous activity of PU (Hofst?tter et al. 2002). (A)
The US-related cf input occurs while there is an active CS-trace (Trace subplot), in
this case following the CS with an ISI of 3s. LTD predominates over LTP under
these conditions (Weight subplot). Because the PU membrane potential (PU)
remains above spiking threshold, PU is active and supplies constant inhibition to
DN (DN) while in the CS-mode. Thus, DN cannot repolarize and remains inactive
so that no CR is triggered. (B) Later in the experiment, the synaptic weight of the
pf-PU-synapse (Weight) has been reduced due to previous LTD. As a result,
following a CS-related pf input, the PU potential (PU subplot) falls below the
spiking threshold, which leads to a pause in PU spiking. The DN membrane
potential repolarises, so that rebound spikes are emitted (DN subplot). This
rebound excitation triggers a CR. DN inhibition of IO prevents US related cfactivity. Thus, although a US signal is still presented to the circuit, the reinforcing
US pathway is blocked. These conditions induce only LTP, raising the synaptic
weight of the pf-PU-synapse (Weight subplot).
The results we obtained were broadly consistent with those reported in the
biological literature (Ito 1984; Kim and Thompson 1997). The correct operation of
the circuit can be seen in the cell traces illustrating the properties of the aVLSI
circuit components before significant learning (Figure 2 A), and after a CR is
expressed (Figure 2B). Long-term acquisition experiments (25 blocks of 10 trials

each over 50 minutes) showed that chip functions remained stable over a long time
period. In each trial the CS was followed by a US with a fixed ISI of 3s; the inter
trial interval (ITI) was 12s. The number of effective CRs shows an initial fast
learning phase followed by a stable phase with higher percentages of effective CRs
(Figure 3B). In the stable phase the percentage of effective CRs per block
fluctuates around 80-90%. There are fluctuations of up to 500ms in the CR latency
caused by the interaction of LTD and LTP in the stable phase, but the average CR
latency remains fairly constant.
Figure 4 shows the average of five acquisition experiments (5 blocks of 10 trials
per experiment) for ISIs of 2.5s, 3s and 3.5s. The curves are similar in shape to the
ones in the long-term experiment. The CR latency quickly adjusts to match the ISI
and remains stable thereafter (Figure 4A). The effect of the ISI-dependent learning
rule can be seen in two ways: firstly, the shorter the ISI, the faster the stable phase
is reached, denoting faster learning. Secondly, the shorter the ISI, the better the
performance in terms of percentage of effective CRs (Figure 4B). The parameters
of the chip were tuned to optimally encode short ISIs in the range of 1.75s to 4.5s.
Separate experiments showed that the chip could also adapt rapidly to changes in
the ISI within this range after initial learning.

(Error bar = 1 std. dev.)

Figure 3: Long-term changes in CR latency (A) and % effective CRs (B) per block
of 10 CSs during acquisition. Experiment length = 50min., ISI = 3s, ITI = 12s.

(Error bar = 1 std. dev.)

Figure 4: Average of five acquisition experiments per block of 10 CSs for ISIs of
2.5s ( ), 3s (*) and 3.5s ( ). (A) Avg. CR latency. (B) Avg. % effective CRs.

4

Ro b o t a s s o ci a ti v e l ea rn i n g ex p eri men t s

The ?in vivo? learning capability of the chip was evaluated by interfacing it to a
robot and observing its behavior in an unsupervised obstacle avoidance task.
Experiments were performed using a Khepera microrobot (K-team, Lausanne,
Switzerland, Figure 5A) in a circular arena with striped walls (Figure 5C). The
robot was equipped with 6 proximal infra-red (IR) sensors (Figure 5B). Activation
of these sensors (US) due to a collision triggered a turn of ~110? in the opposite
direction (UR). A line camera (64 pixels x 256 gray-levels) constituted the distal
sensor, with detection of a certain spatial frequency (~0.14 periods/degree)
signalling the CS. Visual CSs and collision USs were conveyed to CSpathway and
USpathway on the chip. The activation of CRpathway triggered a motor CR: a 1s
long regression followed by a turn of ~180?. Communication between the chip and
the robot was performed using Matlab on a PC. The control program could be
downloaded to the robot's processor, allowing the robot to act fully autonomously.
In each experiment, the robot was placed in the circular arena exploring its
environment with a constant speed of ~4 cm/s. A spatial frequency CS was
detected at some distance when the robot approached the wall, followed by a
collision with the wall, stimulating the IR sensors and thus triggering a US.
Consequently the CS was correlated with the US, predicting it. The ISIs of these
stimuli were variable, due to noise in sensor sampling, and variations in the angle
at which the robot approached the wall.

Figure 5: (A) Khepera microrobot with aVLSI chip mounted on top. (B) Only the
forward sensors were used during the experiments. (C) The environment: a 60cm
diameter circular arena surrounded by a 15cm high wall. A pattern of vertical,
equally sized black and white bars was placed on the wall.
Associative learning mediated by the cerebellum chip significantly altered the
robot's behavior in the obstacle avoidance task (Figure 6) over the course of each
experiment. In the initial learning phase, the behavior was UR driven: the robot
drove forwards until it collided with the wall, only then performing a turn (Figure
6A1). In the trained phase, the robot usually turned just before it collided with the
wall (Figure 6A2), reducing the number of collisions. The positions of the robot
when a CS, US or CR event occurred in these two phases are shown in Figure 6B1

and B2. The CRs were not expressed immediately after the CSs, but rather with a
CR latency adjusted to just prevent collisions (USs). Not all USs were avoided in
the trained phase due to some excessively short ISIs (Figure 7) and normal
extinction processes over many unreinforced trials. After the learning phase the
percentage of effective CRs fluctuated between 70% and 100% (Figure 7).

Figure 6: Learning performance of the robot. (Top row) Trajectories of the robot.
The white circle with the black dot in the center indicates the beginning of
trajectories. (Bottom row) The same periods of the experiment examined at the
circuit level: = CS, * = US, = CR. (A1, B1) Beginning of the experiment (CS
3-15). (A2, B2) Later in the experiment (CS 32-44).

Figure 7: Trends in learning behavior (average of 5 experiments, 25 min. each). 90
CSs were presented in each experiment. Error bars indicate one standard deviation.
(A) Average percentage of effective CRs over 9 blocks of 10 CSs. (B) Number of
CS occurrences ( ), US occurrences (*) and CR occurrences ( ).

5

Di s cu s s i o n

We have presented one of the first examples of a biologically constrained model of
learning implemented in hardware. Our aVLSI cerebellum chip supports the
acquisition and extinction of adaptively timed responses under noisy, real world

conditions. These results provide further evidence for the role of the cerebellar
circuit embedded in a synaptic feedback loop in the learning of adaptive behavior,
and pave the way for the creation of artefacts with embedded ultra low-power
learning capabilities.

6

Ref eren ces

Fiala, J. C., Grossberg, S. and Bullock, D. (1996). Metabotropic glutamate receptor
activation in cerebellar Purkinje cells as substrate for adaptive timing of the classical
conditioned eye-blink response. Journal of Neuroscience 16: 3760-3774.
Hansel, C., Linden, D. J. and D'Angelo, E. (2001). Beyond parallel fiber LTD, the diversity
of synaptic and nonsynaptic plasticity in the cerebellum. Nature Neuroscience 4: 467-475.
Hesslow, G. (1994). Inhibition of classical conditioned eyeblink response by stimulation of
the cerebellar cortex in decerebrate cat. Journal of Physiology 476: 245-256.
Hofst?tter, C., Mintz, M. and Verschure, P. F. M. J. (2002). The cerebellum in action: a
simulation and robotics study. European Journal of Neuroscience 16: 1361-1376.
Indiveri, G. (2003). A low-power adaptive integrate-and-fire neuron circuit. IEEE
International Symposium on Circuits and Systems, Bangkok, Thailand, 4: 820-823.
Ito, M. (1984). The modifiable neuronal network of the cerebellum. Japanese Journal of
Physiology 5: 781-792.
Kim, J. J. and Thompson, R. F. (1997). Cerebellar circuits and synaptic mechanisms
involved in classical eyeblink conditioning. Trends in the Neurosciences 20(4): 177-181.
Kim, J. J. and Thompson, R. F. (1997). Cerebellar circuits and synaptic mechanisms
involved in classical eyeblink conditioning. Trend. Neurosci. 20: 177-181.
Kramer, J. and Hofst?tter, C. (2002). An aVLSI model of cerebellar mediated associative
learning. Telluride Workshop, CO, USA.
McCormick, D. A. and Thompson, R. F. (1984). Neuronal response of the rabbit cerebellum
during acquisition and performance of a classical conditioned nictitating membrane-eyelid
response. J. Neurosci. 4: 2811-2822.
Pavlov, I. P. (1927). Conditioned Reflexes, Oxford University Press.
Steinmetz, J. E., Lavond, D. G. and Thompson, R. F. (1989). Classical conditioning in
rabbits using pontine nucleus stimulation as a conditioned stimulus and inferior olive
stimulation as an unconditioned stimulus. Synapse 3: 225-233.
Sutton, R. S. and Barto, A. G. (1990). Time derivate models of Pavlovian Reinforcement
Learning and Computational Neuroscience: Foundations of Adaptive Networks., MIT press:
chapter 12, 497-537.
Thompson, R. F., Thompson, J. K., Kim, J. J. and Shinkman, P. G. (1998). The nature of
reinforcement in cerebellar learning. Neurobiology of Learning and Memory 70: 150-176.
Verschure, P. F. M. J. and Mintz, M. (2001). A real-time model of the cerebellar circuitry
underlying classical conditioning: A combined simulation and robotics study.
Neurocomputing 38-40: 1019-1024.

"
2003,Training a Quantum Neural Network,,2363-training-a-quantum-neural-network.pdf,Abstract Missing,"Training a Quantum Neural Network

Bob Ricks
Department of Computer Science
Brigham Young University
Provo, UT 84602
cyberbob@cs.byu.edu

Dan Ventura
Department of Computer Science
Brigham Young University
Provo, UT 84602
ventura@cs.byu.edu

Abstract
Most proposals for quantum neural networks have skipped over the problem of how to train the networks. The mechanics of quantum computing
are different enough from classical computing that the issue of training
should be treated in detail. We propose a simple quantum neural network
and a training method for it. It can be shown that this algorithm works
in quantum systems. Results on several real-world data sets show that
this algorithm can train the proposed quantum neural networks, and that
it has some advantages over classical learning algorithms.

1

Introduction

Many quantum neural networks have been proposed [1], but very few of these proposals
have attempted to provide an in-depth method of training them. Most either do not mention
how the network will be trained or simply state that they use a standard gradient descent
algorithm. This assumes that training a quantum neural network will be straightforward and
analogous to classical methods. While some quantum neural networks seem quite similar
to classical networks [2], others have proposed quantum networks that are vastly different
[3, 4, 5]. Several different network structures have been proposed, including lattices [6]
and dots [4]. Several of these networks also employ methods which are speculative or
difficult to do in quantum systems [7, 8]. These significant differences between classical
networks and quantum neural networks, as well as the problems associated with quantum
computation itself, require us to look more deeply at the issue of training quantum neural
networks. Furthermore, no one has done empirical testing on their training methods to
show that their methods work with real-world problems.
It is an open question what advantages a quantum neural network (QNN) would have over
a classical network. It has been shown that QNNs should have roughly the same computational power as classical networks [7]. Other results have shown that QNNs may work best
with some classical components as well as quantum components [2].
Quantum searches can be proven to be faster than comparable classical searches. We leverage this idea to propose a new training method for a simple QNN. This paper details such a
network and how training could be done on it. Results from testing the algorithm on several
real-world problems show that it works.

2

Quantum Computation

Several necessary ideas that form the basis for the study of quantum computation are briefly
reviewed here. For a good treatment of the subject, see [9].
2.1

Linear Superposition

Linear superposition is closely related to the familiar mathematical principle of linear combination of vectors. Quantum systems are described by a wave function ? that exists in a
Hilbert space. The Hilbert space has a set
P of states, |?i i, that form a basis, and the system
is described by a quantum state |?i = i ci |?i i. |?i is said to be coherent or to be in a
linear superposition of the basis states |?i i, and in general the coefficients ci are complex.
A postulate of quantum mechanics is that if a coherent system interacts in any way with its
environment (by being measured, for example), the superposition is destroyed. This loss
of coherence is governed by the wave function ?. The coefficients ci are called probability
2
amplitudes, and |ci | gives the probability of |?i being measured in the state |?i i . Note
that the wave function ? describes a real physical system that must collapse to exactly one
basis state. Therefore, the probabilities governed by the amplitudes ci must sum to unity. A
two-state quantum system is used as the basic unit of quantum computation. Such a system
is referred to as a quantum bit or qubit and naming the two states |0i and |1i, it is easy to
see why this is so.
2.2

Operators

Operators on a Hilbert space describe how one wave function is changed into another and
they may be represented as matrices acting on vectors (the notation |?i indicates a column
vector and the h?| a [complex conjugate] row vector). Using operators, an eigenvalue equation can be written A |?i i = ai |?i i, where ai is the eigenvalue. The solutions |?i i to such
an equation are called eigenstates and can be used to construct the basis of a Hilbert space
as discussed in Section 2.1. In the quantum formalism, all properties are represented as operators whose eigenstates are the basis for the Hilbert space associated with that property
and whose eigenvalues are the quantum allowed values for that property. It is important
to note that operators in quantum mechanics must be linear operators and further that they
must be unitary.
2.3

Interference

Interference is a familiar wave phenomenon. Wave peaks that are in phase interfere constructively while those that are out of phase interfere destructively. This is a phenomenon
common to all kinds of wave mechanics from water waves to optics. The well known
double slit experiment demonstrates empirically that at the quantum level interference also
applies to the probability waves of quantum mechanics. The wave function interferes with
itself through the action of an operator ? the different parts of the wave function interfere
constructively or destructively according to their relative phases just like any other kind of
wave.
2.4

Entanglement

Entanglement is the potential for quantum systems to exhibit correlations that cannot be
accounted for classically. From a computational standpoint, entanglement seems intuitive
enough ? it is simply the fact that correlations can exist between different qubits ? for example if one qubit is in the |1i state, another will be in the |1i state. However, from a physical
standpoint, entanglement is little understood. The questions of what exactly it is and how

it works are still not resolved. What makes it so powerful (and so little understood) is the
fact that since quantum states exist as superpositions, these correlations exist in superposition as well. When coherence is lost, the proper correlation is somehow communicated
between the qubits, and it is this ?communication? that is the crux of entanglement. Mathematically, entanglement may be described using the density matrix formalism. The density
matrix ?? of a quantum state |?i is defined as ?? = |?i h?| For example, the quantum
? ?
1
1
1
1 ? 1 ?
?
?
?
state |?i = 2 |00i + 2 |01i appears in vector form as |?i = 2 ? ? and it may
0
0
?
?
1 1 0 0
? 1 1 0 0 ?
also be represented as the density matrix ?? = |?i h?| = 21 ?
while the
0 0 0 0 ?
0 0 0 0
?
?
1 0 0 1
? 0 0 0 0 ?
state |?i = ?12 |00i + ?12 |11i is represented as ?? = |?i h?| = 12 ?
0 0 0 0 ?
1 0 0 1
where the matrices and vectors
are
indexed
by
the
state
labels
00,...,
11.
Notice that ??

 

1
0
1
1
?
where ? is the normal tensor
can be factorized as ?? = 12
0 0
1 1
product. On the other hand, ?? can not be factorized. States that can not be factorized are
said to be entangled, while those that can be factorized are not. There are different degrees
of entanglement and much work has been done on better understanding and quantifying it
[10, 11]. Finally, it should be mentioned that while interference is a quantum property that
has a classical cousin, entanglement is a completely quantum phenomenon for which there
is no classical analog. It has proven to be a powerful computational resource in some cases
and a major hindrance in others.
To summarize, quantum computation can be defined as representing the problem to be
solved in the language of quantum states and then producing operators that drive the system
(via interference and entanglement) to a final state such that when the system is observed
there is a high probability of finding a solution.
2.5

An Example ? Quantum Search

One of the best known quantum algorithms searches an unordered database quadratically
faster than any classical method [12, 13]. The algorithm begins with a superposition of
all N data items and depends upon an oracle that can recognize the target of the search.
Classically, searching such a database requires
O(N ) oracle calls; however, on a quan?
tum computer, the task requires only O( N ) oracle calls. Each oracle call consists of a
quantum operator that inverts the phase of the search target. An ?inversion
about average?
?
operator then shifts amplitude towards the target state. After ?/4 ? N repetitions of this
process, the system is measured and with high probability, the desired datum is the result.

3

A Simple Quantum Neural Network

We would like a QNN with features that make it easy for us to model, yet powerful enough
to leverage quantum physics. We would like our QNN to:
? use known quantum algorithms and gates
? have weights which we can measure for each node

? work in classical simulations of reasonable size
? be able to transfer knowledge to classical systems
We propose a QNN that operates much like a classical ANN composed of several layers
of perceptrons ? an input layer, one or more hidden layers and an output layer. Each layer
is fully connected to the previous layer. Each hidden layer computes a weighted sum of
the outputs of the previous layer. If this is sum above a threshold, the node goes high,
otherwise it stays low. The output layer does the same thing as the hidden layer(s), except
that it also checks its accuracy against the target output of the network. The network as a
whole computes a function by checking which output bit is high. There are no checks to
make sure exactly one output is high. This allows the network to learn data sets which have
one output high or binary-encoded outputs.

Figure 1: Simple QNN to compute XOR function
The QNN in Figure 1 is an example of such a network, with sufficient complexity to compute the XOR function. Each input node i is represented by a register, |?ii . The two hidden
nodes compute a weighted sum of the inputs, |?ii1 and |?ii2 , and compare the sum to a
threshold weight, |?ii0 . If the weighted sum is greater than the threshold the node goes
high. The |?ik represent internal calculations that take place at each node. The output layer
works similarly, taking a weighted sum of the hidden nodes and checking against a threshold. The QNN then checks each computed output and compares it to the target output, |?ij
sending |?ij high when they are equivalent. The performance of the network is denoted
by |?i, which is the number of computed outputs equivalent to their corresponding target
output.
At the quantum gate level, the network will require O(blm + m2 ) gates for each node of
the network. Here b is the number of bits used for floating point arithmetic in |?i, l is the
number of bits for each weight and m is the number of inputs to the node [14]-[15].
The overall network works as follows on a training set. In our example, the network has
two input parameters, so all n training examples will have two input registers. These are
represented as |?i11 to |?in2 . The target answers are kept in registers |?i11 to |?in2 . Each
hidden or output node has a weight vector, represented by |?ii , each vector containing
weights for each of its inputs. After classifying a training example, the registers |?i1 and
|?i2 reflect the networks ability to classify that the training example. As a simple measure
of performance, we increment |?i by the sum of all |?ii . When all training examples have

Figure 2: QNN Training

been classified, |?i will be the sum of the output nodes that have the correct answer throughout the training set and will range between zero and the number of training examples times
the number of output nodes.

4

Using Quantum Search to Learn Network Weights

One possibility for training this kind of a network is to search through the possible weight
vectors for one which is consistent with the training data. Quantum searches have been
used already in quantum learning [16] and many of the problems associated with them
have already been explored [17]. We would like to find a solution which classifies all
training examples correctly; in other words we would like |?i = n ? m where n is the
number of training examples and m is the number of output nodes. Since we generally do
not know how many weight vectors will do this, we use a generalization of the original
search algorithm [18], intended for problems where the number of solutions t is unknown.
The basic idea is that we will put |?i into a superposition of all possible weight vectors and
search for one which classifies all training examples correctly.
We start out with |?i as a superposition of all possible weight vectors. All other registers
(|?i, |?i, |?i), besides the inputs and target outputs are initialized to the state |0i. We
then classify each training example, updating the performance register, |?i. By using a superposition we classify the training examples with respect to every possible weight vector
simultaneously. Each weight vector is now entangled with |?i in such a way that |?i corresponds with how well every weight vector classifies all the training data. In this case, the
oracle for the quantum search is |?i = n ? m, which corresponds to searching for a weight
vector which correctly classifies the entire set.
Unfortunately, searching the weight vectors while entangled with |?i would cause unwanted weight vectors to grow that would be entangled with the performance metric we
are looking for. The solution is to disentangle |?i from the other registers after inverting
the phase of those weights which match the search criteria, based on |?i. To do this the
entire network will need to be uncomputed, which will unentangle all the registers and set
them back to their initial values. This means that the network will need to be recomputed

each time we make an oracle call and after each measurement.
There are at least two things about this algorithm that are undesirable. First, not all training
data will have any solution networks that correctly classify all training instances. This
means that nothing will be marked by the search oracle, so every weight vector will have
an equal chance of being measured. It is also possible that even when a solution does
exist, it is not desirable because it over fits the training data. Second, p
the amount of time
needed to find a vector which correctly classifies the training set is O( 2b /t), which has
exponential complexity with respect to the number of bits in the weight vector.
One way to deal with the first problem is to search until we find a solution which covers an
acceptable percentage, p, of the training data. In other words, the search oracle is modified
to be |?i ? n ? m ? p. The second problem is addressed in the next section.

5

Piecewise Weight Learning

Our quantum search algorithm gives us a good polynomial speed-up to the exponential task
of finding a solution to the QNN. This algorithm does not scale well, in fact it is exponential
in the total number of weights in the network and the bits per weight. Therefore, we propose
a randomized training algorithm which searches each node?s weight vector independently.
The network starts off, once again, with training examples in |?i, the corresponding answers in |?i, and zeros in all the other registers. A node is randomly selected and its
weight vector, |?ii , is put into superposition. All other weight vectors start with random
classical initial weights. We then search for a weight vector for this node that causes the
entire network to classify a certain percentage, p, of the training examples correctly. This is
repeated, iteratively decreasing p, until a new weight vector is found. That weight is fixed
classically and the process is repeated randomly for the other nodes.
Searching each node?s weight vector separately is, in effect, a random search through the
weight space where we select weight vectors which give a good level of performance for
each node. Each node takes on weight vectors that tend to increase performance with some
amount of randomness that helps keep it out of local minima. This search can be terminated
when an acceptable level of performance has been reached.
There are a few improvements to the basic design which help speed convergence. First,
to insure that hidden nodes find weight vectors that compute something useful, a small
performance penalty is added to weight vectors which cause a hidden node to output the
same value for all training examples. This helps select weight vectors which contain useful
information for the output nodes. Since each output node?s performance is independent
of the performance or all output nodes, the algorithm only considers the accuracy of the
output node being trained when training an output node.

6

Results

We first consider the canonical XOR problem. Each of the hidden and the output nodes
are thresholded nodes with three weights, one for each input and one for the threshold. For
each weight 2 bits are used. Quantum search did well on this problem, finding a solution
in an average of 2.32 searches.
The randomized search algorithm also did well on the XOR problem. After an average of
58 weight updates, the algorithm was able to correctly classify the training data. Since this
is a randomized algorithm both in the number of iterations of the search algorithm before
measuring and in the order which nodes update their weight vectors, the standard deviation
for this method was much higher, but still reasonable. In the randomized search algorithm,

an epoch refers to finding and fixing the weight of a single node.
We also tried the randomized search algorithm for a few real-world machine learning problems: lenses, Hayes-Roth and the iris datasets [19]. The lenses data set is a data set that
tries to predict whether people will need soft contact lenses, hard contact lenses or no contacts. The iris dataset details features of three different classes of irises. The Hayes-Roth
dataset classifies people into different classes depending several attributes.

Data Set
Iris
Lenses
Hayes-Roth

# Weight
Qubits
32
42
68

Epochs
23,000
22,500
5 ? 106

Weight
Updates
225
145
9,200

Output
Accuracy
98.23%
98.35%
88.76%

Training
Accuracy
97.79%
100.0%
82.98%

Backprop
96%
92%
83%

Table 1: Training Results
The lenses data set can be solved with a network that has three hidden nodes. After between
a few hundred to a few thousand iterations it usually finds a solution. This may be because
it has a hard time with 2 bit weights, or because it is searching for perfect accuracy. The
number of times a weight was fixed and updated was only 225 for this data set. The iris data
set was normalized so that each input had a value between zero and one. The randomized
search algorithm found the correct target for 97.79% of the output nodes.
Our results for the Hayes-Roth problem were also quite good. We used four hidden nodes
with two bit weights for the hidden nodes. We had to normalize the inputs to range
from zero to one once again so the larger inputs would not dominate the weight vectors.
The algorithm found the correct target for 88.86% of the output nodes correctly in about
5,000,000 epochs. Note that this does not mean that it classified 88.86% of the training
examples correctly since we are checking each output node for accuracy on each training example. The algorithm actually classified 82.98% of the training set correctly, which
compares well with backpropagation?s 83% [20].

7

Conclusions and Future Work

This paper proposes a simple quantum neural network and a method of training it which
works well in quantum systems. By using a quantum search we are able to use a wellknown algorithm for quantum systems which has already been used for quantum learning.
The algorithm is able to search for solutions that cover an arbitrary percentage of the training set. This could be very useful for problems which require a very accurate solution. The
drawback is that it is an exponential algorithm, even with the significant quadratic speedup.
A randomized version avoids some of the exponential increases in complexity with problem
size. This algorithm is exponential in the number of qubits of each node?s weight vector
instead of in the composite weight vector of the entire network. This means the complexity
of the algorithm increases with the number of connections to a node and the precision of
each individual weight, dramatically decreasing complexity for problems with large numbers of nodes. This could be a great improvement for larger problems. Preliminary results
for both algorithms have been very positive.
There may be quantum methods which could be used to improve current gradient descent
and other learning algorithms. It may also be possible to combine some of these with a
quantum search. An example would be to use gradient descent to try and refine a composite weight vector found by quantum search. Conversely, a quantum search could start with
the weight vector of a gradient descent search. This would allow the search to start with an

accurate weight vector and search locally for weight vectors which improve overall performance. Finally the two methods could be used simultaneously to try and take advantage of
the benefits of each technique.
Other types of QNNs may be able to use a quantum search as well since the algorithm
only requires a weight space which can be searched in superposition. In addition, more
traditional gradient descent techniques might benefit from a quantum speed-up themselves.

References
[1] Alexandr Ezhov and Dan Ventura. Quantum neural networks. In Ed. N. Kasabov, editor, Future
Directions for Intelligent Systems and Information Science. Physica-Verlang, 2000.
[2] Ajit Narayanan and Tammy Menneer. Quantum artificial neural network architectures and
components. In Information Sciences, volume 124 nos. 1-4, pages 231?255, 2000.
[3] M. V. Altaisky. Quantum neural network. Technical report, 2001. http://xxx.lanl.gov/quantph/0107012.
[4] E. C. Behrman, J. Niemel, J. E. Steck, and S. R. Skinner. A quantum dot neural network. In
Proceedings of the 4th Workshop on Physics of Computation, pages 22?24. Boston, 1996.
[5] Fariel Shafee.
Neural networks with c-not gated nodes.
Technical report, 2002.
http://xxx.lanl.gov/quant-ph/0202016.
[6] Yukari Fujita and Tetsuo Matsui. Quantum gauged neural network: U(1) gauge theory. Technical report, 2002. http://xxx.lanl.gov/cond-mat/0207023.
[7] S. Gupta and R. K. P. Zia. Quantum neural networks. In Journal of Computer and System
Sciences, volume 63 No. 3, pages 355?383, 2001.
[8] E. C. Behrman, V. Chandrasheka, Z. Wank, C. K. Belur, J. E. Steck, and S. R. Skinner. A quantum neural network computes entanglement. Technical report, 2002. http://xxx.lanl.gov/quantph/0202131.
[9] Michael A. Nielsen and Isaac L. Chuang. Quantum computation and quantum information.
Cambridge University Press, 2000.
[10] V. Vedral, M. B. Plenio, M. A. Rippin, and P. L. Knight. Quantifying entanglement. In Physical
Review Letters, volume 78(12), pages 2275?2279, 1997.
[11] R. Jozsa. Entanglement and quantum computation. In S. Hugget, L. Mason, K.P. Tod, T. Tsou,
and N.M.J. Woodhouse, editors, The Geometric Universe, pages 369?379. Oxford University
Press, 1998.
[12] Lov K. Grover. A fast quantum mechanical algorithm for database search. In Proceedings of
the 28th ACM STOC, pages 212?219, 1996.
[13] Lov K. Grover. Quantum mechanics helps in searching for a needle in a haystack. In Physical
Review Letters, volume 78, pages 325?328, 1997.
[14] Peter Shor. Polynomial-time algorithms for prime factorization and discrete logarithms on a
quantum computer. In SIAM Journal of Computing, volume 26 no. 5, pages 1484?1509, 1997.
[15] Vlatko Vedral, Adriano Barenco, and Artur Ekert. Quantum networks for elementary arithmetic
operations. In Physical Review A, volume 54 no. 1, pages 147?153, 1996.
[16] Dan Ventura and Tony Martinez. Quantum associative memory. In Information Sciences, volume 124 nos. 1-4, pages 273?296, 2000.
[17] Alexandr Ezhov, A. Nifanova, and Dan Ventura. Distributed queries for quantum associative
memory. In Information Sciences, volume 128 nos. 3-4, pages 271?293, 2000.
[18] Michel Boyer, Gilles Brassard, Peter H?yer, and Alain Tapp. Tight bounds on quantum searching. In Proceedings of the Fourth Workshop on Physics and Computation, pages 36?43, 1996.
[19] C.L. Blake and C.J. Merz.
UCI repository of machine learning databases, 1998.
http://www.ics.uci.edu/?mlearn/MLRepository.html.
[20] Frederick Zarndt. A comprehensive case study: An examination of machine learning and connectionist algorithms. Master?s thesis, Brigham Young University, 1995.

"
2016,Improved Techniques for Training GANs,Poster,6125-improved-techniques-for-training-gans.pdf,"We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: Our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.","Improved Techniques for Training GANs

Tim Salimans
tim@openai.com

Ian Goodfellow
ian@openai.com

Wojciech Zaremba
woj@openai.com

Alec Radford
alec@openai.com

Vicki Cheung
vicki@openai.com

Xi Chen
peter@openai.com

Abstract
We present a variety of new architectural features and training procedures that we
apply to the generative adversarial networks (GANs) framework. Using our new
techniques, we achieve state-of-the-art results in semi-supervised classification on
MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans
cannot distinguish from real data, and CIFAR-10 samples that yield a human error
rate of 21.3%. We also present ImageNet samples with unprecedented resolution
and show that our methods enable the model to learn recognizable features of
ImageNet classes.

1

Introduction

Generative adversarial networks [1] (GANs) are a class of methods for learning generative models
based on game theory. The goal of GANs is to train a generator network G(z; ? (G) ) that produces
samples from the data distribution, pdata (x), by transforming vectors of noise z as x = G(z; ? (G) ).
The training signal for G is provided by a discriminator network D(x) that is trained to distinguish
samples from the generator distribution pmodel (x) from real data. The generator network G in turn
is then trained to fool the discriminator into accepting its outputs as being real.
Recent applications of GANs have shown that they can produce excellent samples [2, 3]. However,
training GANs requires finding a Nash equilibrium of a non-convex game with continuous, highdimensional parameters. GANs are typically trained using gradient descent techniques that are
designed to find a low value of a cost function, rather than to find the Nash equilibrium of a game.
When used to seek for a Nash equilibrium, these algorithms may fail to converge [4].
In this work, we introduce several techniques intended to encourage convergence of the GANs game.
These techniques are motivated by a heuristic understanding of the non-convergence problem. They
lead to improved semi-supervised learning peformance and improved sample generation. We hope
that some of them may form the basis for future work, providing formal guarantees of convergence.
All code and hyperparameters may be found at https://github.com/openai/improved-gan.

2

Related work

Several recent papers focus on improving the stability of training and the resulting perceptual quality
of GAN samples [2, 3, 5, 6]. We build on some of these techniques in this work. For instance, we
use some of the ?DCGAN? architectural innovations proposed in Radford et al. [3], as discussed
below.
One of our proposed techniques, feature matching, discussed in Sec. 3.1, is similar in spirit to
approaches that use maximum mean discrepancy [7, 8, 9] to train generator networks [10, 11].
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

Another of our proposed techniques, minibatch features, is based in part on ideas used for batch
normalization [12], while our proposed virtual batch normalization is a direct extension of batch
normalization.
One of the primary goals of this work is to improve the effectiveness of generative adversarial
networks for semi-supervised learning (improving the performance of a supervised task, in this case,
classification, by learning on additional unlabeled examples). Like many deep generative models,
GANs have previously been applied to semi-supervised learning [13, 14], and our work can be seen
as a continuation and refinement of this effort. In concurrent work, Odena [15] proposes to extend
GANs to predict image labels like we do in Section 5, but without our feature matching extension
(Section 3.1) which we found to be critical for obtaining state-of-the-art performance.

3

Toward Convergent GAN Training

Training GANs consists in finding a Nash equilibrium to a two-player non-cooperative game.
Each player wishes to minimize its own cost function, J (D) (? (D) , ? (G) ) for the discriminator and
J (G) (? (D) , ? (G) ) for the generator. A Nash equilibirum is a point (? (D) , ? (G) ) such that J (D) is at a
minimum with respect to ? (D) and J (G) is at a minimum with respect to ? (G) . Unfortunately, finding Nash equilibria is a very difficult problem. Algorithms exist for specialized cases, but we are not
aware of any that are feasible to apply to the GAN game, where the cost functions are non-convex,
the parameters are continuous, and the parameter space is extremely high-dimensional.
The idea that a Nash equilibrium occurs when each player has minimal cost seems to intuitively motivate the idea of using traditional gradient-based minimization techniques to minimize each player?s
cost simultaneously. Unfortunately, a modification to ? (D) that reduces J (D) can increase J (G) , and
a modification to ? (G) that reduces J (G) can increase J (D) . Gradient descent thus fails to converge
for many games. For example, when one player minimizes xy with respect to x and another player
minimizes ?xy with respect to y, gradient descent enters a stable orbit, rather than converging to
x = y = 0, the desired equilibrium point [16]. Previous approaches to GAN training have thus
applied gradient descent on each player?s cost simultaneously, despite the lack of guarantee that this
procedure will converge. We introduce the following techniques that are heuristically motivated to
encourage convergence:
3.1

Feature matching

Feature matching addresses the instability of GANs by specifying a new objective for the generator
that prevents it from overtraining on the current discriminator. Instead of directly maximizing the
output of the discriminator, the new objective requires the generator to generate data that matches
the statistics of the real data, where we use the discriminator only to specify the statistics that we
think are worth matching. Specifically, we train the generator to match the expected value of the
features on an intermediate layer of the discriminator. This is a natural choice of statistics for the
generator to match, since by training the discriminator we ask it to find those features that are most
discriminative of real data versus data generated by the current model.
Letting f (x) denote activations on an intermediate layer of the discriminator, our new objective for
the generator is defined as: ||Ex?pdata f (x) ? Ez?pz (z) f (G(z))||22 . The discriminator, and hence
f (x), are trained in the usual way. As with regular GAN training, the objective has a fixed point
where G exactly matches the distribution of training data. We have no guarantee of reaching this
fixed point in practice, but our empirical results indicate that feature matching is indeed effective in
situations where regular GAN becomes unstable.
3.2

Minibatch discrimination

One of the main failure modes for GAN is for the generator to collapse to a parameter setting where
it always emits the same point. When collapse to a single mode is imminent, the gradient of the
discriminator may point in similar directions for many similar points. Because the discriminator
processes each example independently, there is no coordination between its gradients, and thus no
mechanism to tell the outputs of the generator to become more dissimilar to each other. Instead,
all outputs race toward a single point that the discriminator currently believes is highly realistic.
After collapse has occurred, the discriminator learns that this single point comes from the generator,
but gradient descent is unable to separate the identical outputs. The gradients of the discriminator

2

then push the single point produced by the generator around space forever, and the algorithm cannot
converge to a distribution with the correct amount of entropy. An obvious strategy to avoid this type
of failure is to allow the discriminator to look at multiple data examples in combination, and perform
what we call minibatch discrimination.
The concept of minibatch discrimination is quite general: any discriminator model that looks
at multiple examples in combination, rather than in isolation, could potentially help avoid collapse of the generator. In fact, the successful application of batch normalization in the discriminator by Radford et al. [3] is well explained from this perspective. So far, however, we
have restricted our experiments to models that explicitly aim to identify generator samples that
are particularly close together. One successful specification for modelling the closeness between
examples in a minibatch is as follows: Let f (xi ) ? RA denote a vector of features for input xi , produced by some intermediate layer in the discriminator. We then multiply the vector
f (xi ) by a tensor T ? RA?B?C , which results in a matrix Mi ? RB?C . We then compute
the L1 -distance between the rows of the resulting matrix Mi across samples i ? {1, 2, . . . , n}
and apply a negative exponential (Fig. 1): cb (xi , xj ) = exp(?||Mi,b ? Mj,b ||L1 ) ? R.
The output o(xi ) for this minibatch layer for a sample xi
is then defined as the sum of the cb (xi , xj )?s to all other
samples:
n
X
o(xi )b =
cb (xi , xj ) ? R
j=1

h

i
o(xi ) = o(xi )1 , o(xi )2 , . . . , o(xi )B ? RB
o(X) ? Rn?B
Next, we concatenate the output o(xi ) of the minibatch
layer with the intermediate features f (xi ) that were its Figure 1: Figure sketches how miniinput, and we feed the result into the next layer of the batch discrimination works. Features
discriminator. We compute these minibatch features sep- f (xi ) from sample xi are multiplied
arately for samples from the generator and from the train- through a tensor T , and cross-sample
ing data. As before, the discriminator is still required to distance is computed.
output a single number for each example indicating how
likely it is to come from the training data: The task of the discriminator is thus effectively still to
classify single examples as real data or generated data, but it is now able to use the other examples in
the minibatch as side information. Minibatch discrimination allows us to generate visually appealing
samples very quickly, and in this regard it is superior to feature matching (Section 6). Interestingly,
however, feature matching was found to work much better if the goal is to obtain a strong classifier
using the approach to semi-supervised learning described in Section 5.
3.3

Historical averaging

Pt
When applying this technique, we modify each player?s cost to include a term ||? ? 1t i=1 ?[i]||2 ,
where ?[i] is the value of the parameters at past time i. The historical average of the parameters can
be updated in an online fashion so this learning rule scales well to long time series. This approach is
loosely inspired by the fictitious play [17] algorithm that can find equilibria in other kinds of games.
We found that our approach was able to find equilibria of low-dimensional, continuous non-convex
games, such as the minimax game with one player controlling x, the other player controlling y, and
value function (f (x) ? 1)(y ? 1), where f (x) = x for x < 0 and f (x) = x2 otherwise. For
these same toy games, gradient descent fails by going into extended orbits that do not approach the
equilibrium point.
3.4

One-sided label smoothing

Label smoothing, a technique from the 1980s recently independently re-discovered by Szegedy et.
al [18], replaces the 0 and 1 targets for a classifier with smoothed values, like .9 or .1, and was
recently shown to reduce the vulnerability of neural networks to adversarial examples [19].
Replacing positive classification targets with ? and negative targets with ?, the optimal discriminator
data (x)+?pmodel (x)
becomes D(x) = ?ppdata
(x)+pmodel (x) . The presence of pmodel in the numerator is problematic
because, in areas where pdata is approximately zero and pmodel is large, erroneous samples from

3

pmodel have no incentive to move nearer to the data. We therefore smooth only the positive labels to
?, leaving negative labels set to 0.
3.5

Virtual batch normalization

Batch normalization greatly improves optimization of neural networks, and was shown to be highly
effective for DCGANs [3]. However, it causes the output of a neural network for an input example
x to be highly dependent on several other inputs x0 in the same minibatch. To avoid this problem
we introduce virtual batch normalization (VBN), in which each example x is normalized based on
the statistics collected on a reference batch of examples that are chosen once and fixed at the start
of training, and on x itself. The reference batch is normalized using only its own statistics. VBN is
computationally expensive because it requires running forward propagation on two minibatches of
data, so we use it only in the generator network.

4

Assessment of image quality

Generative adversarial networks lack an objective function, which makes it difficult to
compare performance of different models.
One intuitive metric of performance can be
obtained by having human annotators judge the visual quality of samples [2].
We
automate this process using Amazon Mechanical Turk (MTurk), using the web interface in figure Fig. 2 (live at http://infinite-chamber-35121.herokuapp.com/
cifar-minibatch/), which we use to ask annotators to distinguish between generated data
and real data. The resulting quality assessments of our models are described in Section 6.

Figure 2: Web interface given to annotators. Annotators are asked to distinguish computer generated images from
real ones.

A downside of using human annotators is that the metric
varies depending on the setup of the task and the motivation of the annotators. We also find that results change
drastically when we give annotators feedback about their
mistakes: By learning from such feedback, annotators are
better able to point out the flaws in generated images, giving a more pessimistic quality assessment. The left column of Fig. 2 presents a screen from the annotation process, while the right column shows how we inform annotators about their mistakes.

As an alternative to human annotators, we propose an automatic method to evaluate samples, which
we find to correlate well with human evaluation: We apply the Inception model1 [20] to every
generated image to get the conditional label distribution p(y|x). Images that contain meaningful
objects should have a conditional label distribution p(y|x)
with low entropy. Moreover, we expect
R
the model to generate varied images, so the marginal p(y|x = G(z))dz should have high entropy.
Combining these two requirements, the metric that we propose is: exp(Ex KL(p(y|x)||p(y))), where
we exponentiate results so the values are easier to compare. Our Inception score is closely related
to the objective used for training generative models in CatGAN [14]: Although we had less success
using such an objective for training, we find it is a good metric for evaluation that correlates very
well with human judgment. We find that it?s important to evaluate the metric on a large enough
number of samples (i.e. 50k) as part of this metric measures diversity.

5

Semi-supervised learning

Consider a standard classifier for classifying a data point x into one of K possible classes. Such
a model takes in x as input and outputs a K-dimensional vector of logits {l1 , . . . , lK }, that can
exp(lj )
be turned into class probabilities by applying the softmax: pmodel (y = j|x) = PK exp(l
. In
k)
k=1
supervised learning, such a model is then trained by minimizing the cross-entropy between the
observed labels and the model predictive distribution pmodel (y|x).
1
We use the pretrained Inception model from http://download.tensorflow.org/models/image/
imagenet/inception-2015-12-05.tgz. Code to compute the Inception score with this model will be made
available by the time of publication.

4

We can do semi-supervised learning with any standard classifier by simply adding samples from
the GAN generator G to our data set, labeling them with a new ?generated? class y = K + 1, and
correspondingly increasing the dimension of our classifier output from K to K + 1. We may then
use pmodel (y = K + 1 | x) to supply the probability that x is fake, corresponding to 1 ? D(x) in
the original GAN framework. We can now also learn from unlabeled data, as long as we know that
it corresponds to one of the K classes of real data by maximizing log pmodel (y ? {1, . . . , K}|x).
Assuming half of our data set consists of real data and half of it is generated (this is arbitrary), our
loss function for training the classifier then becomes
L = ?Ex,y?pdata (x,y) [log pmodel (y|x)] ? Ex?G [log pmodel (y = K + 1|x)]
= Lsupervised + Lunsupervised , where
Lsupervised = ?Ex,y?pdata (x,y) log pmodel (y|x, y < K + 1)
Lunsupervised = ?{Ex?pdata (x) log[1 ? pmodel (y = K + 1|x)] + Ex?G log[pmodel (y = K + 1|x)]},
where we have decomposed the total cross-entropy loss into our standard supervised loss function
Lsupervised (the negative log probability of the label, given that the data is real) and an unsupervised
loss Lunsupervised which is in fact the standard GAN game-value as becomes evident when we substitute D(x) = 1 ? pmodel (y = K + 1|x) into the expression:
Lunsupervised = ?{Ex?pdata (x) log D(x) + Ez?noise log(1 ? D(G(z)))}.
The optimal solution for minimizing both Lsupervised and Lunsupervised is to have
exp[lj (x)] = c(x)p(y=j, x)?j<K+1 and exp[lK+1 (x)] = c(x)pG (x) for some undetermined scaling function c(x). The unsupervised loss is thus consistent with the supervised loss in
the sense of Sutskever et al. [13], and we can hope to better estimate this optimal solution from
the data by minimizing these two loss functions jointly. In practice, Lunsupervised will only help if
it is not trivial to minimize for our classifier and we thus need to train G to approximate the data
distribution. One way to do this is by training G to minimize the GAN game-value, using the
discriminator D defined by our classifier. This approach introduces an interaction between G and
our classifier that we do not fully understand yet, but empirically we find that optimizing G using
feature matching GAN works very well for semi-supervised learning, while training G using GAN
with minibatch discrimination does not work at all. Here we present our empirical results using this
approach; developing a full theoretical understanding of the interaction between D and G using this
approach is left for future work.
Finally, note that our classifier with K + 1 outputs is over-parameterized: subtracting a general
function f (x) from each output logit, i.e. setting lj (x) ? lj (x) ? f (x)?j, does not change the
output of the softmax. This means we may equivalently fix lK+1 (x) = 0?x, in which case Lsupervised
becomes the standard supervised loss function of our original
classifier with K classes, and our
PK
Z(x)
discriminator D is given by D(x) = Z(x)+1
, where Z(x) = k=1 exp[lk (x)].
5.1

Importance of labels for image quality

Besides achieving state-of-the-art results in semi-supervised learning, the approach described above
also has the surprising effect of improving the quality of generated images as judged by human
annotators. The reason appears to be that the human visual system is strongly attuned to image
statistics that can help infer what class of object an image represents, while it is presumably less
sensitive to local statistics that are less important for interpretation of the image. This is supported
by the high correlation we find between the quality reported by human annotators and the Inception
score we developed in Section 4, which is explicitly constructed to measure the ?objectness? of a
generated image. By having the discriminator D classify the object shown in the image, we bias it to
develop an internal representation that puts emphasis on the same features humans emphasize. This
effect can be understood as a method for transfer learning, and could potentially be applied much
more broadly. We leave further exploration of this possibility for future work.

5

6

Experiments

We performed semi-supervised experiments on MNIST, CIFAR-10 and SVHN, and sample generation experiments on MNIST, CIFAR-10, SVHN and ImageNet. We provide code to reproduce the
majority of our experiments.
6.1

MNIST

The MNIST dataset contains 60, 000 labeled
images of digits. We perform semi-supervised
training with a small randomly picked fraction
of these, considering setups with 20, 50, 100,
and 200 labeled examples. Results are averaged
over 10 random subsets of labeled data, each
chosen to have a balanced number of examples
from each class. The remaining training images
are provided without labels. Our networks have
5 hidden layers each. We use weight normalization [21] and add Gaussian noise to the output Figure 3: (Left) samples generated by model durof each layer of the discriminator. Table 1 sum- ing semi-supervised training. Samples can be
marizes our results.
clearly distinguished from images coming from
Samples generated by the generator during MNIST dataset. (Right) Samples generated with
semi-supervised learning using feature match- minibatch discrimination. Samples are coming (Section 3.1) do not look visually appealing pletely indistinguishable from dataset images.
(left Fig. 3). By using minibatch discrimination
instead (Section 3.2) we can improve their visual quality. On MTurk, annotators were able to distinguish samples in 52.4% of cases (2000 votes total), where 50% would be obtained by random
guessing. Similarly, researchers in our institution were not able to find any artifacts that would allow them to distinguish samples. However, semi-supervised learning with minibatch discrimination
does not produce as good a classifier as does feature matching.
Model
20
DGN [22]
Virtual Adversarial [23]
CatGAN [14]
Skip Deep Generative Model [24]
Ladder network [25]
Auxiliary Deep Generative Model [24]
Our model
Ensemble of 10 of our models

Number of incorrectly predicted test examples
for a given number of labeled samples
50
100

1677 ? 452
1134 ? 445

221 ? 136
142 ? 96

333 ? 14
212
191 ? 10
132 ? 7
106 ? 37
96 ? 2
93 ? 6.5
86 ? 5.6

200

90 ? 4.2
81 ? 4.3

Table 1: Number of incorrectly classified test examples for the semi-supervised setting on permutation invariant MNIST. Results are averaged over 10 seeds.

6.2

CIFAR-10
Model
1000
Ladder network [25]
CatGAN [14]
Our model
Ensemble of 10 of our models

21.83?2.01
19.22?0.54

Test error rate for
a given number of labeled samples
2000
4000

19.61?2.09
17.25?0.66

20.40?0.47
19.58?0.46
18.63?2.32
15.59?0.47

8000

17.72?1.82
14.87?0.89

Table 2: Test error on semi-supervised CIFAR-10. Results are averaged over 10 splits of data.
CIFAR-10 is a small, well studied dataset of 32 ? 32 natural images. We use this data set to study
semi-supervised learning, as well as to examine the visual quality of samples that can be achieved.
For the discriminator in our GAN we use a 9 layer deep convolutional network with dropout and
weight normalization. The generator is a 4 layer deep CNN with batch normalization. Table 2
summarizes our results on the semi-supervised learning task.
6

Figure 4: Samples generated during semi-supervised training on CIFAR-10 with feature matching
(Section 3.1, left) and minibatch discrimination (Section 3.2, right).

When presented with 50% real and 50% fake data generated by our best CIFAR-10 model, MTurk
users correctly categorized 78.7% of images correctly. However, MTurk users may not be sufficiently familiar with CIFAR-10 images or sufficiently motivated; we ourselves were able to categorize images with > 95% accuracy. We validated the Inception score described above by observing
that MTurk accuracy drops to 71.4% when the data is filtered by using only the top 1% of samples
according to the Inception score. We performed a series of ablation experiments to demonstrate that
our proposed techniques improve the Inception score, presented in Table 3. We also present images
for these ablation experiments?in our opinion, the Inception score correlates well with our subjective judgment of image quality. Samples from the dataset achieve the highest value. All the models
that even partially collapse have relatively low scores. We caution that the Inception score should be
used as a rough guide to evaluate models that were trained via some independent criterion; directly
optimizing Inception score will lead to the generation of adversarial examples [26].

Samples
Model
Score ? std.

Real data
11.24 ? .12

Our methods
8.09 ? .07

-VBN+BN
7.54 ? .07

-L+HA
6.86 ? .06

-LS
6.83 ? .06

-L
4.36 ? .04

-MBF
3.87 ? .03

Table 3: Table of Inception scores for samples generated by various models for 50, 000 images.
Score highly correlates with human judgment, and the best score is achieved for natural images.
Models that generate collapsed samples have relatively low score. This metric allows us to avoid
relying on human evaluations. ?Our methods? includes all the techniques described in this work,
except for feature matching and historical averaging. The remaining experiments are ablation experiments showing that our techniques are effective. ?-VBN+BN? replaces the VBN in the generator
with BN, as in DCGANs. This causes a small decrease in sample quality on CIFAR. VBN is more
important for ImageNet. ?-L+HA? removes the labels from the training process, and adds historical
averaging to compensate. HA makes it possible to still generate some recognizable objects. Without
HA, sample quality is considerably reduced (see ?-L?). ?-LS? removes label smoothing and incurs a
noticeable drop in performance relative to ?our methods.? ?-MBF? removes the minibatch features
and incurs a very large drop in performance, greater even than the drop resulting from removing the
labels. Adding HA cannot prevent this problem.

6.3

SVHN

For the SVHN data set, we used the same architecture and experimental setup as for CIFAR-10.
Figure 5 compares against the previous state-of-the-art, where it should be noted that the model
7

of [24] is not convolutional, but does use an additional data set of 531131 unlabeled examples. The
other methods, including ours, are convolutional and do not use this data.
Model

Percentage of incorrectly predicted test examples
for a given number of labeled samples
500
1000
2000

Virtual Adversarial [23]
Stacked What-Where Auto-Encoder [27]
DCGAN [3]
Skip Deep Generative Model [24]
Our model
Ensemble of 10 of our models

24.63
23.56
22.48
16.61?0.24
8.11 ? 1.3
5.88 ? 1.0

18.44 ? 4.8

6.16 ? 0.58

Figure 5: (Left) Error rate on SVHN. (Right) Samples from the generator for SVHN.
6.4

ImageNet

We tested our techniques on a dataset of unprecedented scale: 128 ? 128 images from the
ILSVRC2012 dataset with 1,000 categories. To our knowledge, no previous publication has applied a generative model to a dataset with both this large of a resolution and this large a number
of object classes. The large number of object classes is particularly challenging for GANs due to
their tendency to underestimate the entropy in the distribution. We extensively modified a publicly
available implementation of DCGANs2 using TensorFlow [28] to achieve high performance, using
a multi-GPU implementation. DCGANs without modification learn some basic image statistics and
generate contiguous shapes with somewhat natural color and texture but do not learn any objects.
Using the techniques described in this paper, GANs learn to generate objects that resemble animals,
but with incorrect anatomy. Results are shown in Fig. 6.

Figure 6: Samples generated from the ImageNet dataset. (Left) Samples generated by a DCGAN.
(Right) Samples generated using the techniques proposed in this work. The new techniques enable
GANs to learn recognizable features of animals, such as fur, eyes, and noses, but these features are
not correctly combined to form an animal with realistic anatomical structure.

7

Conclusion

Generative adversarial networks are a promising class of generative models that has so far been
held back by unstable training and by the lack of a proper evaluation metric. This work presents
partial solutions to both of these problems. We propose several techniques to stabilize training
that allow us to train models that were previously untrainable. Moreover, our proposed evaluation
metric (the Inception score) gives us a basis for comparing the quality of these models. We apply
our techniques to the problem of semi-supervised learning, achieving state-of-the-art results on a
number of different data sets in computer vision. The contributions made in this work are of a
practical nature; we hope to develop a more rigorous theoretical understanding in future work.
2

https://github.com/carpedm20/DCGAN-tensorflow

8

References
[1] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, et al. Generative adversarial nets. In NIPS, 2014.
[2] Emily Denton, Soumith Chintala, Arthur Szlam, and Rob Fergus. Deep generative image models using a
laplacian pyramid of adversarial networks. arXiv preprint arXiv:1506.05751, 2015.
[3] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
[4] Ian J Goodfellow. On distinguishability criteria for estimating generative models. arXiv preprint
arXiv:1412.6515, 2014.
[5] Daniel Jiwoong Im, Chris Dongjoo Kim, Hui Jiang, and Roland Memisevic. Generating images with
recurrent adversarial networks. arXiv preprint arXiv:1602.05110, 2016.
[6] Donggeun Yoo, Namil Kim, Sunggyun Park, Anthony S Paek, and In So Kweon. Pixel-level domain
transfer. arXiv preprint arXiv:1603.07442, 2016.
[7] Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Sch?olkopf. Measuring statistical dependence with hilbert-schmidt norms. In Algorithmic learning theory, pages 63?77. Springer, 2005.
[8] Kenji Fukumizu, Arthur Gretton, Xiaohai Sun, and Bernhard Sch?olkopf. Kernel measures of conditional
dependence. In NIPS, volume 20, pages 489?496, 2007.
[9] Alex Smola, Arthur Gretton, Le Song, and Bernhard Sch?olkopf. A hilbert space embedding for distributions. In Algorithmic learning theory, pages 13?31. Springer, 2007.
[10] Yujia Li, Kevin Swersky, and Richard S. Zemel. Generative moment matching networks. CoRR,
abs/1502.02761, 2015.
[11] Gintare Karolina Dziugaite, Daniel M Roy, and Zoubin Ghahramani. Training generative neural networks
via maximum mean discrepancy optimization. arXiv preprint arXiv:1505.03906, 2015.
[12] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
[13] Ilya Sutskever, Rafal Jozefowicz, Karol Gregor, et al. Towards principled unsupervised learning. arXiv
preprint arXiv:1511.06440, 2015.
[14] Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks. arXiv preprint arXiv:1511.06390, 2015.
[15] Augustus Odena. Semi-supervised learning with generative adversarial networks. arXiv preprint
arXiv:1606.01583, 2016.
[16] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. 2016. MIT Press.
[17] George W Brown. Iterative solution of games by fictitious play. Activity analysis of production and
allocation, 13(1):374?376, 1951.
[18] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the Inception Architecture for
Computer Vision. ArXiv e-prints, December 2015.
[19] David Warde-Farley and Ian Goodfellow. Adversarial perturbations of deep neural networks. In Tamir
Hazan, George Papandreou, and Daniel Tarlow, editors, Perturbations, Optimization, and Statistics, chapter 11. 2016. Book in preparation for MIT Press.
[20] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. arXiv preprint arXiv:1512.00567, 2015.
[21] Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. arXiv preprint arXiv:1602.07868, 2016.
[22] Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised
learning with deep generative models. In Neural Information Processing Systems, 2014.
[23] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, and Shin Ishii. Distributional smoothing
by virtual adversarial examples. arXiv preprint arXiv:1507.00677, 2015.
[24] Lars Maal?e, Casper Kaae S?nderby, S?ren Kaae S?nderby, and Ole Winther. Auxiliary deep generative
models. arXiv preprint arXiv:1602.05473, 2016.
[25] Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semi-supervised
learning with ladder networks. In Advances in Neural Information Processing Systems, 2015.
[26] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, et al. Intriguing properties of neural networks.
arXiv preprint arXiv:1312.6199, 2013.
[27] Junbo Zhao, Michael Mathieu, Ross Goroshin, and Yann Lecun. Stacked what-where auto-encoders.
arXiv preprint arXiv:1506.02351, 2015.
[28] Mart??n Abadi, Ashish Agarwal, Paul Barham, et al. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.

9

"
2013,"B-test: A Non-parametric, Low Variance Kernel Two-sample Test",Poster,5081-b-test-a-non-parametric-low-variance-kernel-two-sample-test.pdf,"We propose a family of maximum mean discrepancy (MMD) kernel two-sample tests that have low sample complexity and are consistent. The test has a hyperparameter that allows one to control the tradeoff between sample complexity and computational time. Our family of tests, which we denote as B-tests, is both computationally and statistically efficient, combining favorable properties of previously proposed MMD two-sample tests.  It does so by better leveraging samples to produce low variance estimates in the finite sample case, while avoiding a quadratic number of kernel evaluations and complex null-hypothesis approximation as would be required by tests relying on one sample U-statistics. The B-test uses a smaller than quadratic number of kernel evaluations and avoids completely the computational burden of complex null-hypothesis approximation while maintaining consistency and probabilistically conservative thresholds on Type I error. Finally, recent results of combining multiple kernels transfer seamlessly to our hypothesis test, allowing a further increase in discriminative power and decrease in sample complexity.","B-tests: Low Variance Kernel Two-Sample Tests

Matthew Blaschko
Arthur Gretton
Wojciech Zaremba
?
Gatsby Unit
Center for Visual Computing
Equipe
GALEN
?
University College London
Inria Saclay
Ecole
Centrale Paris
United Kingdom
Ch?atenay-Malabry, France
Ch?atenay-Malabry, France
{woj.zaremba,arthur.gretton}@gmail.com, matthew.blaschko@inria.fr

Abstract
A family of maximum mean discrepancy (MMD) kernel two-sample tests is introduced. Members of the test family are called Block-tests or B-tests, since the test
statistic is an average over MMDs computed on subsets of the samples. The choice
of block size allows control over the tradeoff between test power and computation
time. In this respect, the B-test family combines favorable properties of previously proposed MMD two-sample tests: B-tests are more powerful than a linear
time test where blocks are just pairs of samples, yet they are more computationally efficient than a quadratic time test where a single large block incorporating all
the samples is used to compute a U-statistic. A further important advantage of the
B-tests is their asymptotically Normal null distribution: this is by contrast with
the U-statistic, which is degenerate under the null hypothesis, and for which estimates of the null distribution are computationally demanding. Recent results on
kernel selection for hypothesis testing transfer seamlessly to the B-tests, yielding
a means to optimize test power via kernel choice.

1

Introduction

Given two samples {xi }ni=1 where xi ? P i.i.d., and {yi }ni=1 , where yi ? Q i.i.d, the two sample
problem consists in testing whether to accept or reject the null hypothesis H0 that P = Q, vs the
alternative hypothesis HA that P and Q are different. This problem has recently been addressed
using measures of similarity computed in a reproducing kernel Hilbert space (RKHS), which apply
in very general settings where P and Q might be distributions over high dimensional data or structured objects. Kernel test statistics include the maximum mean discrepancy [10, 6] (of which the
energy distance is an example [18, 2, 22]), which is the distance between expected features of P and
Q in the RKHS; the kernel Fisher discriminant [12], which is the distance between expected feature
maps normalized by the feature space covariance; and density ratio estimates [24]. When used in
testing, it is necessary to determine whether the empirical estimate of the relevant similarity measure is sufficiently large as to give the hypothesis P = Q low probability; i.e., below a user-defined
threshold ?, denoted the test level. The test power denotes the probability of correctly rejecting the
null hypothesis, given that P 6= Q.
The minimum variance unbiased estimator MMDu of the maximum mean discrepancy, on the basis
of n samples observed from each of P and Q, is a U-statistic, costing O(n2 ) to compute. Unfortunately, this statistic is degenerate under the null hypothesis H0 that P = Q, and its asymptotic
distribution takes the form of an infinite weighted sum of independent ?2 variables (it is asymptotically Gaussian under the alternative hypothesis HA that P 6= Q). Two methods for empirically
estimating the null distribution in a consistent way have been proposed: the bootstrap [10], and a
method requiring an eigendecomposition of the kernel matrices computed on the merged samples
from P and Q [7]. Unfortunately, both procedures are computationally demanding: the former costs
O(n2 ), with a large constant (the MMD must be computed repeatedly over random assignments
of the pooled data); the latter costs O(n3 ), but with a smaller constant, hence can in practice be
1

faster than the bootstrap. Another approach is to approximate the null distribution by a member
of a simpler parametric family (for instance, a Pearson curve approximation), however this has no
consistency guarantees.
More recently, an O(n) unbiased estimate MMDl of the maximum mean discrepancy has been proposed [10, Section 6], which is simply a running average over independent pairs of samples from P
and Q. While this has much greater variance than the U-statistic, it also has a simpler null distribution: being an average over i.i.d. terms, the central limit theorem gives an asymptotically Normal
distribution, under both H0 and HA . It is shown in [9] that this simple asymptotic distribution makes
it easy to optimize the Hodges and Lehmann asymptotic relative efficiency [19] over the family of
kernels that define the statistic: in other words, to choose the kernel which gives the lowest Type II
error (probability of wrongly accepting H0 ) for a given Type I error (probability of wrongly rejecting H0 ). Kernel selection for the U-statistic is a much harder question due to the complex form of
the null distribution, and remains an open problem.
It appears that MMDu and MMDl fall at two extremes of a spectrum: the former has the lowest
variance of any n-sample estimator, and should be used in limited data regimes; the latter is the
estimator requiring the least computation while still looking at each of the samples, and usually
achieves better Type II error than MMDu at a given computational cost, albeit by looking at much
more data (the ?limited time, unlimited data? scenario). A major reason MMDl is faster is that its
null distribution is straightforward to compute, since it is Gaussian and its variance can be calculated
at the same cost as the test statistic. A reasonable next step would be to find a compromise between
these two extremes: to construct a statistic with a lower variance than MMDl , while retaining an
asymptotically Gaussian null distribution (hence remaining faster than tests based on MMDu ). We
study a family of such test statistics, where we split the data into blocks of size B, compute the
quadratic-time MMDu on each block, and then average the resulting statistics. We call the resulting
tests B-tests. As long as we choose the size B of blocks such that n/B ? ?, we are still guaranteed
asymptotic Normality by the central limit theorem, and the null distribution can be computed at the
same cost as the test statistic. For a given sample size n, however, the power of the test can increase
dramatically over the MMDl test, even for moderate block sizes B, making much better use of the
available data with only a small increase in computation.
The block averaging scheme was originally proposed in [13], as an instance of a two-stage Ustatistic, to be applied when the degree of degeneracy of the U-statistic is indeterminate. Differences
with respect to our method are that Ho and Shieh compute the block statistics by sampling with
replacement [13, (b) p. 863], and propose to obtain the variance of the test statistic via Monte
Carlo, jackknife, or bootstrap techniques, whereas we use closed form expressions. Ho and Shieh
further suggest an alternative two-stage U-statistic in the event that the degree of degeneracy is
known; we return to this point in the discussion. While we confine ourselves to the MMD in this
paper, we emphasize that the block approach applies to a much broader variety of test situations
where the null distribution cannot easily be computed, including the energy distance and distance
covariance [18, 2, 22] and Fisher statistic [12] in the case of two-sample testing, and the HilbertSchmidt Independence Criterion [8] and distance covariance [23] for independence testing. Finally,
the kernel learning approach of [9] applies straightforwardly, allowing us to maximize test power
over a given kernel family. Code is available at http://github.com/wojzaremba/btest.

2

Theory

In this section we describe the mathematical foundations of the B-test. We begin with a brief review
of kernel methods, and of the maximum mean discrepancy. We then present our block-based average
MMD statistic, and derive its distribution under the H0 (P = Q) and HA (P 6= Q) hypotheses. The
central idea employed in the construction of the B-test is to generate a low variance MMD estimate
by averaging multiple low variance kernel statistics computed over blocks of samples. We show
simple sufficient conditions on the block size for consistency of the estimator. Furthermore, we
analyze the properties of the finite sample estimate, and propose a consistent strategy for setting the
block size as a function of the number of samples.
2.1

Definition and asymptotics of the block-MMD

Let Fk be an RKHS defined on a topological space X with reproducing kernel k, and P a Borel
probability measure on X . The mean embedding of P in Fk , written ?k (p) ? Fk is defined such
2

250
HA histogram

250

H0 histogram

HA histogram

approximated 5% quantile of H0

H histogram
0

approximated 5% quantile of H

200

0

200

150
150

100
100

50
50

0
?4
0
?0.05

?0.04

?0.03

?0.02

?0.01

0

0.01

0.02

0.03

0.04

?2

0

2

4

6

8

10
?3

x 10

0.05

(a) B = 2. This setting corresponds to the MMDl
statistic [10].

(b) B = 250

Figure 1: Empirical distributions under H0 and HA for different regimes of B for the music experiment
(Section 3.2). In both plots, the number of samples is fixed at 500. As we vary B, we trade off the quality of the
finite sample Gaussian approximation to the null distribution, as in Theorem 2.3, with the variances of the H0
and HA distributions, as outlined in Section 2.1. In (b) the distribution under H0 does not resemble a Gaussian
(it does not pass a level 0.05 Kolmogorov-Smirnov (KS) normality test [16, 20]), and a Gaussian approximation
results in a conservative test threshold (vertical green line). The remaining empirical distributions all pass a KS
normality test.

that Ex?p f (x) = hf, ?k (p)iFk for all f ? Fk , and exists for all Borel probability measures when
k is bounded and continuous [3, 10]. The maximum mean discrepancy (MMD) between a Borel
probability measure P and a second Borel probability measure Q is the squared RKHS distance
between their respective mean embeddings,
2

?k (P, Q) = k?k (P ) ? ?k (Q)kFk = Exx0 k(x, x0 ) + Eyy0 k(y, y 0 ) ? 2Exy k(x, y),

(1)

0

where x denotes an independent copy of x [11]. Introducing the notation z = (x, y), we write
?k (P, Q) = Ezz0 hk (z, z 0 ),

h(z, z 0 ) = k(x, x0 ) + k(y, y 0 ) ? k(x, y 0 ) ? k(x0 , y).

(2)

When the kernel k is characteristic, then ?k (P, Q) = 0 iff P = Q [21]. Clearly, the minimum
variance unbiased estimate MMDu of ?k (P, Q) is a U-statistic.
By analogy with MMDu , we make use of averages of h(x, y, x0 , y 0 ) to construct our two-sample
test. We denote by ??k (i) the ith empirical estimate MMDu based on a subsample of size B, where
n
1?i? B
(for notational purposes, we will index samples as though they are presented in a random
fixed order). More precisely,
??k (i) =

1
B(B ? 1)

iB
X

iB
X

h(za , zb ).

(3)

a=(i?1)B+1 b=(i?1)B+1,b6=a

The B-test statistic is an MMD estimate obtained by averaging the ??k (i). Each ??k (i) under H0
converges to an infinite sum of weighted ?2 variables [7]. Although setting B = n would lead to the
lowest variance estimate of the MMD, computing sound thresholds for a given p-value is expensive,
involving repeated bootstrap sampling [5, 14], or computing the eigenvalues of a Gram matrix [7].
In contrast, we note that ??k (i)i=1,..., n are i.i.d. variables, and averaging them allows us to apply
B
the central limit theorem in order to estimate p-values from a normal distribution. We denote the
average of the ??k (i) by ??k ,
n
B
BX
??k =
??k (i).
(4)
n i=1
We would like to apply the central limit theorem to variables ??k (i)i=1,..., n . It remains for us to
B
derive the distribution of ??k under H0 and under HA . We rely on the result from [11, Theorem 8]
for HA . According to our notation, for every i,
3

Theorem 2.1 Assume 0 < E(h2 ) < ?, then under HA , ??k converges in distribution to a Gaussian
according to
D

1

B 2 (?
?k (i) ? MMD2 ) ? N (0, ?u2 ),

where ?u2 = 4 Ez [(Ez0 h(z, z 0 ))2 ? Ez,z0 (h(z, z 0 ))]2 .

(5)

This in turn implies that
D

??k (i) ? N (MMD2 , ?u2 B ?1 ).
For an average of {?
?k (i)}i=1,..., Bn , the central limit theorem implies that under HA ,



D
?1
??k ? N MMD2 , ?u2 (Bn/B)
= N MMD2 , ?u2 n?1 .

(6)

(7)

This result shows that the distribution of HA is asymptotically independent of the block size, B.
Turning to the null hypothesis, [11, Theorem 8] additionally implies that under H0 for every i,
Theorem 2.2
D

B ??k (i) ?

?
X

?l [zl2 ? 2],

(8)

l=1

where zl ? N (0, 2)2 i.i.d, ?l are the solutions to the eigenvalue equation
Z
? x0 )?l (x)dp(x) = ?l ?l (x0 ),
k(x,

(9)

X

? i , xj ) := k(xi , xj ) ? Ex k(xi , x) ? Ex k(x, xj ) + Ex,x0 k(x, x0 ) is the centered RKHS kernel.
and k(x
P?
As a consequence, under H0 , ??k (i) has expected variance 2B ?2 l=1 ?2 . We will denote this
variance by CB ?2 . The central limit theorem implies that under H0 ,

?1 

D
??k ? N 0, C B 2 n/B
= N 0, C(nB)?1

(10)

The asymptotic distributions for ??k under H0 and HA are Gaussian, and consequently it is easy
to calculate the distribution quantiles and test thresholds. Asymptotically, it is always beneficial to
increase B, as the distributions for ? under H0 and HA will be better separated. For consistency, it
is sufficient to ensure that n/B ? ?.
A related strategy of averaging over data blocks to deal with large sample sizes has recently been
developed in [15], with the goal of efficiently computing bootstrapped estimates of statistics of
interest (e.g. quantiles or biases). Briefly, the approach splits the data (of size n) into s subsamples
each of size B, computes an estimate of the n-fold bootstrap on each block, and averages these
estimates. The difference with respect to our approach is that we use the asymptotic distribution
of the average over block statistics to determine a threshold for a hypothesis test, whereas [15] is
concerned with proving the consistency of a statistic obtained by averaging over bootstrap estimates
on blocks.
2.2

Convergence of Moments

In this section, we analyze the convergence of the moments of the B-test statistic, and comment on
potential sources of bias.
?k (i)).
The central limit theorem implies that the empirical mean of {?
?k (i)}i=1,..., Bn converges to E(?
2
2
n
Moreover it states that the variance {?
?k (i)}i=1,..., B converges to E(?
?k (i)) ? E(?
?k (i) ). Finally, all
remaining moments tend to zero, where the rate of convergence for the jth moment is of the order
 j+1
n
2
[1]. This indicates that the skewness dominates the difference of the distribution from a
B
Gaussian.
4

Under both H0 and HA , thresholds computed from normal distribution tables are asymptotically unbiased. For finite samples sizes, however, the bias under H0 can be more severe. From Equation (8)
we have that under H0 , the summands, ??k (i), converge in distribution to infinite weighted sums of
?2 distributions. Every unweighted term of this infinite sum has distribution N (0, 2)2 , which has
finite skewness equal to 8. The skewness for the entire sum is finite and positive,
?
X
C=
8?3l ,
(11)
l=1

as ?l ? 0 for all l due to the positive definiteness of the kernel k. The skew for the mean of the
??k (i) converges to 0 and is positively biased. At smaller sample sizes, test thresholds obtained from
the standard Normal table may therefore be inaccurate, as they do not account for this skew. In our
experiments, this bias caused the tests to be overly conservative, with lower Type I error than the
design level required (Figures 2 and 5).
2.3

Finite Sample Case

In the finite sample case, we apply the Berry-Ess?een theorem, which gives conservative bounds on
the `? convergence of a series of finite sample random variables to a Gaussian distribution [4].
2
2
Theorem 2.3 Let X1 , X2 , . . . , Xn be i.i.d. variables. E(X
> 0, and
1 ) = 0, E(X1 ) = ?
Pn
Xi
3
i=1
E(|X1 | ) = ? < ?. Let Fn be a cumulative distribution of ?n? , and let ? denote the standard
normal distribution. Then for every x,

|Fn (x) ? ?(x)| ? C?? ?3 n?1/2 ,

(12)

where C < 1.
This result allows us to ensure fast point-wise convergence of the B-test. We have that ?(?
?k ) =
O(1), i.e., it is dependent only on the underlying distributions of the samples and not on the sample
size. The number of i.i.d. samples is nB ?1 . Based on Theorem 2.3, the point-wise error can be
O(1)
B2
upper bounded by
= O( ?
) under HA . Under H0 , the error can be bounded by
3?
n
n
?1
O(B

O(1)
3
O(B ?2 ) 2

?n =

)2

B

3.5
O( B?n ).

B

While the asymptotic results indicate that convergence to an optimal predictor is fastest for larger
B, the finite sample results support decreasing the size of B in order to have a sufficient number
n
of samples for application of the central limit theorem. As long as B ? ? and B
? ?, the
assumptions of the B-test are fulfilled.
By varying B, we make a fundamental tradeoff in the construction of our two sample test. When B
is small, we have many samples, hence the null distribution is close to the asymptotic limit provided
by the central limit theorem, and the Type I error is estimated accurately. The disadvantage of a
small B is a lower test power for a given sample size. Conversely, if we increase B, we will have
a lower variance empirical distribution for H0 , hence higher test power, but we may have a poor
estimate of the number of Type I errors (Figure 1). A sensible family of heuristics therefore is to set
B = [n? ]

(13)

for some 0 < ? < 1, where we round to the nearest integer. In this setting the number of samples
(1??)
available for application of the central
]. For given ? computational
 limit theorem will be [n
1+?
complexity of the B-test is O n
. We note that any value of ? ? (0, 1) yields a consistent
1
estimator.
 We have chosen ? = 2 in the experimental results section, with resulting complexity
1.5
O n
: we emphasize that this is a heuristic, and just one choice that fulfils our assumptions.

3

Experiments

We have conducted experiments on challenging synthetic and real datasets in order to empirically
measure (i) sample complexity, (ii) computation time, and (iii) Type I / Type II errors. We evaluate
B-test performance in comparison to the MMDl and MMDu estimators, where for the latter we
compare across different strategies for null distribution quantile estimation.
5

Method

Kernel parameters
?=1

B-test

? = median
multiple kernels

Pearson curves
Gamma approximation
Gram matrix spectrum
Bootstrap
Pearson curves
Gamma approximation
Gram matrix spectrum
Bootstrap

Additional
parameters
B=2
B =?8
B= n
any B
B=2
B =p8
B = n2

Minimum number
of samples
26400
3850
886
> 60000
37000
5400
1700

Computation
time (s)
0.0012
0.0039
0.0572

B=n

186
183
186
190

387.4649
0.2667
407.3447
129.4094

?=1

Consistent
X
X
X
X
X
X
X

0.0700
0.1295
0.8332

?
?
X
X
?
?
X
X

> 60000, or 2h
per iteration
timeout

? = median

Table 1: Sample complexity for tests on the distributions described in Figure 3. The fourth column indicates
the minimum number of samples necessary to achieve Type I and Type II errors of 5%. The fifth column is the
computation time required for 2000 samples, and is not presented for settings that have unsatisfactory sample
complexity.
0.08

0.08

Empirical Type I error
Expected
? Type I error
B= n

0.07

0.08

Empirical Type I error
Expected
? Type I error
B= n

0.07

0.06

0.06

0.05

0.05

0.05

0.04
0.03

0.04
0.03

0.02

0.02

0.01

0.01

0
2

4

8

16
32
Size of inner block

(a)

64

128

Type I error

0.06
Type I error

Type I error

0.07

0
2

Empirical Type I error
Expected
p Type I error
B = n2

0.04
0.03
0.02
0.01

4

8

16
32
Size of inner block

(b)

64

128

0
2

4

8

16
32
Size of inner block

64

128

(c)

Figure 2: Type I errors on the distributions shown in Figure 3 for ? = 5%: (a) MMD, single kernel, ? = 1, (b)
MMD, single kernel, ? set to the median pairwise distance, and (c) MMD, non-negative linear combination of
multiple kernels. The experiment was repeated 30000 times. Error bars are not visible at this scale.

3.1

Synthetic data

Following previous work on kernel hypothesis testing [9], our synthetic distributions are 5 ? 5 grids
of 2D Gaussians. We specify two distributions, P and Q. For distribution P each Gaussian has
identity covariance matrix, while for distribution Q the covariance is non-spherical. Samples drawn
from P and Q are presented in Figure 3. These distributions have proved to be very challenging for
existing non-parametric two-sample tests [9].
We employed three different kernel selection strategies
in the hypothesis test. First, we used a Gaussian kernel
with ? = 1, which approximately matches the scale of
the variance of each Gaussian in mixture P . While this
is a somewhat arbitrary default choice, we selected it as
it performs well in practice (given the lengthscale of the
(a) Distribution P
(b) Distribution Q
data), and we treat it as a baseline. Next, we set ? equal
to the median pairwise distance over the training data, Figure 3: Synthetic data distributions P and
which is a standard way to choose the Gaussian kernel Q. Samples belonging to these classes are
bandwidth [17], although it is likewise arbitrary in this difficult to distinguish.
context. Finally, we applied a kernel learning strategy, in
which the kernel was optimized to maximize the test power for the alternative P 6= Q [9]. This
approach returned a non-negative linear combination combination of base kernels, where half the
data were used in learning the kernel weights (these data were excluded from the testing phase).
The base kernels in our experiments were chosen to be Gaussian, with bandwidths in the set ? ?
{2?15 , 2?14 , . . . , 210 }. Testing was conducted using the remaining half of the data.
6

B?test, a single kernel, ? = 1
B?test, a single kernel, ? = median
B?test kernel selection
Tests estimating MMDu with ?=1

Tests estimating MMDu with ?=median
Emprical number of Type II errors

For comparison with the quadratic time U statistic MMDu [7, 10], we evaluated four
null distribution estimates: (i) Pearson curves,
(ii) gamma approximation, (iii) Gram matrix
spectrum, and (iv) bootstrap. For methods using Pearson curves and the Gram matrix spectrum, we drew 500 samples from the null distribution estimates to obtain the 1 ? ? quantiles,
for a test of level ?. For the bootstrap, we fixed
the number of shuffles to 1000. We note that
Pearson curves and the gamma approximation
are not statistically consistent. We considered
only the setting with ? = 1 and ? set to the
median pairwise distance, as kernel selection is
not yet solved for tests using MMDu [9].

1
0.8
0.6
0.4
0.2
0

1

10

2

10
Size of inner block

3

10

Figure 4: Synthetic experiment: number of Type II er-

In the first experiment we set the Type I error to rors vs B, given a fixed probability ? of Type I erbe 5%, and we recorded the Type II error. We rors. As B grows, the Type II error drops quickly when
conducted these experiments on 2000 samples the kernel is appropriately chosen. The kernel selecover 1000 repetitions, with varying block size, tion method is described in [9], and closely approxB. Figure 4 presents results for different kernel imates the baseline performance of the well-informed
choice strategies, as a function of B. The me- user choice of ? = 1.
dian heuristic performs extremely poorly in this
experiment. As discussed in [9, Section 5], the reason for this failure is that the lengthscale of the
difference between the distributions P and Q differs from the lengthscale of the main data variation
as captured by the median, which gives too broad a kernel for the data.
In the second experiment, our aim was to compare the empirical sample complexity of the various
methods. We again fixed the same Type I error for all methods, but this time we also fixed a Type
II error of 5%, increasing the number of samples until the latter error rate was achieved. Column
four of Table 1 shows the number of samples required in each setting to achieve these error rates.
We additionally compared the computational efficiency of the various methods. The computation
time for each method with a fixed sample size of 2000 is presented in column five of Table 1. All
experiments were run on a single 2.4 GHz core.
Finally, we evaluated the empirical Type I error for ? = 5% and increasing B. Figure 2 displays the
empirical Type I error, where we note the location of the ? = 0.5 heuristic in Equation (13). For the
user-chosen kernel (? = 1, Figure 2(a)), the number of Type I errors closely matches the targeted
test level. When median heuristic is used, however, the test is overly conservative, and makes fewer
Type I errors than required (Figure 2(b)). This indicates that for this choice of ?, we are not in the
asymptotic regime, and our Gaussian null distribution approximation is inaccurate. Kernel selection
via the strategy of [9] alleviates this problem (Figure 2(c)). This setting coincides with a block size
substantially larger than 2 (MMDl ), and therefore achieves lower Type II errors while retaining the
targeted Type I error.
3.2

Musical experiments

In this set of experiments, two amplitude modulated Rammstein songs were compared (Sehnsucht
vs. Engel, from the album Sehnsucht). Following the experimental setting in [9, Section 5], samples
from P and Q were extracts from AM signals of time duration 8.3 ? 10?3 seconds in the original
audio. Feature extraction was identical to [9], except that the amplitude scaling parameter
? was set
to 0.3 instead of 0.5. As the feature vector had size 1000 we set the block size B =
1000 =
32. Table 2 summarizes the empirical Type I and Type II errors over 1000 repetitions, and the
average computation times. Figure 5 shows the average number of Type I errors as a function of
B: in this case, all kernel selection strategies result in conservative tests (lower Type I error than
required), indicating that more samples are needed to reach the asymptotic regime. Figure 1 shows
the empirical H0 and HA distributions for different B.

4

Discussion

We have presented experimental results both on a difficult synthetic problem, and on real-world data
from amplitude modulated audio recordings. The results show that the B-test has a much better
7

Kernel
parameters

Method

Additional
parameters
B =?2
B= n
B =?2
B= n
B =p2
B = n2

?=1
B-test

? = median
multiple kernels

Gram matrix spectrum
Bootstrap
Gram matrix spectrum
Bootstrap

Type I error

Type II error

0.038
0.006
0.043
0.026
0.0481
0.025

0.927
0.597
0.786
0
0.867
0.012

Computational
time (s)
0.039
1.276
0.047
1.259
0.607
18.285

0
0.01
0
0.01

0
0
0
0

160.1356
121.2570
286.8649
122.8297

?=1
B = 2000
? = median

Table 2: A comparison of consistent tests on the music experiment described in Section 3.2. Here computation
time is reported for the test achieving the stated error rates.

0.08

0.08

Empirical Type I error
Expected
? Type I error
B= n

0.07

0.08

Empirical Type I error
Expected
? Type I error
B= n

0.07

0.06

0.06

0.05

0.05

0.05

0.04
0.03

Type I error

0.06
Type I error

Type I error

0.07

0.04
0.03

0.04
0.03

0.02

0.02

0.02

0.01

0.01

0.01

0
2

4

8

16
32
Size of inner block

(a)

64

128

0
2

4

8

16
32
Size of inner block

(b)

64

128

Empirical Type I error
Expected
p Type I error
B = n2

0
2

4

8

16
32
Size of inner block

64

128

(c)

Figure 5: Empirical Type I error rate for ? = 5% on the music data (Section 3.2). (a) A single kernel test with
? = 1, (b) A single kernel test with ? = median, and (c) for multiple kernels. Error bars are not visible at this
scale. The results broadly follow the trend visible from the synthetic experiments.

sample complexity than MMDl over all tested kernel selection strategies. Moreover, it is an order
of magnitude faster than any test that consistently estimates the null distribution for MMDu (i.e.,
the Gram matrix eigenspectrum and bootstrap estimates): these estimates are impractical at large
sample sizes, due to their computational complexity. Additionally, the B-test remains statistically
consistent, with the best convergence rates achieved for large B. The B-test combines the best
features of MMDl and MMDu based two-sample tests: consistency, high statistical efficiency, and
high computational efficiency.
A number of further interesting experimental trends may be seen in these results. First, we have
observed that the empirical Type I error rate is often conservative, and is less than the 5% targeted
by the threshold based on a Gaussian null distribution assumption (Figures 2 and 5). In spite of this
conservatism, the Type II performance remains strong (Tables 1 and 2), as the gains in statistical
power of the B-tests improve the testing performance (cf. Figure 1). Equation (7) implies that the
size of B does not influence the asymptotic variance under HA , however we observe in Figure 1 that
the empirical variance of HA drops with larger B. This is because, for these P and Q and small B,
the null and alternative distributions have considerable overlap. Hence, given the distributions are
effectively indistinguishable at these sample sizes n, the variance of the alternative distribution as a
function of B behaves more like that of H0 (cf. Equation (10)). This effect will vanish as n grows.
Finally, [13] propose an alternative approach for U-statistic based testing when the degree of degeneracy is known: a new U-statistic (the TU-statistic) is written in terms of products of centred
U-statistics computed on the individual blocks, and a test is formulated using this TU-statistic. Ho
and Shieh show that a TU-statistic based test can be asymptotically more powerful than a test using
a single U-statistic on the whole sample, when the latter is degenerate under H0 , and nondegenerate
under HA . It is of interest to apply this technique to MMD-based two-sample testing.
Acknowledgments We thank Mladen Kolar for helpful discussions. This work is partially funded by ERC
Grant 259112, and by the Royal Academy of Engineering through the Newton Alumni Scheme.

8

References
[1] Bengt Von Bahr. On the convergence of moments in the central limit theorem. The Annals of
Mathematical Statistics, 36(3):pp. 808?818, 1965.
[2] L. Baringhaus and C. Franz. On a new multivariate two-sample test. J. Multivariate Anal.,
88:190?206, 2004.
[3] A. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and
Statistics. Kluwer, 2004.
[4] Andrew C Berry. The accuracy of the gaussian approximation to the sum of independent
variates. Transactions of the American Mathematical Society, 49(1):122?136, 1941.
[5] B. Efron and R. J. Tibshirani. An Introduction to the Bootstrap. Chapman & Hall, 1993.
[6] M. Fromont, B. Laurent, M. Lerasle, and P. Reynaud-Bouret. Kernels based tests with nonasymptotic bootstrap approaches for two-sample problems. In COLT, 2012.
[7] A Gretton, K Fukumizu, Z Harchaoui, and BK Sriperumbudur. A fast, consistent kernel twosample test. In Advances in Neural Information Processing Systems 22, pages 673?681, 2009.
[8] A. Gretton, K. Fukumizu, C.-H. Teo, L. Song, B. Sch?olkopf, and A. J. Smola. A kernel
statistical test of independence. In Advances in Neural Information Processing Systems 20,
pages 585?592, Cambridge, MA, 2008. MIT Press.
[9] A Gretton, B Sriperumbudur, D Sejdinovic, H Strathmann, S Balakrishnan, M Pontil, and
K Fukumizu. Optimal kernel choice for large-scale two-sample tests. In Advances in Neural
Information Processing Systems 25, pages 1214?1222, 2012.
[10] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Sch?olkopf, and Alexander
Smola. A kernel two-sample test. J. Mach. Learn. Res., 13:723?773, March 2012.
[11] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Sch?olkopf, and Alexander J.
Smola. A kernel method for the two-sample-problem. In NIPS, pages 513?520, 2006.
[12] Z. Harchaoui, F. Bach, and E. Moulines. Testing for homogeneity with kernel Fisher discriminant analysis. In NIPS, pages 609?616. MIT Press, Cambridge, MA, 2008.
[13] H.-C. Ho and G. Shieh. Two-stage U-statistics for hypothesis testing. Scandinavian Journal
of Statistics, 33(4):861?873, 2006.
[14] Norman Lloyd Johnson, Samuel Kotz, and Narayanaswamy Balakrishnan. Continuous univariate distributions. Distributions in statistics. Wiley, 2nd edition, 1994.
[15] A. Kleiner, A. Talwalkar, P. Sarkar, and M. I. Jordan. A scalable bootstrap for massive data.
Journal of the Royal Statistical Society, Series B, In Press.
[16] Andrey N Kolmogorov. Sulla determinazione empirica di una legge di distribuzione. Giornale
dellIstituto Italiano degli Attuari, 4(1):83?91, 1933.
[17] B Sch?olkopf. Support vector learning. Oldenbourg, M?unchen, Germany, 1997.
[18] D. Sejdinovic, A. Gretton, B. Sriperumbudur, and K. Fukumizu. Hypothesis testing using
pairwise distances and associated kernels. In ICML, 2012.
[19] R. Serfling. Approximation Theorems of Mathematical Statistics. Wiley, New York, 1980.
[20] Nickolay Smirnov. Table for estimating the goodness of fit of empirical distributions. The
Annals of Mathematical Statistics, 19(2):279?281, 1948.
[21] B. Sriperumbudur, A. Gretton, K. Fukumizu, G. Lanckriet, and B. Sch?olkopf. Hilbert space
embeddings and metrics on probability measures. Journal of Machine Learning Research,
11:1517?1561, 2010.
[22] G. Sz?ekely and M. Rizzo. Testing for equal distributions in high dimension. InterStat, (5),
November 2004.
[23] G. Sz?ekely, M. Rizzo, and N. Bakirov. Measuring and testing dependence by correlation of
distances. Ann. Stat., 35(6):2769?2794, 2007.
[24] M. Yamada, T. Suzuki, T. Kanamori, H. Hachiya, and M. Sugiyama. Relative density-ratio
estimation for robust distribution comparison. Neural Computation, 25(5):1324?1370, 2013.

9

"
1997,Perturbative M-Sequences for Auditory Systems Identification,,1430-perturbative-m-sequences-for-auditory-systems-identification.pdf,Abstract Missing,"Perturbative M-Sequences for Auditory
Systems Identification

Mark Kvale and Christoph E. Schreiner?
Sloan Center for Theoretical Neurobiology, Box 0444
University of California, San Francisco
513 Parnassus Ave, San Francisco, CA 94143

Abstract
In this paper we present a new method for studying auditory systems based on m-sequences. The method allows us to perturbatively study the linear response of the system in the presence of
various other stimuli, such as speech or sinusoidal modulations.
This allows one to construct linear kernels (receptive fields) at the
same time that other stimuli are being presented. Using the method
we calculate the modulation transfer function of single units in the
inferior colli cui us of the cat at different operating points and discuss
nonlinearities in the response.

1

Introduction

A popular approach to systems identification, i.e., identifying an accurate analytical model for the system behavior, is to use Volterra or Wiener expansions to
model behavior via functional Taylor or orthogonal polynomial series, respectively
[Marmarelis and Marmarelis1978]. Both approaches model the response r(t) as a
linear combination of small powers of the stimulus s(t). Although effective for mild
nonlinearities, deriving the linear combinations becomes numerically unstable for
highly nonlinear systems. A more serious problem is that many biological systems
are adaptive, i.e., the system behavior is dependent on the stimulus ensemble. For
instance, [Rieke et al.1995] found that in the auditory nerve of the bullfrog linearity
and information rates depended sensitively on whether a white noise or naturalistic
ensemble is used.
One approach to handling these difficulties is to forgo the full expansion, and simply compute the linear response to small (perturbative) stimuli in the presence
of various different ensembles, or operating points. By collecting linear responses
? Email: kvale@phy.ucsf.edu and chris@phy.ucsf.edu

Perturbative M-Sequences for Auditory Systems Identification

181

from different operating points, one may fit nonlinear responses as one fits a nonlinear function with a piecewise linear approximation. For adaptive systems the
same procedure would be applied, with different operating points corresponding to
different points along the time axis. Perturbative stimuli have wide application in
condensed-matter physics, where they are used to characterize linear responses such
as resistance, elasticity and viscosity, and in engineering, perturbative analyses are
used in circuit analysis (small signal models) and structural diagnostics (vibration
analysis). In neurophysiology, however, perturbative stimuli are unknown.
An effective stimulus for calculating the perturbative linear response of a system
is the m-sequence. M-sequences have a long history of use in engineering and the
physical sciences, with applications ranging from systems identification to cryptography and cellular communication. In physiology, m-sequences have been used
primarily to compute system kernels [Marmarelis and Marmarelisl978], especially
in the visual system [Pinter and Nabet1987]. In this work, we use perturbative msequences to study the linear response of single units in the inferior colli cui us of a
cat to amplitude-modulated (AM) stimuli. We add a small m-sequence signal to
an AM carrier, which allows us to study the linear behavior of the system near a
particular operating point in a non-destructive manner, i.e., without changing the
operating point. Perturbative m-sequences allow one to calculate linear responses
near the particular stimuli under study with only a little extra effort, and allow us
to characterize the system over a wide range of stimuli, such as sinusoidal AM and
naturalistic stimuli.
The auditory system we selected to study was the response of single units in the
central nucleus of the inferior colliculus (IC) of an anaesthetised cat. Single unit
responses were recorded extraceUularly. Action potentials were amplified and stored
on DAT tape, and were discriminated offline using a commercial computer-based
spike sorter (Brainwave). 20 units were recorded, of which 10 yielded sufficiently
stable responses to be analyzed.

2

M-Sequences and Linear Systems

A binary m-sequence is a two-level pseudo-random sequence of +1's and -1's. The
sequence length is L = 2n - 1, where n is the order of the sequence. Typically, a
binary m-sequence can be generated by a shift register with n bits and feedback
connections derived from an irreducible polynomial over the multiplicative group Z2
[Golomb1982]. For linear systems identification, m-sequences have two important
properties. The first is that m-sequences have nearly zero mean: ~~':OI m[t] = -l.
The second is that the autocorrelation function takes on the impulse-like form
L-l

Smm(T)

= ~ m[t]m[t + T] =

{L-1

if T = 0
otherwise

(1)

Impulse stimuli also have a 8-function autocorrelation function. In the context
of perturbative stimuli, the advantage of an m-sequence stimulus over an impulse
stimulus is that for a given signal to noise ratio, an m-sequence perturbation stays
much closer to the original signal (in the least squares sense) than an impulse perturbation. Thus the perturbed signal does not stray as far from the operating point
and measurement of linear response about that operating point is more accurate.
We model the IC response with a system F through which a scalar stimulus s(t) is
passed to give a response r(t):

r(t) = F[s(t)].

(2)

M. Kvale and C. E. Schreiner

182

For the purposes of this section, the functional F is taken to be a linear functional
plus a DC component. In real experiments, the input and output signal are sampled
into discrete sequences with t becoming an integer indexing the sequence. Then the
system can be written as the discrete convolution
L-1

r[t] = ho

+L

(3)

h[ttls[t - t1]

with kernels ho and h[tt] to be determined. We assume that the system has a finite
memory of M time steps (with perhaps a delay) so that at most M of the h[t]
coefficients are nonzero. To determine the kernels perturbatively, we add a small
amount of m-sequence to a base stimulus so:
s[t] = so[t]

+ am[t].

(4)

Cross-correlating the response with the original m-sequence yields
L-l

L-l

t=o

t=o

L m[t]r[t + r] = L

L-l L- l
m[t]ho +

LL

h[ttlm[t]so[t + r - ttl

t=o tl =0

L-1 L-l

+L

L

ah[tdm[t]m[t + r - tl]'

(5)

t=o tl=O

Using the sum formula for am -sequence above, the first sum in Eq. (5) can be
simplified to -ho. Using the autocorrelation Eq. (1), the third sum in Eq. (5)
simplifies, and we find
L-l
Rrm(r)

= a(L + l)h[r] -

ho - a

L

L-l L-l

h[tl]

+L

tl =0

L

h[tt]m[t]so[t + r -

ttl

(6)

t=o tl =0

Although the values for the kernels h(t) are set implicitly by this equation, the
terms on the right hand side of Eq. (6) are widely different in size for large Land
the equation can be simplified. As is customary in auditory systems, we assume
the DC response ho is small. To estimate the size of the other terms, we compute
statistical estimates of their sizes and look at their scaling with the parameters.
The term a L:~-==~ h[tt] is a sum of M kernel elements; they may be correlated or
uncorrelated, so a conservative estimate of their size is on the order of O( aM).
The last term in (6) is more subtle. We rewrite it as
L-l L-l

LL

L-l
h[tdm[t]so[t + r -

h=O t=o

ttl

=

L

h[tt]p[r, ttl

tl=O

L-l

L m[t]so[t + r - ttl

(7)

t=o

The time series of the ambient stimulus so[t] and m-sequence m[t] are assumed to
be uncorrelated. By the central limit theorem, the sum p[r, tl] will then have an
average of zero with a standard deviation of 0(L 1 / 2 ). If in turn, the terms p[r, ttl
are un correlated with the kernels h[tl], we have that
L-l L-l

L L h[tt]m[t]so[t + r - ttl '"" 0(MI/2 Ll/2)

tl=O t=o

(8)

Perturbative M-Sequences for Auditory Systems Identification

183

If N cycles of the m-sequence are performed, in which sort] is different for each
cycle, all the terms in Eq. (6) scale with N as O(N), except for the double sum.
By the same central limits arguments above, the double sum scales as O(Nl/2).

Putting all these results together into Eq. (6) and solving for the kernels yields
h(r)

a(L1+ 1) Rrm(r) - 0 (

~) + 0 (aN~~~1/2 )

M
1
a(L + 1) Rrm(r) - C L
1

Ml/2
2
+ C aNI/2?1/2'

.
(9)

with the constants C1 , C2 '"" O(h[r]) depending neural firing rate, statistics, etc.,
determined from experiment. If we take the kernel element h(r) to be the first term
in Eq. 9, then the last two terms in Eq. (9) contribute errors in determining the
kernel and can be thought of as noise. Both error terms vanish as L -+ 00 and the
procedure is asymptotically exact for arbitrary uncorrelated stimuli sort]. In order
for the cross-correlation Ram (r) to yield a good estimate, the inequalities
(10)
must hold. In practice, the kernel memory is much smaller than the sequence length,
and the second inequality is the stricter bound. The second inequality represents a
tradeoff among sequence length, number of trials and the size of the perturbation for
a given level of systematic noise in the kernel estimate. For instance, if L = 215 - 1,
N = 10, M = 30, and noise floor at 10%, the perturbation should be larger than
a = 0.095. If no signal sort] is present, then the O(Ml/2a- 1(NL)-1/2) term drops
out and the usual m-sequence cross-correlation result is recovered.

3

M-Sequences for Modulation Response

Previous work, e.g., [M011er and Rees1986, Langner and Schreinerl988] has shown
that many of the cells in the inferior colliculus are tuned not only to a characteristic
frequency, but are also tuned to a best frequency of modulation of the carrier. A
highly simplified model of the IC unit response to sound stimuli is the Ll- N - L2
cascade filter, with L1 a linear tank circuit with a transfer function matching that
of the frequency tuning curve, N a nonlinear rectifying unit, and L2 a linear circuit with a transfer function matching that of the modulation transfer function.
Detecting this modulation is an inherently nonlinear operation and N is not well
approximated by a linear kernel. Thus IC modulation responses will not be well
characterized by ordinary m-sequence stimuli using the methods described in Section 2.
A better approach is to bypass the Ll - N demodulation step entirely and concentrate on measuring L2. This can be accomplished by creating a modulation
m-sequence:
(11)
s[t] = a (so[t] + bm[t]) sin[wet],
where Iso[t]1 :::; 1 is the ambient signal, i.e., the operating point, m[t] E [-1,1] is an
m-sequence added with amplitude b, and We is the carrier frequency. Demodulation
gives the effective input stimulus

sm[t] = a (so[t] + bm[t]) .

(12)

Note that there is little physiological evidence for a purely linear rectifier N. In
fact, both the work of [M011er and Rees1986, Rees and M011er1987] and ours below
show that there is a nonlinear modulation response. Taking a modulation transfer

184

M. Kvale and C. E. Schreiner

function seriously, however, implies that one assumes that modulation response
is linear, which implies that the static nonlinearity used is something like a halfwave rectifier. Linearity is used here as a convenient assumption for organizing the
stimulus and asking whether nonlinearities exist.
For full m-sequence modulation (so[t] = 1 and b = 1) the stimulus Sm and the
neural response can be used to compute, via the Lee--Schetzen cross-correlation,
the modulation transfer function for the L2 system. Alternatively, for b ? 1, the
m-sequence is a perturbation on the underlying modulation envelope sort]. The
derivation above shows that the linear modulation kernel can also be calculated
using a Lee--Schetzen cross-correlation. M-sequences at full modulation depth were
first used by [M0ller and Rees1986, Rees and M011erI987] to calculate white-noise
kernels. Here, we are using m-sequence in a different way-we are calculating the
small-signal properties around the stimulus sort].
The m-sequences used in this experiment were of length 215 -1 = 32,767. For each
unit, 10 cycles of the m-sequence were presented back-to-back. After determining
the characteristic frequency of a unit, stimuli were presented which never differed
from the characteristic frequency by more than 500 Hz. Figure 1 depicts the sinusoidal and m-sequence components and their combined result. The stimuli were
presented in random order so as to mitigate adaptation effects.

Figure 1: A depiction of stimuli used in the experiment. The top
a pure sine wave modulation at modulation depth 0.8. The middle
an m-sequence modulation at depth 1.0. The bottom graph shows a
m-sequence modulation at depth 0.2 added to a sinusoidal modulation

4

graph shows
graph shows
perturbative
at depth 0.8.

Results

Figure 2 shows the spike rates for both the pure sinusoid and the combined sinusoid
and m-sequence stimuli. Note that the rates are nearly the same, indicating that
the perturbation did not have a large effect on the average response of the unit.
The unit shows an adaptation in firing rate over the 10 trials, but we did not find

Perturbative M-Sequences for Auditory Systems Identification

185

a statistically significant change in the kernels of different trials in any of the units.

G----e sinusoid
~ sinusoid + m-sequence

.-...

o

Q)

~

80.0

C/)

Q)
~

'5.

~

60.0

Q)

?i
""40.0

100.0

200.0

300.0

400.0

500.0

Time (sec)

Figure 2: A plot of the unit firing rates for both the pure sinusoid and the sinusoid +
m-sequence stimuli. The carrier frequency is 9 kHz and is close to the characteristic
frequency of the neuron. The sinusoidal modulation has a frequency of 20 Hz and
the m-sequence modulation has a frequency of 800 sec-I .
Figure 3 shows modulation response kernels at several different values of the modulation depth. Note that if the system was a linear, superposition would cause all
the kernels to be equivalent; in fact it is seen that the nonlinearities are of the same
magnitude as the linear response. In this particular unit, the triphasic behavior
at small modulation depths gives way to monophasic behavior at high modulation
depths and an FFT of the kernel shows that the bandwidth of the modulation
transfer function also broadens with increasing depth.

5

Discussion

In this paper, we have introduced a new type of stimulus, perturbative m-sequences,
for the study of auditory systems and derived their properties. We then applied
perturbative m-sequences to the analysis of the modulation response of units in the
Ie, and found the linear response at a few different operation point. We demonstrated that the nonlinear response in the presence of sinusoidal modulations are
nearly as large as the linear response and thus a description of unit response with
only an MTF is incomplete. We believe that perturbative stimuli can be an effective
tool for the analysis of many systems whose units phase lock to a stimulus.
The main limiting factor is the systematic noise discussed in section 2, but it is
possible to trade off duration of measurement and size of the perturbation to achieve
good results. The m-sequence stimuli also make it possible to derive higher order
information [Sutter1987] and with a suitable noise floor, it may be possible to derive
second-order kernels as well.
This work was supported by The Sloan foundation and ONR grant number N0001494-1-0547.

M. Kvale and C. E. Schreiner

186

Response vs. modulation depth
sine wave @40Hz + pert. m-sequence

90.0

- 0.2
- 0.4
-0.6
-0.8
-1.0

70.0
50.0
CD
:::>

""C

""""
a.

30.0

E

ns

10.0
-10.0
-30.0
-50.0
0.0

5.0

10.0

15.0

20.0

time from spike (milliseconds)

Figure 3: A plot of the temporal kernels derived from perturbative m-sequence
stimuli in conjunction with sinusoidal modulations at various modulation depth.
The y-axis units are amplitude per spike and the x-axis is in milliseconds before the
spike.

References
[Golomb1982] S. W. Golomb. Shift Register Sequences. Aegean Park Press, Laguna
Hills, CA, 1982.
[Langner and Schreiner1988] G. Langner and C. E. Schreiner. Periodicity coding
in the inferior colliculus of the cat: 1. neuronal mechanisms. Journal of Neurophysiology, 60: 1799-1822, 1988.
[Marmarelis and Marmarelis1978] Panos Z. Marmarelis and Vasilis Z. Marmarelis.
Analysis of Physiological Systems. Plenum Press, New York, NY, 10011, 1978.
[M011er and Rees1986] Aage R. M011er and Adrian Rees. Dynamic properties of
single neurons in the inferior colliculus of the rat. Hearing Research, 24:203-215,
1986.
[Pinter and Nabet1987] Robert B. Pinter and Bahram Nabet. Nonlinear Vision.
CRC Press, Boca Raton, FL, 1987.
[Rees and M011er1987] Adrian Rees and Aage R. M01ler. Stimulus properties influencing the responses of inferior colliculus neurons to amplitude-modulated
sounds. Hearing Research, 27:129-143, 1987.
[Rieke et al.1995] F. Rieke, D. A. Bodnar, and W. Bialek. Naturalistic stimuli
increase the rate and efficiency of information transmission by primary auditory
afi""erents. Proceedings of the Royal Society of London. Series B, 262:259-265,
1995.
[Sutter1987] E. E. Sutter. A practical non-stochastic approach to nonlinear timedomain analysis. In Vasilis Z. Marmarelis, editor, Advanced Methods of Physiological Modeling, Vol. 1, pages 303-315. Biomedical Simulations Resource, University of Southern California, Los Angeles, CA 90089-1451, 1987.

"
2008,Overlaying classifiers: a practical approach for optimal ranking,,3469-overlaying-classifiers-a-practical-approach-for-optimal-ranking.pdf,"ROC curves are one of the most widely used displays to evaluate performance of scoring functions. In the paper, we propose a statistical method for directly optimizing the ROC curve. The target is known to be the regression function up to an increasing transformation and this boils down to recovering the level sets of the latter. We propose to use classifiers obtained by empirical risk minimization of a weighted classification error and then to construct a scoring rule by overlaying these classifiers. We show the consistency and rate of convergence to the optimal ROC curve of this procedure in terms of supremum norm and also, as a byproduct of the analysis, we derive an empirical estimate of the optimal ROC curve.","Overlaying classifiers:
a practical approach for optimal ranking

St?ephan Cl?emenc?on
Telecom Paristech (TSI) - LTCI UMR Institut Telecom/CNRS 5141
stephan.clemencon@telecom-paristech.fr
Nicolas Vayatis
ENS Cachan & UniverSud - CMLA UMR CNRS 8536
vayatis@cmla.ens-cachan.fr

Abstract
ROC curves are one of the most widely used displays to evaluate performance
of scoring functions. In the paper, we propose a statistical method for directly
optimizing the ROC curve. The target is known to be the regression function up
to an increasing transformation and this boils down to recovering the level sets of
the latter. We propose to use classifiers obtained by empirical risk minimization of
a weighted classification error and then to construct a scoring rule by overlaying
these classifiers. We show the consistency and rate of convergence to the optimal
ROC curve of this procedure in terms of supremum norm and also, as a byproduct
of the analysis, we derive an empirical estimate of the optimal ROC curve.

1

Introduction

In applications such as medical diagnosis, credit risk screening or information retrieval, one aims at
ordering instances under binary label information. The problem of ranking binary classification data
is known in the machine learning literature as the bipartite ranking problem ([FISS03], [AGH+ 05],
[CLV08]). A natural approach is to find a real-valued scoring function which mimics the order
induced by the regression function. A classical performance measure for scoring functions is the
Receiver Operating Characteristic (ROC) curve which plots the rate of true positive against false
positive ([vT68], [Ega75]). The ROC curve offers a graphical display which permits to judge rapidly
how a scoring rule discriminates the two populations (positive against negative). A scoring rule
whose ROC curve is close to the diagonal line does not discriminate at all, while the one lying above
all others is the best possible choice. From a statistical learning perspective, risk minimization (or
performance maximization) strategies for bipartite ranking have been based mostly on a popular
summary of the ROC curve known as the Area Under a ROC Curve (AUC - see [CLV08], [FISS03],
[AGH+ 05]) which corresponds to the L1 -metric on the space of ROC curves. In the present paper,
we propose a statistical methodology to estimate the optimal ROC curve in a stronger sense than
the AUC sense, namely in the sense of the supremum norm. In the same time, we will explain how
to build a nearly optimal scoring function. Our approach is based on a simple observation: optimal
scoring functions can be represented from the collection of level sets of the regression function.
Hence, the bipartite ranking problem may be viewed as a ?continuum? of classification problems
with asymmetric costs where the targets are the level sets. In a nonparametric setup, regression
or density level sets can be estimated with plug-in methods ([Cav97], [RV06], [AA07], [WN07],
...). Here, we take a different approach based on a weighted empirical risk minimization principle.
We provide rates of convergence with which an optimal point of the ROC curve can be recovered
according to this principle. We also develop a practical ranking method based on a discretization of
the original problem. From the resulting classifiers and their related empirical errors, we show how
1

to build a linear-by-part estimate of the optimal ROC curve and a quasi-optimal piecewise constant
scoring function. Rate bounds in terms of the supremum norm on ROC curves for these procedures
are also established.
The rest of the paper is organized as follows: in Section 2, we present the problem and give some
properties of ROC curves, in Section 3, we provide a statistical result for the weighted empirical risk
minimization, and in Section 4, we develop the main results of the paper which describe the statistical performance of a scoring rule based on overlaying classifiers as well as the rate of convergence
of the empirical estimate of the optimal ROC curve.

2

Bipartite ranking, scoring rules and ROC curves

Setup. We study the ranking problem for classification data with binary labels. The data are assumed
to be generated as i.i.d. copies of a random pair (X, Y ) ? X ? {?1, +1} where X is a random
descriptor living in the measurable space X and Y represents its binary label (relevant vs. irrelevant,
healthy vs. sick, ...). We denote by P = (?, ?) the distribution of (X, Y ), where ? is the marginal
distribution of X and ? is the regression function (up to an affine transformation): ?(x) = P{Y =
1 | X = x}, x ? X . We will also denote by p = P{Y = 1} the proportion of positive labels.
In the sequel, we assume that the distribution ? is absolutely continuous with respect to Lebesgue
measure.
Optimal scoring rules. We consider the approach where the ordering can be derived by the means
of a scoring function s : X ? R: one expects that the higher the value s(X) is, the more likely the
event ?Y = +1? should be observed. The following definition sets the goal of learning methods in
the setup of bipartite ranking.
Definition 1 (Optimal scoring functions) The class of optimal scoring functions is given by the set
S ? = { s? = T ? ? | T : [0, 1] ? R strictly increasing }.
Interestingly, it is possible to make the connection between an arbitrary (bounded) optimal scoring
function s? ? S ? and the distribution P (through the regression function ?) completely explicit.
Proposition 1 (Optimal scoring functions representation, [CV08]) A bounded scoring function
s? is optimal if and only if there exist a nonnegative integrable function w and a continuous random
variable V in (0, 1) such that:
?x ? X ,

s? (x) = inf s? + E (w(V ) ? I{?(x) > V }) .
X

A crucial consequence of the last proposition is that solving the bipartite ranking problem amounts
to recovering the collection {x ? X | ?(x) > u}u?(0,1) of level sets of the regression function ?.
Hence, the bipartite ranking problem can be seen as a collection of overlaid classification problems.
This view was first introduced in [CV07] and the present paper is devoted to the description of a
statistical method implementing this idea.
ROC curves. We now recall the concept of ROC curve and explain why it is a natural choice of
performance measure for the ranking problem with classification data. We consider here only true
ROC curves which correspond to the situation where the underlying distribution is known. First,
we need to introduce some notations. For a given scoring rule s, the conditional cdfs of the random
variable s(X) are denoted by Gs and Hs . We also set, for all z ? R:
? s (z) = 1 ? Gs (z) = P {s(X) > z | Y = +1} ,
G
? s (z) = 1 ? Hs (z) = P {s(X) > z | Y = ?1} .
H
to be the residual conditional cdfs of the random variable s(X). When s = ?, we shall denote the
??, H
? ? respectively.
previous functions by G? , H ? , G
We introduce the notation Q(Z, ?) to denote the quantile of order 1 ? ? for the distribution of a
random variable Z conditioned on the event Y = ?1. In particular, the following quantile will be
of interest:
? ??1 (?) ,
Q? (?) = Q(?(X), ?) = H
2

where we have used here the notion of generalized inverse F ?1 of a c`adl`ag function F : F ?1 (z) =
inf{t ? R | F (t) ? z}. We now turn to the definition of the ROC curve.
Definition 2 (True ROC curve) The ROC curve of a scoring function s is the parametric curve:

? s (z), G
? s (z)
z 7? H
for thresholds z ? R. It can also be defined as the plot of the function:
?s ? H
? ?1 (?) = G
? s (Q(s(X), ?)) .
ROC(s, ? ) : ? ? [0, 1] 7? G
s
By convention, points of the curve corresponding to possible jumps (due to possible degenerate
points of Hs or Gs ) are connected by line segments, so that the ROC curve is always continuous.
For s = ?, we take the notation ROC? (?) = ROC(?, ?).
? s is also called the true positive rate while H
? s is the false positive rate, so that
The residual cdf G
the ROC curve is the plot of the true positive rate against the false positive rate.
Note that, as a functional criterion, the ROC curve induces a partial order over the space of all scoring functions. Some scoring function might provide a better ranking on some part of the observation
space and a worst one on some other. A natural step to take is to consider local properties of the
ROC curve in order to focus on best instances but this is not straightforward as explained in [CV07].
We expect optimal scoring functions to be those for which the ROC curve dominates all the others
for all ? ? (0, 1). The next proposition highlights the fact that the ROC curve is relevant when
evaluating performance in the bipartite ranking problem.
Proposition 2 The class S ? of optimal scoring functions provides the best possible ranking with
respect to the ROC curve. Indeed, for any scoring function s, we have:
?? ? (0, 1) , ROC? (?) ? ROC(s, ?) ,
and ?s? ? S ? , ?? ? (0, 1) , ROC(s? , ?) = ROC? (?).
The following result will be needed later.
Proposition 3 We assume that the optimal ROC curve is differentiable. Then, we have, for any ?
such that Q? (?) < 1:
d
1?p
Q? (?)
ROC? (?) =
?
.
d?
p
1 ? Q? (?)
For proofs of the previous propositions and more details on true ROC curves, we refer to [CV08].

3

Recovering a point on the optimal ROC curve

We consider here the problem of recovering a single point of the optimal ROC curve from a sample
of i.i.d. copies {(Xi , Yi )}i=1,...,n of (X, Y ). This amounts to recovering a single level set of the
regression function ? but we aim at controlling the error in terms of rates of false positive and true
positive. For any measurable set C ? X , we set the following notations:
?(C) = P(X ? C | Y = ?1) and ?(C) = P(X ? C | Y = +1) .
We also define the weighted classification error:
L? (C) = 2p(1 ? ?) (1 ? ?(C)) + 2(1 ? p)? ?(C) ,
with ? ? (0, 1) being the asymmetry factor.
Proposition 4 The optimal set for this error measure is C?? = {x : ?(x) > ?}. We have indeed,
for all C ? X :
L? (C?? ) ? L? (C) .
Also the optimal error is given by:
L? (C?? ) = 2E min{?(1 ? ?(X)), (1 ? ?)?(X)} .
The excess risk for an arbitrary set C can be written:
L? (C) ? L? (C?? ) = 2E (| ?(X) ? ? | I{X ? C?C?? }) ,
where ? stands for the symmetric difference between sets.
3

The empirical counterpart of the weighted classification error can be defined as:
n
n
X
2(1 ? ?) X
? ? (C) = 2?
L
I{Yi = ?1, Xi ? C} +
I{Yi = +1, Xi ?
/ C} .
n i=1
n
i=1

This leads to consider the weighted empirical risk minimizer over a class C of candidate sets:
? ? (C).
C?? = arg min L
C?C

The next result provides rates of of convergence of the weighted empirical risk minimizer C?? to the
best set in the class in terms of the two types of error ? and ?.
Theorem 1 Let ? ? (0, 1). Assume that C is of finite VC dimension V and contains C?? . Suppose
also that both G? and H ? are twice continuously differentiable with strictly positive first derivatives
and that ROC? has a bounded second derivative. Then, for all ? > 0, there exist constants c(V )
independent of ? such that, with probability at least 1 ? ?:

1
c(V )
log(1/?) 3
|?(C?? ) ? ?(C?? )| ? p
.
?
n
p(1 ? ?)
?
The same result
palso holds for the excess risk of C? in terms of the rate ? of true positive with a
factor term of (1 ? p)? in the denominator instead .
It is noteworthy that, while convergence in terms of classification error is expected to be of the order
of n?1/2 , its two components corresponding to the rate of false positive and true positive present
slower rates.

4

Nearly optimal scoring rule based on overlaying classifiers

Main result. We now propose to collect the classifiers studied in the previous section in order to
build a scoring function for the bipartite ranking problem. From Proposition 1, we can focus on
optimal scoring rules of the form:
Z
?
s (x) = I{x ? C?? } ?(d?),
(1)
where the integral is taken w.r.t. any positive measure ? with same support as the distribution of
?(X).
Consider a fixed partition ?0 = 0 < ?1 ? . . . ? ?K ? 1 = ?K+1 of the interval (0, 1). We can
then construct an estimator of s? by overlaying a finite collection of (estimated) density level sets
C??1 , . . . , C??K :
K
X
s?(x) =
I{x ? C??i },
i=1

which may be seen as an empirical version of a discrete version of the target s? .
In order to consider the performance of such an estimator, we need to compare the ROC curve of s? to
the optimal ROC curve. However, if the sequence {C??i }i=1,...,K is not decreasing, the computation
of the ROC curve as a function of the errors of the overlaying classifiers becomes complicated.
The main result of the paper is the next theorem which is proved for a modified sequence which
yields to a different estimator. We introduce: {C??i }1?i?K defined by:
C??1 = C??1 and C??i+1 = C??i ? C??i+1 for all i ? {1, . . . , K ? 1} .
The corresponding scoring function is then given by:
s?K (x) =

K
X

I{x ? C??i } .

i=1

4

(2)

Hence, the ROC curve of s?K is simply the broken line that connects the knots (?(C??i ), ?(C??i )),
0 ? i ? K + 1.
The next result offers a rate bound in the ROC space, equipped with a sup-norm. Up to our knowledge, this is the first result on the generalization ability of decision rules in such a functional space.
Theorem 2 Under the same assumptions as in Theorem 1 and with the previous notations, we set
K = Kn ? n1/8 . Fix  > 0. Then, there exists a constant c such that, with probability at least
1 ? ?, we have:
c log(1/?)
sup |ROC? (?) ? ROC(?sK , ?)| ?
.
n1/4
??[,1?]
Remark 1 (P ERFORMANCE OF CLASSIFIERS AND ROC CURVES .) In the present paper, we have
adopted a scoring approach to ROC analysis which is somehow related to the evaluation of the
performance of classifiers in ROC space. Using combinations of such classifiers to improve performance in terms of ROC curves has also been pointed out in [BDH06] and [BCT07].
Remark 2 (P LUG - IN ESTIMATOR OF THE REGRESSION FUNCTION .) Note that taking ? = ?
the
measure over [0, 1] in the expression of s? leads to the regression function ?(x) =
R Lebesgue
I{x ? C?? } d?. An estimator for the regression function could be the following: ??K (x) =
PK+1
?
i=1 (?i ? ?i?1 )I{x ? C?i }.
Remark 3 (A DAPTIVITY OF THE PARTITION .) A natural extension of the approach would be to
consider a flexible partition (?i )i which could possibly be adaptively chosen depending on the local
regularity of the ROC curve. For now, it is not clear how to extend the method of the paper to
take into account adaptive partitions, however we have investigated such partitions corresponding
to different approximation schemes of the optimal ROC curve elsewhere ([CV08]), but the rates of
convergence obtained in the present paper are faster.
Optimal ROC curve approximation and estimation. We now provide some insights on the previous result. The key for the proof of Theorem 2 is the idea of a piecewise linear approximation of
the optimal ROC curve.
We introduce some notations. Let ?0 = 0 < ?1 < . . . < ?K < ?K+1 = 1 be a given partition
of [0, 1] such that maxi?{0,...,K} {?i+1 ? ?i } ? ?. Set: ?i ? {0, . . . , K + 1}, ?i? = ?(C??i ) and
?i? = ?(C??i ).
The broken line that connects the knots {(?i? , ?i? ); 0 ? i ? K + 1} provides a piecewise linear
(concave) approximation/interpolation of the optimal ROC curve ROC? . In the spirit of the finite
element method (FEM, see [dB01] for instance), we introduce the ?hat functions? defined by:
?
?
?i ? {1, . . . , K ? 1}, ??i ( ? ) = ?( ? ; (?i?1
, ?i? )) ? ?( ? ; (?i? , ?i+1
)),
with the notation ?(?, (?1 , ?2 )) = (? ? ?1 )/(?2 ? ?1 ) ? I{? ? [?1 , ?2 ]} for all ?1 < ?2 . We
?
also set ??K ( ? ) = ?( ? ; (?K
, 1)) for notational convenience. The piecewise linear approximation
?
of ROC may then be written as:
K
X
?
] (?) =
ROC
?i? ??i (?) .
i=1
?

] (?), we propose: i) to find an estimate C??i of the
In order to obtain an empirical estimator of ROC
true level set C??i based on the training sample {(Xi , Yi )}i=1,...,n as in Section 3, ii) to compute the
corresponding errors ?
? i and ??i using a test sample {(Xi0 , Yi0 )}i=1,...,n . Hence we define:
n
n
1 X
1 X
0
0
?
I{Xi ? C, Yi = ?1} and ?i (C) =
I{Xi0 ? C, Yi0 = +1},
?
? i (C) =
n? i=1
n+ i=1
Pn
with n+ = i=1 I{Yi0 = +1} = n ? n? . We set ?
?i = ?
? i (C??i ) and ??i = ??i (C??i ). We propose
?
] (?):
the following estimator of ROC
\? (?) =
ROC

K
X
i=1

5

??i ??i (?),

where ??K (?) = ?(.; (?
?K , 1)) and ??i (?) = ?(.; (?
? i?1 , ?
? i )) ? ?(.; (?
?i, ?
? i+1 )) for 1 ? i < K.
?
[
Hence, ROC is the broken line connecting the empirical knots {(?
?i , ?i ); 0 ? i ? K + 1}.
The next result takes the form of a deviation bound for the estimation of the optimal ROC curve.
It quantifies the order of magnitude of a confidence band in supremum norm around an empirical
estimate based on the previous approximation scheme with empirical counterparts.
Theorem 3 Under the same assumptions as in Theorem 1 and with the previous notations, set K =
Kn ? n1/6 . Fix  > 0. Then, there exists a constant c such that, with probability at least 1 ? ?,

1/3
\? (?) ? ROC? (?)| ? c?1 log(n/?)
sup |ROC
.
n
??[,1?]

5

Conclusion

We have provided a strategy based on overlaid classifiers to build a nearly-optimal scoring function.
Statistical guarantees are provided in terms of rates of convergence for a functional criterion which
is the ROC space equipped with a supremum norm. This is the first theoretical result of this nature.
To conclude, we point out that ROC analysis raises important and novel issues for statistical learning
and we hope that the present contribution gives a flavor of possible research directions.

Appendix - Proof section
Proof of Theorem 1. The idea of the proof is to relate the excess risk in terms of ?-error to the excess
risk in terms of weighted classification error. First we re-parameterize the weighted classification
error. Set C(?) = {x ? X | ?(x) > Q? (?)} and:
`? (?) = L? (C(?)) = 2(1 ? p)? ? + 2p(1 ? ?)(1 ? ROC? (?))
Since ROC? is assumed to be differentiable and using Proposition 3, it is easy to check that the
value ?? = ?(C?? ) minimizes `? (?). Denote by `?? = `? (?? ). It follows from a Taylor expansion
of `? (?) around ?? at the second order that there exists ?0 ? [0, 1] such that:
d2
ROC? (?0 ) (? ? ?? )2
d?2
Using also the fact that ROC? dominates any other curve of the ROC space, we have: ?C ? X
measurable, ?(C) ? ROC? (?(C)). Also, by assumption, there exists m such that: ?? ? [0, 1],
d2
?
?
?
d?2 ROC (?) ? ?m. Hence, since `? (?(C? )) = L? (C? ), we have:



2
1
L? (C?? ) ? L? (C?? ) .
?(C?? ) ? ?(C?? ) ?
mp(1 ? ?)
`? (?) = `?? ? p(1 ? ?)

We have obtained the desired inequality. It remains to get the rate of convergence for the weighted
empirical risk.
Now set: F ? = pG? + (1 ? p)H ? . We observe that: ?t > 0, P(|?(X) ? ?| ? t) = F ? (? +
t) ? F ? (? ? t) ? 2t supu (F ? )0 (u). We have thus shown that the distribution satisfies a modified
Tsybakov?s margin condition [Tsy04], for all ? ? [0, 1], of the form:
?

P(|?(X) ? ?| ? t) ? D t 1?? .
with ? = 1/2 and D = 2 supu (F ? )0 (u). Adapting slightly the argument used in [Tsy04], [BBL05],
we have that, under the modified margin condition, there exists a constant c such that, with probability 1 ? ?:
 1

log(1/?) 2??
?
?
?
L? (C? ) ? L? (C? ) ? c
.
n
Proof of Theorem 2. We note ?
? i = ?(C??i ), ??i = ?(C??i ) and also ??i ( ? ) = ?( ? ; (?
?i?1 , ?
? i )) ?
PK ? ?
?( ? ; (?
?i , ?
? i+1 )). We then have ROC(?sK , ?) =
?
?
(?)
and
we
can
use
the
following
i
i
i=1
6

decomposition, for any ? ? [0, 1]:
?

ROC (?) ? ROC(?sK , ?) =

?

ROC (?) ?

K
X

!
ROC (?
?i )??i (?)
?

+

i=1

K
X
(ROC? (?
?i ) ? ??i )??i (?) .
i=1

It is well-known folklore in linear approximation theory ([dB01]) that if s?K is a piecewise constant
scoring function whose ROC curve interpolates the points {(?
? i , ROC? (?
? i ))}i=0,...,K of the optimal
ROC curve, then we can bound the first term (which is positive), ?? ? [0, 1], by:
?

d2
1
ROC? (?) ? max (?
?i+1 ? ?
? i )2 .
inf
0?i?K
8 ??[0,1] d?2

Now, to control the second term, we upper bound the following quantity:
|ROC? (?
?i ) ? ??i | ? sup
??[0,1]

d
ROC? (?) ? |?
?i ? ?i? | + |?i? ? ??i |
d?

We further bound: |?
?i ? ?i? | ? |?
?i ? ?i | + |?i ? ?i? | where ?i = ?(C?i ). In order to deal with the
first term, the next lemma will be needed:
Lemma 1 We have, for all k ? {1, . . . , K}:
?(C?k ) = ?(C?k ) + (k ? 1)OP (n?1/4 ) .
where the notation OP (1) is used for a r.v. which is bounded in probability.
From the lemma, it follows that: max1?i?K |?
?i ? ?i | = OP (Kn?1/4 ). We can then use Theorem
1 with ? replaced by ?/K to get that max1?i?K |?i ? ?i? | = OP ((n?1 log K)1/3 ). The same
inequalities hold with the ??s. It remains to control the quantity ?
? i+1 ? ?
? i . We have:
|?
? i+1 ? ?
? i |? max | ?(C?k ) ? ?(C?k?1 ) | +K OP (n?1/4 ) .
1?k?K

We have that:
?
max | ?(C?k ) ? ?(C?k?1 ) |? 2 max | ?(C?k ) ? ?(Ck? ) | + max | ?(Ck? ) ? ?(Ck?1
)|

1?k?K

1?k?K

1?k?K

As before, we have that the first term is of the order (log K/n)1/3 and since the second derivative
of the optimal ROC curve is bounded, the second term is of the order K ?1 . Eventually, we choose
K in order to optimize the quantity: K ?2 + (log K/n)2/3 + K 2 n?1/2 + Kn?1/4 + (log K/n)1/3 .
As only the first and the third term matter, this leads to the choice of K = Kn ? n1/8 .
Proof of Lemma 1.
We have that ?(C?2 ) = ?(C?2 ) + ?(C?1 \ C?2 ). Therefore, since C1? ? C2? and observing that
?(C?1 \ C?2 ) = ?(((C?1 \ C1? ) ? (C?1 ? C1? )) \ ((C?2 \ C2? ) ? (C?2 ? C2? )) ,
it suffices to use the additivity of the probability measure ?(.) to get: ?(C?2 ) = ?(C?2 ) + OP (n?1/4 ).
Eventually, errors are stacked and we obtain the result.
Proof of Theorem 3.
We use the following decomposition, for any fixed ? ? (0, 1):
!
!
K
K
X
X
?
?
?
?
\? (?) ?
\? (?)?ROC (?) = ROC
ROC
ROC (?
?i )??i (?) +
ROC (?
?i )??i (?) ? ROC (?) .
i=1

i=1

Therefore, we have by a triangular inequality: ?? ? [0, 1],


K


X
 \?

?
?
ROC (?
?i )?i (?) ? max |??i ? ?i | + |?i ? ?i? | + |ROC? (?i? ) ? ROC? (?
?i )| .
ROC (?) ?
 1?i?K

i=1

7

And, by the finite increments theorem, we have:
|ROC

?

(?i? )

?

? ROC (?
?i )| ?

!
d
?
sup
ROC (?) (|?i? ? ?i | + |?i ? ?
? i |) .
d?
??[0,1]

For the other term, we use the same result on approximation as in the proof of Theorem 2:


K
X

1
d2


?
?
ROC (?
?i )??i (?) ? ROC (?) ? ?
ROC? (?) ? max (?
? i+1 ? ?
? i )2
inf

0?i?K


8
??[0,1] d?2
i=1
?
max (?
?i+1 ? ?
? i ) ? max (?i+1
? ?i? ) + 2 max |?i? ? ?i | + 2 max |?
?i ? ?i | .

0?i?K

0?i?K

1?i?K

1?i?K

?1/2

?
We recall that: max1?i?K |?
?i ? ?i |. = OP (Kn
). Moreover, max0?i?K {?i+1
? ?i? } is of the
?1
?
order of K . And with probability at least 1 ? ?, we have that max1?i?K |?i ? ?i | is bounded as
in Theorem 1, except that ? is replaced by ?/K in the bound. Eventually, we get the generalization
bound: K ?2 + (log K/n)1/3 , which is optimal for a number of knots: K ? n1/6 .

References
[AA07]

J.-Y. Audibert and A.Tsybakov. Fast learning rates for plug-in classifiers. Annals of
statistics, 35(2):608?633, 2007.
[AGH+ 05] S. Agarwal, T. Graepel, R. Herbrich, S. Har-Peled, and D. Roth. Generalization bounds
for the area under the ROC curve. J. Mach. Learn. Res., 6:393?425, 2005.
[BBL05] S. Boucheron, O. Bousquet, and G. Lugosi. Theory of Classification: A Survey of Some
Recent Advances. ESAIM: Probability and Statistics, 9:323?375, 2005.
[BCT07] M. Barreno, A.A. Cardenas, and J.D. Tygar. Optimal ROC curve for a combination of
classifiers. In NIPS?07, 2007.
[BDH06] F.R. Bach, D.Heckerman, and Eric Horvitz. Considering cost asymmetry in learning
classifiers. Journal of Machine Learning Research, 7:1713?1741, 2006.
[Cav97]
L. Cavalier. Nonparametric estimation of regression level sets. Statistics, 29:131?160,
1997.
[CLV08] S. Cl?emenc?on, G. Lugosi, and N. Vayatis. Ranking and empirical risk minimization of
U-statistics. The Annals of Statistics, 36(2):844?874, 2008.
[CV07]
S. Cl?emenc?on and N. Vayatis. Ranking the best instances. Journal of Machine Learning
Research, 8:2671?2699, 2007.
[CV08]
S. Cl?emenc?on and N. Vayatis. Tree-structured ranking rules and approximation of the
optimal ROC curve. Technical Report hal-00268068, HAL, 2008.
[dB01]
C. de Boor. A practical guide to splines. Springer, 2001.
[Ega75]
J.P. Egan. Signal Detection Theory and ROC Analysis. Academic Press, 1975.
[FISS03] Y. Freund, R. D. Iyer, R. E. Schapire, and Y. Singer. An efficient boosting algorithm for
combining preferences. Journal of Machine Learning Research, 4:933?969, 2003.
[RV06]
P. Rigollet and R. Vert. Fast rates for plug-in estimators of density level sets. Technical
Report arXiv:math/0611473v2, arXiv:math/0611473v2, 2006.
[Tsy04]
A. Tsybakov. Optimal aggregation of classifiers in statistical learning. Annals of Statistics, 32(1):135?166, 2004.
[vT68]
H.L. van Trees. Detection, Estimation, and Modulation Theory, Part I. Wiley, 1968.
[WN07]
R. Willett and R. Nowak. Minimax optimal level set estimation. IEEE Transactions on
Image Processing, 16(12):2965?2979, 2007.

8

"
2010,Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression,,4145-predicting-execution-time-of-computer-programs-using-sparse-polynomial-regression.pdf,"Predicting the execution time of computer programs is an important but challenging problem in the community of computer systems. Existing methods require experts to perform detailed analysis of program code in order to construct predictors or select important features. We recently developed a new system to automatically extract a large number of features from program execution on sample inputs, on which prediction models can be constructed without expert knowledge. In this paper we study the construction of predictive models for this problem. We propose the SPORE (Sparse POlynomial REgression) methodology to build accurate prediction models of program performance using feature data collected from program execution on sample inputs. Our two SPORE algorithms are able to build relationships between responses (e.g., the execution time of a computer program) and features, and select a few from hundreds of the retrieved features to construct an explicitly sparse and non-linear model to predict the response variable. The compact and explicitly polynomial form of the estimated model could reveal important insights into the computer program (e.g., features and their non-linear combinations that dominate the execution time), enabling a better understanding of the program?s behavior. Our evaluation on three widely used computer programs shows that SPORE methods can give accurate prediction with relative error less than 7% by using a moderate number of training data samples. In addition, we compare SPORE algorithms to state-of-the-art sparse regression algorithms, and show that SPORE methods, motivated by real applications, outperform the other methods in terms of both interpretability and prediction accuracy.","Predicting Execution Time of Computer Programs
Using Sparse Polynomial Regression

Ling Huang
Intel Labs Berkeley
ling.huang@intel.com
Byung-Gon Chun
Intel Labs Berkeley
byung-gon.chun@intel.com

Jinzhu Jia
UC Berkeley
jzjia@stat.berkeley.edu
Petros Maniatis
Intel Labs Berkeley
petros.maniatis@intel.com

Bin Yu
UC Berkeley
binyu@stat.berkeley.edu
Mayur Naik
Intel Labs Berkeley
mayur.naik@intel.com

Abstract
Predicting the execution time of computer programs is an important but challenging problem in the community of computer systems. Existing methods require experts to perform detailed analysis of program code in order to construct predictors
or select important features. We recently developed a new system to automatically
extract a large number of features from program execution on sample inputs, on
which prediction models can be constructed without expert knowledge. In this
paper we study the construction of predictive models for this problem. We propose the SPORE (Sparse POlynomial REgression) methodology to build accurate
prediction models of program performance using feature data collected from program execution on sample inputs. Our two SPORE algorithms are able to build
relationships between responses (e.g., the execution time of a computer program)
and features, and select a few from hundreds of the retrieved features to construct an explicitly sparse and non-linear model to predict the response variable.
The compact and explicitly polynomial form of the estimated model could reveal
important insights into the computer program (e.g., features and their non-linear
combinations that dominate the execution time), enabling a better understanding
of the program?s behavior. Our evaluation on three widely used computer programs shows that SPORE methods can give accurate prediction with relative error
less than 7% by using a moderate number of training data samples. In addition, we
compare SPORE algorithms to state-of-the-art sparse regression algorithms, and
show that SPORE methods, motivated by real applications, outperform the other
methods in terms of both interpretability and prediction accuracy.

1 Introduction
Computing systems today are ubiquitous, and range from the very small (e.g., iPods, cellphones,
laptops) to the very large (servers, data centers, computational grids). At the heart of such systems
are management components that decide how to schedule the execution of different programs over
time (e.g., to ensure high system utilization or efficient energy use [11, 15]), how to allocate to each
program resources such as memory, storage and networking (e.g., to ensure a long battery life or fair
resource allocation), and how to weather anomalies (e.g., flash crowds or attacks [6, 17, 24]).
These management components typically must make guesses about how a program will perform
under given hypothetical inputs, so as to decide how best to plan for the future. For example,
consider a simple scenario in a data center with two computers, fast computer A and slow computer
B, and a program waiting to run on a large file f stored in computer B. A scheduler is often faced
1

with the decision of whether to run the program at B, potentially taking longer to execute, but
avoiding any transmission costs for the file; or moving the file from B to A but potentially executing
the program at A much faster. If the scheduler can predict accurately how long the program would
take to execute on input f at computer A or B, he/she can make an optimal decision, returning
results faster, possibly minimizing energy use, etc.
Despite all these opportunities and demands, uses of prediction have been at best unsophisticated
in modern computer systems. Existing approaches either create analytical models for the programs
based on simplistic assumptions [12], or treat the program as a black box and create a mapping function between certain properties of input data (e.g., file size) and output response [13]. The success
of such methods is highly dependent on human experts who are able to select important predictors
before a statistical modeling step can take place. Unfortunately, in practice experts may be hard to
come by, because programs can get complex quickly beyond the capabilities of a single expert, or
because they may be short-lived (e.g., applications from the iPhone app store) and unworthy of the
attention of a highly paid expert. Even when an expert is available, program performance is often
dependent not on externally visible features such as command-line parameters and input files, but
on the internal semantics of the program (e.g., what lines of code are executed).
To address this problem (lack of expert and inherent semantics), we recently developed a new system [7] to automatically extract a large number of features from the intermediate execution steps of
a program (e.g., internal variables, loops, and branches) on sample inputs; then prediction models
can be built from those features without the need for a human expert.
In this paper, we propose two Sparse POlynomial REgression (SPORE) algorithms that use the
automatically extracted features to predict a computer program?s performance. They are variants of
each other in the way they build the nonlinear terms into the model ? SPORE-LASSO first selects
a small number of features and then entertains a full nonlinear polynomial expansion of order less
than a given degree; while SPORE-FoBa chooses adaptively a subset of the full expanded terms
and hence allows possibly a higher order of polynomials. Our algorithms are in fact new general
methods motivated by the computer performance prediction problem. They can learn a relationship
between a response (e.g., the execution time of a computer program given an input) and the generated
features, and select a few from hundreds of features to construct an explicit polynomial form to
predict the response. The compact and explicit polynomial form reveals important insights in the
program semantics (e.g., the internal program loop that affects program execution time the most).
Our approach is general, flexible and automated, and can adapt the prediction models to specific
programs, computer platforms, and even inputs.
We evaluate our algorithms experimentally on three popular computer programs from web search
and image processing. We show that our SPORE algorithms can achieve accurate predictions with
relative error less than 7% by using a small amount of training data for our application, and that our
algorithms outperform existing state-of-the-art sparse regression algorithms in the literature in terms
of interpretability and accuracy.
Related Work. In prior attempts to predict program execution time, Gupta et al. [13] use a variant of
decision trees to predict execution time ranges for database queries. Ganapathi et al. [11] use KCCA
to predict time and resource consumption for database queries using statistics on query texts and
execution plans. To measure the empirical computational complexity of a program, Trendprof [12]
constructs linear or power-law models that predict program execution counts. The drawbacks of such
approaches include their need for expert knowledge about the program to identify good features, or
their requirement for simple input-size to execution time correlations.
Seshia and Rakhlin [22, 23] propose a game-theoretic estimator of quantitative program properties,
such as worst-case execution time, for embedded systems. These properties depend heavily on the
target hardware environment in which the program is executed. Modeling the environment manually
is tedious and error-prone. As a result, they formulate the problem as a game between their algorithm
(player) and the program?s environment (adversary), where the player seeks to accurately predict the
property of interest while the adversary sets environment states and parameters.
Since expert resource is limited and costly, it is desirable to automatically extract features from program codes. Then machine learning techniques can be used to select the most important features
to build a model. In statistical machine learning, feature selection methods under linear regression models such as LASSO have been widely studied in the past decade. Feature selection with
2

non-linear models has been studied much less, but has recently been attracting attention. The most
notable are the SpAM work with theoretical and simulation results [20] and additive and generalized forward regression [18]. Empirical studies with data of these non-linear sparse methods are
very few [21]. The drawback of applying the SpAM method in our execution time prediction problem is that SpAM outputs an additive model and cannot use the interaction information between
features. But it is well-known that features of computer programs interact to determine the execution time [12]. One non-parametric modification of SpAM to replace the additive model has been
proposed [18]. However, the resulting non-parametric models are not easy to interpret and hence are
not desirable for our execution time prediction problem. Instead, we propose the SPORE methodology and propose efficient algorithms to train a SPORE model. Our work provides a promising
example of interpretable non-linear sparse regression models in solving real data problems.

2 Overview of Our System
Our focus in this paper is on algorithms for feature selection and model building. However we first
review the problem within which we apply these techniques to provide context [7]. Our goal is to
predict how a given program will perform (e.g., its execution time) on a particular input (e.g., input
files and command-line parameters). The system consists of four steps.
First, the feature instrumentation step analyzes the source code and automatically instruments it
to extract values of program features such as loop counts (how many times a particular loop has
executed), branch counts (how many times each branch of a conditional has executed), and variable
values (the k first values assigned to a numerical variable, for some small k such as 5).
Second, the profiling step executes the instrumented program with sample input data to collect values
for all created program features and the program?s execution times. The time impact of the data
collection is minimal.
Third, the slicing step analyzes each automatically identified feature to determine the smallest subset
of the actual program that can compute the value of that feature, i.e., the feature slice. This is the
cost of obtaining the value of the feature; if the whole program must execute to compute the value,
then the feature is expensive and not useful, since we can just measure execution time and we have
no need for prediction, whereas if only a little of the program must execute, the feature is cheap and
therefore possibly valuable in a predictive model.
Finally, the modeling step uses the feature values collected during profiling along with the feature
costs computed during slicing to build a predictive model on a small subset of generated features.
To obtain a model consisting of low-cost features, we iterate over the modeling and slicing steps,
evaluating the cost of selected features and rejecting expensive ones, until only low-cost features are
selected to construct the prediction model. At runtime, given a new input, the selected features are
computed using the corresponding slices, and the model is used to predict execution time from the
feature values.
The above description is minimal by necessity due to space constraints, and omits details on the
rationale, such as why we chose the kinds of features we chose or how program slicing works.
Though important, those details have no bearing in the results shown in this paper.
At present our system targets a fixed, overprovisioned computation environment without CPU job
contention or network bandwidth fluctuations. We therefore assume that execution times observed
during training will be consistent with system behavior on-line. Our approach can adapt to modest
change in execution environment by retraining on different environments. In our future research, we
plan to incorporate candidate features of both hardware (e.g., configurations of CPU, memory, etc)
and software environment (e.g., OS, cache policy, etc) for predictive model construction.

3 Sparse Polynomial Regression Model
Our basic premise for predictive program analysis is that a small but relevant set of features may explain the execution time well. In other words, we seek a compact model?an explicit form function
of a small number of features?that accurately estimates the execution time of the program.
3

To make the problem tractable, we constrain our models to the multivariate polynomial family, for at
least three reasons. First, a ?good program? is usually expected to have polynomial execution time in
some (combination of) features. Second, a polynomial model up to certain degree can approximate
well many nonlinear models (due to Taylor expansion). Finally, a compact polynomial model can
provide an easy-to-understand explanation of what determines the execution time of a program,
providing program developers with intuitive feedback and a solid basis for analysis.
For each computer program, our feature instrumentation procedure outputs a data set with n samples
as tuples of {yi , xi }ni=1 , where yi ? R denotes the ith observation of execution time, and xi denotes
the ith observation of the vector of p features. We now review some obvious alternative methods to
modeling the relationship between Y = [yi ] and X = [xi ], point out their drawbacks, and then we
proceed to our SPORE methodology.
3.1 Sparse Regression and Alternatives
Least square regression is widely used for finding the best-fitting f (x, ?) to a given set of responses
yi by minimizing the sum of the squares of the residuals [14]. Regression with subset selection
finds for each k ? {1, 2, . . . , m} the feature subset of size k that gives the smallest residual sum of
squares. However, it is a combinatorial optimization and is known to be NP-hard [14]. In recent
years a number of efficient alternatives based on model regularization have been proposed. Among
them, LASSO [25] finds the selected features with coefficients ?? given a tuning parameter ? as
follows:
X
1
?? = arg min kY ? X?k22 + ?
|?j |.
(1)
? 2
j
LASSO effectively enforces many ?j ?s to be 0, and selects a small subset of features (indexed by
non-zero ?j ?s) to build the model, which is usually sparse and has better prediction accuracy than
models created by ordinary least square regression [14] when p is large. Parameter ? controls the
complexity of the model: as ? grows larger, fewer features are selected.
Being a convex optimization problem is an important advantage of the LASSO method since several
fast algorithms exist to solve the problem efficiently even with large-scale data sets [9, 10, 16, 19].
Furthermore, LASSO has convenient theoretical and empirical properties. Under suitable assumptions, it can recover the true underlying model [8, 25]. Unfortunately, when predictors are highly
correlated, LASSO usually cannot select the true underlying model. The adaptive-LASSO [29]
defined below in Equation (2) can overcome this problem
1
?? = arg min kY ? X?k22 + ?
? 2

X ?j
| |,
wj
j

(2)

where wj can be any consistent estimate of ?. Here we choose wj to be a ridge estimate of ?:
wj = (X T X + 0.001I)?1 X T Y,
where I is the identity matrix.
Technically LASSO can be easily extended to create nonlinear models (e.g., using polynomial basis
functions up to degree d of all p features). However, this approach gives us p+d
terms, which is
d
very large when p is large (on the order of thousands) even for small d, making regression computationally expensive. We give two alternatives to fit the sparse polynomial regression model next.
3.2 SPORE Methodology and Two Algorithms
Our methodology captures non-linear effects of features?as well as non-linear interactions among
features?by using polynomial basis functions over those features (we use terms to denote the polynomial basis functions subsequently). We expand the feature set x = {x1 x2 . . . xk }, k ? p to
all the terms in the expansion of the degree-d polynomial (1 + x1 + . . . + xk )d , and use the terms
to construct a multivariate polynomial function f (x, ?) for the regression. We define expan(X, d)
as the mapping from the original data matrix X to a new matrix with the polynomial expansion
terms up to degree d as the columns. For example, using a degree-2 polynomial with feature set
4

x = {x1 x2 }, we expand out (1 + x1 + x2 )2 to get terms 1, x1 , x2 , x21 , x1 x2 , x22 , and use them as
basis functions to construct the following function for regression:
expan ([x1 , x2 ], 2) = [1, [x1 ], [x2 ], [x21 ], [x1 x2 ], [x22 ]],
f (x, ?)

=

?0 + ?1 x1 + ?2 x2 + ?3 x21 + ?4 x1 x2 + ?5 x22 .

Complete expansion on all p features is not necessary, because many of them have little contribution to the execution time. Motivated by this execution time application, we propose a general
methodology called SPORE which is a sparse polynomial regression technique. Next, we develop
two algorithms to fit our SPORE methodology.
3.2.1 SPORE-LASSO: A Two-Step Method
For a sparse polynomial model with only a few features, if we can preselect a small number of
features, applying the LASSO on the polynomial expansion of those preselected features will still
be efficient, because we do not have too many polynomial terms. Here is the idea:
Step 1: Use the linear LASSO algorithm to select a small number of features and filter out (often
many) features that hardly have contributions to the execution time.
Step 2: Use the adaptive-LASSO method on the expanded polynomial terms of the selected features
(from Step 1) to construct the sparse polynomial model.
Adaptive-LASSO is used in Step 2 because of the collinearity of the expanded polynomial features.
Step 2 can be computed efficiently if we only choose a small number of features in Step 1. We
present the resulting SPORE-LASSO algorithm in Algorithm 1 below.
Algorithm 1 SPORE-LASSO
Input: response Y , feature data X, maximum degree d, ?1 , ?2
Output: Feature index S, term index St , weights ?? for d-degree polynomial basis.
1: ?
? = arg min? 21 kY ? X?k22 + ?1 k?k1
2: S = {j : ?
? j 6= 0}
3: Xnew = expan(X(S), d)
T
T
4: w = (Xnew
Xnew + 0.001I)?1 Xnew
Y
P ?
1
2
5: ?? = arg min? 2 kY ? Xnew ?k2 + ?2 j | wjj |
6: St = {j : ??j 6= 0}
X(S) in Step 3 of Algorithm 1 is a sub-matrix of X containing only columns from X indexed by
S. For a new observation with feature vector X = [x1 , x2 , . . . , xp ], we first get the selected feature
vector X(S), then obtain the polynomial terms Xnew = expan(X(S), d), and finally we compute
? Note that the prediction depends on the choice of ?1 , ?2 and
the prediction: Y? = Xnew ? ?.
maximum degree d. In this paper, we fix d = 3. ?1 and ?2 are chosen by minimizing the Akaike
Information Criterion (AIC) on the LASSO solution paths. The AIC is defined as n log(kY ? Y? k22 )+
2s, where Y? is the fitted Y and s is the number of polynomial terms selected in the model. To be
precise, for the linear LASSO step (Step 1 of Algorithm 1), a whole solution path with a number of
?1 can be obtained using the algorithm in [10]. On the solution path, for each fixed ?1 , we compute
a solution path with varied ?2 for Step 5 of Algorithm 1 to select the polynomial terms. For each
?2 , we calculate the AIC, and choose the (?1 , ?2 ) with the smallest AIC.
One may wonder whether Step 1 incorrectly discards features required for building a good model
in Step 2. We next show theoretically this is not the case. Let S be a subset of {1, 2, . . . , p} and
its complement S c = {1, 2, . . . , p} \ S. Write the feature matrix X as X = [X(S), X(S c)]. Let
response Y = f (X(S)) + ?, where f (?) is any function and ? is additive noise. Let n be the number
of observations and s the size of S. We assume that X is deterministic, p and s are fixed, and ??i s are
i.i.d. and follow the Gaussian distribution with mean 0 and variance ? 2 . Our results also hold for
zero mean sub-Gaussian noise with parameter ? 2 . More general results regarding general scaling of
n, p and s can also be obtained.
Under the following conditions, we show that Step 1 of SPORE-LASSO, the linear LASSO, selects
the relevant features even if the response Y depends on predictors X(S) nonlinearly:
5

1. The columns (Xj , j = 1, . . . , p) of X are standardized:

1
T
n Xj Xj

= 1, for all j;

2. ?min ( n1 X(S)T X(S)) ? c with a constant c > 0;
3. min |(X(S)T X(S))?1 X(S)T f (X(S))| > ? with a constant ? > 0;
4.

T
T
?1
T
XS
XS
]f (XS )
c [I?XS (XS XS )
n

<

??c
?
,
2 s+1

for some 0 < ? < 1;

5. kXSTc XS (XST XS )?1 k? ? 1 ? ?;
where ?min (?) denotes the minimum eigenvalue of a matrix, kAk? is defined as maxi
and the inequalities are defined element-wise.

hP

j

i
|Aij |

Theorem 3.1. Under the conditions above, with probability ? 1 as n ? ?, there exists
some ?, such that ?? = (??S , ??S c ) is the unique solution of the LASSO (Equation (1)), where
??j 6= 0, for all j ? S and ??S c = 0.
Remark. The first two conditions are trivial: Condition 1 can be obtained by rescaling while Condition 2 assumes that the design matrix composed of the true predictors in the model is not singular.
Condition 3 is a reasonable condition which means that the linear projection of the expected response to the space spanned by true predictors is not degenerated. Condition 4 is a little bit tricky;
it says that the irrelevant predictors (XS c ) are not very correlated with the ?residuals? of E(Y ) after
its projection onto XS . Condition 5 is always needed when considering LASSO?s model selection
consistency [26, 28]. The proof of the theorem is included in the supplementary material.
3.2.2 Adaptive Forward-Backward: SPORE-FoBa
Using all of the polynomial expansions of a feature subset is not flexible. In this section, we propose
the SPORE-FoBa algorithm, a more flexible algorithm using adaptive forward-backward searching
over the polynomially expanded data: during search step k with an active set T (k) , we examine one
new feature Xj , and consider a small candidate set which consists of the candidate feature Xj , its
higher order terms, and the (non-linear) interactions between previously selected features (indexed
by S) and candidate feature Xj with total degree up to d, i.e., terms with form
X
dl ? d.
(3)
Xjd1 ?l?S Xldl , with d1 > 0, dl ? 0, and d1 +
Algorithm 2 below is a short description of the SPORE-FoBa, which uses linear FoBa [27] at step
5and 6. The main idea of SPORE-FoBa is that a term from the candidate set is added into the model
if and only if adding this term makes the residual sum of squares (RSS) decrease a lot. We scan all
of the terms in the candidate set and choose the one which makes the RSS drop most. If the drop in
the RSS is greater than a pre-specified value ?, we add that term to the active set, which contains the
currently selected terms by the SPORE-FoBa algorithm. When considering deleting one term from
the active set, we choose the one that makes the sum of residuals increase the least. If this increment
is small enough, we delete that term from our current active set.
Algorithm 2 SPORE-FoBa
Input: response Y , feature columns X1 , . . . , Xp , the maximum degree d
Output: polynomial terms and the weights
1: Let T = ?
2: while true do
3:
for j = 1, . . . , p do
4:
Let C be the candidate set that contains non-linear and interaction terms from Equation (3)
5:
Use Linear FoBa to select terms from C to form the new active set T .
6:
Use Linear FoBa to delete terms from T to form a new active set T .
7:
if no terms can be added or deleted then
8:
break

6

0.2

0.2

SPORE?LASSO
SPORE?FoBa

0.1

0.05

0
0

0.1

0.05

0.1

0.2
0.3
0.4
0.5
Percentage of Training data

(a) Lucene

0.6

0
0

SPORE?LASSO
SPORE?FoBa

0.15
Prediction Error

0.15
Prediction Error

Prediction Error

0.15

0.2

SPORE?LASSO
SPORE?FoBa

0.1

0.05

0.1

0.2
0.3
0.4
0.5
Percentage of Training data

(b) Find Maxima

0.6

0
0

0.1

0.2
0.3
0.4
0.5
Percentage of Training data

0.6

(c) Segmentation

Figure 1: Prediction errors of our algorithms across the three data sets varying training-set fractions.

4 Evaluation Results
We now experimentally demonstrate that our algorithms are practical, give highly accurate predictors for real problems with small training-set sizes, compare favorably in accuracy to other state-ofthe-art sparse-regression algorithms, and produce interpretable, intuitive models.
To evaluate our algorithms, we use as case studies three programs: the Lucene Search Engine [4],
and two image processing algorithms, one for finding maxima and one for segmenting an image
(both of which are implemented within the ImageJ image processing framework [3]). We chose
all three programs according to two criteria. First and most importantly, we sought programs with
high variability in the predicted measure (execution time), especially in the face of otherwise similar
inputs (e.g., image files of roughly the same size for image processing). Second, we sought programs
that implement reasonably complex functionality, for which an inexperienced observer would not
be able to trivially identify the important features.
Our collected datasets are as follows. For Lucene, we used a variety of text input queries from
two corpora: the works of Shakespeare and the King James Bible. We collected a data set with
n = 3840 samples, each of which consists of an execution time and a total of p = 126 automatically
generated features. The time values are in range of (0.88, 1.13) with standard deviation 0.19. For
the Find Maxima program within the ImageJ framework, we collected n = 3045 samples (from an
equal number of distinct, diverse images obtained from three vision corpora [1, 2, 5]), and a total of
p = 182 features. The execution time values are in range of (0.09, 2.99) with standard deviation
0.24. Finally, from the Segmentation program within the same ImageJ framework on the same image
set, we collected again n = 3045 samples, and a total of p = 816 features for each. The time values
are in range of (0.21, 58.05) with standard deviation 3.05. In all the experiments, we fix degree
d = 3 for polynomial expansion, and normalized each column of feature data into range [0, 1].
Prediction Error. We first show that our algorithms predict accurately, even when training on a
small number of samples, in both absolute and relative terms. The accuracy measure we use is the
P y?i ?yi
relative prediction error defined as n1t
| yi |, where nt is the size of the test data set, and y?i ?s
and yi ?s are the predicted and actual responses of test data, respectively.
We randomly split every data set into a training set and a test set for a given training-set fraction,
train the algorithms and measure their prediction error on the test data. For each training fraction,
we repeat the ?splitting, training and testing? procedure 10 times and show the mean and standard
deviation of prediction error in Figure 1. We see that our algorithms have high prediction accuracy,
even when training on only 10% or less of the data (roughly 300 - 400 samples). Specifically,
both of our algorithms can achieve less than 7% prediction error on both Lucene and Find Maxima
datasets; on the segmentation dataset, SPORE-FoBa achieves less than 8% prediction error, and
SPORE-LASSO achieves around 10% prediction error on average.
Comparisons to State-of-the-Art. We compare our algorithms to several existing sparse regression
methods by examining their prediction errors at different sparsity levels (the number of features used
in the model), and show our algorithms can clearly outperform LASSO, FoBa and recently proposed
non-parametric greedy methods [18] (Figure 2). As a non-parametric greedy algorithm, we use Additive Forward Regression (AFR), because it is faster and often achieves better prediction accuracy
than Generalized Forward Regression (GFR) algorithms. We use the Glmnet Matlab implementa7

tion of LASSO and to obtain the LASSO solution path [10]. Since FoBa and SPORE-FoBa naturally
produce a path by adding or deleting features (or terms), we record the prediction error at each step.
When two steps have the same sparsity level, we report the smallest prediction error. To generate
the solution path for SPORE-LASSO, we first use Glmnet to generate a solution path for linear
LASSO; then at each sparsity level k, we perform full polynomial expansion with d = 3 on the
selected k features, obtain a solution path on the expanded data, and choose the model with the
smallest prediction error among all models computed from all active feature sets of size k. From the
figure, we see that our SPORE algorithms have comparable performance, and both of them clearly
achieve better prediction accuracy than LASSO, FoBa, and AFR. None of the existing methods can
build models within 10% of relative prediction error. We believe this is because execution time of a
computer program often depends on non-linear combinations of different features, which is usually
not well-handled by either linear methods or the additive non-parametric methods. Instead, both of
our algorithms can select 2-3 high-quality features and build models with non-linear combinations
of them to predict execution time with high accuracy.

Prediction Error

Prediction Error

0.4
0.3
0.2
0.1
0
1

LASSO
FoBa
AFR
SPORE?LASSO
SPORE?FoBa

0.5
0.4
0.3
0.2
0.1

2

3

4
Sparsity

5

(a) Lucene

6

7

0
1

LASSO
FoBa
AFR
SPORE?LASSO
SPORE?FoBa

0.5
Prediction Error

LASSO
FoBa
AFR
SPORE?LASSO
SPORE?FoBa

0.5

0.4
0.3
0.2
0.1

2

3

4
Sparsity

5

(b) Find Maxima

6

7

0
1

2

3

4
Sparsity

5

6

7

(c) Segmentation

Figure 2: Performance of the algorithms: relative prediction error versus sparsity level.
Model Interpretability. To gain better understanding, we investigate the details of the model constructed by SPORE-FoBa for Find Maxima. Our conclusions are similar for the other case studies,
but we omit them due to space. We see that with different training set fractions and with different
sparsity configurations, SPORE-FoBa can always select two high-quality features from hundreds of
automatically generated ones. By consulting with experts of the Find Maxima program, we find that
the two selected features correspond to the width (w) and height (h) of the region of interest in the
image, which may in practice differ from the actual image width and height. Those are indeed the
most important factors for determining the execution time of the particular algorithm used. For a
10% training set fraction and ? = 0.01, SPORE-FoBa obtained
f (w, h) = 0.1 + 0.22w + 0.23h + 1.93wh + 0.24wh2
which uses non-linear feature terms(e.g., wh, wh2 ) to predict the execution time accurately (around
5.5% prediction error). Especially when Find Maxima is used as a component of a more complex
image processing pipeline, this model would not be the most obvious choice even an expert would
pick. On the contrary, as observed in our experiments, neither the linear nor the additive sparse
methods handle well such nonlinear terms, and result in inferior prediction performance. A more
detailed comparison across different methods is the subject of our on-going work.

5 Conclusion
In this paper, we proposed the SPORE (Sparse POlynomial REgression) methodology to build the
relationship between execution time of computer programs and features of the programs. We introduced two algorithms to learn a SPORE model, and showed that both algorithms can predict
execution time with more than 93% accuracy for the applications we tested. For the three test cases,
these results present a significant improvement (a 40% or more reduction in prediction error) over
other sparse modeling techniques in the literature when applied to this problem. Hence our work
provides one convincing example of using sparse non-linear regression techniques to solve real
problems. Moreover, the SPORE methodology is a general methodology that can be used to model
computer program performance metrics other than execution time and solve problems from other
areas of science and engineering.
8

References
[1] Caltech 101 Object Categories. http://www.vision.caltech.edu/Image_Datasets/
Caltech101/Caltech101.html.
[2] Event Dataset. http://vision.stanford.edu/lijiali/event_dataset/.
[3] ImageJ. http://rsbweb.nih.gov/ij/.
[4] Mahout. lucene.apache.org/mahout.
[5] Visual Object Classes Challenge 2008. http://pascallin.ecs.soton.ac.uk/challenges/
VOC/voc2008/.
[6] S. Chen, K. Joshi, M. A. Hiltunen, W. H. Sanders, and R. D. Schlichting. Link gradients: Predicting the
impact of network latency on multitier applications. In INFOCOM, 2009.
[7] B.-G. Chun, L. Huang, S. Lee, P. Maniatis, and M. Naik. Mantis: Predicting system performance through
program analysis and modeling. Technical Report, 2010. arXiv:1010.0019v1 [cs.PF].
[8] D. Donoho. For most large underdetermined systems of equations, the minimal 1-norm solution is the
sparsest solution. Communications on Pure and Applied Mathematics, 59:797829, 2006.
[9] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics,
32(2):407?499, 2002.
[10] J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1), 2010.
[11] A. Ganapathi, H. Kuno, U. Dayal, J. L. Wiener, A. Fox, M. Jordan, and D. Patterson. Predicting multiple
metrics for queries: Better decisions enabled by machine learning. In ICDE, 2009.
[12] S. Goldsmith, A. Aiken, and D. Wilkerson. Measuring empirical computational complexity. In FSE,
2007.
[13] C. Gupta, A. Mehta, and U. Dayal. PQR: Predicting query execution times for autonomous workload
management. In ICAC, 2008.
[14] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer, 2009.
[15] M. Isard, V. Prabhakaran, J. Currey, U. Wieder, K. Talwar, and A. Goldberg. Quincy: fair scheduling for
distributed computing clusters. In Proceedings of SOSP?09, 2009.
[16] S.-J. Kim, K. Koh, M. Lustig, S. Boyd, and D. Gorinevsky. An interior-point method for large-scale
l1-regularized least squares. IEEE Journal on Selected Topics in Signal Processing, 1(4):606?617, 2007.
[17] Z. Li, M. Zhang, Z. Zhu, Y. Chen, A. Greenberg, and Y.-M. Wang. WebProphet: Automating performance
prediction for web services. In NSDI, 2010.
[18] H. Liu and X. Chen. Nonparametric greedy algorithm for the sparse learning problems. In NIPS 22, 2009.
[19] M. Osborne, B. Presnell, and B. Turlach. On the lasso and its dual. Journal of Computational and
Graphical Statistics, 9(2):319?337, 2000.
[20] P. Ravikumar, J. Lafferty, H. Liu, and L. Wasserman. Sparse additive models. Journal of the Royal
Statistical Society: Series B(Statistical Methodology), 71(5):1009?1030, 2009.
[21] P. Ravikumar, V. Vu, B. Yu, T. Naselaris, K. Kay, J. Gallant, and C. Berkeley. Nonparametric sparse hierarchical models describe v1 fmri responses to natural images. Advances in Neural Information Processing
Systems (NIPS), 21, 2008.
[22] S. A. Seshia and A. Rakhlin. Game-theoretic timing analysis. In Proceedings of the IEEE/ACM International Conference on Computer-Aided Design (ICCAD), pages 575?582. IEEE Press, Nov. 2008.
[23] S. A. Seshia and A. Rakhlin. Quantitative analysis of systems using game-theoretic learning. ACM
Transactions on Embedded Computing Systems (TECS), 2010. To appear.
[24] M. Tariq, A. Zeitoun, V. Valancius, N. Feamster, and M. Ammar. Answering what-if deployment and
configuration questions with wise. In ACM SIGCOMM, 2008.
[25] R. Tibshirani. Regression shrinkage and selection via the lasso. J. Royal. Statist. Soc B., 1996.
[26] M. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using l1-constrained
quadratic programming (Lasso). IEEE Trans. Information Theory, 55:2183?2202, 2009.
[27] T. Zhang. Adaptive forward-backward greedy algorithm for sparse learning with linear models. Advances
in Neural Information Processing Systems, 22, 2008.
[28] P. Zhao and B. Yu. On model selection consistency of Lasso. The Journal of Machine Learning Research,
7:2563, 2006.
[29] H. Zou. The adaptive lasso and its oracle properties. Journal of the American Statistical Association,
101(476):1418?1429, 2006.

9

"
2009,Statistical Analysis of Semi-Supervised Learning: The Limit of Infinite Unlabelled Data,,3652-statistical-analysis-of-semi-supervised-learning-the-limit-of-infinite-unlabelled-data.pdf,"We study the behavior of the popular Laplacian Regularization method for Semi-Supervised Learning at the regime of a fixed number of labeled points but a large number of unlabeled points.  We show that in $\R^d$, $d \geq 2$, the method is actually not well-posed, and as the number of unlabeled points increases the solution degenerates to a noninformative function.  We also contrast the method with the Laplacian Eigenvector method, and discuss the ``smoothness assumptions associated with this alternate method.","Semi-Supervised Learning with the Graph Laplacian:
The Limit of Infinite Unlabelled Data
Boaz Nadler
Dept. of Computer Science and Applied Mathematics
Weizmann Institute of Science
Rehovot, Israel 76100
boaz.nadler@weizmann.ac.il

Nathan Srebro
Toyota Technological Institute
Chicago, IL 60637
nati@uchicago.edu

Xueyuan Zhou
Dept. of Computer Science
University of Chicago
Chicago, IL 60637
zhouxy@cs.uchicago.edu

Abstract
We study the behavior of the popular Laplacian Regularization method for SemiSupervised Learning at the regime of a fixed number of labeled points but a large
number of unlabeled points. We show that in Rd , d > 2, the method is actually not
well-posed, and as the number of unlabeled points increases the solution degenerates to a noninformative function. We also contrast the method with the Laplacian
Eigenvector method, and discuss the ?smoothness? assumptions associated with
this alternate method.

1

Introduction and Setup

In this paper we consider the limit behavior of two popular semi-supervised learning (SSL) methods
based on the graph Laplacian: the regularization approach [15] and the spectral approach [3]. We
consider the limit when the number of labeled points is fixed and the number of unlabeled points
goes to infinity. This is a natural limit for SSL as the basic SSL scenario is one in which unlabeled
data is virtually infinite. We can also think of this limit as ?perfect? SSL, having full knowledge
of the marginal density p(x). The premise of SSL is that the marginal density p(x) is informative
about the unknown mapping y(x) we are trying to learn, e.g. since y(x) is expected to be ?smooth?
in some sense relative to p(x). Studying the infinite-unlabeled-data limit, where p(x) is fully known,
allows us to formulate and understand the underlying smoothness assumptions of a particular SSL
method, and judge whether it is well-posed and sensible. Understanding the infinite-unlabeled-data
limit is also a necessary first step to studying the convergence of the finite-labeled-data estimator.
We consider the following setup: Let p(x) be an unknown smooth density on a compact domain ? ?
Rd with a smooth boundary. Let y : ? ? Y be the unknown function we wish to estimate. In case of
regression Y = R whereas in binary classification Y = {?1, 1}. The standard (transductive) semisupervised learning problem is formulated as follows: Given l labeled points, (x1 , y1 ), . . . , (xl , yl ),
with yi = y(xi ), and u unlabeled points xl+1 , . . . , xl+u , with all points xi sampled i.i.d. from p(x),
the goal is to construct an estimate of y(xl+i ) for any unlabeled point xl+i , utilizing both the labeled
and the unlabeled points. We denote the total number of points by n = l + u. We are interested in
the regime where l is fixed and u ? ?.
1

2

SSL with Graph Laplacian Regularization

We first consider the following graph-based approach formulated by Zhu et. al. [15]:
y?(x) = arg min In (y)
subject to y(xi ) = yi , i = 1, . . . , l
y

where
In (y) =

1 X
Wi,j (y(xi ) ? y(xj ))2
n2 i,j

(1)

(2)

is a Laplacian regularization term enforcing ?smoothness? with respect to the n?n similarity matrix
W . This formulation has several natural interpretations in terms of, e.g. random walks and electrical
circuits [15]. These interpretations, however, refer to a fixed graph, over a finite set of points with
given similarities.
In contrast, our focus here is on the more typical scenario where the points xi ? Rd are a random
sample from a density p(x), and W is constructed based on this sample. We would like to understand
the behavior of the method in terms of the density p(x), particularly in the limit where the number
of unlabeled points grows. Under what assumptions on the target labeling y(x) and on the density
p(x) is the method (1) sensible?
The answer, of course, depends on how the matrix W is constructed. We consider the common
situation where the similarities are obtained by applying some decay filter to the distances:


kxi ?xj k
(3)
Wi,j = G
?

where G : R+ ? R+ is some function with an adequately fast decay. Popular choices are the
2
Gaussian filter G(z) = e?z /2 or the ?-neighborhood graph obtained by the step filter G(z) = 1z<1 .
For simplicity, we focus here on the formulation (1) where the solution is required to satisfy the
constraints at the labeled points exactly. In practice, the hard labeling constraints are often replaced
with a softer loss-based data term, which is balanced against the smoothness term In (y), e.g. [14, 6].
Our analysis and conclusions apply to such variants as well.
Limit of the Laplacian Regularization Term
As the number of unlabeled examples grows the regularization term (2) converges to its expectation,
where the summation is replaced by integration w.r.t. the density p(x):
Z Z


?
k
lim In (y) = I (?) (y) =
(y(x) ? y(x? ))2 p(x)p(x? )dxdx? .
G kx?x
(4)
?
n??

?

?

In the above limit, the bandwidth ? is held fixed. Typically, one would also drive the bandwidth ?
to zero as n ? ?. There are two reasons for this choice. First, from a practical perspective, this
makes the similarity matrix W sparse so it can be stored and processed. Second, from a theoretical
perspective, this leads to a clear and well defined limit of the smoothness
regularization term In (y),
p
at least when ? ? 0 slowly enough1 , namely when ? = ?( d log n/n). If ? ? 0 as n ? ?,
and as long as n? d / log n ? ?, then after appropriate normalization, the regularizer converges to
a density weighted gradient penalty term [7, 8]:
Z
lim C?dd+2 In (y) = lim C?dd+2 I (?) (y) = J(y) =
k?y(x)k2 p(x)2 dx
(5)
n??
??0
?
R
where C = Rd kzk2 G(kzk)dz, and assuming 0 < C < ? (which is the case for both the Gaussian
and the step filters). This energy functional J(f ) therefore encodes the notion of ?smoothness? with
respect to p(x) that is the basis of the SSL formulation (1) with the graph constructions specified by
(3). To understand the behavior and appropriateness of (1) we must understand this functional and
the associated limit problem:
y?(x) = arg min J(y)
subject to y(xi ) = yi , i = 1, . . . , l
(6)
y

p
When ? = o( d 1/n) then all non-diagonal weights Wi,j vanish (points no longer have
p any ?close by?
neighbors). We are not aware of an analysis covering the regime where ? decays roughly as d 1/n, but would
be surprised if a qualitatively different meaningful limit is reached.
1

2

3

Graph Laplacian Regularization in R1

We begin by considering the solution of (6) for one dimensional data, i.e. d = 1 and x ? R. We first
consider the situation where the support of p(x) is a continuous interval ? = [a, b] ? R (a and/or
b may be infinite). Without loss of generality, we assume the labeled data is sorted in increasing
order a 6 x1 < x2 < ? ? ? < xl 6 b. Applying the theory of variational calculus, the solution y?(x)
satisfies inside each interval (xi , xi+1 ) the Euler-Lagrange equation


d
dy
p2 (x)
= 0.
dx
dx
Performing two integrations and enforcing the constraints at the labeled points yields
Rx
1/p2 (t)dt
i
y(x) = yi + R xxi+1
(yi+1 ? yi )
for xi 6 x 6 xi+1
1/p2 (t)dt
xi

(7)

with y(x) = x1 for a 6 x 6 x1 and y(x) = xl for xl 6 x 6 b. If the support of p(x) is a union of
disjoint intervals, the above analysis and the form of the solution applies in each interval separately.

The solution (7) seems reasonable and desirable from the point of view of the ?smoothness? assumptions: when p(x) is uniform, the solution interpolates linearly between labeled data points, whereas
across low-density regions, where p(x) is close to zero, y(x) can change abruptly. Furthermore,
the regularizer J(y) can be interpreted as a Reproducing Kernel Hilbert Space (RKHS) squared
semi-norm, giving us additional insight into this choice of regularizer:
Rb
Theorem 1. Let p(x) be a smooth density on ? = [a, b] ? R such that Ap = 41 a 1/p2 (t)dt < ?.
Then, J(f ) can be written as a squared semi-norm J(f ) = kf k2Kp induced by the kernel
Z ?

 x



1
Kp (x, x? ) = Ap ? 12 
dt
(8)
.
2
 x p (t) 

with a null-space of all constant functions. That is, kf kKp is the norm of the projection of f onto
the RKHS induced by Kp .

If p(x) is supported on several disjoint intervals, ? = ?i [ai , bi ], then J(f ) can be written as a
squared semi-norm induced by the kernel
R ?

( Rb
i dt
1
1  x dt 
?

 if x, x? ? [ai , bi ]
2
2
?
4
p
(t)
2
p
(t)
x
ai
(9)
Kp (x, x ) =
0
if x ? [ai , bi ], x? ? [aj , bj ], i 6= j
with a null-space spanned by indicator functions 1[ai ,bi ] (x) on the connected components of ?.
Proof. For any f (x) =

P

?i Kp (x, xi ) in the RKHS induced by Kp :
Z  2
X
df
J(f ) =
p2 (x)dx =
?i ?j Jij
dx
i,j
Z
d
d
where Jij =
Kp (x, xi ) Kp (x, xj )p2 (x)dx
dx
dx
i

(10)

When xi and xj are in different connected components of ?, the gradients of Kp (?, xi ) and Kp (?, xj )
are never non-zero together and Jij = 0 = Kp (xi , xj ). When they are in the same connected
component [a, b], and assuming w.l.o.g. a 6 xi 6 xj 6 b:
""Z
#
Z xj
Z b
xi
1
1
?1
1
Jij =
dt +
dt +
dt
2
2
4 a p2 (t)
xi p (t)
xj p (t)
Z
Z
1 xj 1
1 b 1
(11)
=
dt ?
dt = Kp (xi , xj ).
4 a p2 (t)
2 xi p2 (t)
P
Substituting Jij = Kp (xi , xj ) into (10) yields J(f ) =
?i ?j Kp (xi , xj ) = kf kKp .
3

Combining Theorem 1 with the Representer Theorem [13] establishes that the solution of (6) (or of
any variant where the hard constraints are replaced by a data term) is of the form:
y(x) =

l
X

?j Kp (x, xj ) +

j=1

X

?i 1[ai ,bi ] (x),

i

where i ranges over the connected components [ai , bi ] of ?, and we have:
J(y) =

l
X

?i ?j Kp (xi , xj ).

(12)

i,j=1

Viewing the regularizer as kyk2Kp suggests understanding (6), and so also its empirical approximation (1), by interpreting Kp (x, x? ) as a density-based ?similarity measure? between x and x? . This
similarity measure indeed seems sensible: for a uniform density it is simply linearly decreasing as a
function of the distance. When the density is non-uniform, two points are relatively similar only if
they are connected by a region in which 1/p2 (x) is low, i.e. the density is high, but are much less
?similar?, i.e. related to each other, when connected by a low-density region. Furthermore, there is
no dependence between points in disjoint components separated by zero density regions.

4

Graph Laplacian Regularization in Higher Dimensions

The analysis of the previous section seems promising, at it shows that in one dimension, the SSL
method (1) is well posed and converges to a sensible limit. Regretfully, in higher dimensions this is
not the case anymore. In the following theorem we show that the infimum of the limit problem (6) is
zero and can be obtained by a sequence of functions which are certainly not a sensible extrapolation
of the labeled points.
Theorem 2. Let p(x) be a smooth density over Rd , d > 2, bounded from above by some constant
pmax , and let (x1 , y1 ), . . . , (xl , yl ) be any (non-repeating) set of labeled examples. There exist continuous functions y? (x), for any ? > 0, all satisfying the constraints y? (xj ) = yj , j = 1, . . . , l, such
??0

??0

that J(y? ) ?? 0 but y? (x) ?? 0 for all x 6= xj , j = 1, . . . , l.
Proof. We present a detailed proof for the case of l = 2 labeled points. The generalization of the
proof to more labeled points is straightforward. Furthermore, without loss of generality, we assume
the first labeled point is at x0 = 0 with y(x0 ) = 0 and the second labeled point is at x1 with
kx1 k = 1 and y(x1 ) = 1. In addition, we assume that the ball B1 (0) of radius one centered around
the origin is contained in ? = {x ? Rd | p(x) > 0}.
We first consider the case d > 2. Here, for any ? > 0, consider the function


y? (x) = min kxk
,
1
?

which indeed satisfies the two constraints y? (xi ) = yi , i = 0, 1. Then,
Z
Z
p2 (x)
pmax
J(y? ) =
dx
6
dx = p2max Vd ?d?2
2
2
?
?
B? (0)
B? (0)

(13)

where Vd is the volume of a unit ball in Rd . Hence, the sequence of functions y? (x) satisfy the
constraints, but for d > 2, inf ? J(y? ) = 0.
For d = 2, a more extreme example is necessary: consider the functions
 2 

y? (x) = log kxk? +? log 1+?
for kxk 6 1
?

and y? (x) = 1 for kxk > 1. These functions satisfy the two constraints y? (xi ) = yi , i = 0, 1 and:
Z
Z 1
4p2max
kxk2
2
4
r2
?i2
h
?
?i
J(y? ) = h ? 1+?
p
(x)dx
6
1+? 2
(kxk2 +?)2
(r 2 +?)2 2?rdr
log

6

?

4?p2max
h ?
?i
1+? 2
log
?

log

B1 (0)

log


1+?
?

=

4?p2max ??0
 ??
log 1+?
?
4

0.

?

0

The implication of Theorem 2 is that regardless of the values at the labeled points, as u ? ?, the
solution of (1) is not well posed. Asymptotically, the solution has the form of an almost everywhere constant function, with highly localized spikes near the labeled points, and so no learning
is performed. In particular, an interpretation in terms of a density-based kernel Kp , as in the onedimensional case, is not possible.
Our analysis also carries over to a formulation where a loss-based data term replaces the hard label
constraints, as in
l
1X
y? = arg min
(y(xj ) ? yj )2 + ?In (y)
y(x) l
j=1

In the limit of infinite unlabeled data, functions of the form y? (x) above have a zero data penalty
term (since they exactly match the labels) and also drive the regularization term J(y) to zero. Hence,
it is possible to drive the entire objective functional (the data term plus the regularization term) to
zero with functions that do not generalize at all to unlabeled points.
4.1

Numerical Example

We illustrate the phenomenon detailed by Theorem 2 with a simple example. Consider a density
p(x) in R2 , which is a mixture of two unit variance spherical Gaussians, one per class, centered at
the origin and at (4, 0). We sample a total of n = 3000 points, and label two points from each of
the two components (four total). We then construct a similarity matrix using a Gaussian filter with
? = 0.4.
Figure 1 depicts the predictor y?(x) obtained from (1). In fact, two different predictors are shown,
obtained by different numerical methods for solving (1). Both methods are based on the observation
that the solution y?(x) of (1) satisfies:
y?(xi ) =

n
X
j=1

Wij y?(xj ) /

n
X

Wij

on all unlabeled points i = l + 1, . . . , l + u.

(14)

j=1

Combined with the constraints of (1), we obtain a system of linear equations that can be solved
by Gaussian elimination (here invoked through MATLAB?s backslash operator). This is the method
used in the top panels of Figure 1. Alternatively, (14) can be viewed as an update equation for y?(xi ),
which can be solved via the power method, or label propagation [2, 6]: start with zero labels on the
unlabeled points and iterate (14), while keeping the known labels on x1 , . . . , xl . This is the method
used in the bottom panels of Figure 1.
As predicted, y?(x) is almost constant for almost all unlabeled points. Although all values are very
close to zero, thresholding at the ?right? threshold does actually produce sensible results in terms of
the true -1/+1 labels. However, beyond being inappropriate for regression, a very flat predictor is still
problematic even from a classification perspective. First, it is not possible to obtain a meaningful
confidence measure for particular labels. Second, especially if the size of each class is not known apriori, setting the threshold between the positive and negative classes is problematic. In our example,
setting the threshold to zero yields a generalization error of 45%.
The differences between the two numerical methods for solving (1) also point out to another problem
with the ill-posedness of the limit problem: the solution is numerically very un-stable.
A more quantitative evaluation, that also validates that the effect in Figure 1 is not a result of choosing a ?wrong? bandwidth ?, is given in Figure 2. We again simulated data from a mixture of two
Gaussians, one Gaussian per class, this time in 20 dimensions, with one labeled point per class, and
an increasing number of unlabeled points. In Figure 2 we plot the squared error, and the classification error of the resulting predictor y?(x). We plot the classification error both when a threshold
of zero is used (i.e. the class is determined by sign(?
y (x))) and with the ideal threshold minimizing
the test error. For each unlabeled sample size, we choose the bandwidth ? yielding the best test
performance (this is a ?cheating? approach which provides a lower bound on the error of the best
method for selecting the bandwidth). As the number of unlabeled examples increases the squared
error approaches 1, indicating a flat predictor. Using a threshold of zero leads to an increase in the
classification error, possibly due to numerical instability. Interestingly, although the predictors become very flat, the classification error using the ideal threshold actually improves slightly. Note that
5

DIRECT INVERSION

SQUARED ERROR

SIGN ERROR: 45%
10
y(x) > 0
y(x) < 0

1
5
0

OPTIMAL BANDWIDTH

1

6

0.95

4

0.9

2

0.85

0

0

?1
10

0
200
400
600
800
0?1 ERROR (THRESHOLD=0)
0.32

?5
10

0

5
?10

0

?10
?5

?5

POWER METHOD

0

5

10

SIGN ERR: 17.1
10

1

0

0.3

1

0.28

0.5

0.26

0

0
200 400 600 800
0?1 ERROR (IDEAL THRESHOLD)
0.19

5

0

200
400 600
800
OPTIMAL BANDWIDTH

0

200 400 600 800
OPTIMAL BANDWIDTH

1.5

8

0

?1
10

0.18

6

0.17

4

?5
10

0

5
?10

0
?5

?10
?5

0

5

10

Figure 1: Left plots: Minimizer of Eq. (1). Right plots:
the resulting classification according to sign(y). The four
labeled points are shown by green squares. Top: minimization via Gaussian elimination (MATLAB backslash).
Bottom: minimization via label propagation with 1000 iterations - the solution has not yet converged, despite small
residuals of the order of 2 ? 10?4 .

0.16

0

200

400

600

800

2

0

200

400

600

800

Figure 2: Squared error (top), classification error
with a threshold of zero (center) and minimal classification error using ideal threhold (bottom), of the
minimizer of (1) as a function of number of unlabeled points. For each error measure and sample
size, the bandwidth minimizing the test error was
used, and is plotted.

ideal classification performance is achieved with a significantly larger bandwidth than the bandwidth
minimizing the squared loss, i.e. when the predictor is even flatter.
4.2

Probabilistic Interpretation, Exit and Hitting Times

As mentioned above, the Laplacian regularization method (1) has a probabilistic interpretation in
terms of a random walk on the weighted graph. Let x(t) denote a random walk
P on the graph with
transition matrix M = D?1 W where D is a diagonal matrix with Dii = j Wij . Then, for the
binary classification case with yi = ?1 we have [15]:

h
i

y?(xi ) = 2 Pr x(t) hits a point labeled +1 before hitting a point labeled -1  x(0) = xi ? 1

We present an interpretation of our analysis in terms of the limiting properties of this random walk.
Consider, for simplicity, the case where the two classes are separated by a low density region. Then,
the random walk has two intrinsic quantities of interest. The first is the mean exit time from one
cluster to the other, and the other is the mean hitting time to the labeled points in that cluster. As the
number of unlabeled points increases and ? ? 0, the random walk converges to a diffusion process
[12]. While the mean exit time then converges to a finite value corresponding to its diffusion analogue, the hitting time to a labeled point increases to infinity (as these become absorbing boundaries
of measure zero). With more and more unlabeled data the random walk will fully mix, forgetting
where it started, before it hits any label. Thus, the probability of hitting +1 before ?1 will become
uniform across the entire graph, independent of the starting location xi , yielding a flat predictor.

5

Keeping ? Finite

At this point, a reader may ask whether the problems found in higher dimensions are due to taking
the limit ? ? 0. One possible objection is that there is an intrinsic characteristic scale for the data
?0 where (with high probability) all points at a distance kxi ? xj k < ?0 have the same label. If this
is the case, then it may not necessarily make sense to take values of ? < ?0 in constructing W .
However, keeping ? finite while taking the number of unlabeled points to infinity does not resolve
the problem. On the contrary, even the one-dimensional case becomes ill-posed in this case. To
see this, consider a function y(x) which is zero everywhere except at the labeled points, where
y(xj ) = yj . With a finite number of labeled points of measure zero, I (?) (y) = 0 in any dimension
6

y

50 points

500 points

3500 points

1

1

1

0.5

0.5

0.5

0

0

0

?0.5

?0.5

?0.5

?1

?2

0

2

4

6

?1

?2

0

2

4

6

?1

?2

0

2

4

6

x

Figure 3: Minimizer of (1) for a 1-d problem with a fixed ? = 0.4, two labeled points and an increasing number
of unlabeled points.

and for any fixed ? > 0. While this limiting function is discontinuous, it is also possible to construct
??0
a sequence of continuous functions y? that all satisfy the constraints and for which I (?) (y? ) ?? 0.
This behavior is illustrated in Figure 3. We generated data from a mixture of two 1-D Gaussians
centered at the origin and at x = 4, with one Gaussian labeled ?1 and the other +1. We used
two labeled points at the centers of the Gaussians and an increasing number of randomly drawn
unlabeled points. As predicted, with a fixed ?, although the solution is reasonable when the number
of unlabeled points is small, it becomes flatter, with sharp spikes on the labeled points, as u ? ?.

6

Fourier-Eigenvector Based Methods

Before we conclude, we discuss a different approach for SSL, also based on the Graph Laplacian,
suggested by Belkin and Niyogi [3]. Instead of using the Laplacian as a regularizer, constraining
candidate predictors y(x) non-parametrically to those with small In (y) values, here the predictors
are constrained to the low-dimensional space spanned by the first few eigenvectors of the Laplacian:
The similarity matrix W is computed as before, and P
the Graph Laplacian matrix L = D ? W is
considered (recall D is a diagonal matrix with Dii = j Wij ). Only predictors
Pp
y?(x) = j=1 aj ej
(15)
spanned by the first p eigenvectors e1 , . . . , ep of L (with smallest eigenvalues) are considered. The
coefficients aj are chosen by minimizing a loss function on the labeled data, e.g. the squared loss:
Pl
(?
a1 , . . . , a
?p ) = arg min j=1 (yj ? y?(xj ))2 .
(16)

Unlike the Laplacian Regularization method (1), the Laplacian Eigenvector method (15)?(16) is
well posed in the limit u ? ?. This follows directly from the convergence of the eigenvectors of
the graph Laplacian to the eigenfunctions of the corresponding Laplace-Beltrami operator [10, 4].
Eigenvector based methods were shown empirically to provide competitive generalization performance on a variety of simulated and real world problems. Belkin and Niyogi [3] motivate the
approach by arguing that ?the eigenfunctions of the Laplace-Beltrami operator provide a natural basis for functions on the manifold and the desired classification function can be expressed in such a
basis?. In our view, the success of the method is actually not due to data lying on a low-dimensional
manifold, but rather due to the low density separation assumption, which states that different class labels form high-density clusters separated by low density regions. Indeed, under this assumption and
with sufficient separation between the clusters, the eigenfunctions of the graph Laplace-Beltrami operator are approximately piecewise constant in each of the clusters, as in spectral clustering [12, 11],
providing a basis for a labeling that is constant within clusters but variable across clusters. In other
settings, such as data uniformly distributed on a manifold but without any significant cluster structure, the success of eigenvector based methods critically depends on how well can the unknown
classification function be approximated by a truncated expansion with relatively few eigenvectors.
We illustrate this issue with the following three-dimensional example: Let p(x) denote the uniform
density in the box [0, 1] ? [0, 0.8] ? [0, 0.6], where the box lengths are different to prevent eigenvalue
multiplicity. Consider learning three different functions, y1 (x) = 1x1 >0.5 , y2 (x) = 1x1 >x2 /0.8 and
y3 (x) = 1x2 /0.8>x3 /0.6 . Even though all three functions are relatively simple, all having a linear
separating boundary between the classes on the manifold, as shown in the experiment described in
Figure 4, the Eigenvector based method (15)?(16) gives markedly different generalization performances on the three targets. This happens both when the number of eigenvectors p is set to p = l/5
as suggested by Belkin and Niyogi, as well as for the optimal (oracle) value of p selected on the test
set (i.e. a ?cheating? choice representing an upper bound on the generalization error of this method).
7

Prediction Error (%)

p = #labeled points/5
40

optimal p

20 labeled points

40

Approx. Error

50
20

20
0

20

20

40

60

# labeled points

0

10
20

40

60

# labeled points

0

0

5

10

15

# eigenvectors

0

0

5

10

15

# eigenvectors

Figure 4: Left three panels: Generalization Performance of the Eigenvector Method (15)?(16) for the three
different functions described in the text. All panels use n = 3000 points. Prediction counts the number of sign
agreements with the true labels. Rightmost panel: best fit when many (all 3000) points are used, representing
the best we can hope for with a few leading eigenvectors.

The reason for this behavior is that y2 (x) and even more so y3 (x) cannot be as easily approximated
by the very few leading eigenfunctions?even though they seem ?simple? and ?smooth?, they are
significantly more complicated than y1 (x) in terms of measure of simplicity implied by the Eigenvector Method. Since the density is uniform, the graph Laplacian converges to the standard Laplacian and its eigenfunctions have the form ?i,j,k (x) = cos(i?x1 ) cos(j?x2 /0.8) cos(k?x3 /0.6),
making it hard to represent simple decision boundaries which are not axis-aligned.

7

Discussion

Our results show that a popular SSL method, the Laplacian Regularization method (1), is not wellbehaved in the limit of infinite unlabeled data, despite its empirical success in various SSL tasks.
The empirical success might be due to two reasons.
First, it is possible that with a large enough number of labeled points relative to the number of
unlabeled points, the method is well behaved. This regime, where the number of both labeled and
unlabeled points grow while l/u is fixed, has recently been analyzed by Wasserman and Lafferty
[9]. However, we do not find this regime particularly satisfying as we would expect that having
more unlabeled data available should improve performance, rather than require more labeled points
or make the problem ill-posed. It also places the user in a delicate situation of choosing the ?just
right? number of unlabeled points without any theoretical guidance.
Second, in our experiments we noticed that although the predictor y?(x) becomes extremely flat, in
binary tasks, it is still typically possible to find a threshold leading to a good classification performance. We do not know of any theoretical explanation for such behavior, nor how to characterize
it. Obtaining such an explanation would be very interesting, and in a sense crucial to the theoretical
foundation of the Laplacian Regularization method. On a very practical level, such a theoretical understanding might allow us to correct the method so as to avoid the numerical instability associated
with flat predictors, and perhaps also make it appropriate for regression.
The reason that the Laplacian regularizer (1) is ill-posed in the limit is that the first order gradient
is not a sufficient penalty in high dimensions. This fact is well known in spline theory, where the
d
Sobolev Embedding Theorem [1] indicates one must control at least d+1
2 derivatives in R . In the
context of Laplacian regularization, this can be done using the iterated Laplacian: replacing the
d+1
graph Laplacian matrix L = D ? W , where D is the diagonal degree matrix, with L 2 (matrix to
d+1
the 2 power). In the infinite unlabeled data limit, this corresponds to regularizing all order- d+1
2
(mixed) partial derivatives. In the typical case of a low-dimensional manifold in a high dimensional
ambient space, the order of iteration should correspond to the intrinsic, rather then ambient, dimensionality, which poses a practical problem of estimating this usually unknown dimensionality. We
are not aware of much practical work using the iterated Laplacian, nor a good understanding of its
appropriateness for SSL.
A different approach leading to a well-posed solution is to include also an ambient regularization
term [5]. However, the properties of the solution and in particular its relation to various assumptions
about the ?smoothness? of y(x) relative to p(x) remain unclear.
Acknowledgments The authors would like to thank the anonymous referees for valuable suggestions. The research of BN was supported by the Israel Science Foundation (grant 432/06).
8

References
[1] R.A. Adams, Sobolev Spaces, Academic Press (New York), 1975.
[2] A. Azran, The rendevous algorithm: multiclass semi-supervised learning with Markov Random Walks,
ICML, 2007.
[3] M. Belkin, P. Niyogi, Using manifold structure for partially labelled classification, NIPS, vol. 15, 2003.
[4] M. Belkin and P. Niyogi, Convergence of Laplacian Eigenmaps, NIPS 19, 2007.
[5] M. Belkin, P. Niyogi and S. Sindhwani, Manifold Regularization: A Geometric Framework for Learning
from Labeled and Unlabeled Examples, JMLR, 7:2399-2434, 2006.
[6] Y. Bengio, O. Delalleau, N. Le Roux, label propagation and quadratic criterion, in Semi-Supervised
Learning, Chapelle, Scholkopf and Zien, editors, MIT Press, 2006.
[7] O. Bosquet, O. Chapelle, M. Hein, Measure Based Regularization, NIPS, vol. 16, 2004.
[8] M. Hein, Uniform convergence of adaptive graph-based regularization, COLT, 2006.
[9] J. Lafferty, L. Wasserman, Statistical Analysis of Semi-Supervised Regression, NIPS, vol. 20, 2008.
[10] U. von Luxburg, M. Belkin and O. Bousquet, Consistency of spectral clustering, Annals of Statistics, vol.
36(2), 2008.
[11] M. Meila, J. Shi. A random walks view of spectral segmentation, AI and Statistics, 2001.
[12] B. Nadler, S. Lafon, I.G. Kevrekidis, R.R. Coifman, Diffusion maps, spectral clustering and eigenfunctions of Fokker-Planck operators, NIPS, vol. 18, 2006.
[13] B. Sch?olkopf, A. Smola, Learning with Kernels, MIT Press, 2002.
[14] D. Zhou, O. Bousquet, T. Navin Lal, J. Weston, B. Sch?olkopf, Learning with local and global consistency,
NIPS, vol. 16, 2004.
[15] X. Zhu, Z. Ghahramani, J. Lafferty, Semi-Supervised Learning using Gaussian fields and harmonic functions, ICML, 2003.

9

"
2014,Do Convnets Learn Correspondence?,Poster,5420-do-convnets-learn-correspondence.pdf,"Convolutional neural nets (convnets) trained from massive labeled datasets have substantially improved the state-of-the-art in image classification and object detection. However, visual understanding requires establishing correspondence on a finer level than object category. Given their large pooling regions and training from whole-image labels, it is not clear that convnets derive their success from an accurate correspondence model which could be used for precise localization. In this paper, we study the effectiveness of convnet activation features for tasks requiring correspondence. We present evidence that convnet features localize at a much finer scale than their receptive field sizes, that they can be used to perform intraclass aligment as well as conventional hand-engineered features, and that they outperform conventional features in keypoint prediction on objects from PASCAL VOC 2011.","Do Convnets Learn Correspondence?

Jonathan Long
Ning Zhang
Trevor Darrell
University of California ? Berkeley
{jonlong, nzhang, trevor}@cs.berkeley.edu

Abstract
Convolutional neural nets (convnets) trained from massive labeled datasets [1]
have substantially improved the state-of-the-art in image classification [2] and object detection [3]. However, visual understanding requires establishing correspondence on a finer level than object category. Given their large pooling regions and
training from whole-image labels, it is not clear that convnets derive their success
from an accurate correspondence model which could be used for precise localization. In this paper, we study the effectiveness of convnet activation features for
tasks requiring correspondence. We present evidence that convnet features localize at a much finer scale than their receptive field sizes, that they can be used to
perform intraclass aligment as well as conventional hand-engineered features, and
that they outperform conventional features in keypoint prediction on objects from
PASCAL VOC 2011 [4].

1

Introduction

Recent advances in convolutional neural nets [2] dramatically improved the state-of-the-art in image
classification. Despite the magnitude of these results, many doubted [5] that the resulting features
had the spatial specificity necessary for localization; after all, whole image classification can rely
on context cues and overly large pooling regions to get the job done. For coarse localization, such
doubts were alleviated by record breaking results extending the same features to detection on PASCAL [3].
Now, the same questions loom on a finer scale. Are the modern convnets that excel at classification
and detection also able to find precise correspondences between object parts? Or do large receptive
fields mean that correspondence is effectively pooled away, making this a task better suited for
hand-engineered features?
In this paper, we provide evidence that convnet features perform at least as well as conventional
ones, even in the regime of point-to-point correspondence, and we show considerable performance
improvement in certain settings, including category-level keypoint prediction.
1.1

Related work

Image alignment Image alignment is a key step in many computer vision tasks, including face
verification, motion analysis, stereo matching, and object recognition. Alignment results in correspondence across different images by removing intraclass variability and canonicalizing pose.
Alignment methods exist on a supervision spectrum from requiring manually labeled fiducial points
or landmarks, to requiring class labels, to fully unsupervised joint alignment and clustering models.
Congealing [6] is an unsupervised joint alignment method based on an entropy objective. Deep
congealing [7] builds on this idea by replacing hand-engineered features with unsupervised feature
learning from multiple resolutions. Inspired by optical flow, SIFT flow [8] matches densely sampled
SIFT features for correspondence and has been applied to motion prediction and motion transfer. In
Section 3, we apply SIFT flow using deep features for aligning different instances of the same class.
1

Keypoint localization Semantic parts carry important information for object recognition, object
detection, and pose estimation. In particular, fine-grained categorization, the subject of many recent
works, depends strongly on part localization [9, 10]. Large pose and appearance variation across
examples make part localization for generic object categories a challenging task.
Most of the existing works on part localization or keypoint prediction focus on either facial landmark
localization [11] or human pose estimation. Human pose estimation has been approached using tree
structured methods to model the spatial relationships between parts [12, 13, 14], and also using
poselets [15] as an intermediate step to localize human keypoints [16, 17]. Tree structured models
and poselets may struggle when applied to generic objects with large articulated deformations and
wide shape variance.
Deep learning Convolutional neural networks have gained much recent attention due to their success in image classification [2]. Convnets trained with backpropagation were initially succesful in
digit recognition [18] and OCR [19]. The feature representations learned from large data sets have
been found to generalize well to other image classification tasks [20] and even to object detection
[3, 21]. Recently, Toshev et al. [22] trained a cascade of regression-based convnets for human pose
estimation and Jain et al. [23] combine a weak spatial model with deep learning methods.
The latter work trains multiple small, independent convnets on 64 ? 64 patches for binary bodypart detection. In contrast, we employ a powerful pretained ImageNet model that shares mid-elvel
feature representations among all parts in Section 5.
Several recent works have attempted to analyze and explain this overwhelming success. Zeiler and
Fergus [24] provide several heuristic visualizations suggesting coarse localization ability. Szegedy
et al. [25] show counterintuitive properties of the convnet representation, and suggest that individual
feature channels may not be more semantically meaningful than other bases in feature space. A
concurrent work [26] compares convnet features with SIFT in a standard descriptor matching task.
This work illuminates and extends that comparison by providing visual analysis and by moving
beyond single instance matching to intraclass correspondence and keypoint prediction.
1.2

Preliminaries

We perform experiments using a network architecture almost identical1 to that popularized by
Krizhevsky et al. [2] and trained for classification using the 1.2 million images of the ILSVRC
2012 challenge dataset [1]. All experiments are implemented using caffe [27], and our network
is the publicly available caffe reference model. We use the activations of each layer as features,
referred to as convn, pooln, or fcn for the nth convolutional, pooling, or fully connected layer,
respectively. We will use the term receptive field, abbreviated rf, to refer to the set of input pixels
that are path-connected to a particular unit in the convnet.

2

Feature visualization

In this section and Figures 1 and 2, we provide a
novel visual investigation of the effective pooling regions of convnet features.

Table 1: Convnet receptive field sizes and strides,
for an input of size 227 ? 227.

In Figure 1, we perform a nonparametric reconlayer
rf size
stride
struction of images from features in the spirit
conv1 11 ? 11
4?4
of HOGgles [28]. Rather than paired dictionary
conv2 51 ? 51
8?8
learning, however, we simply replace patches
conv3 99 ? 99
16 ? 16
with averages of their top-k nearest neighbors
conv4 131 ? 131 16 ? 16
in a convnet feature space. To do so, we first
conv5 163 ? 163 16 ? 16
compute all features at a particular layer, repool5 195 ? 195 32 ? 32
sulting in an 2d grid of feature vectors. We associate each feature vector with a patch in the
original image at the center of the corresponding receptive field and with size equal to the receptive
field stride. (Note that the strides of the receptive fields are much smaller than the receptive fields
1

Ours reverses the order of the response normalization and pooling layers.

2

conv4

conv5

uniform rf

5 neighbors

1 neighbor

5 neighbors

1 neighbor

conv3

Figure 1: Even though they have large receptive fields, convnet features carry local information at
a finer scale. Upper left: given an input image, we replaced 16 ? 16 patches with averages over
1 or 5 nearest neighbor patches, computed using convnet features centered at those patches. The
yellow square illustrates one input patch, and the black squares show the corresponding rfs for the
three layers shown. Right: Notice that the features retrieve reasonable matches for the centers of
their receptive fields, even though those rfs extend over large regions of the source image. In the
?uniform rf? column, we show the best that could be expected if convnet features discarded all
spatial information within their rfs, by choosing input patches uniformly at random from conv3sized neighborhoods. (Best viewed electronically.)
themselves, which overlap. Refer to Table 1 above for specific numbers.) We replace each such
patch with an average over k nearest neighbor patches using a database of features densely computed on the images of PASCAL VOC 2011. Our database contains at least one million patches for
every layer. Features are matched by cosine similarity.
Even though the feature rfs cover large regions of the source images, the specific resemblance of
the resulting images shows that information is not spread uniformly throughout those regions. Notable features (e.g., the tires of the bicycle and the facial features of the cat) are replaced in their
corresponding locations. Also note that replacement appears to become more semantic and less
visually specific as the layer deepens: the eyes and nose of the cat get replaced with differently colored or shaped eyes and noses, and the fur gets replaced with various animal furs, with the diversity
increasing with layer number.
Figure 2 gives a feature-centric rather than image-centric view of feature locality. For each column,
we first pick a random seed feature vector (computed from a PASCAL image), and find k nearest
neighbor features, again by cosine similarity. Instead of averaging only the centers, we average
the entire receptive fields of the neighbors. The resulting images show that similar features tend to
respond to similar colors specifically in the centers of their receptive fields.

3

conv4

conv5

500 nbrs 50 nbrs 5 nbrs

conv3

Figure 2: Similar convnet features tend to have similar receptive field centers. Starting from a
randomly selected seed patch occupying one rf in conv3, 4, or 5, we find the nearest k neighbor
features computed on a database of natural images, and average together the corresponding receptive
fields. The contrast of each image has been expanded after averaging. (Note that since each layer
is computed with a stride of 16, there is an upper bound on the quality of alignment that can be
witnessed here.)

3

Intraclass alignment

We conjecture that category learning implicitly aligns instances by pooling over a discriminative
mid-level representation. If this is true, then such features should be useful for post-hoc alignment
in a similar fashion to conventional features. To test this, we use convnet features for the task of
aligning different instances of the same class. We approach this difficult task in the style of SIFT
flow [8]: we retrieve near neighbors using a coarse similarity measure, and then compute dense
correspondences on which we impose an MRF smoothness prior which finally allows all images to
be warped into alignment.
Nearest neighbors are computed using fc7 features. Since we are specifically testing the quality of
alignment, we use the same nearest neighbors for convnet or conventional features, and we compute
both types of features at the same locations, the grid of convnet rf centers in the response to a single
image.
Alignment is determined by solving an MRF formulated on this grid of feature locations. Let p be a
point on this grid, let fs (p) be the feature vector of the source image at that point, and let ft (p) be the
feature vector of the target image at that point. For each feature grid location p of the source image,
there is a vector w(p) giving the displacement of the corresponding feature in the target image. We
use the energy function
X
X
kfs (p) ? ft (p + w(p))k2 + ?
kw(p) ? w(q)k22 ,
E(w) =
p

(p,q)?E

where E are the edges of a 4-neighborhood graph and ? is the regularization parameter. Optimization is performed using belief propagation, with the techniques suggested in [29]. Message passing
is performed efficiently using the squared Euclidean distance transform [30]. (Unlike the L1 regularization originally used by SIFT flow [8], this formulation maintains rotational invariance of w.)
Based on its performance in the next section, we use conv4 as our convnet feature, and SIFT with
descriptor radius 20 as our conventional feature. From validation experiments, we set ? = 3 ? 10?3
for both conv4 and SIFT features (which have a similar scale).
Given the alignment field w, we warp target to source using bivariate spline interpolation (implemented in SciPy [31]). Figure 3 gives examples of alignment quality for a few different seed images,
using both SIFT and convnet features. We show five warped nearest neighbors as well as keypoints
transferred from those neighbors.
We quantitatively assess the alignment by measuring the accuracy of predicted keypoints. To obtain
good predictions, we warp 25 nearest neighbors for each target image, and order them from smallest
to greatest deformation energy (we found this method to outperform ordering using the data term).
We take the predicted keypoints to be the median points (coordinate-wise) of the top five aligned
keypoints according to this ordering.
We assess correctness using mean PCK [32]. We consider a ground truth keypoint to be correctly
predicted if the prediction lies within a Euclidean distance of ? times the maximum of the bounding
4

five nearest neighbors

SIFT flow

conv4 flow

SIFT flow

conv4 flow

target image

Figure 3: Convnet features can bring different instances of the same class into good alignment at
least as well (on average) as traditional features. For each target image (left column), we show
warped versions of five nearest neighbor images aligned with conv4 flow (first row), and warped
versions aligned with SIFT flow [8] (second row). Keypoints from the warped images are shown
copied to the target image. The cat shows a case where convnet features perform better, while the
bicycle shows a case where SIFT features perform better. (Note that each instance is warped to a
square bounding box before alignment. Best viewed in color.)
Table 2: Keypoint transfer accuracy using convnet flow, SIFT flow, and simple copying from nearest
neighbors. Accuracy (PCK) is shown per category using ? = 0.1 (see text) and means are also
shown for the stricter values ? = 0.05 and 0.025. On average, convnet flow performs as well as
SIFT flow, and performs a bit better for stricter tolerances.
aero bike bird
conv4 flow 28.2 34.1 20.4
SIFT flow 27.6 30.8 19.9
NN transfer 18.3 24.8 14.5

boat
17.1
17.5
15.4

bttl
50.6
49.4
48.1

bus
36.7
36.4
27.6

car
20.9
20.7
16.0

mean
conv4 flow
SIFT flow
NN transfer

cat
19.6
16.0
11.1

chair
15.7
16.1
12.0

cow
25.4
25.0
16.8

? = 0.1
24.9
24.7
19.9

table
12.7
16.1
15.7

dog
18.7
16.3
12.7

? = 0.05
11.8
10.9
7.8

horse
25.9
27.7
20.2

mbike
23.1
28.3
18.5

prsn
21.4
20.2
18.7

plant
40.2
36.4
33.4

sheep
21.1
20.5
14.0

sofa
14.5
17.2
15.5

train
18.3
19.9
14.6

tv
33.3
32.9
30.0

mean
24.9
24.7
19.9

? = 0.025
4.08
3.55
2.35

box width and height, picking some ? ? [0, 1]. We compute the overall accuracy for each type of
keypoint, and report the average over keypoint types. We do not penalize predicted keypoints that
are not visible in the target image.
Results are given in Table 2. We show per category results using ? = 0.1, and mean results for
? = 0.1, 0.05, and 0.025. Indeed, convnet learned features are at least as capable as SIFT at
alignment, and better than might have been expected given the size of their receptive fields.

4

Keypoint classification

In this section, we specifically address the ability of convnet features to understand semantic information at the scale of parts. As an initial test, we consider the task of keypoint classification:
given an image and the coordinates of a keypoint on that image, can we train a classifier to label the
keypoint?
5

Table 3: Keypoint classification accuracies, in percent, on the twenty categories of PASCAL 2011
val, trained with SIFT or convnet features. The best SIFT and convnet scores are bolded in each
category.
aero
SIFT 10 36
(radius) 20 37
40 35
80 33
160 27
conv 1 16
(layer) 2 37
3 42
4 44
5 44

bike
42
50
54
43
36
14
43
50
53
51

bird
36
39
37
37
34
15
40
46
49
49

boat
32
35
41
42
38
19
35
41
42
41

bttl
67
74
76
75
72
20
69
76
78
77

bus
64
67
68
66
59
29
63
69
70
68

car
40
47
47
42
35
15
38
46
45
44

cat
37
40
37
30
25
22
44
52
55
53

chair
33
36
39
43
39
16
35
39
41
39

cow
37
43
40
36
30
17
40
45
48
45

table
60
68
69
70
67
29
61
64
68
63

dog
34
38
36
31
27
17
38
47
51
50

horse mbike prsn
39
38
29
42
48
33
42
49
32
36
51
27
32
46
25
14
16
15
40
44
34
48
52
40
51
53
41
49
52
39

(a)

(a) cat left eye

plant sheep sofa
63
37 42
70
44 52
69
39 52
70
35 49
70
29 48
33
18 12
65
39 41
74
46 50
76
49 52
73
47 47

train
64
68
74
69
66
27
63
71
73
71

tv
75
77
78
77
76
29
72
77
76
75

mean
45
50
51
48
44
20
47
54
56
54

(b)

Figure 5: Cross validation scores for cat
keypoint classification as a function of
the SVM parameter C. In (a), we plot
mean accuracy against C for five different convnet features; in (b) we plot
the same for SIFT features of different
sizes. We use C = 10?6 for all experiments in Table 3.

(b) cat nose

Figure 4: Convnet features show fine
localization ability, even beyond their
stride and in cases where SIFT features
do not perform as well. Each plot is
a 2D histogram of the locations of the
maximum responses of a classifer in a
21 by 21 pixel rectangle taken around a
ground truth keypoint.

For this task we use keypoint data [15] on the twenty classes of PASCAL VOC 2011 [4]. We extract
features at each keypoint using SIFT [33] and using the column of each convnet layer whose rf
center lies closest to the keypoint. (Note that the SIFT features will be more precisely placed as a
result of this approximation.) We trained one-vs-all linear SVMs on the train set using SIFT at five
different radii and each of the five convolutional layer activations as features (in general, we found
pooling and normalization layers to have lower performance). We set the SVM parameter C = 10?6
for all experiments based on five-fold cross validation on the training set (see Figure 5).
Table 3 gives the resulting accuracies on the val set. We find features from convnet layers consistently perform at least as well as and often better than SIFT at this task, with the highest performance
coming from layers conv4 and conv5. Note that we are specifically testing convnet features trained
only for classification; the same net could be expected to achieve even higher performance if trained
for this task.
Finally, we study the precise location understanding of our classifiers by computing their responses
with a single-pixel stride around ground truth keypoint locations. For two example keypoints (cat
left eye and nose), we histogram the locations of the maximum responses within a 21 pixel by 21
pixel rectangle around the keypoint, shown in Figure 4. We do not include maximum responses
that lie on the boundary of this rectangle. While the SIFT classifiers do not seem to be sensitive
to the precise locations of the keypoints, in many cases the convnet ones seem to be capable of
localization finer than their strides, not just their receptive field sizes. This observation motivates
our final experiments to consider detection-based localization performance.

6

5

Keypoint prediction

We have seen that despite their large receptive field sizes, convnets work as well as the handengineered feature SIFT for alignment and slightly better than SIFT for keypoint classification.
Keypoint prediction provides a natural follow-up test. As in Section 3, we use keypoint annotations
from PASCAL VOC 2011, and we assume a ground truth bounding box.
Inspired in part by [3, 34, 23], we train sliding window part detectors to predict keypoint locations
independently. R-CNN [3] and OverFeat [34] have both demonstrated the effectiveness of deep convolutional networks on the generic object detection task. However, neither of them have investigated
the application of CNNs for keypoint prediction.2 R-CNN starts from bottom-up region proposal
[35], which tends to overlook the signal from small parts. OverFeat, on the other hand, combines
convnets trained for classification and for regression and runs in multi-scale sliding window fashion.
We rescale each bounding box to 500 ? 500 and compute conv5 (with a stride of 16 pixels). Each
cell of conv5 contains one 256-dimensional descriptor. We concatenate conv5 descriptors from a
local region of 3 ? 3 cells, giving an overall receptive field size of 195 ? 195 and feature dimension
of 2304. For each keypoint, we train a linear SVM with hard negative mining. We consider the ten
closest features to each ground truth keypoint as positive examples, and all the features whose rfs
do not contain the keypoint as negative examples. We also train using dense SIFT descriptors for
comparison. We compute SIFT on a grid of stride eight and bin size of eight using VLFeat [36]. For
SIFT, we consider features within twice the bin size from the ground truth keypoint to be positives,
while samples that are at least four times the bin size away are negatives.
We augment our SVM detectors with a spherical Gaussian prior over candidate locations constructed
by nearest neighbor matching. The mean of each Gaussian is taken to be the location of the keypoint
in the nearest neighbor in the training set found using cosine similarity on pool5 features, and we
use a fixed standard deviation of 22 pixels. Let s(Xi ) be the output score of our local detector for
keypoint Xi , and let p(Xi ) be the prior score. We combine these to yield a final score f (Xi ) =
s(Xi )1?? p(Xi )? , where ? ? [0, 1] is a tradeoff parameter. In our experiments, we set ? = 0.1 by
cross validation. At test time, we predict the keypoint location as the highest scoring candidate over
all feature locations.
We evaluate the predicted keypoints using the measure PCK introduced in Section 3, taking ? = 0.1.
A predicted keypoint is defined as correct if the distance between it and the ground truth keypoint is
less than ? ? max(h, w) where h and w are the height and width of the bounding box. The results
using conv5 and SIFT with and without the prior are shown in Table 4. From the table, we can see
that local part detectors trained on the conv5 feature outperform SIFT by a large margin and that the
prior information is helpful in both cases. To our knowledge, these are the first keypoint prediction
results reported on this dataset. We show example results from five different categories in Figure
6. Each set consists of rescaled bounding box images with ground truth keypoint annotations and
predicted keypoints using SIFT and conv5 features, where each color corresponds to one keypoint.
As the figure shows, conv5 outperforms SIFT, often managing satisfactory outputs despite the
challenge of this task. A small offset can be noticed for some keypoints like eyes and noses, likely
due to the limited stride of our scanning windows. A final regression or finer stride could mitigate
this issue.

6

Conclusion

Through visualization, alignment, and keypoint prediction, we have studied the ability of the intermediate features implicitly learned in a state-of-the-art convnet classifier to understand specific,
local correspondence. Despite their large receptive fields and weak label training, we have found in
all cases that convnet features are at least as useful (and sometimes considerably more useful) than
conventional ones for extracting local visual information.
Acknowledgements This work was supported in part by DARPA?s MSEE and SMISC programs, by NSF
awards IIS-1427425, IIS-1212798, and IIS-1116411, and by support from Toyota.
2

But see works cited in Section 1.1 regarding keypoint localization.

7

Table 4: Keypoint prediction results on PASCAL VOC 2011. The numbers give average accuracy
of keypoint prediction using the criterion described in Section 3, PCK with ? = 0.1.
SIFT
SIFT+prior
conv5
conv5+prior

aero
17.9
33.5
38.5
50.9

Groundtruth

bike
16.5
36.9
37.6
48.8

bird
15.3
22.7
29.6
35.1

boat
15.6
23.1
25.3
32.5

bttl
25.7
44.0
54.5
66.1

SIFT+prior

bus
21.7
42.6
52.1
62.0

car
22.0
39.3
28.6
45.7

cat
12.6
22.1
31.5
34.2

chair
11.3
18.5
8.9
21.4

cow
7.6
23.5
30.5
41.1

table
6.5
11.2
24.1
27.2

dog
12.5
20.6
23.7
29.3

horse
18.3
32.2
35.8
46.8

Groundtruth

conv5+prior

mbike
15.1
33.9
29.9
45.6

prsn
15.9
26.7
39.3
47.1

plant
21.3
30.6
38.2
42.5

sheep
14.7
25.7
30.5
38.8

SIFT+prior

sofa
15.1
26.5
24.5
37.6

train
9.2
21.9
41.5
50.7

tv
19.9
32.4
42.0
45.6

mean
15.7
28.4
33.3
42.5

conv5+prior

Figure 6: Examples of keypoint prediction on five classes of the PASCAL dataset: aeroplane, cat,
cow, potted plant, and horse. Each keypoint is associated with one color. The first column is the
ground truth annotation, the second column is the prediction result of SIFT+prior and the third
column is conv5+prior. (Best viewed in color).
References
[1] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR, 2009.
[2] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
[3] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In CVPR, 2014.
[4] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman.
The
PASCAL Visual Object Classes Challenge 2011 (VOC2011) Results.
http://www.pascalnetwork.org/challenges/VOC/voc2011/workshop/index.html.
[5] Debate on Yann LeCun?s Google+ page. https://plus.google.com/+YannLeCunPhD/posts/JBBFfv2XgWM.
Accessed: 2014-5-31.
[6] G. B. Huang, V. Jain, and E. Learned-Miller. Unsupervised joint alignment of complex images. In ICCV,
2007.

8

[7] G. B. Huang, M. A. Mattar, H. Lee, and E. Learned-Miller. Learning to align from scratch. In NIPS,
2012.
[8] C. Liu, J. Yuen, and A. Torralba. Sift flow: Dense correspondence across scenes and its applications.
PAMI, 33(5):978?994, 2011.
[9] J. Liu and P. N. Belhumeur. Bird part localization using exemplar-based models with enforced pose and
subcategory consistenty. In ICCV, 2013.
[10] T. Berg and P. N. Belhumeur. POOF: Part-based one-vs.-one features for fine-grained categorization, face
verification, and attribute estimation. In CVPR, 2013.
[11] P. N. Belhumeur, D. W. Jacobs, D. J. Kriegman, and N. Kumar. Localizing parts of faces using a consensus
of exemplars. In CVPR, 2011.
[12] Y. Yang and D. Ramanan. Articulated pose estimation using flexible mixtures of parts. In CVPR, 2011.
[13] M. Sun and S. Savarese. Articulated part-based model for joint object detection and pose estimation. In
ICCV, 2011.
[14] X. Zhu and D. Ramanan. Face detection, pose estimation, and landmark localization in the wild. In
CVPR, 2012.
[15] L. Bourdev and J. Malik. Poselets: Body part detectors trained using 3d human pose annotations. In
ICCV, 2009.
[16] G. Gkioxari, B. Hariharan, R. Girshick, and J. Malik. Using k-poselets for detecting people and localizing
their keypoints. In CVPR, 2014.
[17] G. Gkioxari, P. Arbelaez, L. Bourdev, and J. Malik. Articulated pose estimation using discriminative
armlet classifiers. In CVPR, 2013.
[18] Y. LeCun, B. Boser, J.S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to hand-written zip code recognition. In Neural Computation, 1989.
[19] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.
In Proceedings of the IEEE, pages 2278?2324, 1998.
[20] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. DeCAF: A deep convolutional activation feature for generic visual recognition. In ICML, 2014.
[21] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun. Pedestrian detection with unsupervised multistage feature learning. In CVPR, 2013.
[22] A. Toshev and C. Szegedy. DeepPose: Human pose estimation via deep neural networks. In CVPR, 2014.
[23] A. Jain, J. Tompson, M. Andriluka, G. W. Taylor, and C. Bregler. Learning human pose estimation
features with convolutional networks. In ICLR, 2014.
[24] M. D Zeiler and R. Fergus. Visualizing and understanding convolutional neural networks. In ECCV,
2014.
[25] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing
properties of neural networks. In ICLR, 2014.
[26] P. Fischer, A. Dosovitskiy, and T. Brox. Descriptor Matching with Convolutional Neural Networks: a
Comparison to SIFT. ArXiv e-prints, May 2014.
[27] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe:
Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.
[28] C. Vondrick, A. Khosla, T. Malisiewicz, and A. Torralba. HOGgles: Visualizing Object Detection Features. In ICCV, 2013.
[29] P. Felzenszwalb and D. P. Huttenlocher. Efficient belief propagation for early vision. International journal
of computer vision, 70(1):41?54, 2006.
[30] P. Felzenszwalb and D. Huttenlocher. Distance transforms of sampled functions. Technical report, Cornell
University, 2004.
[31] E. Jones, T. Oliphant, P. Peterson, et al. SciPy: Open source scientific tools for Python, 2001.
[32] Y. Yang and D. Ramanan. Articulated human detection with flexible mixtures of parts. In PAMI, 2013.
[33] D.G. Lowe. Object recognition from local scale-invariant features. In ICCV, 1999.
[34] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition,
localization and detection using convolutional networks. In ICLR, 2014.
[35] J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders. Selective search for object recognition. IJCV,
2013.
[36] A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library of computer vision algorithms.
http://www.vlfeat.org/, 2008.

9

"
2009,Spatial Normalized Gamma Processes,,3630-spatial-normalized-gamma-processes.pdf,"Dependent Dirichlet processes (DPs) are dependent sets of random measures, each being marginally Dirichlet process distributed. They are used in Bayesian nonparametric models when the usual exchangebility assumption does not hold. We propose a simple and general framework to construct dependent DPs by marginalizing and normalizing a single gamma process over an extended space. The result is a set of DPs, each located at a point in a space such that  neighboring DPs are more dependent. We describe Markov chain Monte Carlo inference, involving the typical Gibbs sampling and three different Metropolis-Hastings proposals to speed up convergence. We report an empirical study of convergence speeds on a synthetic dataset and demonstrate an application of the model to topic modeling through time.","Spatial Normalized Gamma Processes

Yee Whye Teh
Gatsby Computational Neuroscience Unit
University College London
ywteh@gatsby.ucl.ac.uk

Vinayak Rao
Gatsby Computational Neuroscience Unit
University College London
vrao@gatsby.ucl.ac.uk

Abstract
Dependent Dirichlet processes (DPs) are dependent sets of random measures, each
being marginally DP distributed. They are used in Bayesian nonparametric models
when the usual exchangeability assumption does not hold. We propose a simple
and general framework to construct dependent DPs by marginalizing and normalizing a single gamma process over an extended space. The result is a set of
DPs, each associated with a point in a space such that neighbouring DPs are more
dependent. We describe Markov chain Monte Carlo inference involving Gibbs
sampling and three different Metropolis-Hastings proposals to speed up convergence. We report an empirical study of convergence on a synthetic dataset and
demonstrate an application of the model to topic modeling through time.

1

Introduction

Bayesian nonparametrics have recently garnered much attention in the machine learning and statistics communities, due to their elegant treatment of infinite dimensional objects like functions and
densities, as well as their ability to sidestep the need for model selection. The Dirichlet process (DP)
[1] is a cornerstone of Bayesian nonparametrics, and forms a basic building block for a wide variety
of extensions and generalizations, including the infinite hidden Markov model [2], the hierarchical
DP [3], the infinite relational model [4], adaptor grammars [5], to name just a few.
By itself, the DP is a model that assumes that data are infinitely exchangeable, i.e. the ordering of
data items does not matter. This assumption is false in many situations and there has been a concerted
effort to extend the DP to more structured data. Much of this effort has focussed on defining priors on
collections of dependent random probability measures. [6] expounded on the notion of dependent
DPs, that is, a dependent set of random measures that are all marginally DPs. The property of
being marginally DP here is both due to a desire to construct mathematically elegant solutions, and
also due to the fact that the DP and its implications as a statistical model, e.g. on the behaviour
of induced clusterings of data or asymptotic consistency, are well-understood. In this paper, we
propose a simple and general framework for the construction of dependent DPs on arbitrary spaces.
The idea is based on the fact that just as Dirichlet distributions can be generated by drawing a set
of independent gamma variables and normalizing, the DP can be constructed by drawing a sample
from a gamma process (?P) and normalizing (i.e. it is an example of a normalized random measure
[7, 8]). A ?P is an example of a completely random measure [9]: it has the property that the random
masses it assigns to disjoint subsets are independent. Furthermore, the restriction of a ?P to a subset
is itself a ?P. This implies the following easy construction of a set of dependent DPs: define a ?P
over an extended space, associate each DP with a different region of the space, and define each DP
by normalizing the restriction of the ?P on the associated region. This produces a set of dependent
DPs, with the amount of overlap among the regions controlling the amount of dependence. We call
this model a spatial normalized gamma process (SN?P). More generally, our construction can be
extended to normalizing restrictions of any completely random measure, and we call the resulting
dependent random measures spatial normalized random measures (SNRMs).
1

In Section 2 we briefly describe the ?P. Then we describe our construction of the SN?P in Section 3.
We describe inference procedures based on Gibbs and Metropolis-Hastings sampling in Section 4
and report experimental results in Section 5. We conclude by discussing limitations and possible
extensions of the model as well as related work in Section 6.

2

Gamma Processes

We briefly describe the gamma process (?P) here. A good high-level introduction can be found in
[10]. Let (?, ?) be a measure space on which we would like to define a ?P. Like the DP, realizations
of the ?P are atomic measures with random weighted point masses. We can visualize the point
masses ? ? ? and their corresponding weights w > 0 as points in a product space ? ? [0, ?).
Consider a Poisson process over this product space with mean measure
?(d?dw) = ?(d?)w?1 e?w dw.

(1)

Here ? is a measure on the space (?, ?) and is called the base measure of
R the ?P. A sample from
this Poisson process will yield an infinite set of atoms {?i , wi }?
i=1 since ??[0,?) ?(d?dw) = ?.
A sample from the ?P is then defined as
G=

?
X

wi ??i ? ?P(?).

(2)

i=1

P?
It can be shown that the total mass G(S) = i=1 wi 1(?i ? S) of any measurable subset S ? ? is
simply gamma distributed with shape parameter ?(S), thus the natural name gamma process. Dividing G by G(?), we get a normalized random measure?a random probability measure. Specifically,
we get a sample from the Dirichlet process DP(?):
D = G/G(?) ? DP(?).

(3)

Here we used an atypical parameterization of the DP in terms of the base measure ?. The usual
(equivalent) parameters of the DP are: strength parameter ?(?) and base distribution ?/?(?).
Further, the DP is independent of the normalization: D?
?G(?).
The gamma process is an example of a completely random measure [9]. This means that for mutually
disjoint measurable subsets S1 , . . . , Sn ? ? the random numbers {G(S1 ), . . . , G(Sn )} are mutually
independent. Two straightforward consequences will be of importance in the rest of this paper.
Firstly, if S ? ? then the restriction G0 (d?) = G(d? ? S) onto S is a ?P with base measure
?0 (d?) = ?(d?
R ? S). Secondly, if ? = ?1 ? ?2 is a two dimensional space, Rthen the projection
G00 (d?1 ) = ?2 G(d?1 d?2 ) onto ?1 is also a ?P with base measure ?00 (d?1 ) = ?2 ?(d?1 d?2 ).

3

Spatial Normalized Gamma Processes

In this section we describe our proposal for constructing dependent DPs. Let (?, ?) be a probability
space and T an index space. We wish to construct a set of dependent random measures over (?, ?),
one Dt for each t ? T such that each Dt is marginally DP. Our approach is to define a gamma
process G over an extended space and let each Dt be a normalized restriction/projection of G.
Because restrictions and projections of gamma processes are also gamma processes, each Dt will
be DP distributed.
To this end, let Y be an auxiliary space and for each t ? T, let Yt ? Y be a measurable set. For any
measure ? over ? ? Y define the restricted projection ?t by
Z
?t (d?) =
?(d?dy) = ?(d? ? Yt ).
(4)
Yt

Note that ?t is a measure over ? for each t ? T. Now let ? be a base measure over the product
space ? ? Y and consider a gamma process
G ? ?P(?)
2

(5)

over ? ? Y. Since restrictions and projections of ?Ps are ?Ps as well, Gt will be a ?P over ? with
base measure ?t :
Z
Gt (d?) =
G(d?dy) ? ?P(?t )
(6)
Yt

Now normalizing,
Dt = Gt /Gt (?) ? DP(?t )

(7)

We call the resulting set of dependent DPs {Dt }t?T spatial normalized gamma processes (SN?Ps).
If the index space is continuous, {Dt }t?T can equivalently be thought of as a measure-valued
stochastic process. The amount of dependence between Ds and Dt for s, t ? T is related to the
amount of overlap between Ys and Yt . Generally, the subsets Yt are defined so that the closer s and
t are in T, the more overlap Ys and Yt have and as a result Ds and Dt are more dependent.
3.1

Examples

We give two examples of SN?Ps, both with index set T = R interpreted as the time line. Generalizations to higher dimensional Euclidean spaces Rn are straightforward. Let H be a base distribution
over ? and ? > 0 be a concentration parameter.
The first example uses Y = R as well, with the subsets being Yt = [t ? L, t + L] for some fixed
window length L > 0. The base measure is ?(d?dy) = ?H(d?)dy/2L. In this case the measurevalued stochastic process {Dt }t?R is stationary. The base measure ?t works out to be:
Z t+L
?t (d?) =
?H(d?)dy/2L = ?H(d?),
(8)
t?L

so that each Dt ? DP(?H) with concentration parameter ? and base distribution H. We can
interpret this SN?P as follows. Each atom in the overall ?P G has a time-stamp y and a time-span
of [y ? L, y + L], so that it will only appear in the DPs Dt within the window t ? [y ? L, y + L]. As
a result, two DPs Ds and Dt will share more atoms the closer s and t are to each other, and no atoms
if |s ? t| > 2L. Further, the dependence between Ds and Dt depends on |s ? t| only, decreasing
with increasing |s ? t| and independent if |s ? t| > 2L.
The second example generalizes the first one by allowing different atoms to have different window
lengths. Each atom now has a time-stamp y and a window length l, so that it appears in DPs in the
window [y ? l, y + l]. Our auxiliary space is thus Y = R ? [0, ?), with Yt = {(y, l) : |y ? t| ? l}
(see Figure 1). Let ?(dl) be a distribution over window lengths in [0, ?). We use the base measure
?(d?dydl) = ?H(d?)dy?(dl)/2l. The restricted projection is then
Z
Z ?
Z t+l
?t (d?) =
?H(d?)dy?(dl)/2l = ?H(d?)
?(dl)
dy/2l = ?H(d?)
(9)
|y?t|?l

0

t?l

so that each Dt is again simply DP(?H). Now Ds and Dt will always be dependent with the amount
of dependence decreasing as |s ? t| increases.
3.2

Interpretation as Mixtures of DPs

Even though the SN?P as described above defines an uncountably infinite number of DPs, in practice
we will only have observations at a finite number of times, say t1 , . . . , tm . We define R as the
smallest collection of disjoint regions of Y such that each Ytj is a union of subsets in R. Thus
m
R = {?m
j=1 Sj : Sj = Ytj or Sj = Y\Ytj , with at least one Sj = Ytj and ?j=1 Sj 6= ?}. For
1 ? j ? m let Rj be the collection of regions in R contained in Ytj , so that ?R?Rj = Ytj . For
each R ? R define
GR (d?) = G(d? ? R)

(10)

We see that each GR is a ?P with base measure ?R (d?) = ?(d? ? R). Normalizing, DR =
GR /GR (?) ? DP(?R ), with DR ?
?DR0 for distinct R, R0 ? R. Now
P
(11)
Dtj (d?) = R?Rj P 0 GR (?)
G 0 (?) DR (d?)
R ?Rj

3

R

L

SCALE = L

t1

t2

t3

Y

Figure 1: The extended space Y?L over which the overall ?P is defined in the second example. Not
shown is the ?-space over which the DPs are defined. Also not shown is the fourth dimension W
needed to define the Poisson process used to construct the ?P. t1 , t2 , t3 ? Y are three times at which
observations are present. The subset Ytj corresponding to each tj is the triangular area touching tj .
The regions in R are the six areas formed by various intersections of the triangular areas.
so each Dtj is a mixture where each component DR is drawn independently from a DP. Further, the
mixing proportions are Dirichlet distributed and independent from the components by virtue of each
GR (?) being gamma distributed and independent from DR . Thus we have the following equivalent
construction for a SN?P:
DR ? DP(?R )
X
Dtj =
?jR DR

gR ? Gamma(?R (?))

for R ? R

gR

for R ? Rj

?jR =

P

R0 ?Rj

R?Rj

gR

(12)

Note that the DPs in this construction are all defined only over ?, and references to the auxiliary
space Y and the base measure ? are only used to define the individual base measures ?R and the
shape parameters of the gR ?s. Figure 1 shows the regions for the second example corresponding to
observations at three times.
The mixture of DPs construction is related to the hierarchical Dirichlet process defined in [11] (not
the one defined by Teh et al [3]). The difference is that the parameters of the prior over the mixing
proportions exactly matches the concentration parameters of the individual DPs. A consequence of
this is that each mixture Dtj is now conveniently also a DP.

4

Inference in the SN?P

The mixture of DPs interpretation of the SN?P makes sampling from the model, and consequently
inference via Markov chain Monte Carlo sampling, easy. In what follows, we describe both Gibbs
sampling and Metropolis-Hastings based updates for a hierarchical model in which the dependent
DPs act as prior distributions over a collection of infinite mixture models. Formally, our observations
now lie in a measurable space (X, ?) equipped with a set of probability measures F? parametrized
by ? ? ?. Observation i at time tj is denoted xji , lies in region rji and is drawn from mixture
component parametrized by ?ji . Thus to augment (12), we have
rji ? Mult({?jR : R ? Rj })

?ji ? Drji

xji ? F?ji

(13)

where rji = R with probability ?jR for each R ? Rj . In words, we first pick a region rji from the
set Rj , then a mixture component ?ji , followed by drawing xji from the mixture distribution.
4.1

Gibb Sampling

We derive a Gibbs sampler for the SN?P where the region DPs DR are integrated out and replaced
by Chinese restaurants. Let cji denote the index of the cluster in Drji to which observation xji is
assigned. We also assume that the base distribution H is conjugate to the mixture distributions F?
so that the cluster parameters are integrated out as well. The Gibbs sampler iteratively resamples the
4

latent variables left: rji ?s, cji ?s and gR ?s. In the following, let mjRc be the number of observations
?ji
from time tj assigned to cluster c in the DP DR in region R, and let fRc
(xji ) be the density of
observation xji conditioned on the other variables currently assigned to cluster c in DR , with its
cluster parameters integrated out. We denote marginal counts with dots, for example m?Rc is the
number of observations (over all times) assigned to cluster c in region R. Superscripts ?ji means
observation xji is excluded.
rji and cji are resampled together; their conditional joint probability given the other variables is:



m?ji
?ji
?Rc
p(rji = R, cji = c|others) ? P gR gr
fRc
(xji )
(14)
m?ji +? (?)
r?Rj

R

?R?

The probability of xji joining a new cluster in region R is



?R (?)
gR
P
p(rji = R, cji = cnew |others) ?
fRcnew (xji )
gr
m?ji +? (?)
r?Rj

?R?

(15)

R

where R ? Rj and c denotes the index of an existing cluster in region R. The updates of the gR ?s
are more complicated as they are coupled and not of standard form:



?mj??
Q
P
?R (?)+m?R? ?1 ?gR Q
p({gR }R?R |others) =
e
(16)
R?R gR
j
R?Rj gR
To sample the gR ?s we introduce auxiliary variables {Aj } to simplify the rightmost term above. In
particular, using the Gamma identity

?mj?? Z ?
P
P
mj?? ?1 ? R?Rj gR Aj
g
dAj
(17)
?(mj?? )
=
A
e
j
R?Rj R
0

we have that (16) is the marginal of {gR }R?R of the distribution:
Y ? (?)+m ?1
Y m ?1 ? P
?R?
R?Rj gR Aj
q({gR }R?R , {Aj }) ?
gRR
e?gR
Aj j?? e

(18)

j

R?R

Now we can Gibbs sample the gR ?s and Aj ?s:
gR |others ?Gamma(?R (?) + m?R? , 1 +
P
Aj |others ?Gamma(mj?? , R?Rj gR )

P

j?JR

Aj )

(19)
(20)

Here JR is the collection of indices j such that R ? Rj .
4.2

Metropolis-Hastings Proposals

To improve convergence and mixing of the Markov chain, we introduce three Metropolis-Hastings
(MH) proposals in addition to the Gibbs sampling updates described above. These propose nonincremental changes in the assignment of observations to clusters and regions, allowing the Markov
chain to traverse to different modes that are hard to reach using Gibbs sampling.
The first proposal (Algorithm 1) proceeds like the split-merge proposal of [12]. It either splits an
existing cluster in a region into two new clusters in the same region, or merges two existing clusters
in a region into a single cluster. To improve the acceptance probability, we use 5 rounds of restricted
Gibbs sampling [12].
The second proposal (Algorithm 2) seeks to move a picked cluster from one region to another.
The new region is chosen from a region neighbouring the current one (for example in Figure 1
the neigbors are the four regions diagonally neighbouring the current one). To improve acceptance
probability we also resample the gR ?s associated with the current and proposed regions. The move
can be invalid if the cluster contains an observation from a time point not associated with the new
region; in this case the move is simply rejected.
The third proposal (Algorithm 3) we considered seeks to combine into one step what would take
two steps under the previous two proposals: splitting a cluster and moving it to a new region (or the
reverse: moving a cluster into a new region and merging it with a cluster therein).
5

Algorithm 1 Split and Merge in the Same Region (MH1)
1: Let S0 be the current state of the Markov chain.
2: Pick a region R with probability proportional to m?R? and two distinct observations in R
3: Construct a launch state S 0 by creating two new clusters, each containing one of the two observations, and running restricted Gibbs sampling
4: if the two observations belong to the same cluster in S0 then
5:
Propose split: run one last round of restricted Gibbs sampling to reach the proposed state S1
6: else
7:
Propose merge: the proposed state S1 is the (unique) state merging the two clusters
8: end if


p(S1 )q(S 0 ?S0 )
9: Accept proposed state S1 according to acceptance probability min 1, p(S0 )q(S 0 ?S1 ) where
p(S) is the posterior probability of state S and q(S 0 ? S) is the probability of proposing state
S from the launch state S 0 .
Algorithm 2 Move (MH2)
1: Pick a cluster c in region R0 with probability proportional to m?R0 c
2: Pick a region R1 neighbouring R0 and propose moving c to R1
3: Propose new weights gR0 , gR1 by sampling both from (19)
4: Accept or reject the move
Algorithm 3 Split/merged Move (MH3)
1: Pick a region R0 , a cluster c contained in R, and a neighbouring region R1 with probability
proportional to the number of observations in c that cannot be assigned to a cluster in R1
2: if c contains observations than can be moved to R1 then
3:
Propose assigning these observations to a new cluster in R1
4: else
5:
Pick a cluster from those in R1 and propose merging it into c
6: end if
7: Propose new weights gR0 , gR1 by sampling from (19)
8: Accept or reject the proposal

5

Experiments

Synthetic data In the first of our experiments, we artificially generated 60 data points at each of
5 times by sampling from a mixture of 10 Gaussians. Each component was assigned a timespan,
ranging from a single time to the entire range of five times. We modelled this data as a collection of
five DP mixture of Gaussians, with a SN?P prior over the five dependent DPs. We used the set-up as
described in the second example. To encourage clusters to be shared across times (i.e. to avoid similar clusters with non-overlapping timespans), we chose the distribution over window lengths ?(w)
to give larger probabilities to larger timespans. Even in this simple model, Gibbs sampling alone
usually did not converge to a good optimum; remaining stuck around local maxima. Figure 2 shows
the evolution of the log-likelihood for 5 different samplers: plain Gibbs sampling, Gibbs sampling
augmented with each of MH proposals 1, 2 and 3, and finally a sampler that interleaved all three
MH samplers with Gibbs sampling. Not surprisingly, the complete sampler converged fastest, with
Gibbs sampling with MH-proposal 2 (Gibbs+MH2) performing nearly as well. Gibbs+MH1 seemed
converge no faster than just Gibbs sampling, with Gibbs+MH3 giving performance somewhere in
between. The fact that Gibbs+MH2 performs so well can be explained by the easy clustering structure of the problem, so that exploring region assignments of clusters rather than cluster assignments
of observations was the challenge faced by the sampler (note its high acceptance rate in Figure 4).
To demonstrate how the additional MH proposals help mixing, we examined how the cluster assignment of observations varied over iterations. At each iteration, we construct a 600 by 600 binary
matrix, with element (i, j) being 1 if observations i and j are assigned to the same cluster. In
Figure 3, we plot the average L1 difference between matrices at different iteration lags. Somewhat counterintuitively, Gibbs+MH1 does much better than Gibbs sampling with all MH proposals.
6

Figure 4: Acceptance rates of the MH proposals
for Gibbs+MH1+MH2+MH3 after burn-in (percentages).

?750

Proposal
MH-Proposal 1
MH-Proposal 2
MH-Proposal 3

?800

?850

?900

Gibbs+MH1
MH2+MH3
Gibbs+MH2
Gibbs+MH3
Gibbs+MH1
Gibbs

?950

?1000
0

100

200

300

400

500

600

700

Synthetic
0.51
11.7
0.22

NIPS
0.6621
0.6548
0.0249

9
8

800

7
6

Figure 2: log-likelihoods (the coloured lines are
ordered at iteration 80 like the legend).

5
0

500

1000

1500

2000

2500

3000

3500

4000

4500

5000

0

500

1000

1500

2000

2500

3000

3500

4000

4500

5000

0

500

1000

1500

2000

2500

3000

8

4000

7
3500

6
3000

2500

10
2000

8

Gibbs+MH1
Gibbs+MH1
MH2+MH3
Gibbs+MH3
Gibbs+MH2
Gibbs

1500

1000

500
1

2

3

4

5

6

7

8

9

6

3500

4000

4500

5000

10

Figure 5: Evolution of the timespan of a cluster.
From top to bottom: Gibbs+MH1+MH2+MH3,
Gibbs+MH2
and
Gibbs+MH1
(pink),
Gibbs+MH3 (black) and Gibbs (magenta).

Figure 3: Dissimilarity in clustering structure vs
lag (the coloured lines are ordered like the legend).

This is because the latter is simultaneously exploring the region assignment of clusters as well. In
Gibbs+MH1, clusters split and merge frequently since they stay in the same regions, causing the
cluster matrix to vary rapidly. In Gibbs+MH1+MH2+MH3, after a split the new clusters often move
into separate regions; so it takes longer before they can merge again. Nonetheless, this demonstrates
the importance of split/merge proposals like MH1 and MH3; [12] studied this in greater detail. We
next examined how well the proposals explore the region assignment of clusters. In particular, at
each step of the Markov chain, we pick the cluster with mean closest to the mean of one of the true
Gaussian mixture components, and tracked how its timespan evolved. Figure 5 shows that without
MH proposal 2, the clusters remain essentially frozen in their initial regions.
NIPS dataset For our next experiment we modelled the proceedings of the first 13 years of NIPS.
The number of word tokens was about 2 million spread over 1740 documents, with about 13000
unique words. We used a model that involves both the SN?P (to capture changes in topic distributions across the years) and the hierarchical Dirichlet process (HDP) [3] (to capture differences
among documents). Each document is modeled using a different DP, with the DPs in year i sharing
the same base distribution Di . On top of this, we place a SN?P (with structure given by the second
example in Section 3.1) prior on {Di }13
i=1 . Consequently, each topic is associated with a distribution
over words, and has a particular timespan. Each document in year i is a mixture over the topics
whose timespan include year i. Our model allows statistical strength to be shared in a more refined
manner than the HDP. Instead of all DPs having the same base distribution, we have 13 dependent
base distributions drawn from the SN?P. The concentration parameters of our DPs were chosen to
encourage shared topics, their magnitude chosen to produce about a 100 topics over the whole corpus on average. Figure 6 shows some of the topics identified by the model and their timespans. For
inference, we used Gibbs sampling, interleaved with all three MH proposals to update the SN?P. the
Markov chain was initialized randomly except that all clusters were assigned to the top-most region
(spanning the 13 years). We calculated per-word perplexity [3] on test documents (about half of all
documents, withheld during training). We obtained an average perplexity of 3023.4, as opposed to
about 3046.5 for the HDP.
7

scale

topic A

(173268 words)

topic B
topic C

(98342 words)

(60385 words)

topic D
topic E
topic F

(20290 words)

(7021 words)

(3223 words)

topic G
topic H
1

2

3

4

5

7

topic I
8

function, model, data, error, learning, probability, distribution

Topic B

model, visual, figure, image, motion, object, field

Topic C

network, memory, neural, state, input, matrix, hopfield

Topic D

rules, rule, language, tree, representations, stress, grammar

Topic E

classifier, genetic, memory, classification, tree, algorithm, data

Topic F

map, brain, fish, electric, retinal, eye, tectal

Topic G

recurrent, time, context, sequence, gamma, tdnn, sequences

Topic H

chain, protein, region, mouse, human, markov, sequence

Topic I

routing, load, projection, forecasting, shortest, demand, packet

(5334 words)

(2074 words)
6

Topic A

9

10

11

(780 words)
12

13

year

Figure 6: Inferred topics with their timespans (the horizontal lines). In parentheses are the number
of words assigned to each topic. On the right are the top ten most probable words in the topics.

Computationally, the 3 MH steps are much cheaper than a round of Gibbs sampling. When trying to
split a large cluster (or merge 2 large clusters), MH proposal 1 can still be fairly expensive because
of the rounds of restricted Gibbs sampling. MH proposal 3 does not face this problem. However
we find that after the burn-in period it tends to have low acceptance rate. We believe we need to
redesign MH proposal 3 to produce more intelligent splits to increase the acceptance rate. Finally,
MH-proposal 2 is the cheapest, both in terms of computation and book-keeping, and has reasonably
high acceptance rate. We ran MH-proposal 2 a hundred times between successive Gibbs sampling
updates. The acceptance rates of the MH proposals (given in Figure 4) are slightly lower than those
reported by [12], where a plain DP mixture model was applied to a simple synthetic data set, and
where split/merge acceptance rates were on the order of 1 to 5 percent.

6

Discussion

We described a conceptually simple and elegant framework for the construction of dependent DPs
based on normalized gamma processes. The resulting collection of random probability measures has
a number of useful properties: the marginal distributions are DPs and the weights of shared atoms
can vary across DPs. We developed auxiliary variable Gibbs and Metropolis-Hastings samplers for
the model and applied it to time-varying topic modelling where each topic has its own time-span.
Since [6] there has been strong interest in building dependent sets of random measures. Interestingly,
the property of each random measure being marginally DP, as originally proposed by [6], is often not
met in the literature, where dependent stochastic processes are defined through shared and random
parameters [3, 14, 15, 11]. Useful dependent DPs had not been found [16] until recently, when
a flurry of models were proposed [17, 18, 19, 20, 21, 22, 23]. However most of these proposals
have been defined only for the real line (interpreted as the time line) and not for arbitrary spaces.
[24, 25, 26, 13] proposed a variety of spatial DPs where the atoms and weights of the DPs are
dependent through Gaussian processes. A model similar to ours was proposed recently in [23],
using the same basic idea of introducing dependencies between DPs through spatially overlapping
regions. This model differs from ours in the content of these shared regions (breaks of a stick in that
case vs a (restricted) Gamma process in ours) and the construction of the DPs (they use the stick
breaking construction of the DP, we normalize the restricted Gamma process). Consequently, the
nature of the dependencies between the DPs differ; for instance, their model cannot be interpreted
as a mixture of DPs like ours.
There are a number of interesting future directions. First, we can allow, at additional complexity, the
locations of atoms to vary using the spatial DP approach [13]. Second, more work need still be done
to improve inference in the model, e.g. using a more intelligent MH proposal 3. Third, although
we have only described spatial normalized gamma processes, it should be straightforward to extend
the approach to spatial normalized random measures [7, 8]. Finally, further investigations into the
properties of the SN?P and its generalizations, including the nature of the dependency between DPs
and asymptotic behavior, are necessary for a complete understanding of these processes.
8

References
[1] T. S. Ferguson. A Bayesian analysis of some nonparametric problems. Annals of Statistics, 1(2):209?230,
1973.
[2] M. J. Beal, Z. Ghahramani, and C. E. Rasmussen. The infinite hidden Markov model. In Advances in
Neural Information Processing Systems, volume 14, 2002.
[3] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical Dirichlet processes. Journal of the
American Statistical Association, 101(476):1566?1581, 2006.
[4] C. Kemp, J. B. Tenenbaum, T. L. Griffiths, T. Yamada, and N. Ueda. Learning systems of concepts with
an infinite relational model. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 21,
2006.
[5] M. Johnson, T. L. Griffiths, and S. Goldwater. Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models. In Advances in Neural Information Processing Systems, volume 19, 2007.
[6] S. MacEachern. Dependent nonparametric processes. In Proceedings of the Section on Bayesian Statistical Science. American Statistical Association, 1999.
[7] L. E. Nieto-Barajas, I. Pruenster, and S. G. Walker. Normalized random measures driven by increasing
additive processes. Annals of Statistics, 32(6):2343?2360, 2004.
[8] L. F. James, A. Lijoi, and I. Pruenster. Bayesian inference via classes of normalized random measures.
ICER Working Papers - Applied Mathematics Series 5-2005, ICER - International Centre for Economic
Research, April 2005.
[9] J. F. C. Kingman. Completely random measures. Pacific Journal of Mathematics, 21(1):59?78, 1967.
[10] J. F. C. Kingman. Poisson Processes. Oxford University Press, 1993.
[11] P. M?uller, F. A. Quintana, and G. Rosner. A method for combining inference across related nonparametric
Bayesian models. Journal of the Royal Statistical Society, 66:735?749, 2004.
[12] S. Jain and R. M. Neal. A split-merge Markov chain Monte Carlo procedure for the Dirichlet process
mixture model. Technical report, Department of Statistics, University of Toronto, 2004.
[13] J. A. Duan, M. Guindani, and A. E. Gelfand. Generalized spatial Dirichlet process models. Biometrika,
94(4):809?825, 2007.
[14] A. Rodr??guez, D. B. Dunson, and A. E. Gelfand. The nested Dirichlet process. Technical Report 2006-19,
Institute of Statistics and Decision Sciences, Duke University, 2006.
[15] D. B. Dunson, Y. Xue, and L. Carin. The matrix stick-breaking process: Flexible Bayes meta analysis. Technical Report 07-03, Institute of Statistics and Decision Sciences, Duke University, 2007.
http://ftp.isds.duke.edu/WorkingPapers/07-03.html.
[16] N. Srebro and S. Roweis. Time-varying topic models using dependent Dirichlet processes. Technical
Report UTML-TR-2005-003, Department of Computer Science, University of Toronto, 2005.
[17] J. E. Griffin and M. F. J. Steel. Order-based dependent Dirichlet processes. Journal of the American
Statistical Association, Theory and Methods, 101:179?194, 2006.
[18] J. E. Griffin. The Ornstein-Uhlenbeck Dirichlet process and other time-varying processes for Bayesian
nonparametric inference. Technical report, Department of Statistics, University of Warwick, 2007.
[19] F. Caron, M. Davy, and A. Doucet. Generalized Polya urn for time-varying Dirichlet process mixtures. In
Proceedings of the Conference on Uncertainty in Artificial Intelligence, volume 23, 2007.
[20] A. Ahmed and E. P. Xing. Dynamic non-parametric mixture models and the recurrent Chinese restaurant
process. In Proceedings of The Eighth SIAM International Conference on Data Mining, 2008.
[21] J. E. Griffin and M. F. J. Steel. Bayesian nonparametric modelling with the Dirichlet process regression
smoother. Technical report, University of Kent and University of Warwick, 2008.
[22] J. E. Griffin and M. F. J. Steel. Generalized spatial Dirichlet process models. Technical report, University
of Kent and University of Warwick, 2009.
[23] Y. Chung and D. B. Dunson. The local Dirichlet process. Annals of the Institute of Mathematical Statistics,
2009. to appear.
[24] S.N. MacEachern, A. Kottas, and A.E. Gelfand. Spatial nonparametric Bayesian models. In Proceedings
of the 2001 Joint Statistical Meetings, 2001.
[25] C. E. Rasmussen and Z. Ghahramani. Infinite mixtures of Gaussian process experts. In Advances in
Neural Information Processing Systems, volume 14, 2002.
[26] A. E. Gelfand, A. Kottas, and S. N. MacEachern. Bayesian nonparametric spatial modeling with Dirichlet
process mixing. Journal of the American Statistical Association, 100(471):1021?1035, 2005.

9

"
2017,Sparse Embedded ,Poster,6924-sparse-embedded-k-means-clustering.pdf,"The $k$-means clustering algorithm is a ubiquitous tool in data mining and machine learning that shows promising performance. However, its high computational cost has hindered its applications in broad domains. Researchers have successfully addressed these obstacles with dimensionality reduction methods. Recently, [1] develop a state-of-the-art random projection (RP) method for faster $k$-means clustering. Their method delivers many improvements over other dimensionality reduction methods. For example, compared to the advanced singular value decomposition based feature extraction approach,  [1] reduce the running time by a factor of $\min \{n,d\}\epsilon^2 log(d)/k$ for data matrix $X \in \mathbb{R}^{n\times d} $ with $n$ data points and $d$ features, while losing only a factor of one in approximation accuracy. Unfortunately, they still require $\mathcal{O}(\frac{ndk}{\epsilon^2log(d)})$ for matrix multiplication and this cost will be prohibitive for large values of $n$ and $d$. To break this bottleneck, we carefully build a sparse embedded $k$-means clustering algorithm which requires $\mathcal{O}(nnz(X))$ ($nnz(X)$ denotes the number of non-zeros in $X$) for fast matrix multiplication. Moreover, our proposed algorithm improves on [1]'s results for approximation accuracy by a factor of one. Our empirical studies corroborate our theoretical findings, and demonstrate that our approach is able to significantly accelerate $k$-means clustering, while achieving satisfactory clustering performance.","Sparse Embedded k-Means Clustering

?
?

Weiwei Liu?,\,?, Xiaobo Shen?,? , Ivor W. Tsang\
School of Computer Science and Engineering, The University of New South Wales
School of Computer Science and Engineering, Nanyang Technological University
\
Centre for Artificial Intelligence, University of Technology Sydney
{liuweiwei863,njust.shenxiaobo}@gmail.com
ivor.tsang@uts.edu.au

Abstract
The k-means clustering algorithm is a ubiquitous tool in data mining and machine
learning that shows promising performance. However, its high computational
cost has hindered its applications in broad domains. Researchers have successfully addressed these obstacles with dimensionality reduction methods. Recently,
[1] develop a state-of-the-art random projection (RP) method for faster k-means
clustering. Their method delivers many improvements over other dimensionality
reduction methods. For example, compared to the advanced singular value decomposition based feature extraction approach, [1] reduce the running time by a
factor of min{n, d}2 log(d)/k for data matrix X ? Rn?d with n data points and
d features, while losing only a factor of one in approximation accuracy. Unfortundk
) for matrix multiplication and this cost will
nately, they still require O( 2 log(d)
be prohibitive for large values of n and d. To break this bottleneck, we carefully
build a sparse embedded k-means clustering algorithm which requires O(nnz(X))
(nnz(X) denotes the number of non-zeros in X) for fast matrix multiplication.
Moreover, our proposed algorithm improves on [1]?s results for approximation
accuracy by a factor of one. Our empirical studies corroborate our theoretical findings, and demonstrate that our approach is able to significantly accelerate k-means
clustering, while achieving satisfactory clustering performance.

1

Introduction

Due to its simplicity and flexibility, the k-means clustering algorithm [2, 3, 4] has been identified
as one of the top 10 data mining algorithms. It has shown promising results in various real world
applications, such as bioinformatics, image processing, text mining and image analysis. Recently, the
dimensionality and scale of data continues to grow in many applications, such as biological, finance,
computer vision and web [5, 6, 7, 8, 9], which poses a serious challenge in designing efficient and
accurate algorithmic solutions for k-means clustering.
To address these obstacles, one prevalent technique is dimensionality reduction, which embeds the
original features into low dimensional space before performing k-means clustering. Dimensionality
reduction encompasses two kinds of approaches: 1) feature selection (FS), which embeds the
data into a low dimensional space by selecting the actual dimensions of the data; and 2) feature
extraction (FE), which finds an embedding by constructing new artificial features that are, for
example, linear combinations of the original features. Laplacian scores [10] and Fisher scores
[11] are two basic feature selection methods. However, they lack a provable guarantee. [12] first
propose a provable singular value decomposition (SVD) feature selection method. It uses the SVD
to find O(klog(k/)/2 ) actual features such that with constant probability the clustering structure
?

The first two authors make equal contributions.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

Table 1: Dimension reduction methods for k-means clustering. The third column corresponds to the
number of selected or extracted features. The fourth column corresponds to the time complexity of
each dimension reduction method. The last column corresponds to the approximation accuracy. N/A
denotes not available. nnz(X) denotes the number of non-zeros in X.  and ? represent the gap to
optimality and the confidence level, respectively. Sparse embedding is abbreviated to SE.
M ETHOD
[13]
F OLKLORE
[12]
[14]
[1]
[15]
T HIS PAPER

D ESCRIPTION
SVD-FE
RP-FE

D IMENSIONS
k
O( log(n)
)
2

T IME
O(nd min{n, d})
O( ndlog(n)
)
2 log(d)

ACCURACY
2
1+

SVD-FS
SVD-FE
RP-FE

)
O( klog(k/)
2
O( k2 )
O( k2 )

O(nd min{n, d})
O(nd min{n, d})
O( 2 ndk
)
log(d)

2+
1+
2+

O(dlog(d)n + dlog(n))
O(nnz(X))

N/A
1+

RP-FE
SE-FE

O( log(n)
)
n
O(max( k+log(1/?)
,
2


6
2 ?

))

is preserved within a factor of 2 + . [13] propose a popular feature extraction approach, where k
artificial features are constructed using the SVD such that the clustering structure is preserved within
a factor of two. Recently, corollary 4.5 in [14]?s study improves [13]?s results, by claiming that O( k2 )
dimensions are sufficient for preserving 1 +  accuracy.
Because SVD is computationally expensive, we focus on another important feature extraction method
that randomly projects the data into low dimensional space. [1] develop a state-of-the-art random
projection (RP) method, which is based on random sign matrices. Compared to SVD-based feature
extraction approaches [14], [1] reduce the running time by a factor of min{n, d}2 log(d)/k 2 , while
losing only a factor of one in approximation accuracy. They also improve the results of the folklore
RP method by a factor of log(n)/k in terms of the number of embedded dimensions and the running
time, while losing a factor of one in approximation accuracy. Compared to SVD-based feature
selection methods, [1] reduce the embedded dimension by a factor of log(k/) and the running time
ndk
by a factor of min{n, d}2 log(d)/k, respectively. Unfortunately, they still require O( 2 log(d)
) for
matrix multiplication and this cost will be prohibitive for large values of n and d.
This paper carefully constructs a sparse matrix for the RP method that only requires O(nnz(X)) for
fast matrix multiplication. Our algorithm is significantly faster than other dimensionality reduction
methods, especially when nnz(X) << nd. Theoretically, we show a provable guarantee for our
algorithm. Given d? = O(max( k+log(1/?)
, 26? )), with probability at least 1 ? O(?), our algorithm
2
preserves the clustering structure within a factor of 1 + , improving on the results of [12] and [1] by
a factor of one for approximation accuracy. These results are summarized in Table 1.
Experiments on three real-world data sets show that our algorithm outperforms other dimension
reduction methods. The results verify our theoretical analysis. We organize this paper as follows.
Section 2 introduces the concept of -approximation k-means clustering and our proposed sparse
embedded k-means clustering algorithm. Section 3 analyzes the provable guarantee for our algorithm
and experimental results are presented in Section 4. The last section provides our conclusions.

Sparse Embedded k-Means Clustering

2

-Approximation k-Means Clustering

2.1

Given X ? Rn?d with n data points and d features. We denote the transpose of the vector/matrix
by superscript 0 and the logarithms to base 2 by log. Let r = rank(X). By using singular value
decomposition (SVD), we have X = U ?V 0 , where ? ? Rr?r is a positive diagonal matrix containing
the singular values of X in decreasing order (?1 ? ?2 ? . . . ? ?r ), and U ? Rn?r and V ? Rd?r
contain orthogonal left and right singular vectors of X. Let Uk and Vk represent U and V with all
but their first k columns zeroed out, respectively, and ?k be ? with all but its largest k singular
values zeroed out. Assume k ? r, [16] have already shown that Xk = Uk ?k Vk0 is the optimal rank k
2

Refer to Section 2.1 for the notations.

2

approximation to X for any unitarily invariant norm, including the Frobenius and spectral norms. The
pseudoinverse of X is given by X + = V ??1 U 0 . Assume Xr|k = X ? Xk . In denotes the n ? n
identity matrix. Let ||X||F be the Frobenius norm of matrix X. For concision, ||A||2 represents the
spectral norm of A if A is a matrix and the Euclidean norm of A if A is a vector. Let nnz(X) denote
the number of non-zeros in X.
The task of k-means clustering is to partition n data points in d dimensions into k clusters. Let ?i
be the centroid of the vectors in cluster i and c(xi ) be the cluster that xi is assigned to. Assume
D ? Rn?k is the indicator matrix which has exactly one non-zero element per row, which denotes
?
cluster membership. The i-th data point belongs to the j-th cluster if and only if Dij = 1/ zj , where
zj denotes the number of data points in cluster j. Note
D0 D = Ik and the i-th row of DD0 X is
Pthat
n
the centroid of xi ?s assigned cluster. Thus, we have i=1 ||xi ? ?c(xi ) ||22 = ||X ? DD0 X||2F . We
formally define the k-means clustering task as follows, which is also studied in [12] and [1].
Definition 1 (k-Means Clustering). Given X ? Rn?d and a positive integer k denoting the number
of clusters. Let D be the set of all n ? k indicator matrices D. The task of k-means clustering is to
solve
min ||X ? DD0 X||2F

(1)

D?D

To accelerate the optimization of problem 1, we aim to find a -approximate solution for problem 1
? ? Rn?d? with d? < d.
by optimizing D (either exactly or approximately) over an embedded matrix X
To measure the quality of approximation, we first define the -approximation embedded matrix:
Definition 2 (-Approximation Embedded Matrix). Given 0 ?  < 1 and a non-negative constant ? .
? ? Rn?d? with d? < d is a -approximation embedded matrix for X, if
X
? ? DD0 X||
? 2 + ? ? (1 + )||X ? DD0 X||2
(1 ? )||X ? DD0 X||2F ? ||X
F
F

(2)

We show that a -approximation embedded matrix is sufficient for approximately optimizing problem
1:
Lemma 1 (-Approximation k-Means Clustering). Given X ? Rn?d and D be the set of all n ? k
? ? Rn?d? with d? < d, let
indicator matrices D, let D? = arg minD?D ||X ? DD0 X||2F . Given X
? ? = arg minD?D ||X
? ? DD0 X||
? 2 . If X
? is a 0 -approximation embedded matrix for X, given
D
F
0
0
? 2 , we have
?
?D
? 0 X||
? 2 ? ?||X
? ?D
? ?D
? ?0 X||
 = 2 /(1 ?  ), then for any ? ? 1, if ||X ? D
F
F
?D
? 0 X||2F ? (1 + )?||X ? D? D?0 X||2F
||X ? D
? ?D
? ?D
? ?0 X||
? 2 ? ||X
? ? D? D?0 X||
? 2 and thus
Proof. By definition, we have ||X
F
F
? ?D
?D
? 0 X||
? 2F ? ?||X
? ? D? D?0 X||
? 2F
||X

(3)

? is a -approximation embedded matrix for X, we have
Since X
? ? D? D?0 X||
? 2 ?(1 + 0 )||X ? D? D?0 X||2 ? ?
||X
F
F
? ?D
?D
? 0 X||
? 2F ?(1 ? 0 )||X ? D
?D
? 0 X||2F ? ?
||X

(4)

Combining Eq.(3) and Eq.(4), we obtain:
?D
? 0 X||2F ? ? ? ||X
? ?D
?D
? 0 X||
? 2F ??||X
? ? D? D?0 X||
? 2F
(1 ? 0 )||X ? D
0

?(1 + 0 )?||X ? D? D? X||2F ? ? ?

(5)

Eq.(5) implies that
?D
? 0 X||2F ? (1 + 0 )/(1 ? 0 )?||X ? D? D?0 X||2F ? (1 + )?||X ? D? D?0 X||2F
||X ? D

(6)

? is an optimal solution for X,
? then it also preserves Remark. Lemma 1 implies that if D
?
?
approximation for X. Parameter ? allows D to be approximately global optimal for X.
3

Algorithm 1 Sparse Embedded k-Means Clustering
Input: X ? Rn?d . Number of clusters k.
Output: -approximate solution for problem 1.
k+log(1/?) 6
1: Set d? = O(max(
, 2 ? )).
2
? with probability 1/d.
?
2: Build a random map h so that for any i ? [d], h(i) = j for j ? [d]
?
3: Construct matrix ? ? {0, 1}d?d with ?i,h(i) = 1, and all remaining entries 0.
4: Construct matrix Q ? Rd?d is a random diagonal matrix whose entries are i.i.d. Rademacher
variables.
? = XQ? and run exact or approximate k-means algorithms on X.
?
5: Compute the product X

2.2

Sparse Embedding

[1] construct a random embedded matrix for fast k-means clustering by post-multiplying X with a
?1
d ? d? random matrix having entries ?1 or ?
with equal probability. However, this method requires
d?

d?

ndk
) for matrix multiplication and this cost will be prohibitive for large values of n and d. To
O( 2 log(d)
break this bottleneck, Algorithm 1 demonstrates our sparse embedded k-means clustering, which
requires O(nnz(X)) for fast matrix multiplication. Our algorithm is significantly faster than other
dimensionality reduction methods, especially when nnz(X) << nd. For an index i taking values in
the set {1, ? ? ? , n}, we write i ? [n].

Next, we state our main theorem to show that XQ? is the -approximation embedded matrix for X:
?

Theorem 1. Let ? and Q be constructed as in Algorithm 1 and R = (Q?)0 ? Rd?d . Given
, 26? )). Then for any X ? Rn?d , with a probability of at least 1 ? O(?),
d? = O(max( k+log(1/?)
2
0
XR is the -approximation embedded matrix for X.

3

Proofs

Let Z = In ? DD0 and tr be the trace notation. Eq.(2) can be formulated as: (1 ? )tr(ZXX 0 Z) ?
?X
? 0 . To prove our
?X
? 0 Z) + ? ? (1 + )tr(ZXX 0 Z). Then, we try to approximate XX 0 with X
tr(Z X
? = XR0 and our goal is to show that tr(ZXX 0 Z) can be approximated
main theorem, we write X
0
0
?X
? 0 ? XX 0 that are
by tr(ZXR RX Z). Lemma 2 provides conditions on the error matrix E = X
?
sufficient to guarantee that X is a -approximation embedded matrix for X. For any two symmetric
matrices A, B ? Rn?n , A  B indicates that B ? A is positive semidefinite. Let ?i (A) denote the
i-th largest eigenvalue of A in absolute value. h?, ?i represents the inner product, and 0n?d denotes an
n ? d zero matrix with all its entries being zero.
?X
? 0 . If we write C? = C + E1 + E2 + E3 + E4 , where:
Lemma 2. Let C = XX 0 and C? = X
(i) E1 is symmetric and ?1 C  E1  1 C.
Pk
(ii) E2 is symmetric, i=1 |?i (E2 )| ? 2 ||Xr|k ||2F , and tr(E2 ) ? ?2 ||Xr|k ||2F .
(iii) The columns of E3 fall in the column span of C and tr(E30 C + E3 ) ? 23 ||Xr|k ||2F .
(iv) The rows of E4 fall in the row span of C and tr(E4 C + E40 ) ? 24 ||Xr|k ||2F .
? is a -approximation embedded matrix for X. Specifically,
and 1 + 2 + ?2 + 3 + 4 = , then X
? ? min{0, tr(E2 )} ? (1 + )tr(ZCZ).
we have (1 ? )tr(ZCZ) ? tr(Z CZ)
The proof can be referred to [17]. Next, we show XR0 is the -approximation embedded matrix for
X. We first present the following theorem:
Theorem 2. Assume r > 2k and let V2k ??Rd?r represent V with all but their first 2k columns
0
0
zeroed out. We define M1 = V2k
, M2 = k/||Xr|k ||F (X ? XV2k V2k
) and M ? R(n+r)?d as
?
containing M1 as its first r rows and M2 as its lower n rows. We construct R = (Q?)0 ? Rd?d ,
4

which is shown in Algorithm 1. Given d? = O(max( k+log(1/?)
, 26? )), then for any X ? Rn?d , with
2
a probability of at least 1 ? O(?), we have
(i) ||(RM 0 )0 (RM 0 ) ? M M 0 ||2 < .
(ii) |||RM20 ||2F ? ||M20 ||2F | ? k.
Proof. To prove the first result, one can easily check that M1 M20 = 0r?n , thus M M 0 is a block
diagonal matrix with an upper left block equal to M1 M10 and lower right block equal to M2 M20 .
The spectral norm of M1 M10 is 1. ||M2 M20 ||2 = ||M2 ||22 =

0
k||X?XV2k V2k
||22
||Xr|k ||2F
0

=

k||Xr|2k ||22
.
||Xr|k ||2F

As

? k||Xr|2k ||22 , we derive ||M2 M20 ||2 ? 1. Since M M is a block diagonal matrix, we
have ||M ||22 = ||M M 0 ||2 = max{||M1 M10 ||2 , ||M2 M20 ||2 } = 1. tr(M1 M10 ) = 2k. tr(M2 M20 ) =
k||Xr|2k ||2F
. As ||Xr|k ||2F ? ||Xr|2k ||2F , we derive tr(M2 M20 ) ? k. Then we have ||M ||2F =
||Xr|k ||2F
tr(M M 0 ) = tr(M1 M10 ) + tr(M2 M20 ) ? 3k. Applying Theorem 6 from [18], we can obtain that
given d? = O( k+log(1/?)
), with a probability of at least 1 ? ?, ||(RM 0 )0 (RM 0 ) ? M M 0 ||2 < .
2
||Xr|k ||2F

The proof of the second result can be found in the Supplementary Materials.
? = XR0 satisfies the conditions of Lemma 2.
Based on Theorem 2, we show that X
Lemma 3. Assume r > 2k and we construct M and R as in Theorem 2. Given d? =
? = XR0
O(max( k+log(1/?)
, 26? )), then for any X ? Rn?d , with a probability of at least 1?O(?), X
2
satisfies the conditions of Lemma 2.
0
Proof. We construct H1 ? Rn?(n+r) as H1 = [XV2k , 0n?n ], thus H1 M = XV2k V2k
. And we set
||Xr|k ||F
||Xr|k ||F
n?(n+r)
0
=
H2 ? R
as H2 = [0n?r , ?k In ], so we have H2 M = ?k M2 = X ? XV2k V2k
Xr|2k and X = H1 M + H2 M and we obtain the following:

?X
? 0 ? XX 0 = XR0 RX 0 ? XX 0 = 
E =X
1 +
2 +
3 +
4

(7)

Where 
1 = H1 M R0 RM 0 H10 ? H1 M M 0 H10 , 
2 = H2 M R0 RM 0 H20 ? H2 M M 0 H20 , 
3 =
0
0 0
0 0
H1 M R RM H2 ? H1 M M H2 and 
4 = H2 M R0 RM 0 H10 ? H2 M M 0 H10 . We bound 
,
1 
,
2

3 and 
4 separately, showing that each corresponds to one of the error terms described in Lemma 2.
Bounding 
.
1
0
0
0
0
E1 = H1 M R0 RM 0 H10 ? H1 M M 0 H10 = XV2k V2k
R0 RV2k V2k
X 0 ? XV2k V2k
V2k V2k
X0

(8)

E1 is symmetric. By Theorem 2, we know that with a probability of at least 1 ? ?, ||(RM 0 )0 (RM 0 ) ?
M M 0 ||2 <  holds. Then we get ?In+r  (RM 0 )0 (RM 0 ) ? M M 0  In+r . And we derive the
following:
?H1 H10  E1  H1 H10

(9)

0
0
0
0 2
For any vector v, v 0 XV2k V2k
V2k V2k
X 0 v = ||V2k V2k
X 0 v||22 ? ||V2k V2k
||2 ||X 0 v||22 =
0
2
0
0
0 0
0
0
0
0
||X v||2 = v XX v, so H1 M M H1 = XV2k V2k V2k V2k X  XX . Since H1 M M 0 H10 =
0
0
0
XV2k V2k
V2k V2k
X 0 = XV2k V2k
X 0 = H1 H10 , we have

H1 H10 = H1 M M 0 H10  XX 0 = C

(10)

Combining Eqs.(9) and (10), we arrive at a probability of at least 1 ? ?,
?C  E1  C

(11)

satisfying the first condition of Lemma 2.
Bounding 
.
2
E2 =H2 M R0 RM 0 H20 ? H2 M M 0 H20
0
0 0
0
0 0
=(X ? XV2k V2k
)R0 R(X ? XV2k V2k
) ? (X ? XV2k V2k
)(X ? XV2k V2k
)

5

(12)

?
E2 is symmetric. Note that H2 just selects M2 from M and scales it by ||Xr|k ||F / k. Using
Theorem 2, we know that with a probability of at least 1 ? ?,
tr(E2 ) =

||Xr|k ||2F
tr(M2 R0 RM20 ? M2 M20 ) ? ||Xr|k ||2F
k

(13)

Applying Theorem 6.2 from [19] and rescaling  , we can obtain a probability of at least 1 ? ?,

0
0
||E2 ||F = ||Xr|2k R0 RXr|2k
? Xr|2k Xr|2k
||F ? ? ||Xr|2k ||2F
(14)
k
Combining Eq.(14), Cauchy-Schwarz inequality and ||Xr|2k ||2F ? ||Xr|k ||2F , we get that with a
probability of at least 1 ? ?,
k
X

|?i (E2 )| ?

?

k||E2 ||F ? ||Xr|k ||2F

(15)

i=1

Eqs.(13) and (15) satisfy the second conditions of Lemma 2.
Bounding 
.
3
E3 =H1 M R0 RM 0 H20 ? H1 M M 0 H20
0
0 0
0
0 0
=XV2k V2k
R0 R(X ? XV2k V2k
) ? XV2k V2k
(X ? XV2k V2k
)

(16)

0
The columns of E3 are in the column span of H1 M = XV2k V2k
, and so in the column span of
2
0
0
0
0
0
0
0
C. ||V2k ||F = tr(V2k V2k ) = 2k. As V2k V = V2k V2k , V2k Xr|2k = V2k
(V ?U 0 ? V2k ?2k U2k
)=
0
0
?2k U2k ? ?2k U2k = 0r?n . Applying Theorem 6.2 from [19] again and rescaling , we can obtain
that with a probability of at least 1 ? ?,

tr(E30 C + E3 ) =||??1 U 0 (H1 M R0 RM 0 H20 ? H1 M M 0 H20 )||2F
0
0
=||V2k
R0 RXr|2k
? 0r?n ||2F ? 2 ||Xr|k ||2F

(17)

Thus, Eq.(17) satisfies the third condition of Lemma 2.
Bounding 
.
4
E4 =H2 M R0 RM 0 H10 ? H2 M M 0 H10
0
0
0
0
=(X ? XV2k V2k
)R0 RV2k V2k
X 0 ? (X ? XV2k V2k
)V2k V2k
X0

(18)

E4 = E30 and thus we immediately have that with a probability of at least 1 ? ?,
tr(E4 C + E40 ) ? 2 ||Xr|k ||2F

(19)

? = XR0 satisfies the
Lastly, Eqs.(11), (13), (15), (17) and (19) ensure that, for any X ? Rn?d , X
conditions of Lemma 2 and is the -approximation embedded matrix for X with a probability of at
least 1 ? O(?).

4

Experiment

4.1

Data Sets and Baselines

We denote our proposed sparse embedded k-means clustering algorithm as SE for short. This section
evaluates the performance of the proposed method on four real-world data sets: COIL20, SECTOR,
RCV1 and ILSVRC2012. The COIL20 [20] and ILSVRC2012 [21] data sets are collected from
website34 , and other data sets are collected from the LIBSVM website5 . The statistics of these data
sets are presented in the Supplementary Materials.
We compare SE with several other dimensionality reduction techniques:
3

http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php
http://www.image-net.org/challenges/LSVRC/2012/
5
https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/
4

6

40
30
20

0

200

400
600
800
# of dimensions

15

10

5

0

1000

k-means
SVD
LLE
LS
RP
PD
SE

0

200

(a) COIL20

400
600
800
# of dimensions

25

20
k-means
RP
PD
SE

15

10

1000

40

Clustering accuracy (in %)

k-means
SVD
LLE
LS
RP
PD
SE

Clustering accuracy (in %)

50

10

30

20

60

Clustering accuracy (in %)

Clustering accuracy (in %)

70

30

20

0

0

(b) SECTOR

200

400
600
800
# of dimensions

k-means
RP
PD
SE

10

1000

0

200

400

600

800

1000

# of dimensions

(c) RCV1

(d) ILSVRC2012

Figure 1: Clustering accuracy of various methods on COIL20, SECTOR, RCV1 and ILSVRC2012
data sets.

SVD
LLE
LS
RP
PD
SE

10-2

0

200

400
600
800
# of dimensions

10

2

100

10-2

1000

SVD
LLE
LS
RP
PD
SE

0

(a) COIL20

200

400
600
800
# of dimensions

10

2

101

100

1000

RP
PD
SE

Preprocessing time (in second)

10-1

Preprocessing time (in second)

100

10-3

103

104
Preprocessing time (in second)

Preprocessing time (in second)

101

104

102

100

10-2

0

(b) SECTOR

200

400
600
800
# of dimensions

1000

RP
PD
SE

0

200

400

600

800

1000

# of dimensions

(c) RCV1

(d) ILSVRC2012

Figure 2: Dimension reduction time of various methods on COIL20, SECTOR, RCV1 and ILSVRC2012 data sets.
102

10-1

101

100

10
10-2

0

200

400
600
800
# of dimensions

(a) COIL20

1000

-1

0

200

400
600
800
# of dimensions

102
k-means
RP
PD
SE

101

1000

(b) SECTOR

10

0

200

400
600
800
# of dimensions

(c) RCV1

1000

Clustering time (in second)

100

103
k-means
SVD
LLE
LS
RP
PD
SE

Clustering time (in second)

k-means
SVD
LLE
LS
RP
PD
SE

Clustering time (in second)

Clustering time (in second)

101

4

k-means
RP
PD
SE

103

0

500

1000

# of dimensions

(d) ILSVRC2012

Figure 3: Clustering time of various methods on COIL20, SECTOR, RCV1 and ILSVRC2012 data
sets.
? SVD: The singular value decomposition or principal components analysis dimensionality
reduction approach.
? LLE: The local linear embedding (LLE) algorithm is proposed by [22]. We use the code
from website6 with default parameters.
? LS: [10] develop the laplacian score (LS) feature selection method. We use the code from
website7 with default parameters.
? PD: [15] propose an advanced compression scheme for accelerating k-means clustering. We
use the code from website8 with default parameters.
? RP: The state-of-the-art random projection method is proposed by [1].
After dimensionality reduction, we run all methods on a standard k-means clustering package, which
is from website9 with default parameters. We also compare all these methods against the standard
k-means algorithm on the full dimensional data sets. To measure the quality of all methods, we report
clustering accuracy based on the labelled information of the input data. Finally, we report the running
6

http://www.cs.nyu.edu/ roweis/lle/
www.cad.zju.edu.cn/home/dengcai/Data/data.html
8
https://github.com/stephenbeckr/SparsifiedKMeans
9
www.cad.zju.edu.cn/home/dengcai/Data/data.html
7

7

times (in seconds) of both the dimensionality reduction procedure and the k-means clustering for all
baselines.
4.2

Results

The experimental results of various methods on all data sets are shown in Figures 1, 2 and 3. The Y
axes of Figures 2 and 3 represent dimension reduction and clustering time in log scale. We can?t get
the results of SVD, LLE and LS within three days on RCV1 and ILSVRC2012 data sets. Thus, these
results are not reported.
From Figures 1, 2 and 3, we can see that:
? As the number of embedded dimensions increases, the clustering accuracy and running times
of all dimensionality reduction methods increases, which is consistent with the empirical
results in [1].
? Our proposed dimensionality reduction method has superior performance compared to the
RP method and other baselines in terms of accuracy, which verifies our theoretical results.
LLE and LS generally underperforms on the COIL20 and SECTOR data sets.
? SVD and LLE are the two slowest methods compared with the other baselines in terms of
dimensionality reduction time. The dimension reduction time of the RP method increases
significantly with the increasing dimensions, while our method obtains a stable and lowest
dimensionality reduction time. We achieve several hundred orders of magnitude faster than
the RP method and other baselines. The results also support our complexity analysis.
? All dimensionality reduction methods are significantly faster than standard k-means algorithm with full dimensions. Finally, we conclude that our proposed method is able to
significantly accelerate k-means clustering, while achieving satisfactory clustering performance.

5

Conclusion

The k-means clustering algorithm is a ubiquitous tool in data mining and machine learning with
numerous applications. The increasing dimensionality and scale of data has provided a considerable
challenge in designing efficient and accurate k-means clustering algorithms. Researchers have
successfully addressed these obstacles with dimensionality reduction methods. These methods
embed the original features into low dimensional space, and then perform k-means clustering on
the embedded dimensions. SVD is one of the most popular dimensionality reduction methods.
However, it is computationally expensive. Recently, [1] develop a state-of-the-art RP method for
faster k-means clustering. Their method delivers many improvements over other dimensionality
reduction methods. For example, compared to an advanced SVD-based feature extraction approach
[14], [1] reduce the running time by a factor of min{n, d}2 log(d)/k, while only losing a factor of
one in approximation accuracy. They also improve the result of the folklore RP method by a factor
of log(n)/k in terms of the number of embedded dimensions and the running time, while losing
ndk
a factor of one in approximation accuracy. Unfortunately, it still requires O( 2 log(d)
) for matrix
multiplication and this cost will be prohibitive for large values of n and d. To break this bottleneck, we
carefully construct a sparse matrix for the RP method that only requires O(nnz(X)) for fast matrix
multiplication. Our algorithm is significantly faster than other dimensionality reduction methods,
especially when nnz(X) << nd. Furthermore, we improve the results of [12] and [1] by a factor
of one for approximation accuracy. Our empirical studies demonstrate that our proposed algorithm
outperforms other dimension reduction methods, which corroborates our theoretical findings.
Acknowledgments
We would like to thank the area chairs and reviewers for their valuable comments and constructive
suggestions on our paper. This project is supported by the ARC Future Fellowship FT130100746,
ARC grant LP150100671, DP170101628, DP150102728, DP150103071, NSFC 61232006 and NSFC
61672235.
8

References
[1] Christos Boutsidis, Anastasios Zouzias, Michael W. Mahoney, and Petros Drineas. Randomized dimensionality reduction for k-means clustering. IEEE Trans. Information Theory, 61(2):1045?1062, 2015.
[2] J. A. Hartigan and M. A. Wong. Algorithm as 136: A k-means clustering algorithm. Applied Statistics,
28(1):100?108, 1979.
[3] Xiao-Bo Shen, Weiwei Liu, Ivor W. Tsang, Fumin Shen, and Quan-Sen Sun. Compressed k-means for
large-scale clustering. In AAAI, pages 2527?2533, 2017.
[4] Xinwang Liu, Miaomiao Li, Lei Wang, Yong Dou, Jianping Yin, and En Zhu. Multiple kernel k-means
with incomplete kernels. In AAAI, pages 2259?2265, 2017.
[5] Tom M. Mitchell, Rebecca A. Hutchinson, Radu Stefan Niculescu, Francisco Pereira, Xuerui Wang,
Marcel Adam Just, and Sharlene D. Newman. Learning to decode cognitive states from brain images.
Machine Learning, 57(1-2):145?175, 2004.
[6] Jianqing Fan, Richard Samworth, and Yichao Wu. Ultrahigh dimensional feature selection: Beyond the
linear model. JMLR, 10:2013?2038, 2009.
[7] Jorge S?nchez, Florent Perronnin, Thomas Mensink, and Jakob J. Verbeek. Image classification with the
fisher vector: Theory and practice. International Journal of Computer Vision, 105(3):222?245, 2013.
[8] Yiteng Zhai, Yew-Soon Ong, and Ivor W. Tsang. The emerging ?big dimensionality?. IEEE Computational
Intelligence Magazine, 9(3):14?26, 2014.
[9] Weiwei Liu and Ivor W. Tsang. Making decision trees feasible in ultrahigh feature and label dimensions.
Journal of Machine Learning Research, 18(81):1?36, 2017.
[10] Xiaofei He, Deng Cai, and Partha Niyogi. Laplacian score for feature selection. In NIPS, pages 507?514,
2005.
[11] Donald H. Foley and John W. Sammon Jr. An optimal set of discriminant vectors. IEEE Trans. Computers,
24(3):281?289, 1975.
[12] Christos Boutsidis, Michael W. Mahoney, and Petros Drineas. Unsupervised feature selection for the
k-means clustering problem. In NIPS, pages 153?161, 2009.
[13] Petros Drineas, Alan M. Frieze, Ravi Kannan, Santosh Vempala, and V. Vinay. Clustering in large graphs
and matrices. In Proceedings of the Tenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages
291?299, 1999.
[14] Dan Feldman, Melanie Schmidt, and Christian Sohler. Turning big data into tiny data: Constant-size
coresets for k-means, PCA and projective clustering. In Proceedings of the Twenty-Fourth Annual ACMSIAM Symposium on Discrete Algorithms, pages 1434?1453, 2013.
[15] Farhad Pourkamali Anaraki and Stephen Becker. Preconditioned data sparsification for big data with
applications to PCA and k-means. IEEE Trans. Information Theory, 63(5):2954?2974, 2017.
[16] Leon Mirsky. Symmetric gauge functions and unitarily invariant norms. The Quarterly Journal of
Mathematics, 11:50?59, 1960.
[17] Michael B. Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina Persu. Dimensionality
reduction for k-means clustering and low rank approximation. In Proceedings of the Forty-Seventh Annual
ACM on Symposium on Theory of Computing, pages 163?172, 2015.
[18] Michael B. Cohen, Jelani Nelson, and David P. Woodruff. Optimal approximate matrix product in terms
of stable rank. In 43rd International Colloquium on Automata, Languages, and Programming, pages
11:1?11:14, 2016.
[19] Daniel M. Kane and Jelani Nelson. Sparser johnson-lindenstrauss transforms. Journal of the ACM,
61(1):4:1?4:23, 2014.
[20] Rong Wang, Feiping Nie, Xiaojun Yang, Feifei Gao, and Minli Yao. Robust 2DPCA with non-greedy
l1-norm maximization for image analysis. IEEE Trans. Cybernetics, 45(5):1108?1112, 2015.
[21] Weiwei Liu, Ivor W. Tsang, and Klaus-Robert M?ller. An easy-to-hard learning paradigm for multiple
classes and multiple labels. Journal of Machine Learning Research, 18(94):1?38, 2017.
[22] Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear embedding.
SCIENCE, 290:2323?2326, 2000.

9

"
2010,LSTD with Random Projections,,3994-lstd-with-random-projections.pdf,"We consider the problem of reinforcement learning in high-dimensional spaces when the number of features is bigger than the number of samples. In particular, we study the least-squares temporal difference (LSTD) learning algorithm when a space of low dimension is generated with a random projection from a high-dimensional space. We provide a thorough theoretical analysis of the LSTD with random projections and derive performance bounds for the resulting algorithm. We also show how the error of LSTD with random projections is propagated through the iterations of a policy iteration algorithm and provide a performance bound for the resulting least-squares policy iteration (LSPI) algorithm.","LSTD with Random Projections

Mohammad Ghavamzadeh, Alessandro Lazaric, Odalric-Ambrym Maillard, R?emi Munos
INRIA Lille - Nord Europe, Team SequeL, France

Abstract
We consider the problem of reinforcement learning in high-dimensional spaces
when the number of features is bigger than the number of samples. In particular,
we study the least-squares temporal difference (LSTD) learning algorithm when
a space of low dimension is generated with a random projection from a highdimensional space. We provide a thorough theoretical analysis of the LSTD with
random projections and derive performance bounds for the resulting algorithm.
We also show how the error of LSTD with random projections is propagated
through the iterations of a policy iteration algorithm and provide a performance
bound for the resulting least-squares policy iteration (LSPI) algorithm.

1

Introduction

Least-squares temporal difference (LSTD) learning [3, 2] is a widely used reinforcement learning
(RL) algorithm for learning the value function V ? of a given policy ?. LSTD has been successfully
applied to a number of problems especially after the development of the least-squares policy iteration
(LSPI) algorithm [9], which extends LSTD to control problems by using it in the policy evaluation
step of policy iteration. More precisely, LSTD computes the fixed point of the operator ?T ? , where
T ? is the Bellman operator of policy ? and ? is the projection operator onto a linear function
space. The choice of the linear function space has a major impact on the accuracy of the value
function estimated by LSTD, and thus, on the quality of the policy learned by LSPI. The problem
of finding the right space, or in other words the problems of feature selection and discovery, is an
important challenge in many areas of machine learning including RL, or more specifically, linear
value function approximation in RL.
To address this issue in RL, many researchers have focused on feature extraction and learning.
Mahadevan [13] proposed a constructive method for generating features based on the eigenfunctions
of the Laplace-Beltrami operator of the graph built from observed system trajectories. Menache et
al. [16] presented a method that starts with a set of features and then tunes both features and the
weights using either gradient descent or the cross-entropy method. Keller et al. [7] proposed an
algorithm in which the state space is repeatedly projected onto a lower dimensional space based on
the Bellman error and then states are aggregated in this space to define new features. Finally, Parr et
al. [17] presented a method that iteratively adds features to a linear approximation architecture such
that each new feature is derived from the Bellman error of the existing set of features.
A more recent approach to feature selection and discovery in value function approximation in RL is
to solve RL in high-dimensional feature spaces. The basic idea here is to use a large number of features and then exploit the regularities in the problem to solve it efficiently in this high-dimensional
space. Theoretically speaking, increasing the size of the function space can reduce the approximation error (the distance between the target function and the space) at the cost of a growth in the
estimation error. In practice, in the typical high-dimensional learning scenario when the number of
features is larger than the number of samples, this often leads to the overfitting problem and poor
prediction performance. To overcome this problem, several approaches have been proposed including regularization. Both `1 and `2 regularizations have been studied in value function approximation
in RL. Farahmand et al. presented several `2 -regularized RL algorithms by adding `2 -regularization
to LSTD and modified Bellman residual minimization [4] as well as fitted value iteration [5], and
proved finite-sample performance bounds for their algorithms. There have also been algorithmic
work on adding `1 -penalties to the TD [12], LSTD [8], and linear programming [18] algorithms.
1

In this paper, we follow a different approach based on random projections [21]. In particular, we
study the performance of LSTD with random projections (LSTD-RP). Given a high-dimensional
linear space F, LSTD-RP learns the value function of a given policy from a small (relative to the
dimension of F) number of samples in a space G of lower dimension obtained by linear random
projection of the features of F. We prove that solving the problem in the low dimensional random
space instead of the original high-dimensional space reduces the estimation error at the price of a
?controlled? increase in the approximation error of the original space F. We present the LSTDRP algorithm and discuss its computational complexity in Section 3. In Section 4, we provide the
finite-sample analysis of the algorithm. Finally in Section 5, we show how the error of LSTD-RP is
propagated through the iterations of LSPI.

2

Preliminaries

For a measurable space with domain X , we let S(X ) and B(X ; L) denote the set of probability
measures over X and the space of measurable functions with domain X and bounded in absolute
value by 0 < L < ?, respectively. For a measure R? ? S(X ) and a measurable function f :
X ? R, we define the `2 (?)-norm of f as ||f ||2? = f (x)2 ?(dx), the supremum norm of f as
||f ||? = sup
and for a set of n states X1 , . . . , Xn ? X the empirical norm
f as
Pnx?X |f (x)|,
Pof
n
1
2
2
n
2
||f ||n = n t=1 f (Xt ) . Moreover, for a vector u ? R we write its `2 -norm as ||u||2 = i=1 u2i .
We consider the standard RL framework [20] in which a learning agent interacts with a stochastic
environment and this interaction is modeled as a discrete-time discounted Markov decision process
(MDP). A discount MDP is a tuple M = hX , A, r, P, ?i where the state space X is a bounded closed
subset of a Euclidean space, A is a finite (|A| < ?) action space, the reward function r : X ?A ? R
is uniformly bounded by Rmax , the transition kernel P is such that for all x ? X and a ? A,
P (?|x, a) is a distribution over X , and ? ? (0, 1) is a discount factor. A deterministic policy ? :
X ? A is a mapping from states to actions. Under a policy ?, the
 MDP M is reduced to a Markov
chain M? = hX , R? , P ? , ?i with reward R? (x) = r x, ?(x) , transition kernel P ? (?|x) = P ?
|x, ?(x) , and stationary distribution ?? (if it admits one). The value function of a policy ?, V ? , is
max
) ? B(X ; Vmax ) defined
the unique fixed-point of the Bellman operator T ? : B(X ; Vmax = R1??
R
?
?
?
by (T V )(x) = R (x) + ? X P (dy|x)V (y). We also define the optimal value function V ? as
the unique fixed-point of the
operator T ? : B(X ; Vmax ) ? B(X ; Vmax ) defined
 optimal Bellman
R
?
by (T V )(x) = maxa?A r(x, a) + ? X P (dy|x, a)V (y) . Finally, we denote
 by T the truncation
operator at threshold Vmax , i.e., if |f (x)| > Vmax then T (f )(x) = sgn f (x) Vmax .
To approximate a value function V ? B(X ; Vmax ), we first define a linear function space F spanned
by the basis functions ?j ? B(X ; L), j = 1, . . . , D, i.e., F = {f? | f? (?) = ?(?)> ?, ? ? RD },
>
where ?(?) = ?1 (?), . . . , ?D (?) is the feature vector. We define the orthogonal projection of
V onto the space F w.r.t. norm ? as ?F V = arg minf ?F ||V ? f ||? . From F we can generate a d-dimensional (d < D) random space G = {g? | g? (?) = ? (?)> ?, ? ? Rd }, where
>
the feature vector ? (?) = ?1 (?), . . . , ?d (?)
is defined as ? (?) = A?(?) with A ? Rd?D
be a random matrix whose elements are drawn i.i.d. from a suitable distribution, e.g., Gaussian
N (0, 1/d). Similar to the space F, we define the orthogonal projection of V onto the space G
w.r.t. norm ? as ?G V = arg ming?G ||V ? g||? . Finally, for any function f? ? F, we define
m(f? ) = ||?||2 supx?X ||?(x)||2 .

3

LSTD with Random Projections

The objective of LSTD with random projections (LSTD-RP) is to learn the value function of a
given policy from a small (relative to the dimension of the original space) number of samples in a
low-dimensional linear space defined by a random projection of the high-dimensional space. We
show that solving the problem in the low dimensional space instead of the original high-dimensional
space reduces the estimation error at the price of a ?controlled? increase in the approximation error.
In this section, we introduce the notations and the resulting algorithm, and discuss its computational
complexity. In Section 4, we provide the finite-sample analysis of the algorithm.
We use the linear spaces F and G with dimensions D and d (d < D) as defined in Section 2. Since in
the following the policy is fixed, we drop the dependency of R? , P ? , V ? , and T ? on ? and simply
use R, P , V , and T . Let {Xt }nt=1 be a sample path (or trajectory) of size n generated by the Markov
2

chain M? , and let v ? Rn and r ? Rn , defined as vt = V (Xt ) and rt = R(Xt ), be the value and
reward vectors of this trajectory. Also, let ? = [? (X1 )> ; . . . ; ? (Xn )> ] be the feature matrix
defined at these n states and Gn = {?? | ? ? Rd } ? Rn be the corresponding vector space. We
b G : Rn ? Gn the orthogonal projection onto Gn , defined by ?
b G y = arg minz?G ||y ?
denote by ?
n
Pn
z||n , where ||y||2n = n1 t=1 yt2 . Similarly, we can define the orthogonal projection onto Fn =
b F y = arg minz?F ||y ? z||n , where ? = [?(X1 )> ; . . . ; ?(Xn )> ] is the
{?? | ? ? RD } as ?
n
b G y and
feature matrix defined at {Xt }nt=1 . Note that for any y ? Rn , the orthogonal projections ?
b
?F y exist and are unique.
We consider the pathwise-LSTD algorithm introduced in [11]. Pathwise-LSTD takes a single trajectory {Xt }nt=1 of size n generated by the Markov chain as input and returns the fixed point of the
b G Tb , where Tb is the pathwise Bellman operator defined as Tb y = r + ? Pby. The
empirical operator ?
n
operator Pb : R ? Rn is defined as (Pby)t = yt+1 for 1 ? t < n and (Pby)n = 0. As shown
b G , it
in [11], Tb is a ?-contraction in `2 -norm, thus together with the non-expansive property of ?
n
b
guarantees the existence and uniqueness of the pathwise-LSTD fixed point v? ? R , v? = ?G Tb v?.
?
Note that the uniqueness of v? does not imply the uniqueness of the parameter ?? such that v? = ??.

n
LSTD-RP D, d, {Xt }n
Cost
t=1 , {R(Xt )}t=1 , ?, ?
Compute
? the reward vector rn?1 ; rt = R(Xt )
O(n)
? the high-dimensional feature matrix ?n?D = [?(X1 )> ; . . . ; ?(Xn )> ]
O(nD)
? the projection matrix Ad?D whose elements are i.i.d. samples from N (0, 1/d)
O(dD)
? the low-dim feature matrix ?n?d = [? (X1 )> ; . . . ; ? (Xn )> ] ; ? (?) = A?(?)
O(ndD)
? the matrix Pb? = ?0n?d = [? (X2 )> ; . . . ; ? (Xn )> ; 0> ]
O(nd)
?bd?1 = ?> r
? A?d?d = ?> (? ? ??0 )
,
O(nd + nd2 ) + O(nd)
? O(d2 + d3 )
return either ?? = A??1?b or ?? = A?+?b (A?+ is the Moore-Penrose pseudo-inverse of A)

Figure 1: The pseudo-code of the LSTD with random projections (LSTD-RP) algorithm.
Figure 1 contains the pseudo-code and the computational cost of the LSTD-RP algorithm. The total
computational cost of LSTD-RP is O(d3 + ndD), while the computational cost of LSTD in the
high-dimensional space F is O(D3 +?nD2 ). As we will see, the analysis of Section 4 suggests
that the value of d should be set to O( n). In this case the numerical complexity of LSTD-RP is
O(n3/2 D), which is better than O(D3 ), the cost of LSTD in F when n < D (the case considered
in this paper). Note that the cost of making a prediction is D in LSTD in F and dD in LSTD-RP.

4

Finite-Sample Analysis of LSTD with Random Projections

In this section, we report the main theoretical results of the paper. In particular, we derive a performance bound for LSTD-RP in the Markov design setting, i.e., when the LSTD-RP solution is
compared to the true value function only at the states belonging to the trajectory used by the algorithm (see Section 4 in [11] for a more detailed discussion). We then derive a condition on the
number of samples to guarantee the uniqueness of the LSTD-RP solution. Finally, from the Markov
design bound we obtain generalization bounds when the Markov chain has a stationary distribution.
4.1

Markov Design Bound

Theorem 1. Let F and G be linear spaces with dimensions D and d (d < D) as defined in Section 2.
Let {Xt }nt=1 be a sample path generated by the Markov chain M? , and v, v? ? Rn be the vectors
whose components are the value function and the LSTD-RP solution at {Xt }nt=1 . Then for any
? > 0, whenever d ? 15 log(8n/?), with probability 1 ? ? (the randomness is w.r.t. both the random
sample path and the random projection), v? satisfies
""
#
r
r
8 log(8n/?)
1
?Vmax L
d
b
b
||v??
v ||n ? p
||v ? ?F v||n +
m(?F v) +
2
d
1
?
?
?
n
1??

3

r

!
8 log(4d/?)
1
+
,
n
n
(1)

where the random variable ?n is the smallest strictly positive eigenvalue of the sample-based Gram
b F v) = m(f? ) with f? be any function in F such that f? (Xt ) =
matrix n1 ?> ?. Note that m(?
b F v)t for 1 ? t ? n.
(?
Before stating the proof of Theorem 1, we need to prove the following lemma.
Lemma 1. Let F and G be linear spaces with dimensions D and d (d < D) as defined in Section 2.
Let {Xi }ni=1 be n states and f? ? F. Then for any ? > 0, whenever d ? 15 log(4n/?), with
probability 1 ? ? (the randomness is w.r.t. the random projection), we have
8 log(4n/?)
m(f? )2 .
(2)
d
Proof. The proof relies on the application of a variant of Johnson-Lindenstrauss (JL) lemma which
states that the inner-products are approximately preserved by the application of the random matrix
A (see e.g., Proposition 1 in [14]). For any ? > 0, we set 2 = d8 log(4n/?). Thus for d ?
15 log(4n/?), we have  ? 3/4 and as a result 2 /4 ? 3 /6 ? 2 /8 and d ? log(4n/?)
2 /4?3 /6 . Thus, from
Proposition 1 in [14], for all 1 ? i ? n, we have |?(Xi ) ? ? ? A?(Xi ) ? A?| ? ||?||2 ||?(Xi )||2 ?
 m(f? ) with high probability. From this result, we deduce that with probability 1 ? ?
inf ||f? ? g||2n ?

g?G

inf ||f? ? g||2n ? ||f? ? gA? ||2n =

g?G

n
8 log(4n/?)
1X
|?(Xi ) ? ? ? A?(Xi ) ? A?|2 ?
m(f? )2 .
n i=1
d

Proof of Theorem 1. For any fixed space G, the performance of the LSTD-RP solution can be
bounded according to Theorem 1 in [10] as
1
b G v||n + ?Vmax L
||v ? v?||n ? p
||v ? ?
1??
1 ? ?2

r

d 
?n

r

8 log(2d/? 0 )
1
+
,
n
n

(3)

with probability 1 ? ? 0 (w.r.t. the random sample path). From the triangle inequality, we have
b G v||n ? ||v ? ?
b F v||n + ||?
bFv ? ?
b G v||n = ||v ? ?
b F v||n + ||?
bFv ? ?
b G (?
b F v)||n .
||v ? ?

(4)

The equality in Eq. 4 comes from the fact that for any vector g ? G, we can write ||v ? g||2n =
b F v||2 +||?
b F v?g||2 . Since ||v? ?
b F v||n is independent of g, we have arg inf g?G ||v?g||2 =
||v? ?
n
n
n
bGv = ?
b G (?
b F v). From Lemma 1, if d ? 15 log(4n/? 00 ), with
b F v ? g||2n , and thus, ?
arg inf g?G ||?
probability 1 ? ? 00 (w.r.t. the choice of A), we have
r
bFv ? ?
b G (?
b F v)||n ?
||?

8 log(4n/? 00 )
b F v).
m(?
d

(5)

We conclude from a union bound argument that Eqs. 3 and 5 hold simultaneously with probability
at least 1 ? ? 0 ? ? 00 . The claim follows by combining Eqs. 3?5, and setting ? 0 = ? 00 = ?/2.
Remark 1. Using Theorem 1, we can compare the performance of LSTD-RP with the performance
of LSTD directly applied in the high-dimensional space F. Let v? be the LSTD solution in F, then
up to constants, logarithmic, and dominated factors, with high probability, v? satisfies
p
1
b F v||n + 1 O( D/n).
||v ? v?||n ? p
||v ? ?
2
1
?
?
1??

(6)

p
By comparing Eqs. 1 and 6, we notice that 1) the estimation error
p of v? is of order O( d/n), and
thus, is smaller than the estimation error of v?, which is of order O( D/n), and 2) the approximation
b F v||n , plus an additional term that depends on
error of v? is the approximation error of v?, ||v ? ?
p
b F v) and decreases with d, the dimensionality of G, with the rate O( 1/d). Hence, LSTD-RP
m(?
may have a better performance than solving LSTD in F whenever this additional term is smaller than
b F v) highly depends on the value function
the gain achieved in the estimation error. Note that m(?
V that is being approximated and the features of the space F. It is important to carefully tune the
value of d as both the estimation error and the additional approximation error in Eq. 1 depend on
d. For instance, while a small value of d significantly reduces the estimation error (and the need for
samples), it may amplify the additional approximation error term, and thus, reduce the advantage of
LSTD-RP over LSTD. We may get an idea on how to select the value of d by optimizing the bound
4

b F v)
m(?
d=
?Vmax L

s

n?n (1 ? ?)
.
1+?

(7)

?
Therefore, when n samples are available the optimal value for d is of the order O( n). Using the
value of d in Eq. 7, we can rewrite the bound of Eq. 1 as (up to the dominated term 1/n)
||v ? v?||n ? p

p
1
b F v||n + 1
8 log(8n/?)
||v ? ?
1??
1 ? ?2

q
b F v)
?Vmax L m(?

1 ? ? 1/4
. (8)
n?n (1 + ?)

Using Eqs. 6 and 8, it would be easier to compare the performance of LSTD-RP and LSTD in space
b F v). For further discussion on m(?
b F v) refer to [14] and
F, and observe the role of the term m(?
for the case of D = ? to Section 4.3 of this paper.
Remark 2. As discussed in the introduction, when the dimensionality D of F is much bigger than
the number of samples n, the learning algorithms are likely to overfit the data. In this case, it is
reasonable to assume that the target vector v itself belongs to the vector space Fn . We state this
condition using the following assumption:
Assumption 1. (Overfitting). For any set of n points {Xi }ni=1 , there exists a function f ? F such
that f (Xi ) = V (Xi ), 1 ? i ? n .
Assumption 1 is equivalent to require that the rank of the empirical Gram matrices n1 ?> ? to be
bigger than n. Note that Assumption 1 is likely to hold whenever D  n, because in this case we
can expect that the features to be independent enough on {Xi }ni=1 so that the rank of n1 ?> ? to be
bigger than n (e.g., if the features are linearly independent on the samples, it is sufficient to have
D ? n). Under Assumption 1 we can remove the empirical approximation error term in Theorem 1
and deduce the following result.
Corollary 1. Under Assumption 1 and the conditions of Theorem 1, with probability 1 ? ? (w.r.t. the
random sample path and the random space), v? satisfies
1
||v ? v?||n ? p
1 ? ?2

4.2

r

8 log(8n/?)
b F v) + ?Vmax L
m(?
d
1??

r

d 
?n

r

8 log(4d/?)
1
+
.
n
n

Uniqueness of the LSTD-RP Solution

While the results in the previous section hold for any Markov chain, in this section we assume that
the Markov chain M? admits a stationary distribution ? and is exponentially fast ?-mixing with
? b, ?, i.e., its ?-mixing coefficients satisfy ?i ? ?? exp(?bi? ) (see e.g., Sections 8.2
parameters ?,
and 8.3 in [10] for a more detailed definition of ?-mixing processes). As shown in [11, 10], if
? exists, it would be possible to derive a condition for the existence and uniqueness of the LSTD
solution depending on the number of samples and the smallest eigenvalue ofR the Gram matrix defined
according to the stationary distribution ?, i.e., G ? RD?D , Gij = ?i (x)?j (x)?(dx). We
now discuss the existence and uniqueness of the LSTD-RP solution. Note that as D increases, the
smallest eigenvalue of G is likely to become smaller and smaller. In fact, the more the features in F,
the higher the chance for some of them to be correlated under ?, thus leading to an ill-conditioned
matrix G. On the other hand, since d < D, the probability that d independent random combinations
of ?i lead to highly correlated features ?j is relatively small. RIn the following we prove that the
smallest eigenvalue of the Gram matrix H ? Rd?d , Hij = ?i (x)?j (x)?(dx) in the random
space G is indeed bigger than the smallest eigenvalue of G with high probability.
Lemma 2. Let ? > 0 andp
F and G be linear spaces with dimensions D and d (d < D) as defined in
Section 2 with D > d + 2 2d log(2/?) + 2 log(2/?). Let the elements of the projection matrix A be
Gaussian random variables drawn from N (0, 1/d). Let the Markov chain M? admit a stationary
distribution ?. Let G and H be the Gram matrices according to ? for the spaces F and G, and ?
and ? be their smallest eigenvalues. We have with probability 1 ? ? (w.r.t. the random space)
??

D
?
d

r
1?

d
?
D

r

2 log(2/?)
D

!2
.

(9)

Proof. Let ? ? Rd be the eigenvector associated to the smallest eigenvalue ? of H, from the
definition of the features ? of G (H = AGA> ) and linear algebra, we obtain
5

?||?||22 = ? > ?? = ? > H? = ? > AGA> ? ? ?||A> ?||22 = ? ? > AA> ? ? ? ? ||?||22 ,

(10)

?
where ? is the smallest eigenvalue of the random matrix AA> , or in?other words, ? is the smallest
?
singular value of the D ? d random matrix A> , i.e., smin (A> ) = ?. We now define B = dA.
Note that if the elements of A are drawn from the Gaussian distribution N (0, 1/d), the elements
of B are standard Gaussian random variables, and thus, the smallest eigenvalue of AA> , ?, can be
written as ? = s2min (B > )/d. There has been extensive work on extreme singular values of random
matrices (see e.g., [19]). For a D ? d random matrix with independent standard normal random
variables, such as B > , we have with probability 1 ? ? (see [19] for more details)
smin (B > ) ?

?

p
?
D ? d ? 2 log(2/?) .

(11)

From Eq. 11 and the relation between ? and smin (B > ), we obtain
D
??
d

r
1?

d
?
D

r

2 log(2/?)
D

!2
,

(12)

with probability 1 ? ?. The claim follows by replacing the bound for ? from Eq. 12 in Eq. 10.
The result of Lemma 2 is for Gaussian random matrices. However, it would be possible to extend
this result using non-asymptotic bounds for the extreme singular values of more general random
matrices [19]. Note that in Eq. 9, D/d is always greater than 1 and the term in the parenthesis
approaches 1 for large values of D. Thus, we can conclude that with high probability the smallest
eigenvalue ? of the Gram matrix H of the randomly generated low-dimensional space G is bigger
than the smallest eigenvalue ? of the Gram matrix G of the high-dimensional space F.
Lemma 3. Let ? > 0 andp
F and G be linear spaces with dimensions D and d (d < D) as defined in
Section 2 with D > d + 2 2d log(2/?) + 2 log(2/?). Let the elements of the projection matrix A be
Gaussian random variables drawn from N (0, 1/d). Let the Markov chain M? admit a stationary
distribution ?. Let G be the Gram matrix according to ? for space F and ? be its smallest eigenvalue.
Let {Xt }nt=1 be a trajectory of length n generated by a stationary ?-mixing process with stationary
distribution ?. If the number of samples n satisfies
288L2 d ?(n, d, ?/2)
max
n>
?D



?(n, d, ?/2)
,1
b

1/?

r
1?

d
?
D

r

2 log(2/?)
D

!?2
,

(13)


? , then with probability
where ?(n, d, ?) = 2(d + 1) log n + log ?e + log+ max{18(6e)2(d+1) , ?}
1 ? ?, the features ?1 , . . . , ?d are linearly independent on the states {Xt }nt=1 , i.e., ||g? ||n = 0
implies ? = 0, and the smallest eigenvalue ?n of the sample-based Gram matrix n1 ?> ? satisifies
?
?
?n ? ? =

v
?
?
s
(
)1/?
u
r
? r
2
u 2?(n, d, ? )
2
log(
)
?(n, d, 2? )
? D?
d
t
? ?
2
1?
? 6L
?
max
,1
>0.
2
d
D
D
n
b
(14)

Proof. The proof follows similar steps as in Lemma 4 in [10]. A sketch of the proof is available
in [6].
By comparing Eq. 13 with Eq. 13 in [10], we can see that the number of samples needed for the
empirical Gram matrix n1 ?> ? in G to be invertible with high probability is less than that for its
counterpart n1 ?> ? in the high-dimensional space F.
4.3

Generalization Bound

In this section, we show how Theorem 1 can be generalized to the entire state space X when the
Markov chain M? has a stationary distribution ?. We consider the case in which the samples
{Xt }nt=1 are obtained by following a single trajectory in the stationary regime of M? , i.e., when X1
is drawn from ?. As discussed in Remark 2 of Section 4.1, it is reasonable to assume that the highdimensional space F contains functions that are able to perfectly fit the value function V in any finite
number n (n < D) of states {Xt }nt=1 , thus we state the following theorem under Assumption 1.
6

Theorem 2. Let ? > 0 and F and G be linear spaces with dimensions D and d (d < D) as defined
in Section 2 with d ? 15 log(8n/?). Let {Xt }nt=1 be a path generated by a stationary ?-mixing
process with stationary distribution ?. Let V? be the LSTD-RP solution in the random space G. Then
under Assumption 1, with probability 1 ? ? (w.r.t. the random sample path and the random space),
2
||V ? T (V? )||? ? p
1 ? ?2

r

8 log(24n/?)
2?Vmax L
m(?F V ) +
d
1??

r r
8 log(12d/?) 1 
d
+
+  , (15)
?
n
n

where ? is a lower bound on the eigenvalues of the Gram matrix n1 ?> ? defined by Eq. 14 and
s
 = 24Vmax

2?(n, d, ?/3)
max
n



?(n, d, ?/3)
,1
b

1/?
.

with ?(n, d, ?) defined as in Lemma 3. Note that T in Eq. 15 is the truncation operator defined in
Section 2.
Proof. The proof is a consequence of applying concentration of measures inequalities for ?-mixing
processes and linear spaces (see Corollary 18 in [10]) on the term ||V ? T (V? )||n , using the fact that
||V ? T (V? )||n ? ||V ? V? ||n , and using the bound of Corollary 1. The bound of Corollary 1 and
the lower bound on ?, each one holding with probability 1 ? ? 0 , thus, the statement of the theorem
(Eq. 15) holds with probability 1 ? ? by setting ? = 3? 0 .
Remark 1. An interesting property of the bound in Theorem 2 is that the approximation error of
V in space F, ||V ? ?F V ||? , does not appear and the error of the LSTD solution in the randomly
projected space only depends on the dimensionality d of G and the number of samples n. However
this property is valid only when Assumption 1 holds, i.e., at most for n ? D. An interesting case
here is when the dimension of F is infinite (D = ?), so that the bound is valid for any number
of samples n. In [15], two approximation spaces F of infinite dimension were constructed based
on a multi-resolution set of features that are rescaled and translated versions of a given mother
function. In the case that the mother function is a wavelet, the resulting features, called scrambled
wavelets, are linear combinations of wavelets at all scales weighted by Gaussian coefficients. As a
results, the corresponding approximation space is a Sobolev space H s (X ) with smoothness of order
s > p/2, where p is the dimension of the state space X . In this case, for a function f? ? H s (X ),
it is proved that the `2 -norm of the parameter ? is equal to the norm of the function in H s (X ), i.e.,
||?||2 = ||f? ||H s (X ) . We do not describe those results further and refer the interested readers to [15].
What is important about the results of [15] is that it shows that it is possible to consider infinite
dimensional function spaces for which supx ||?(x)||2 is finite and ||?||2 is expressed in terms of the
norm of f? in F. In such cases, m(?F V ) is finite and the bound of Theorem 2, which does not
contain any approximation error of V in F, holds for any n. Nonetheless, further investigation is
needed to better understand the role of ||f? ||H s (X ) in the final bound.
Remark 2. As discussed in the introduction, regularization methods have been studied in solving
high-dimensional RL problems. Therefore, it is interesting to compare our results for LSTD-RP with
those reported in [4] for `2 -regularized LSTD. Under Assumption 1, when D = ?, by selecting the
features as described in the previous remark and optimizing the value of d as in Eq. 7, we obtain
||V ? T (V? )||? ? O


q
||f? ||H s (X ) n?1/4 .

(16)

Although the setting considered in [4] is different than ours (e.g., the samples are i.i.d.), a qualitative comparison of Eq. 16 with the bound in Theorem 2 of [4] shows a striking similarity in the
performance of the two algorithms. In fact, they both contain the Sobolev norm of the target function and have a similar dependency on the number of samples with a convergence rate of O(n?1/4 )
(when the smoothness of the Sobolev space in [4] is chosen to be half of the dimensionality of X ).
This similarity asks for further investigation on the difference between `2 -regularized methods and
random projections in terms of prediction performance and computational complexity.

5

LSPI with Random Projections

In this section, we move from policy evaluation to policy iteration and provide a performance bound
for LSPI with random projections (LSPI-RP), i.e., a policy iteration algorithm that uses LSTD-RP
at each iteration. LSPI-RP starts with an arbitrary initial value function V?1 ? B(X ; Vmax ) and
its corresponding greedy policy ?0 . At the first iteration, it approximates V ?0 using LSTD-RP and
7

returns a function V?0 , whose truncated version V?0 = T (V?0 ) is used to build the policy for the second
iteration. More precisely, ?1 is a greedy policy w.r.t. V?0 . So, at each iteration k, a function V?k?1 is
computed as an approximation to V ?k?1 , and then truncated, V?k?1 , and used to build the policy ?k .1
Note that in general, the measure ? ? S(X ) used to evaluate the final performance of the LSPIRP algorithm might be different from the distribution used to generate samples at each iteration.
Moreover, the LSTD-RP performance bounds require the samples to be collected by following the
policy under evaluation. Thus, we need Assumptions 1-3 in [10] in order to 1) define a lowerbounding distribution ? with constant C < ?, 2) guarantee that with high probability a unique
LSTD-RP solution exists at each iteration, and 3) define the slowest ?-mixing process among all the
mixing processes M?k with 0 ? k < K.
Theorem 3. Let ? > 0 and F and G be linear spaces with dimensions D and d (d < D) as defined
in Section 2 with d ? 15 log(8Kn/?). At each iteration k, we generate a path of size n from the
stationary ?-mixing process with stationary distribution ?k?1 = ??k?1 . Let n satisfy the condition in
Eq. 13 for the slower ?-mixing process. Let V?1 be an arbitrary initial value function, V?0 , . . . , V?K?1
(V?0 , . . . , V?K?1 ) be the sequence of value functions (truncated value functions) generated by LSPIRP, and ?K be the greedy policy w.r.t. V?K?1 . Then, under Assumption 1 and Assumptions 1-3
in [10], with probability 1 ? ? (w.r.t. the random samples and the random spaces), we have
?

||V ? V

?K

s r
""
p
8 log(24Kn/?)
2Vmax
C
(1 + ?) CC?,? p
sup ||?(x)||2
d
x?X
1 ? ? 2 ??
s
#
)
r
K?1
2?Vmax L
d  8 log(12Kd/?)
1
+
+
+ E + ? 2 Rmax ,
1??
??
n
n

4?
||? ?
(1 ? ?)2

(

(17)

where C?,? is the concentrability term from Definition 2 in [1], ?? is the smallest eigenvalue of the
Gram matrix of space F w.r.t. ?, ?? is ? from Eq. 14 in which ? is replaced by ?? , and E is  from
Theorem 2 written for the slowest ?-mixing process.
Proof. The proof follows similar lines as in the proof of Thm. 8 in [10] and is available in [6].
Remark. The most critical issue about Theorem 3 is the validity of Assumptions 1-3 in [10]. It
is important to note that Assumption 1 is needed to bound the performance of LSPI independent
from the use of random projections (see [10]). On the other hand, Assumption 2 is explicitly related
to random projections and allows us to bound the term m(?F V ). In order for this assumption to
hold, the features {?j }D
j=1 of the high-dimensional space F should be carefully chosen so as to be
linearly independent w.r.t. ?.

6

Conclusions

Learning in high-dimensional linear spaces is particularly appealing in RL because it allows to have
a very accurate approximation of value functions. Nonetheless, the larger the space, the higher
the need of samples and the risk of overfitting. In this paper, we introduced an algorithm, called
LSTD-RP, in which LSTD is run in a low-dimensional space obtained by a random projection of
the original high-dimensional space. We theoretically analyzed the performance of LSTD-RP and
showed that it solves the problem of overfitting (i.e., the estimation error depends on the value of
the low dimension) at the cost of a slight worsening in the approximation accuracy compared to the
high-dimensional space. We also analyzed the performance of LSPI-RP, a policy iteration algorithm
that uses LSTD-RP for policy evaluation. The analysis reported in the paper opens a number of interesting research directions such as: 1) comparison of LSTD-RP to `2 and `1 regularized approaches,
and 2) a thorough analysis of the case when D = ? and the role of ||f? ||H s (X ) in the bound.
Acknowledgments This work was supported by French National Research Agency through the
projects EXPLO-RA n? ANR-08-COSI-004 and LAMPADA n? ANR-09-EMER-007, by Ministry
of Higher Education and Research, Nord-Pas de Calais Regional Council and FEDER through the
?contrat de projets e? tat region 2007?2013?, and by PASCAL2 European Network of Excellence.
1
Note that the MDP model is needed to generate a greedy policy ?k . In order to avoid the need for the
model, we can simply move to LSTD-Q with random projections. Although the analysis of LSTD-RP can be
extended to action-value functions and LSTD-RP-Q, for simplicity we use value functions in the following.

8

References
[1] A. Antos, Cs. Szepesvari, and R. Munos. Learning near-optimal policies with Bellman-residual
minimization based fitted policy iteration and a single sample path. Machine Learning Journal,
71:89?129, 2008.
[2] J. Boyan. Least-squares temporal difference learning. Proceedings of the 16th International
Conference on Machine Learning, pages 49?56, 1999.
[3] S. Bradtke and A. Barto. Linear least-squares algorithms for temporal difference learning.
Machine Learning, 22:33?57, 1996.
[4] A. M. Farahmand, M. Ghavamzadeh, Cs. Szepesv?ari, and S. Mannor. Regularized policy
iteration. In Proceedings of Advances in Neural Information Processing Systems 21, pages
441?448. MIT Press, 2008.
[5] A. M. Farahmand, M. Ghavamzadeh, Cs. Szepesv?ari, and S. Mannor. Regularized fitted Qiteration for planning in continuous-space Markovian decision problems. In Proceedings of
the American Control Conference, pages 725?730, 2009.
[6] M. Ghavamzadeh, A. Lazaric, O. Maillard, and R. Munos. LSPI with random projections.
Technical Report inria-00530762, INRIA, 2010.
[7] P. Keller, S. Mannor, and D. Precup. Automatic basis function construction for approximate
dynamic programming and reinforcement learning. In Proceedings of the Twenty-Third International Conference on Machine Learning, pages 449?456, 2006.
[8] Z. Kolter and A. Ng. Regularization and feature selection in least-squares temporal difference
learning. In Proceedings of the Twenty-Sixth International Conference on Machine Learning,
pages 521?528, 2009.
[9] M. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning
Research, 4:1107?1149, 2003.
[10] A. Lazaric, M. Ghavamzadeh, and R. Munos. Finite-sample analysis of least-squares policy
iteration. Technical Report inria-00528596, INRIA, 2010.
[11] A. Lazaric, M. Ghavamzadeh, and R. Munos. Finite-sample analysis of LSTD. In Proceedings
of the Twenty-Seventh International Conference on Machine Learning, pages 615?622, 2010.
[12] M. Loth, M. Davy, and P. Preux. Sparse temporal difference learning using lasso. In IEEE
Symposium on Approximate Dynamic Programming and Reinforcement Learning, pages 352?
359, 2007.
[13] S. Mahadevan. Representation policy iteration. In Proceedings of the Twenty-First Conference
on Uncertainty in Artificial Intelligence, pages 372?379, 2005.
[14] O. Maillard and R. Munos. Compressed least-squares regression. In Proceedings of Advances
in Neural Information Processing Systems 22, pages 1213?1221, 2009.
[15] O. Maillard and R. Munos. Brownian motions and scrambled wavelets for least-squares regression. Technical Report inria-00483014, INRIA, 2010.
[16] I. Menache, S. Mannor, and N. Shimkin. Basis function adaptation in temporal difference
reinforcement learning. Annals of Operations Research, 134:215?238, 2005.
[17] R. Parr, C. Painter-Wakefield, L. Li, and M. Littman. Analyzing feature generation for valuefunction approximation. In Proceedings of the Twenty-Fourth International Conference on
Machine Learning, pages 737?744, 2007.
[18] M. Petrik, G. Taylor, R. Parr, and S. Zilberstein. Feature selection using regularization in
approximate linear programs for Markov decision processes. In Proceedings of the TwentySeventh International Conference on Machine Learning, pages 871?878, 2010.
[19] M. Rudelson and R. Vershynin. Non-asymptotic theory of random matrices: extreme singular
values. In Proceedings of the International Congress of Mathematicians, 2010.
[20] R. Sutton and A. Barto. Reinforcement Learning: An Introduction. MIP Press, 1998.
[21] S. Vempala. The Random Projection Method. American Mathematical Society, 2004.

9

"
2010,Active Learning Applied to Patient-Adaptive Heartbeat Classification,,4091-active-learning-applied-to-patient-adaptive-heartbeat-classification.pdf,"While clinicians can accurately identify different types of heartbeats in electrocardiograms (ECGs) from different patients, researchers have had limited success in applying supervised machine learning to the same task. The problem is made challenging by the variety of tasks, inter- and intra-patient differences, an often severe class imbalance, and the high cost of getting cardiologists to label data for individual patients. We address these difficulties using active learning to perform patient-adaptive and task-adaptive heartbeat classification. When tested on a benchmark database of cardiologist annotated ECG recordings, our method had considerably better performance than other recently proposed methods on the two primary classification tasks recommended by the Association for the Advancement of Medical Instrumentation. Additionally, our method required over 90% less patient-specific training data than the methods to which we compared it.","Active Learning Applied to Patient-Adaptive
Heartbeat Classification
John V. Guttag
CSAIL, MIT
guttag@csail.mit.edu

Jenna Wiens
CSAIL, MIT
jwiens@csail.mit.edu

Abstract
While clinicians can accurately identify different types of heartbeats in electrocardiograms (ECGs) from different patients, researchers have had limited success
in applying supervised machine learning to the same task. The problem is made
challenging by the variety of tasks, inter- and intra-patient differences, an often
severe class imbalance, and the high cost of getting cardiologists to label data
for individual patients. We address these difficulties using active learning to perform patient-adaptive and task-adaptive heartbeat classification. When tested on
a benchmark database of cardiologist annotated ECG recordings, our method had
considerably better performance than other recently proposed methods on the two
primary classification tasks recommended by the Association for the Advancement of Medical Instrumentation. Additionally, our method required over 90%
less patient-specific training data than the methods to which we compared it.

1

Introduction

In 24 hours an electrocardiogram (ECG) can record over 100,000 heartbeats for a single patient.
Of course, a physician is not likely to look at all of them. Automated analysis of long-term ECG
recordings can help physicians understand a patient?s physiological state and his/her risk for adverse
cardiovascular outcomes [1] [2]. Often, an important step in such analysis is labeling the different
types of heartbeats. This labeling reduces an ECG to a set of symbols transferable across patients.
Trained clinicians can successfully identify over a dozen different types of heartbeats in ECG recordings. However, researchers have had limited success using supervised machine learning techniques
to do the same. The problem is made challenging by the inter-patient differences present in the morphology and timing characteristics of the ECGs produced by compromised cardiovascular systems.
The variation in the physiological systems that produce the data means that a classifier trained on
even a large set of patients will yield unpredictable results when applied to a new cardiac patient.
For this reason, global classifiers are highly unreliable and therefore not widely used in practice [3].
Hu et al was one of the first to describe an automatic patient-adaptive ECG beat classifier [4]. It
distinguished ventricular ectopic beats (VEBs), from non-VEBs. This work employed a mixture of
experts approach, combining a global classifier with a local classifier trained on the first 5 minutes of
the test patient?s record. Similarly, de Chazal et al augmented the performance of a global heartbeat
classifier by including patient-specific expert knowledge for each test patient. Their local classifier
was trained on the first 500 labeled beats of each record [3]. More recently, Ince et al developed
a patient-adaptive classification scheme using artificial neural networks by incorporating the first 5
minutes of each test recording in the training set [5] .
Based on the results from these three studies, it is clear that patient-adaptive classifiers provide
increased classification accuracy. Unfortunately, patient-adaptive classifiers are not used in practice
because they require an unrealistic amount of labor to produce a cardiologist-labeled patient-specific
training set. Furthermore, by sampling all of the patient-specific training data from one portion of
1

the ECG, one is at risk for over-fitting to that patient?s physiological state in time. Given a longterm record, which is likely to contain high intra-patient differences, it is likely that constructing the
training set in this manner will not yield a good representation of the patient?s ECG.
There has been some success with hand-coded rule-based algorithms for heartbeat classification.
Hamilton et al developed a rule-based algorithm for detecting one type of particularly dangerous
ectopic heartbeat, the premature ventricular contraction (PVC) [6]. While reasonably accurate, rulebased algorithms are inflexible, since they can only be used for a single classification task. And to be
useful in practice, a classifier should not only be capable of adapting to new patients, but also to new
classification problems, since the classification task in question can change depending on the patient
or even the clinician. Since the field of ECG research is continuously evolving, tools to analyze the
signal should be capable of adapting.
In this paper, we show how active learning can be successfully applied to the problems of both
patient-adaptive and task-adaptive heartbeat classification. We developed our method with a clinical setting in mind: initially it requires no labeled data, it has no user-specified parameters, and
achieves good performance on an imbalanced data set. Applied to data from the MIT-BIH Arrhythmia Database our method outperforms current state-of-the-art machine learning heartbeat classification techniques and uses less training data. Moreover, our approach outperforms a rule-based
algorithm designed to detect an important class of abnormal beat. Finally, we discuss how the classification method performed when used in a prospective experiment with two cardiologists.

2

Background

We begin with a brief background on the signal of interest, the ECG. Since we will consider different heartbeat classification tasks we first present a few examples of heartbeat classes and ECG
abnormalities.
2.1

The ECG and ECG Abnormalities

An ECG records a patient?s cardiac electrical activity by measuring the potential differences at the
surface of the patient?s body. In most healthy patients, the ECG, measured from Lead II, begins with
a P-wave, is followed by a QRS complex and ends with a T-wave. Figure 1(a) shows an example
of the ECG of a normal sinus rhythm beat (N). The exact morphology and timing of the different
portions of the wave depend on the patient and lead placement.
1.8
RR interval

Amplitude (mv)

Amplitude(mV)

1.2
1
0.8
0.6

QT interval

T
P

0.2
0
?0.2
0

Q
0.2

pre?RR
interval

2.5

1.4

0.4

1.4

3
R

post?RR
interval

1.2

2

1

1.5

0.8

Amplitude (mv)

1.6

1
0.5
0

0.6
0.4
0.2

?0.5

0

?1

?0.2

S
0.4

0.6

0.8
Time (s)

(a)

1

1.2

1.4

1.6

?1.5
0

0.5

1

1.5
Time(s)

(b)

2

2.5

3

?0.4
0

0.5

1

1.5
Time (s)

2

2.5

3

(c)

Figure 1: Normal sinus rhythm beats like the ones shown in (a) originate from the pacemaker cells
of the sinoatrial node. Premature ventricular contractions (b) and atrial premature beats (c) are two
examples of ectopic beats.
Cardiac abnormalities can disrupt the heart?s normal sinus rhythm, and, depending on their type
and frequency, can vary from benign to life threatening. Examples of ectopic beats (beats that
do not originate in the sinoatrial node) are shown in Figures 1(b) and 1(c). Premature ventricular
contractions (PVCs), originate in the ventricles instead of in the pacemaker cells of the sinoatrial
node. They are common in patients who have suffered an acute myocardial infarction [7] and may
indicate that a patient is at increased risk for more serious ventricular arrhythmias and sudden cardiac
death [8]. When the electrical impulse originates from the atria, an atrial premature beat is recorded
by the ECG as shown in Figure 1(c). Atrial premature beats tend not to be life threatening.
2

Because of their specific timing and morphology characteristics these two types of abnormal beats
are generally distinguishable by trained cardiologists, but there are many exceptions. Not only can
abnormalities vary from patient to patient, but the same recording may contain beats that belong
to the same class but all look quite different. Figure 2 shows an example of an ECG containing
multiform PVCs.

Figure 2: Each PVC is marked by a ?V? and each normal sinus rhythm beat is marked by a ???. The
PVC morphology varies greatly among patients and even within recordings from a single patient.

3

Methods

In this section we describe the two main components of our heartbeat classification scheme. We
begin, with the process of feature extraction and then present the classification method.
3.1

Feature Extraction

Before extracting feature vectors, we pre-process and segment the ECG. We used PhysioNet?s automated R-peak detector to detect the R-peaks of each heartbeat [9]. Next, we removed baseline
wander from the signals using the method described in [10]. Once pre-processed, the data was segmented into individual heartbeats based on fixed intervals before and after the R-peak, so that each
beat contained the same number of samples.
Our goal was to develop a feature vector that worked well not only across patients but also across different heartbeat classification tasks. This led us to use a combination of the ECG features proposed
in [10],[11], and [12]. The elements of the feature vector, x, are described in Table 1.
Table 1: Heartbeat features used in experiments.
Features
x1 , ..., x60
x61 , x62 , x63
x64 , x65 , x66
x67

Description
? Wavelet coefficients from the last 5 levels of a 6 level wavelet decomposition using
a Daubechies 2 wavelet
? The normalized energy in different segments of the beat
? The pre and post RR intervals normalized by a local average, and the average RR interval
? Morphological distance between the current beat the record?s median beat

The last, and most novel, feature in Table 1 is a measure of the morphological distance between
the represented beat and the median beat for a patient (recalculated every 500 beats). The feature is
based on the dynamic time warping algorithm used in [12] to measure the morphological distance
between a fixed interval that contains a portion of the Q-T intervals of two beats.
3.2

Classification

Our goal was to develop a clinically useful patient-adaptive heartbeat classification method for solving different binary heartbeat classification problems. We designed the classifier for use in a clinical
setting, where physicians have little time to label beats, let alone tune classifier parameters. Thus,
it was important that the method should require few cardiologist-labeled heartbeats, and have no
user-defined parameters. Based on these goals we developed the algorithm presented below, which
combines different ideas from the literature [13-16].
3

Inputs:
(a) Unlabeled data {x1 , ..., xn }
(b) Max number of initial clusters per clustering, k
(c) SVM cost parameter C
(d) Stopping precision 
1. Cluster the data using hierarchical clustering with two different linkage criteria, yielding <= 2 ? k clusters.
2. Query the centroid of each cluster. Add these points to the initially empty set of labeled examples.
3. If the expert labeled all the points as belonging to the same class, stop, else k = 1.
4. Train a linear SVM based on the labeled examples.
5. Apply the SVM to all of the data.
6. If all data that lies on or within the margin is labeled, stop.
7. Re-cluster data that lie on or within the margin using hierarchical clustering with k = k + 1.
8. Query the point from each cluster that lies closest to the current SVM decision boundary.
9. Repeat steps 4-8 until the change in the margin is within  of zero.

Many proposed techniques for SVM active learning assume one starts with some set of labeled data
or, as in [13], the initial training examples are randomly selected. In our application, we start with a
pool of completely unlabeled data. Furthermore, since there is often a severe class imbalance (e.g.,
some multi-thousand beat recordings contain less than a handful of PVCs), choosing a small or even
moderate number of random samples is unlikely to be an effective approach to finding representative
samples of a record. The choice of initial queries is crucial. If beats from only one class are queried
the algorithm could stop prematurely. More generally, the selection of the first set of queries is
independent of the binary task, and therefore the first query should contain at least one example
from each of the beat classes contained in the record. We use clustering in an effort to quickly
identify representative samples from each class.
We experimented with different clustering techniques before choosing hierarchical clustering. On
average hierarchical clustering outperformed other popular clustering techniques like k-means. We
believe this can be attributed to the fact that hierarchical clustering has the ability to produce a
variety of different clusters by modifying the linkage criterion. We chose to use two complementary
linkage criteria in attempt to address the intra-patient variation present in ECG records. The first
metric is average linkage. Average linkage defines the distance between two clusters, q and r, as the
average distance between all pairs of objects in q and r. This linkage is biased toward producing
clusters with similar variances, and has the tendency to merge clusters with small variances. The
second linkage criterion is Ward?s linkage [17], defined in Equation 1.
d(q, r) = ss(qr) ? [ss(q) + ss(r)]

(1)

where ss(qr) is the within-cluster sum of squares for the resulting cluster when q and r are combined. The within-cluster sum of squares, ss(x), is defined as the sum of squares of the distances
between all objects in the cluster and the centroid of the cluster:
nx
1 X
ss(x) =
|xi ?
xj |2
n
x j=1
i=1
nx
X

(2)

Using Ward?s linkage tends to join clusters with a small number of points, and is biased towards
producing clusters with approximately the same number of samples. If presented with an outlier,
Ward?s method tends to assign it to the cluster with the closest centroid, whereas the average linkage
tends to assign it to the densest cluster, where it will have the smallest impact on the maximum
variance [18].
Once the initial queries are labeled, we train a linear SVM, and apply this SVM to all of the data.
We use linear SVMs because most heartbeat classification tasks are close to linearly separable and
because linear SVMs require few tuning parameters. Next, we re-cluster the data on or within the
margin of the SVM, incrementing the max number of clusters with each iteration. We then query a
beat from each cluster that is closest to the SVM decision boundary.
As described above, our algorithm would halt when no unlabeled data lay on or within the margin.
For some records, however, e.g., those with fusion beats - a fusion of normal and abnormal beats
4

- many beats can lie within the margin of the SVM and thus a clinician might end up labeling
hundreds of beats that add little useful information. Intuitively, one should stop querying when
additional training data has little to no effect on the solution. The algorithm, therefore, terminates
when the change in the margin between iterations is within .

4

Experiments & Results

We implemented our algorithm in MATLAB, and used SV Mlight [19] to train the linear SVM at
each iteration. We held the cost parameter of the linear SVM constant, at C = 100, throughout all
experiments. This value was selected based on previous cross-validation experiments. The stopping
precision  was held constant at  = 10?3 . Typical ECG recordings contain beats from 2 to 5 classes
but can contain more; based on this a priori knowledge, we conservatively set k = 10. This value
was held constant throughout all experiments.
To test the utility of our proposed approach for heartbeat classification we ran a series of experiments on data from different patients, and for different classification tasks. First, we compare the
performance of a classifier obtained using our approach to two classifiers recently presented in the
literature. Next, we directly measure the impact active learning has on the classification of heartbeats by creating our own passive learning classifier using the same pre-processing and features as
our proposed active learning method. Finally, we test our method using actual cardiologists.
In our experiments we report the classification performance in terms of sensitivity (SE), specificity
(SP), and positive predictive value (PPV). As an overall measure of performance we use the F-score:
F =

2 ? SE ? P P V
SE + P P V

(3)

The F-score is a commonly-accepted performance evaluation measure in medicine and information
retrieval where one data class (often the positive class) is more important than the other [20]. We
use this measure since the problem of heartbeat classification suffers from severe class imbalance,
and thus the SE (aka recall) and the PPV (aka precision) are more important than SP.
4.1

Classification Performance

We tested performance on the MIT-BIH Arrhythmia Database (MITDB) [9], a widely used benchmark database that contains 48 half-hour ECG recordings, sampled at 360Hz, from 47 different
patients. Twenty-three of these records, labeled 100 to 124 were selected at random from a source
of 4000 recordings. The remaining 25 records, labeled 200 to 234 were selected because they contain rare clinical activity that might not have been represented had all 48 records been chosen at
random. The database contains approximately 109,000 cardiologist labeled heartbeats. Each beat
is labeled as belonging to one of 16 different classes. In some sense, the data in the MITDB is
too good. It was collected at 360Hz, which is a higher sampling rate than is typical for the Holter
monitors used to gather most long term clinical data. To simulate this kind of data, we resampled
the pre-processed ECG signal at 128Hz.
We consider the two main classification tasks proposed by the Association for the Advancement of
Medical Instrumentation (AAMI): detecting ventricular ectopic beats (VEBs), and detecting supraventricular ectopic beats (SVEBs). These two tasks have been the focus of other researchers investigating patient-adaptive heartbeat classification. Recently, Ince et al [5] and de Chazal et al
[3] described methods that combine global information with patient-specific information. Ince et al
trained a global classifier on 245 hand chosen beats from the MITDB, and then adapted the global
classifier by training on labeled data from the first five minutes of each test record. Their reported
results of testing on 44 of the 48 records - all records with paced beats were excluded - from the
MITDB are reported in Table 2. De Chazal et al trained their global classifier on all of the data from
22 patients in the MITDB, and then adapted the global classifier by training on labeled data for the
first 500 beats of each test record. Their reported results of testing on 22 records -different from the
ones used in the global training set- from the MITDB are also reported in Table 2.
For the same two classification tasks we tested our proposed approach and we report the results
when tested on the records reported on in [5] and [3]. In these experiments we exclude the queried
5

beats from the test set, testing only on data the expert hasn?t seen. This was also done in [5] and [3].
Since we query far fewer beats that the other methods, we end up testing on many more beats.
Table 2: Our proposed method outperforms other classifiers for two common classification tasks.
VEB

SVEB

SP

PPV

F-Score

Sens

Spec

PPV

F-Score

Ince et al
84.6% 98.7%
Proposed1
99.0% 99.9%
Chazal et al 94.3% 99.7%
99.6% 99.9%
Proposed2
1
for the 44 records in common
2
for the 22 records in common

87.4%
99.2%
96.2%
99.3%

86.0%
99.1%
95.2%
99.5%

63.5%
88.3%
87.7%
92.0%

99.0%
100.0%
96.2%
100.0%

53.7%
99.2%
47.0%
99.5%

58.2%
93.4%
61.2%
95.6%

Classifier

SE

As Table 2 shows, the method proposed here does considerably better than the methods proposed in
[5] and [3] for each task. For the task of classifying VEBs vs. non-VEBs, our method on average
used 45 labeled beats (compared to roughly 350 beats for [5] and 500 beats for [3]) per record. For
the task of detecting SVEBs, our method used even fewer labeled beats. Recognizing SVEBs is
considerably more difficult than detecting VEBs since the class imbalance problem is even more
severe and supra-ventricular beats are harder to distinguish from normal sinus rhythm beats.
Table 3: Our algorithm outperforms a rule-based classifier designed specifically for the task of
detecting PVCs.
SE

SP

PPV

F-Score

Hamilton et al 92.8%
99.0%
Proposed3
3
for all 48 records

98.4%
100.0%

79.5%
99.3%

85.7%
99.1%

Classifier

Hamilton et al proposed a rule-based classifier for classifying PVCs vs. non-PVCs. Their software
is freely available online, from eplimited.com. We applied their software to all of the records, see
Table 3. Their method does particularly poorly on the four records containing paced beats. Omitting
these four records the F-Score increases to 91.4%, still worse than our method. One advantage of
the rule-based algorithm is that it does not require a labeled training set, whereas on average we
require 45 labeled beats per record. However, unlike our method the rule-based algorithm can only
be used for one task.
4.2

The Impact of Active Learning

We hypothesize that the difference in performance between our method and the other learning-based
methods discussed above is attributable partly to the design of our feature vector and partly to the
method of choosing training data. In order to test this hypothesis we ran an experiment that directly
compares the effect of actively vs. passively selecting the training set, with all other parameters kept
the same (e.g., identical pre-processing, identical feature vectors, etc.).
For each of the 48 records in the MITDB we compare a VEB vs. non-VEB classifier using our
approach, to a linear SVM classifier trained on the first 500 beats of each record. For each patient
we record the number of queries made, as well as the performance of each classifier. Table 4 shows
the classification results for each method across all patients. The column headed ?#Q? gives the
number of beats used for training each classifier, while the column headed ?TP? for true positives,
gives the number of correctly labeled VEBs. The last row gives the totals across all records for each
classification method.
Overall, our classification approach achieves an F-score over 99%, and the passive technique
achieves an F-score of 94%. Compared to the passive approach, active learning used over 90%
less training data, and resulted in over 85% fewer misclassified heartbeats. These results emphasize that fact that active learning can be used to dramatically reduce the labor cost of producing
highly accurate classifiers. That the passive technique performed better than [5] and almost as well
as [3], despite not having any global training data, suggests that our feature vector provides some
advantage.
6

Table 4: Active versus passive learning. Active learning outperforms a passive approach, and uses
over 90% less data.
Active vs. Passive VEB Classification Results
Proposed

4.3

Passive

#Q

TP

TN

FP

FN

#Q

TP

TN

FP

FN

100
101
102
103
104
105
106
107
108
109
111
112
113
114
115
116
117
118
119
121
122
123
124
200
201
202
203
205
207
208
209
210
212
213
214
215
217
219
220
221
222
223
228
230
231
232
233
234

22
19
28
20
30
54
50
31
52
45
22
20
19
51
20
34
20
30
32
24
20
26
32
124
45
41
103
36
109
90
29
90
20
137
53
52
61
41
20
33
20
86
66
30
24
20
91
26

1
0
4
0
2
41
520
59
17
38
1
0
0
43
0
109
0
16
444
1
0
3
41
825
198
19
410
70
203
986
1
190
0
215
256
164
162
63
0
396
0
473
362
1
2
0
830
3

2258
1851
2162
2073
2214
2501
1497
2070
1717
2463
2107
2529
1783
1807
1942
2283
1523
2242
1523
1849
2464
1500
1558
1717
1737
2088
2456
2574
2016
1916
2993
2392
2736
2985
1988
3168
2009
2065
2035
2022
2472
2094
1662
2242
1552
1771
2216
2731

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
15
0
4
0
0
1
0
20
0
0
0
0
0
0
0
7
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
6
1
0
0
34
1
7
6
0
5
0
5
0
0
0
1
0
0
0
0
0
0
0
0
0
0

500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500

0
0
3
0
1
41
507
11
17
36
0
0
0
42
0
106
0
4
444
0
0
3
30
799
0
19
397
65
190
977
0
180
0
157
254
164
159
52
0
393
0
321
356
0
2
0
810
0

2269
1861
2181
2082
2224
2521
1506
2076
1740
2492
2122
2537
1793
1833
1951
2301
1533
2260
1542
1860
2473
1513
1570
1773
1764
2114
2453
2583
2060
1953
3002
2434
2746
3016
2002
3196
2045
2089
2045
2030
2480
2119
1690
2253
1567
1779
2245
2749

0
0
0
0
0
1
0
0
4
0
0
0
0
2
0
0
0
0
0
0
0
0
0
0
0
1
79
0
59
5
0
16
0
12
1
1
0
0
0
0
0
11
0
0
0
0
1
0

1
0
1
0
1
0
13
48
0
2
1
0
0
1
0
3
0
12
0
1
0
0
17
27
198
0
47
6
20
15
1
15
0
63
2
0
3
12
0
3
0
152
6
1
0
0
20
3

Totals

2148

7169

102573

47

66

24000

6540

102427

193

695

Experiments with Clinicians

To get a sense of the feasibility of using our approach in an actual clinical setting, we ran an experiment with two cardiologists and data from another cohort of patients admitted with NSTEACS.
The ECG tracings in this database, unlike those in the MITDB, are not particularly clean, i.e., they
contain a considerable amount of noise and many artifacts. This makes them more representative of
the data with which an algorithm in clinical use is likely to have to deal. We considered 4 randomly
chosen records, from a subset of patients who had experienced at least one episode of ventricular
tachycardia in the 7 day period following randomization. For each record, we consider the first
half-hour, giving us a test set of 8230 heartbeats.
In these experiments we used a slightly different stopping criterion developed earlier. As our algorithm chose beats to be labeled, each cardiologist was presented with an ECG plot of the heartbeat
to be labeled and the beats surrounding it, like the one shown in Figure 3. The cardiologist was
then asked to label it according to the following key: 1=clearly non-PVC , 2 = ambiguous non-PVC,
3=ambiguous PVC, 4=clearly PVC. Because the cardiologists made different choices about how
some beats should be labeled, one was asked to label an average of 15 beats/record and the other
roughly 20 beats/record. The whole process took each cardiologist about 90 seconds per record.
Since the records had not been previously labeled (and it seemed unreasonable to ask our experts
to label all of them), we used the PVC classification software from [6] to provide a label to which
7

1.5

1

mV

0.5

0

?0.5

?1
0

0.4 0.8 1.2 1.6

2

2.4 2.8 3.2 3.6

4

4.4

Time (s)

Figure 3: The classifiers trained using active learning both labeled the delineated beat delineated as a PVC,
whereas the rule-based algorithm labeled it as a non-PVC.

Table 5: Comparison of active earning using two different experts and Hamilton et al. Results are the sum
across four records.

All Records (8230 beats total)
Classifier
Size Training Data
TP
TN
Expert #1
60
191 8038
Expert #2
83
192 8035
0
190 8035
Hamilton et al

FP
0
3
3

FN
1
0
2

we could compare the labels generated by our method. This gave us three independently generated
labels for each beat. When all three classifiers agreed, we assumed that the beat was correctly
classified. Out of a possible 8230 disagreements there were only 6. We asked a third expert to
adjudicate all 6 disagreements, and used this as the gold standard to calculate the results for the
three classifiers shown in Table 5.

5

Summary & Conclusion

The goal of this work was to produce a clinically useful technique for automatically classifying
activity in ECG recordings. The problem is made challenging by the intra- and inter-patient differences present in the morphology and timing characteristics of the ECG produced by compromised
cardiovascular systems and by the variability in the classification tasks that a clinician might want to
perform. We propose to address these difficulties with a method for using active learning to perform
patient-adaptive and task-adaptive heartbeat classification.
When tested on the most widely used benchmark database of cardiologist annotated ECG recordings, our method had better performance than other recently proposed methods on the two primary
classification tasks recommended by AAMI. Additionally, our method required over 90% less training data than the methods to which it was compared. We also showed that our method compares
favorably to a state-of-the-art hand coded algorithm for a third common classification task.
To test out the practical applicability of our method, we conducted a small study with two cardiologists. Both cardiologists were able to use our tool with minimal training, and achieved excellent
classification results with a small amount of labor per record.
These preliminary results are highly encouraging, and suggest that active learning can be used practically in a clinical setting to not only reduce the labor cost but also garner additional improvements
in performance. Of course, there is still room for improvement. In all experiments we used identical
input parameters; further tuning of these parameters may improve results. However, in a clinical setting parameter tuning is impractical, and thus more work to investigate automated parameter tuning
is needed. Based on preliminary experiments we believe that by first learning the optimal number
of initial clusters for each record one can improve performance while decreasing the total number
of required labels. It may also be possible to further reduce the amount of required expert labor by
starting with a global classifier and then adapting it using active learning.
Acknowledgments
We would like to thank Benjamin Scirica, Collin Stultz, and Zeeshan Syed for sharing their expert
knowledge in cardiology and for their participation in our experiments. This work was supported in
part by the NSERC and by Quanta Computer Inc.
8

References
[1] D. V. Exner, K. M. Kavanagh, M. P. Slawnych et al, and for the REFINE Investigators. Noninvasive risk
assessment early after a myocardial infarction: The REFINE study. J Am Coll Cardiol, 50(24):2275?
2284, 2007.
[2] Z. Syed, B. Scirica, S. Mohanavel, P. Sung, C. Cannon, P. Stone, C. Stultz, and J. V. Guttag. Relation to
death within 90 days of non-st-elevation acute coronary syndromes to variability in electrocardiographic
morphology. Am J of Cardiol, 103(3), 2009.
[3] P. de Chazal and R. B. Reilly. A Patient-Adapting Heartbeat Classifier Using ECG Morphology and
Heartbeat Interval Features. Biomedical Engineering, IEEE Transactions on, 53(12):2535?2543, Dec.
2006.
[4] Y. H. Hu, S. Palreddy, and W.J. Tompkins. A Patient-Adaptable ECG Beat Classifier Using a Mixture of
Experts Approach. Biomedical Engineering, IEEE Transactions on, 44(9):891?900, Sept. 1997.
[5] T. Ince, S. Kiranyaz, and M. Gabbouj. A generic and robust system for automated patient-specific classification of ecg signals. IEEE Transactions on Biomedical Engineering, 56(5), May 2009.
[6] P. Hamilton. Open Source ECG Analysis. In Computers in Cardiology, volume 29, pages 101?104, 2002.
[7] J. Bigger, F. Dresdale, and R. Heissenbuttel et. al. Ventricular arrhythmias in ischemic heart disease:
mechanism, prevalence, significance, and management. Prog Cardiovasc Dis, 19:255, 1977.
[8] T. Smilde, D. van Veldhuisen, and M. van den Berg. Prognostic value of heart rate variability and ventricular arrhythmias during 13-year follow up in patients with mild to moderate heart failure. Clinical
Research in Cardiology, 98(4):233?239, 2009.
[9] A. L. Goldberger, L. A. N. Amaral, and L. Glass et al. PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation, 101(23):e215?e220,
2000 (June 13). Circulation Electronic Pages: http://circ.ahajournals.org/cgi/content/full/101/23/e215.
[10] P. de Chazal, M. O?Dwyer, R. B. Reilly, and Senior Member. Automatic Classification of Heartbeats
Using ECG Morphology and Heartbeat Interval Features. IEEE Transactions on Biomedical Engineering,
51:1196?1206, 2004.
[11] K. Sternickel. Automatic pattern recognition in ecg time series. In Computer Methods and Programs in
Biomedicine, Vol: 68, pages 109?115, 2002.
[12] Z. Syed, J. Guttag, and C. Stultz. Clustering and Symbolic Analysis of Cardiovascular Signals: Discovery
and Visualization of Medically Relevant Patterns in Long-term Data Using Limited Prior Knowledge.
EURASIP Journal on Advances in Signal Processing, 2007:97?112, 2007.
[13] S. Tong and D. Koller. Support vector machine active learning with applications to text classification.
Journal of Machine Learning Research, 2:45?66, 2002.
[14] S. Dasgupta and D. Hsu. Hierarchical sampling for active learning. In ICML ?08: Proceedings of the 25th
international conference on Machine learning, pages 208?215, New York, NY, USA, 2008. ACM.
[15] Z. Xu, K. Yu, V. Tresp, X. Xu, and J. Wang. Representative sampling for text classification using support
vector machines. In Proceedings of the twenty-fifth European Conference on Information Retrieval, pages
393?407. Springer, 2003.
[16] H.T. Nguyen and A. Smeulders. Active learning using pre-clustering. In Proceedings of the twenty-first
international conference on Machine learning, page 79, New York, NY, USA, 2004. ACM.
[17] J. H. Ward. Hierarchical grouping to optimize an objective function. Journal of the American Statistical
Association, 58(301):234?244, 1963.
[18] S. Kamvar, D. Klein, and C. Manning. Interpreting and extending classical agglomerative clustering
algorithms using a model-based approach. In Proceedings of nineteenth International Conference on
Machine Learning, pages 283?290, 2002.
[19] T. Joachims. Making Large-scale Support Vector Machine Learning Practical. MIT Press, Cambridge,
MA, USA, 1999.
[20] M. Sokolova, N. Japkowicz, and S. Szpakowicz. Beyond Accuracy, F-score and ROC: a Family of Discriminant Measures for Performance Evaluation, volume 4304 of Lecture Notes in Computer Science,
pages 1015?1021. Springer Berlin/Heidelberg, 2006.

9

"
1992,Using Prior Knowledge in a NNPDA to Learn Context-Free Languages,,587-using-prior-knowledge-in-a-nnpda-to-learn-context-free-languages.pdf,Abstract Missing,"Using Prior Knowledge in a NNPDA to Learn
Context-Free Languages

Sreerupa Das
Dept. of Compo Sc. &
Inst. of Cognitive Sc.
University of Colorado
Boulder, CO 80309

c.

Lee Giles?
NEC Research Inst.
4 Independence Way
Princeton, NJ 08540

Guo-Zheng SUD
""'lnst. for Adv. Compo Studies
University of Maryland
College Park, MD 20742

Abstract
Although considerable interest has been shown in language inference and
automata induction using recurrent neural networks, success of these
models has mostly been limited to regular languages. We have previously demonstrated that Neural Network Pushdown Automaton (NNPDA)
model is capable of learning deterministic context-free languages (e.g.,
anb n and parenthesis languages) from examples. However, the learning
task is computationally intensive. In this paper we discus some ways in
which a priori knowledge about the task and data could be used for efficient
learning. We also observe that such knowledge is often an experimental
prerequisite for learning nontrivial languages (eg. anbncbma m ).

1

INTRODUCTION

Language inference and automata induction using recurrent neural networks has
gained considerable interest in the recent years. Nevertheless, success of these models has been mostly limited to regular languages. Additional information in form of
a priori knowledge has proved important and at times necessary for learning complex languages (Abu-Mostafa 1990; AI-Mashouq and Reed, 1991; Omlin and Giles,
1992; Towell, 1990). They have demonstrated that partial information incorporated
in a connectionist model guides the learning process through constraints for efficient
learning and better generalization.
'Ve have previously shown that the NNPDA model can learn Deterministic Context
65

66

Das, Giles, and Sun

Output

State(t+l)

t

Top-or-stack

Action

00 0 0
~
~
='000 00 00

t

State Neurons

it

State(t)

It;;:::::::..

Input Neurons

it

Input(t)

. .11:

push
pop or no-op

::;;;;::..

0

~

\

External
stack

\
.~~

""

""hig.her order
weights

alphabets on the
stack

'::~::::'.

copy

Read Neurons

11'

Top-of-stack(t)

Figure 1: The figure shows the architecture of a third-order NNPDA. Each weight
relates the product of Input(t), State(t) and Top-of-Stack information to the
State(t+1). Depending on the activation of the Action Neuron, stack action
(namely, push, pop or no operation) is taken and the Top-of-Stack (i.e. value
of Read Neurons) is updated.
Free Languages (DCFLs) from a finite set of examples. However, the learning task
requires considerable amount of time and computational resources. In this paper
we discuss methods in which a priori knowledge, may be incorporated in a N eum!
network Pushdown Automaton (NNPDA) described in (Das, Giles and Sun, 1992;
Giles et aI, 1990; Sun et aI, 1990).

2
2.1

THE NEURAL NETWORK PUSHDOWN AUTOMATA
ARCHITECTURE

The description of the network architecture is necessarily brief, for further details
see the references above. The network consists of a set of recurrent units, called
state neurons and an external stack memory. One state neuron is designated as the
output neuron. The state neurons get input (at every time step) from three sources:
from their own recurrent connections, from the input neurons and from the read
neurons. The input neurons register external inputs which consist of strings of
characters presented one at a time. The read neurons keep track of the symbol(s)
on top of the stack. One non-recurrent state neuron, called the action neuron,
indicates the stack action (push, pop or no-op) at any instance. The architecture
is shown in Figure 1.
The stack used in this model is continuous. Unlike an usual discrete stack where an
element is either present or absent, elements in a continuous stack may be present
in varying degrees (values between [0, 1]). A continuous stack is essential in order

Using Prior Knowledge in a NNPDA to Learn Context-Free Languages

to permit the use of a continuous optimization method during learning. The stack
is manipulated by the continuous valued action neuron. A detailed discussion on
the operations may be found in (Das, Giles and Sun, 1992).

2.2

LEARNABLE CLASS OF LANGUAGES

The class of language learnable by the NNPDA is a proper subset of deterministic
context-free languages. A formal description of a Pushdown Automaton (PDA)
requires two distinct sets of symbols - one is the input symbol set and the other
is the stack symbol set!. We have reduced the complexity of this PDA model in
the following ways: First, we use the same set of symbols for the input and the
stack. Second, when a push operation is performed the symbol pushed on the stack
is the one that is available as the current input. Third, no epsilon transitions are
allowed in the NNPDA. Epsilon transition is one that performs state transition and
stack action without reading in a new input symbol. Unlike a deterministic finite
state automata, a deterministic PDA can make epsilon transitions under certain
restrictions!. Although these simplifications reduce the language class learnable by
NNPDA, nevertheless the languages in this class retain essential properties of eFLs
and is therefore more complex than any regular language.

2.3

TRAINING

The activation of the state neurons s at time step t + 1 may be formulated as follows
(we will only consider third order NNPDA in this paper):
(1)

where g(x) = frac1/1 + exp( -x), i is the activation of the input neurons and r is
the activation of the read neuron and W is the weight matrix of the network. We
use a localized representation for the input and the read symbols. During training,
input sequences are presented one at a time and activations are allowed to propagate
until the end of the string is reached. Once the end is reached the activation of the
output neuron is matched with the target (which is 1.0 for positive string and 0.0 for
a negative string) The learning rule used in the NNPDA is a significantly enhanced
extension to Real Time Recurrent Learning (\Villiams and Zipser, 1989).

2.4

OBJECTIVE FUNCTION

The objective function used to train the network consists of two error terms: one for
positive strings and the other for negative strings. For positive strings we require
(a) the NNPDA must reach a final state and (b) the stack must be empty. This
criterion can be reached by minimizing the error function:

(2)
where So(l) is the activation of an output neuron and L(I) is the stack length, after
a string of length I has been presented as input a character at a time. For negative
1

For details refer to (Hopcroft, 1979).

67

68

Das, Giles, and Sun

avg of total
presentations
# of strings
# of character

parenthesis
w IL wjo IL
2671
5644
10628 29552

postfix
w IL wjo IL
15912
8326
31171
82002

anb n
w IL
wjo IL

108200
358750

>200000
>700000

Table 1: Effect of Incremental Learning (IL) is displayed in this table. The number
of strings and characters required for learning the languages are provided here.
parenthesis
epochs
generalization
number of units

w SSP
50-80
100%
1+1

wjo SSP

150
96.02%
1+1

""'''''''

50-80
100%
2

anbncbma m
w SSP wjo SSP

epochs
generalization
number of units

***
***

anbn
w SSP
wjo SSP

150-250
100%
1+1

150-250
98.97%
2

150-250
100%
1+1

***
***
***

an+mbnc m
w SSP
wjo SSP

Table 2: This table provides some statistics on epochs, generalization and number
of hidden units required for learning with and without selective string presentation
(SSP).
strings, the error function is modified as:

E rror -- { 0so(1) - L(l)

if (so(1) - L(l))
else

> 0.0

(3)

Equation (2) reflects the criterion that, for a negative pattern we require either the
final state so(l)
0.0 or the stack length L(1) to be greater than 1.0 (only when
so(l) = 1.0 and the stack length L(l) is close to zero, the error is high).

=

3

BUILDING IN PRIOR KNOWLEDGE

In practical inference tasks it may be possible to obtain prior knowledge about the
problem domain. In such cases it often helps to build in knowledge into the system
under study. There could be at least two different types of knowledge available
to a model (a) knowledge that depends on the training data with absolutely no
knowledge about the automaton, and (b) partial knowledge about the automaton
being inferred. Some of ways in which knowledge can be provided to the model are
discussed below.
3.1
3.1.1

KNOWLEDGE FROM THE DATA
Incremental Learning

Incremental Learning has been suggested by many (Elman, 1991; Giles et aI, 1990,
Sun et aI, 1990), where the training examples are presented in order of increasing

Using Prior Knowledge in a NNPDA to Learn Context-Free Languages
0.9
with SSP without

SSP - - .

0.8

0.7

.,...,.,
...0
........

0.6

0.5

0.4

0.3

0.2

50

150

100

200

250

epochs

Figure 2: Faster convergence using selective string presentation (SSP) for parenthesis language task.
length. This model of learning starts with a training set containing short simple
strings. Longer strings are added to the training set as learning proceeds.
We believe that incremental learning is very useful when (a) the data presented
contains structure, and (b) the strings learned earlier embody simpler versions of
the task being learned. Both these conditions are valid for context-free languages.
Table 1 provides some results obtained when incremental learning was used. The
figures are averages over several pairs of simulations, each of which were initialized
with the same initial random weights.
3.1.2

Selective Input Presentation

Our training data contained both positive and negative examples. One problem
with training on incorrect strings is that, once a symbol in the string is reached
that makes it negative, no further information is gained by processing the rest of
the string. For example, the fifth a in the string aaaaba ... makes the string a
negative example of the language a""b"", irrespective of what follows it. In order to
incorporate this idea we have introduced the concept of a dead state.
During training, we assume that there is a teacher or an oracle who has knowledge
of the grammar and is able to identify the first (leftmost) occurrence of incorrect
sequence of symbols in a negative string. When such a point is reached in the input
string, further processing of the string is stopped and the network is trained so that
one designated state neuron called the dead state neuron is active. To accommodate
the idea of a dead state in the learning rule, the following change is made: if the
network is being trained on negative strings that end in a dead state then the
length L(l) in the error function in equation (1) is ignored and it simply becomes

69

70

Das, Giles, and Sun
0.5
W/o
wlth 1
wlth 2
wlth 3

0 . 45

IW
IW
IW
IW

--_.
' .. ----

0 .4
0.35

...0
......

..

0 .3

0.
~

0 . 25

0.2
0.15
0.1
0.05

a

1000

2000

3000

4000

5000

6000

7000

8000

9000

No . of gtrlngg

Figure 3: Learning curves when none, one or more initial weights (IW) were set for
postfix language learning task

Error = ~(1 - Sdead{l))2. Since such strings have an negative subsequence, they
cannot be a prefix to any positive string. Therefore at this point we do not care
about the length of the stack. For strings that are either positive or negative but
do not go to a dead state (an example would be a prefix of a positive string); the
objective function remains the same as described earlier in Equations 1 and 2.
Such additional information provided during training resulted in efficient learning,
helped in learning of exact pushdown automata and led to better generalization for
the trained network. Information in this form was often a prerequisite for successfully learning certain languages. Figure 2 shows a typical plot of improvement in
learning when such knowledge is used. Table 2 shows improvements in the statistics
for generalization, number of units needed and number of epochs required for learning. The numbers in the tables were averages over several simulations; changing
the initial conditions resulted in values of similar orders of magnitude.

3.2
3.2.1

KNOWLEDGE ABOUT THE TASK
Knowledge About The Target PDA's Dynamics

One way in which knowledge about the target PDA can be built into a system is
by biasing the initial conditions of the network. This may be done by assigning
predetermined initial values to a selected set of weights (or biases). For example a
third order NNPDA has a dynamics that maps well onto the theoretical model of a
PDA. Both allow a three to two mapping of a similar kind. This is because in the
third order NNPDA, the product of the activations of the input neurons, the read
neurons and the state neurons determine the next state and the next action to be

Using Prior Knowledge in a NNPDA to Learn Context-Free Languages

[.5
.5]
Start

9

[.1 .9]

a/-/push
[.0.6]
Start
[.5 .5]

,
b,-*

e/-/*

O

End
[* .9]

Dead
[.9 *]

(a) PDA for parenthesis

End
[* .9]

(b) PDA for a~n

Figure 4: The figure shows some of the PDAs inferred by the NNPDA. In the figure
the nodes in the graph represent states inferred by the NNPDA and the numbers in
""[]"" indicates the state representations. Every transition is indicated by an arrow
and is labeled as ""x/y /z"" where ""x"" corresponds to the current input symbol, ""y""
corresponds to the symbol on top of the stack and ""z"" corresponds to the action
taken.

taken. It may be possible to determine some of the weights in a third order network
if certain information about the automaton in known. Typical improvement in
learning is shown in Figure 3 for a postfix language learning task.

3.2.2

U sing Structured Examples

Structured examples from a grammar are a set of strings where the order of letter
generation is indicated by brackets. An example would be the string (( ab)c) generated by the rules S ---+ Xc; X ---+ abo Under the current dynamics and limitations
of the model, this information could be interpreted as providing the stack actions
(push and pop) to the NNPDA. Learning the palindrome language is a hard task
because it necessitates remembering a precise history over a long period of time.
The NNPDA was able to learn the palindrome language for two symbols when
structured examples were presented.

4

AUTOMATON EXTRACTION FROM NNPDA

Once the network performs well on the training set, the transition rules in the
inferred PDA can then be deduced. Since the languages learned by the NNPDA so
far corresponded to PDAs with few states, the state representations in the induced
PDA could be inferred by looking at the state neuron activations when presented
with all possible character sequences. For larger PDAs clustering techniques could
be used to infer the state representations. Various clustering techniques for similar
tasks have been discussed in (Das and Das, 1992; Giles et al., 1992). Figure 4 shows
some of the PDAs inferred by the NNPDA.

71

72

Das, Giles, and Sun

5

CONCLUSION

This paper has described some of the ways in which prior knowledge could be used
to learn DCFGs in an NNPDA. Such knowledge is valuable to the learning process
in two ways. It may reduce the solution space, and as a consequence may speed
up the learning process. Having the right restrictions on a given representation can
make learning simple: which reconfirms an old truism in Artificial Intelligence.
References
Y.S. Abu-Mostafa. (1990) Learning from hints in neural networks.

Journal of

Complexity, 6:192-198.

K.A. AI-Mashouq and I.S. Reed . (1991) Including hints in training neural networks.
Neural Computation, 3(3):418-427 .
S. Das and R. Das . (1992) Induction of discrete state-machine by stabilizing a continuous recurrent network using clustering. To appear in CSI Journal of Computer
Science and Informatics. Special Issue on Neural Computing.
S. Das, C.L. Giles, and G.Z. Sun. (1992) Learning context free grammars: capabilities and limitations of neural network with an external stack memory. Proc of
the Fourteenth Annual Conf of the Cognitive Science Society, pp. 791-795. Morgan
Kaufmann, San Mateo, Ca.
J .L. Elman. (1991) Incremental learning, or the importance of starting small. CRL
Tech Report 9101, Center for Research in Language, UCSD, La Jolla, CA.
C.L. Giles, G.Z. Sun, H.H. Chen, Y.C. Lee and D. Chen, (1990) Higher Order
Recurrent Networks & Grammatical Inference, Advances in Neural Information
Processing Systems 2, pp. 380-387, ed. D.S. Touretzky, Morgan Kaufmann, San
Mateo, CA.
C.L. Giles, C.B . Miller, H.H. Chen, G.Z. Sun, and Y.C. Lee. (1992) Learning
and extracting finite state automata with second-order recurrent neural networks.
Neural Computation, 4(3):393-405.

J .E. Hopfcroft and J.D . Ullman. (1979) Introduction to Automata Theory, Languages and Computation. Addison-Wesley, Reading, MA.
C.W. Omlin and C.L. Giles. (1992) Training second-order recurrent neural networks
using hints. Proceedings of the Ninth Int Conf on Machine Learning, pp. 363-368.
D. Sleeman and P. Edwards (eds). Morgan Kaufmann, San Mateo, Ca.
G.Z. Sun, H.H. Chen, C.L. Giles, Y.C . Lee and D. Chen. (1991) Neural networks
with external memory stack that learn context-free grammars from examples. Proc
of the Conf on Information Science and Systems, Princeton U., Vol. II, pp. 649-653.
G.G. Towell, J.W. Shavlik and M.O Noordewier. (1990) Refinement of approximately correct domain theories by knowledge-based neural-networks. In Proc of the
Eighth National Conf on Artificial Intelligence, Boston, MA. pp. 861.
R.J. Williams and D. Zipser. (1989) A learning algorithm for continually running
fully recurrent neural networks. Neural Computation 1(2):270-280.

"
2000,Computing with Finite and Infinite Networks,,1934-computing-with-finite-and-infinite-networks.pdf,Abstract Missing,"Computing with Finite and Infinite Networks

Ole Winther*
Theoretical Physics, Lund University
SOlvegatan 14 A, S-223 62 Lund, Sweden
wint h e r@ nimis.thep.lu. s e

Abstract
Using statistical mechanics results, I calculate learning curves (average
generalization error) for Gaussian processes (GPs) and Bayesian neural
networks (NNs) used for regression. Applying the results to learning a
teacher defined by a two-layer network, I can directly compare GP and
Bayesian NN learning. I find that a GP in general requires CJ (d S )-training
examples to learn input features of order s (d is the input dimension),
whereas a NN can learn the task with order the number of adjustable
weights training examples. Since a GP can be considered as an infinite
NN, the results show that even in the Bayesian approach, it is important
to limit the complexity of the learning machine. The theoretical findings
are confirmed in simulations with analytical GP learning and a NN mean
field algorithm.

1 Introduction
Non-parametric kernel methods such as Gaussian Processes (GPs) and Support Vector Machines (SVMs) are closely related to neural networks (NNs). These may be considered as
single layer networks in a possible infinite dimensional feature space. Both the Bayesian
GP approach and SVMs regularize the learning problem so that only a finite number of the
features (dependent on the amount of data) is used.
Neal [1] has shown that Bayesian NNs converge to GPs in the limit of infinite number of
hidden units and furthermore argued that (1) there is no reason to believe that real-world
problem should require only a 'small' number of hidden units and (2) there are in the
Bayesian approach no reasons (besides computational) to limit the size of the network.
Williams [2] has derived kernels allowing for efficient computation with both infinite feedforward and radial basis networks.
In this paper, I show that learning with a finite rather than infinite networks can make a
profound difference by studying the case where the task to be learned is defined by a large
but finite two-layer NN. A theoretical analysis of the Bayesian approach to learning this
task shows that the Bayesian student makes a learning transition from a linear model to
specialized non-linear one when the number of examples is of the order of the number of
adjustable weights in the network. This effect-which is also seen in the simulations-is a
consequence of the finite complexity of the network. In an infinite network, i.e. a GP on the
*http : // www. th e p . lu .se /t f 2/ s t aff /winth e r /

other hand such a transition will not occur. It will eventually learn the task but it requires
CJ( d S )-training examples to learn features of order s, where d is the input dimension.
Here, I focus entirely on regression. However, the basic conclusions regarding learning
with kernel methods and NNs turn out to be valid more generally, e.g. for classification
unpublished results and [3].
I consider the usual Bayesian setup of supervised learning: A training set DN =
{(Xi, y; ) Ii = 1 ... , N} (x E Rd and y E R) is known and the output for the new input x is predicted by the function f(x) which is sampled from the prior distribution of
model outputs. I will consider both a Gaussian process prior and the prior implied by
a large (but finite) two-layer network. The output noise is taken to be Gaussian, so the
Likelihood becomes p(ylf(x)) = e - (Y- J(X))2 /2 /V27r(T2. The error measure is minus the
log-Likelihood and Bayes regressor (which minimizes the expected error) is the posterior
mean prediction

(f(x)) - Ef f(x) 0 ; p(Yi If(Xi))
EfO; p(y;l f(x;))
,

(1)

where I have introduced Ef , f = f(Xl) "" '"" f(XN) , f(x), to denote an average with respect to the model output prior.
Gaussian processes.

In this case, the model output prior is by definition Gaussian
(2)

where C is the covariance matrix. The covariance matrix is computed from the kernel
(covariance function) C(x, x'). Below I give an explicit example corresponding to an
infinite two-layer network.
Bayesian neural networks The output of the two-layer NN is given by f(x, w , W) =
~:: Wk<f>(Wk . x), where an especially convenient choice of transfer function in what

JK

follows is <f>( z ) = I~ dte- t2 /2/ V2ii. I consider a Bayesian framework (with fixed
known hyperparameters) with a weight prior that factorizes over hidden units p(w, W) =
Ok [P(Wk )p(Wk)] and Gaussian input-to-hidden weights Wk ~ N(O, ~).
From Bayesian NNs to GPs. The prior over outputs for the Bayesian neural network is
p(f)
dwdWp(w , W) 0; J(J(x; ) - f(x ;, w , W)). In the infinite hidden unit limit,
J{ -+ 00, when P(Wk) has zero mean and finite, say unit variance, it follows from the
central limit theorem (eLT) that the prior distribution converges to a Gaussian process
f ~ N(O, C) with kernel [1,2]

=I

C(x, x')

J

dw p(w) <f>(w . x) <f>(w . x')

~ arcsin (J(l + xT;:~:'+ XIT~XI))

(3)

The rest of the paper deals with theoretical statistical mechanics analysis and simulations
for GPs and Bayesian NNs learning tasks defined by either a NN or a GP. For the simulations, I use analytical GP learning (scaling like CJ (N 3 )) [4] and a TAP mean field algorithm
for Bayesian NN.

2 Statistical mechanics of learning
The aim of the average case statistical mechanics analysis is to derive learning curves, i.e.
the expected generalization error as a function of the number of training examples. The
generalization error of the Bayes regressor (f (x)) eq. (1) is
fg

= (((y - (f(X)))2)) ,

(4)

where double brackets (( ... )) = I IIi [dx;dYip(Xi, Yi)] .. . denote an average over both
training examples and the test example (x , y). Rather than using eq. (4) directly, fg will-as
usually done-be derived from the average of the free energy -( (In Z )), where the partition
function is given by

Z = Ef

1

N

V27ru 2

(-~
2:)Yi 2u

exp

f(X i ))2) .

(5)

i

I will not give many details of the actual calculations here since it is beyond the scope of
the paper, but only outline some of the basic assumptions.
2.1

Gaussian processes

The calculation for Gaussian processes is given in another NIPS contribution [5]. The basic
assumption made is that Y- f(x) becomes Gaussian with zero mean 1 under an average over
the training example Y - f(x) ~ N(O , (((y - f(x)) 2))). This assumption can be justified
by the CLT when f(x) is a sum of many random parts contributing on the same scale.
Corrections to the Gaussian assumption may also be calculated [5]. The free energy may
be written in term of a set of order parameters which is found by saddlepoint integration.
Assuming that the teacher is noisy y = f. (x) + 1], (( 1]2)) = uZ, the generalization error is
given by the following equation which depends upon an orderparameter v

uZ + ((f;(x))) -

Ov( v2E f((f(x)f.(x)))2)
2
1 + A 0v Ef((J2(X)))/N

N

=

v

(6)
(7)

where the new normalized measure Ef . . . ex Ef exp (-v((J2(x)))/2) ... has been introduced.
Kernels in feature space. By performing a Karhunen-Loeve expansion, f(x) can be
written as a linear perceptron with weights w p in a possible infinite feature space

f(x)

= LWpAcP p(x)

,

(8)

p

where the features cP p(x) are orthonormal eigenvectors of the covariance function with
eigenvalues Ap: I dxp(x) C (x', X)cP p(x) = ApcP p(X') and I dx p(X) cPpl (x)cPp (x) = Jppl.
The teacher f. (x) may also be expanded in terms of the the features:

f.(x) = L apAcP p(x) ,
p

Using the orthonormality the averages may be found : ((J2(x))) = I: p ApW ~ ,
((f(x)f. (x))) = I: p Apwpa p and ((f;(x))) = I: p Apa ~ . For a Gaussian process prior,
lGeneralization to non-zero mean is straightforward.

the prior over the weight is a spherical Gaussian w ~ N(O , I). Averaging over w, the saddlepoint equations can be written in tenns of the number of examples N, the noise levels
0""2 and 0"";, the eigenvectors of the covariance function Ap and the teacher projections ap:
N

--;;
v

2

(

0""*

Apa~

-1

+ ~ (1 + VAp)2

N (0""2+

L
p

Ap

1 + VAp

)

2

(

0""

+~

Ap
)
(1 + VAp)2

(9)

)-1

(10)

These eqs. are valid for a fixed teacher. However, eq. (9) may also be averaged over the
distribution of teachers. In the Bayes optimal scenario, the teacher is sampled from the
same prior as the student and 0""2 = 0"";. Thus ap ~ N(O, I) implying a~ = 1, where the
average over the teacher is denoted by an overline. In this case the equations reduce to the
Bayes optimal result first derived by Sollich [6]: f. g = f.~ayes = N / v.
Learning finite nets. Next, I consider the case where the teacher is the two-layer network
f*(x) = f(w, W) and the GP student uses the infinite net kernel eq. (3). The average
over the teacher corresponds to an average over the weight prior and since f* (x)f* (Xl) =
C(x, Xl), I get

a~Ap =

!

dxdxlp(x)p(xl)C(x, XI)?p(X)?p(XI) = Ap ,

(11)

where the eigenvalue equation and the orthonormality have been used. The theory therefore
predicts that a GP student (with the infinite network kernel) will have the same learning
curve irre.~pectively of the number of hidden units of the NN teacher. This result is a direct
consequence of the Gaussian assumption made for the average over examples. However,
what is more surprising is that it is found to be a very good approximation in simulations
down to K = 1, i.e. a simple perceptron with a sigmoid non-linearity.
Inner product kernels. I specialize to inner product kernels C(x, Xl) = c(x . xl/d)
and consider large input dimensionality d and input components which are iid with
zero mean and unit variance. The eigenvectors are products of the input components
?p(x) = OmEP Xm and are indexed by subsets of input indices, e.g. p = {I, 2, 42} [3].
The eigenvalues are Ap = cl;IIJ~) with degeneracy nlpl = ( I~I ) R:i d lpl / Ipl!, where Ipi is
the cardinality (in the example above Ipl = 3). Plugging these results into eqs. (9) and (10),
it follows that to learn features that are order s in the inputs, O( d S ) examples are needed.
The same behavior has been predicted for learning in SVMs [3].
The infinite net eq. (3) reduces to an inner product covariance function for
controls the degree on non-linearity of the rule) and large d, X . X R:i d:

C (x, X

I)

(
1/)
2
. (TX. Xl )
= ex?
x d =;: arcsm d (1 + T)

.

~

= TI/ d (T
(12)

Figure 1 shows learning curves for GPs for the infinite network kernel. The mismatch
between theory and simulations is expected to be due to 0(1/ d)-corrections to the eigenvalues Ap. The figure clearly shows that learning of the different order features takes place
on different scales. The stars on the f.g-axis show the theoretical prediction of asymptotic
errorfor N = O( d), O( d3 ), ... (the teacher is an odd function).
2.2

Bayesian neural networks

The limit of large but finite NNs allows for efficient computation since the prior over
functions can be approximated by a Gaussian. The hidden-to-output weights are for sim-

Small N

0.15

= O(d)

= O(d 3 )

0.1

0.4

Eg

Eg

0.2

0.0

o

Large N

20

40

N

60

500

80

1000

N

1500

2000

=

Figure 1: Learning curve for Gaussian processes with the infinite network kernel (d 10,
T = 10 and (}2 = 0.01) for two scales of training examples. The full line is the the
theoretical prediction for the Bayes optimal GP scenario. The two other curves (almost on
top of each other as predicted by theory) are simulations for the Bayes optimal scenario
(dotted line) and for GP learning a neural network with J{ = 30 hidden units (dash-dotted
line).
plicity set to one and we introduce the 'fields' hk(x) = Wk . x and write the output as
f(x, w) = f(h(x)) =
~~ <I>(hk(X)), h(x) = h1 (x), ... , hK(x). In the following, I
discuss the TAP mean field algorithm used to find an approximation to the Bayes regressor
and briefly the theoretical statistical mechanics analysis for the NN task.

.Jx

Mean field algorithm. The derivation sketched here is a straightforward generalization
of previous results for neural networks [7]. The basic cavity assumption [7, 8] is that for
large d, J{ and for a suitable input distribution, the predictive distribution p(J (x) IDN) is
Gaussian:

p(J(x)IDN)

N((J(x)), (J2(x)) - (J(x))2) .
The predictive distribution for the fields h( x) is also assumed to be Gaussian
RJ

p(h(x)IDN)

RJ

N((h(x)) , V) ,

where V = (h(x)h(xf) - (h(x))(h(xf). Using these assumptions, I get an approximate Bayes regressor
(13)

To make predictions, we therefore need the two first moments of the weights since
(hk(x)) = (Wk) . x and Vkl = ~mn XmXn((WmkWnl) - (Wmk)(Wnl)). We can simplify
this in the large d limit by taking the inputs to by iid with zero mean and unit variance:
Vkl RJ (Wk' WI) - (Wk) . (WI). This approximation can be avoided at a substantial computational cost [8]. Furthermore, (Wk' WI) turns out equal to the prior covariance <SkIT / d
[7]. The following exact relation is obtained for the mean weights
(14)

where

p(YiI DN\(Xi, Yi)) =

J

dh(Xi) p(Yi Ih(Xi)) p(h(Xi )IDN\(Xi' y;)) .

0 . 05...,--~--~-~-~~---,

0.04
0.03

E

.

\.

.;.~~

..... ::----

9 0.02

~~~~:':~~~'-~-~-~.-~-~~~-~-=
.-=.-=
. -~~~-~-.----~.

0.01

N
2
4
6
8
10 dK
Figure 2: . Learning curves for Bayesian NNs and GPs. The dashed line is simulations
for the TAP mean field algorithm (d = 30, K = 5, T = 1 and 0- 2 = 0.01) learning a
corresponding NN task, i.e. an approximation to the Bayes optimal scenario. The dashdotted line is the simulations for GPs learning the NN task. Virtually on top of that curve
is the curve for Bayes optimal GP scenario (dotted line). The full lines are the theoretical
prediction. Up to N = Nc = 2.51dK, the learning curves for Bayesian NNs and GPs coincide. At N e , the statistical mechanics theory predicts a first order transition to a specialized
solution for the NN Bayes optimal scenario (lower full line).

p(y;lh(x;)) is the Likelihood and p(h(x;)IDN\(X;, y;)) is a predictive distribution for
h(x;) for a training set where the ith example has been left out. In accordance with above,
I assume p(h(x;) IDN\(Xi, y;)) ~ N((h(x;)hi, V). Finally, generalizing the relation
found in Refs. [7,8], I can relate the reduced mean to the full posterior mean:

(hk(x;)h;

= (hk(x;)) -

L VklD:li
I

to express everything in terms of (Wk) and D:k;, k

= 1, ... , K and i = 1, ... , N .

The mean field eqs. are solved by iteration in D:k; and (Wmk) following the recipe given in
Ref. [8]. The algorithm is tested using a teacher sampled from the NN prior, i.e. the Bayes
optimal scenario. Two types of solutions are found: a linear symmetric and a non-linear
specialized. In the symmetric solution, (Wk) = (WI) and (Wk) . (Wk) = O(T/dK). This
means that the machine is linear (when T ? K). For N = O(dK), a transition to a
specialized solution occurs, where each (Wk), k = 1, ... , K, aligns to a distinct weight
vector of the teacher and (Wk) . (Wk) = O(T/d). The Bayesian student thus learns the
linear features for N = 0 (d). However, unlike the GP, it learns all of the remaining nonlinear features for N = O(dK). The resulting empirical learning curve averaged over 25
independent runs is shown in figure 2. It turned out that setting (hk(xdhi = (hk(x;))
was a necessary heuristic in order to find the specialized solution. The transition to the
specialized solution-although very abrupt for the individual run-is smeared out because it
occurs at different N for each run.
The theoreticalleaming curve is also shown in figure 2. It has been derived by generalizing the results of Ref. [9] for the Gibbs algorithm to the Bayes optimal scenario. The
picture that emerges is in accordance with the empirical findings. The transition to the
specialized solution is predicted to be first order, i.e. with a discontinuous jump in the relevant order parameters at the number of examples N c ( 0- 2 , T ), where the specialized solution
becomes the physical solution (i.e. the lowest free energy solution).
The mean field algorithm cannot completely reproduce the theoretical predictions because
the solution gets trapped in the meta-stable symmetric solution. This is often observed

for first order transitions and should also be observable in the Monte Carlo approach to
Bayesian NNs [1].

3 Discussion
Learning a finite two-layer regression NN using (1) the Bayes optimal algorithm and (2)
the Bayes optimal algorithm for an infinite network (implemented by a GP) is compared.
It is found that the Bayes optimal algorithm can have a very superior performance.
This can be explained as an entropic effect: The infinite network will-although the correct finite network solution is included a priori- have a vanishing probability of finding
this solution. The finite network on the other hand is much more constraint wrt the functions it implements. It can thus--even in the Bayesian setting-give a great payoff to limit
complexity.
For d-dimensional inner product kernel with iid input distribution, it is found that it in
general requires 0 (d S ) training examples to learn features of 0 (s). Unpublished results
and [3] show that these conclusions remain true also for SVM and GP classification.
For SVM hand-written digit recognition, fourth order kernels give good results in practise. Since N = 0(10 4 ) - 0(10 5 ), it can be concluded that the 'effective' dimension,
deffective = 0(10) against typically d = 400, i.e. some inputs must be very correlated
and/or carry very little information. It could therefore be interesting to develop methods
to measure the effective dimension and to extract the important lower dimensional features
rather than performing the classification directly from the images.
Acknowledgments
I am thankful to Manfred Opper for valuable discussions and for sharing his results with
me and to Klaus-Robert Muller for discussions at NIPS. This research is supported by the
Swedish Foundation for Strategic Research.

References
[1] R. Neal, Bayesian Learningfor Neural Networks, Lecture Notes in Statistics, Springer (1996).
[2] C. K. I. Williams, Computing with Infinite Networks, in Neural Information Processing Systems
9, Eds. M. C. Mozer, M. I. Jordan and T. Petsche, 295-301, MIT Press (1997).
[3] R. Dietrich, M. Opper and H. Sompolinsky, Statistical Mechanics of Support Vector Machines,
Phys. Rev. Lett. 82, 2975-2978 (1999).
[4] C. K. I. Williams and C. E. Rasmussen, Gaussian Processes for Regression, In Advances in
Neural Information Processing Systems 8 (NIPS'95). Eds. D. S. Touretzky, M. C. Mozer and
M. E. Hasselmo, 514-520, MIT Press (1996).
[5] D. Malzahn and M. Opper, In this volume.
[6] P. Sollich, Learning Curves for Gaussian Processes, In Advances in Neural Information Processing Systems 11 (NIPS'98), Eds. M. S. Keams, S. A. Solla, and D. A. Cohn, 344-350, MIT
Press (1999).
[7] M. Opper and O. Winther, Mean Field Approach to Bayes Learning in Feed-Forward Neural
Networks, Phys. Rev. Lett. 76,1964-1967 (1996).
[8] M. Opper and O. Winther, Gaussian Processes for Classification: Mean Field Algorithms, Neural Computation 12,2655-2684 (2000).
[9] M. Ahr, M. Biehl and R. Urbanczik, Statistical physics and practical training of soft-committee
machines Eur. Phys. J. B 10,583 (1999).

"
1988,Neural Network Recognizer for Hand-Written Zip Code Digits,,107-neural-network-recognizer-for-hand-written-zip-code-digits.pdf,Abstract Missing,"323

NEURAL NETWORK RECOGNIZER FOR
HAND-WRITTEN ZIP CODE DIGITS
J. S. Denker, W. R. Gardner, H. P. Graf, D. Henderson, R. E. Howard,
W. Hubbard, L. D. Jackel, H. S. Baird, and I. Guyon
AT &T Bell Laboratories
Holmdel, New Jersey 07733

ABSTRACT
This paper describes the construction of a system that recognizes hand-printed
digits, using a combination of classical techniques and neural-net methods. The
system has been trained and tested on real-world data, derived from zip codes seen
on actual U.S. Mail. The system rejects a small percentage of the examples as
unclassifiable, and achieves a very low error rate on the remaining examples. The
system compares favorably with other state-of-the art recognizers. While some of
the methods are specific to this task, it is hoped that many of the techniques will
be applicable to a wide range of recognition tasks.

MOTIVATION
The problem of recognizing hand-written digits is of enormous practical and the~
retical interest [Kahan, Pavlidis, and Baird 1987; Watanabe 1985; Pavlidis 1982].
This project has forced us to formulate and deal with a number of questions ranging from the basic psychophysics of human perception to analog integrated circuit
design.
This is a topic where ""neural net"" techniques are expected to be relevant, since
the task requires closely mimicking human performance, requires massively parallel
processing, involves confident conclusions based on low precision data, and requires
learning from examples. It is also a task that can benefit from the high throughput
potential of neural network hardware.
Many different techniques were needed. This motivated us to compare various classical techniques as well as modern neural-net techniques. This provided valuable
information about the strengths, weaknesses, and range of applicability of the numerous methods.
The overall task is extremely complex, so we have broken it down into a great
number of simpler steps. Broadly speaking, the recognizer is divided into the preprocessor and the classifier. The two main ideas behind the preprocessor are (1) to
remove meaningless variations (i.e. noise) and (2) to capture meaningful variations
(i.e . salient features).
Most of the results reported in this paper are based on a collection of digits taken
from hand-written Zip Codes that appeared on real U.S. Mail passing through the

324

Denker, et al

(j{OL3l-/.jGIJ
OI~.?4--:;

JI<=t

~789

d/~~'f.!Jd,7

012dL/-S-67

87

get

Figure 1: Typical Data
Buffalo, N.Y. post office. Details will be discussed elsewhere [Denker et al., 1989].
Examples of such images are shown in figure 1. The digits were written by many
different people, using a great variety of writing styles and instruments, with widely
varying levels of care.
Important parts of the task can be handled nicely by our lab's custom analog
neural network VLSI chip [Gra! et aI., 1987; Gra! & deVegvar, 1987], allowing us
to perform the necessary computations in a reasonable time. Also, since the chip
was not designed with image processing in mind, this provided a good test of the
chips' versatility.

THE PREPROCESSOR
Acquisition
The first step is to create a digital version of the image. One must find where on
the envelope the zip code is, which is a hard task in itself (Wang and Srihari 1988].
One must also separate each digit from its neighbors. This would be a relatively
simple task if we could assume that a character is contiguous and is disconnected
from its neighbors, but neither of these assumptions holds in practice. It is also
common to find that there are meaningless stray marks in the image.
Acquisition, binarization, location, and preliminary segmentation were performed
by Poetal Service contractors. In some images there were extraneous marks, so we
developed some simple heuristics to remove them while preserving, in most cases,
all segments of a split character.

Scaling and Deskewing
At this point, the size of the image is typically 40 x 60 pixels, although the scaling
routine can accept images that are arbitrarily large, or as small as 5 x 13 pixels. A
translation and scale factor are then applied to make the image fit in a rectangle

Neural Network Recognizer for Hand-Written Zip Code Digits

20 x 32 pixels. The character is centered in the rectangle, and just touches either
the horizontal or vertical edges, whichever way fits. It is clear that any extraneous
marks must be removed before this step, lest the good part of the image be radically
compressed in order to make room for some wild mark. The scaling routine changes
the horizontal and vertical size of the image by the same factor, so the aspect ratio
of the character is preserved.
As shown in figure 1, images can differ greatly in the amount of skew, yet be
considered the same digit. This is an extremely significant noise source. To remove
this noise, we use the methods of [Casey 1970]; see also [Naylor 1971]. That is, we
calculate the XY and YY moments of the image, and apply a linear transformation
that drives the XY moment to zero. The transformation is a pure shear, not a
rotation, because we find that rotation is much less common than skew.
The operations of scaling and deskewing are performed in a single step. This yields
a speed advantage, and, more importantly, eliminates the quantization noise that
would be introduced by storing the intermediate images as pixel maps, were the
calculation carried out in separate steps.

Skeletonization
For the task of digit recognition, the width of the pen used to make the characters is
completely meaningless, and is highly variable. It is important to remove this noise
source. By deleting pixels at the boundaries of thick strokes. After a few iterations
of this process, each stroke will be as thin as possible. The idea is to remove as
many pixels as possible without breaking the connectivity. Connectivity is based
on the 8 nearest neighbors.
This can be formulated as a pattern matching problem - we search the image
looking for situations in which a pixel should be deleted. The qecisions can be
expressed as a convolution, using a rather small kernel, since the identical decision
process is repeated for each location in the image, and the decision depends on the
configuration of the pixel's nearest and next-nearest neighbors.
Figure 2 shows an example of a character before (e) and after (I) skeletonization.
It also shows some of the templates we use for skeletonization, together with an
indication of where (in the given image) that template was active. To visualize the
convolution process, imagine taking a template, laying it over the image in each
possible place, and asking if the template is ""active"" in that place. (The template
is the convolution kernel; we use the two terms practically interchangeably.) The
portrayal of the template uses the following code: Black indicates that if the corresponding pixel in the image is ON, it will contribute +1 to the activity level of
this template. Similarly, gray indicates that the corresponding pixel, if ON, will
contribute -5, reducing the activity of this template. The rest of the pixels don't
matter. If the net activity level exceeds a predetermined threshold, the template
is considered active at this location. The outputs of all the skeletonizer templates

325

326

Denker, et al

a)

b)

c)

d)

Figure 2: Skeletonization
are eombined in a giant logieal OR, that is, whenever any template is aetive, we
eonelude that the pixel presently under the eenter of the template should be deleted.

The skeletonization eomputation involves six nested loops:
for each iteration I
for all X in the image (horizontal eoordinate)
for all Y in the image (vertical eoordinate)
for all T in the set of template shapes
for all P in the template (horizontal)
for all Q in the template (vertical)
compare image element(X +P, Y+Q)
with template(T) element(P, Q)

The inner three loops (the loops over T, P, and Q) are performed in parallel, in
a single cyde of our special-purpose ehip. The outer three loops (1, X, and Y)
are performed serially, calling the ehip repeatedly. The X and Y loops eould be
performed in parallel with no change in the algorithms. The additional parallelism
would require a proportionate increase in hardware.

Neural Network Recognizer for Hand-Written Zip Code Digits

The purpose of template a is to detect pixels at the top edge of a thick horizontal
line. The three ""should be OFF"" (light grey shade in figure 2) template elements
enforce the requirement that this should be a boundary, while the three ""should be
ON"" (solid black shade in figure 2) template elements enforce the requirement that
the line be at least two pixels wide.
Template b is analogous to template a, but rotated 90 degrees. Its purpose is to
detect pixels at the left edge of a thick vertical line.
Template c is similar to, but not exactly the same as, template a rotated 180 degrees.
The distinction is necessary because all templates are applied in parallel. A stroke
that is only two pixels thick ?must not be attacked from both sides at once, lest it be
removed entirely, changing the connectivity of the image . Previous convolutional
line-thinning schemes [Naccache 1984] used templates of size 3 x 3, and therefore
had to use several serial sub-stages. For parallel operation at least 3 x 4 kernels are
needed, and 5 x 5 templates are convenient, powerful, and flexible.

Feature Maps
Having removed the main sources of meaningless variation, we turn to the task of
extracting the meaningful information. It is known from biological studies [Hubel
and Wiesel 1962] that the human vision system is sensitive to certain features that
occur in images, particularly lines and the ends of lines. We therefore designed
detectors for such features. Previous artificial recognizers [\Vatanabe 1985] have
used similar feature extractors.
Once again we use a convolutional method for locating the features of interest - we
check each location in the image to see if each particular feature is present there.
Figure 3 shows some of the templates we use, and indicates where they become
active in an example image. The feature extractor templates are 7 x 7 pixels slightly larger than the skeletonizer templates.
Feature b is designed to detect the right-hand end of (approximately) horizontal
strokes. This can be seen as follows: in order for the template to become active
at a particular point, the image must be able to touch the ""should be ON"" pixels
at the center of the template without touching the surrounding horseshoe-shaped
collection of ""'must be OFF"" pixels. Essentially the only way this can happen is at
the right-hand end of a stroke. (An isolated dot in the image will also activate this
template, but the images, at this stage, are not supposed to contain dots). Feature
d detects (approximately) horizontal strokes.
There are 49 different feature extractor templates. The output of each is stored
separately. These outputs are called feature maps, since they show what feature(s)
occurred where in the image. It is possible, indeed likely, that several different
features will occur in the same place.
Whereas the outputs of all the skeletonizer templates were combined in a very simple
way (a giant OR), the outputs of the feature extractor templates are combined in

327

328

Denker, et al
a)

c)

b)

?
I

~------~.~

~.~------~
Figure 3: Feature Extraction

various artful ways. For example, feature"" and a similar one are O~d to form a
single combined feature that responds to right-hand ends in general. Certain other
features are ANDed to form detectors for arcs (long curved strokes). There are 18
combined features, and these are what is passed to the next stage.
We need to create a compact representation, but starting from the skeletonized
image, we have, instead, created 18 feature maps of the same size. Fortunately, we
can now return to the theme of removing meaningless variation.
If a certain image contains a particular feature (say a left-hand stroke end) in the
upper left corner, it is not really necessary to specify the location of that feature
with great precision. To recognize the Ihope of the feature required considerable
precision at the input to the convolution, but the pOlitiora of the feature does not
require so much precision at the output of the convolution. We call this Coarse
Blocking or Coarse Coding of the feature maps. We find that 3 x 5 is sufficent
resolution.

CLASSIFIERS
If the automatic recognizer is unable to classify a particular zip code digit, it may
be possible for the Post Office to determine the correct destination by other means.
This is costly, but not nearly so costly as a misclassification (substitution error) that
causes the envelope to be sent to the wrong destination. Therefore it is critically

Neural Network Recognizer for Hand-Written Zip Code Digits

important for the system to provide estimates of its confidence, and to reject digits
rather than misclassify them.
The objective is not simply to maximize the number of classified digits, nor to
minimize the number of errors . The objective is to minimize the cost of the whole
operation, and this involves a tradeoff between the rejection rate and the error rate.

Preliminary Inves tigations
Several different classifiers were tried, including Parzen Windows, K nearest neighbors, highly customized layered networks, expert systems, matrix associators, feature spins, and adaptive resonance. We performed preliminary studies to identify
the most promising methods. We determined that the top three methods in this
list were significantly better suited to our task than the others, and we performed
systematic comparisons only among those three.

Classical Clustering Methods
We used two classical clustering techniques, Parzen Windows (PW) and K Nearest Neighbors (KNN), which are nicely described in Duda and Hart [1973]. In
this application, we found (as expected) that they behaved similarly, although PW
consistently outperformed KNN by a small margin. These methods have many
advantages, not the least of which is that they are well motivated and easily understood in terms of standard Bayesian inference theory. They are well suited to
implementation on parallel computers and/or custom hardware. They provide excellent confidence information.
Unlike modern adaptive network methods, PW and KNN require no ""learning
time"", Furthermore the performance was reproducible and responded smoothly to
improvements in the preprocessor and increases in the size of the training set. This
is in contrast to the ""noisy"" performance of typical layered networks. This is convenient, indeed crucial, during exploratory work .

Adaptive Network Methods
In the early phases of the project, we found that neural network methods gave
rather mediocre results. Later, with a high-performance preprocessor, plus a large
training database, we found that a layered network gave the best results, surpassing
even Parzen Windows. We used a network with two stages of processing (i.e., two
layers of weights), with 40 hidden units and using a one-sided objective function (as
opposed to LMS) as described in [Denker and Wittner 1987]. The main theoretical
advantage of the layered network over the classical methods is that it can form
""higher order"" features - conjunctions and disjunctions of the features provided
by our feature extractor. Once the network is trained, it has the advantage that the
classification of each input is very rapid compared to PW or KNN. Furthermore,
the weights represent a compact distillation of the training data and thus have a
smaller memory requirement. The network provides confidence information that is

329

330

Denker, et al

just as good as the classical methods. This is obtained by comparing the activation
level of the most active output against the runner-up unit(s).
To check on the effectiveness of the preprocessing stages, we applied these three
classification schemes (PW, KNN, and the two-layer network) on 256-bit vectors
consisting of raw bit maps of the images - with no skeletonization and no feature
extraction. For each classification scheme, we found the error rate on the raw bit
maps was at least a factor of 5 greater than the error rate on the feature vectors,
thus clearly demonstrating the utility of feature extraction.

TESTING
It is impossible to compare the performance of recognition systems except on identical databases. Using highly motivated ""friendly"" writers, it is possible to get a
dataset that is so clean that practically any algorithm would give outstanding results. On the other hand, if the writers are not motivated to write clearly, the result
will be not classifiable by machines of any sort (nor by humans for that matter).
It would have been much easier to classify digits that were input using a mouse or
bitpad, since the lines in the such an image have zero thickness, and stroke-order
information is available. It would also have been much easier to recognize digits
from a single writer.
The most realistic test data we could obtain was provided by the US Postal Service.
It consists of approximately 10,000 digits (1000 in each category) obtained from the
zip codes on actual envelopes. The data we received had already been binarized
and divided into images of individual digits, rather than multi-digit zip codes, but
no further processing had been done.
On this data set, our best performance is as follows: if 14% of the images are rejected
as unclassifiable, only 1% of the remainder are misclassified. If no images are rejected, approximately 6% are misclassified. Other groups are working with the same
dataset, but their results have not yet been published. Informal communications
indicate that our results are among the best.

CONCLUSIONS
We have obtained very good results on this very difficult task. Our methods include
low-precision and analog processing, massively parallel computation, extraction of
biologically-motivated features, and learning from examples. We feel that this is,
therefore, a fine example of a Neural Information Processing System. We emphasize that old-fashioned engineering, classical pattern recognition, and the latest
learning-from-examples methods were all absolutely necessary. Without the careful
engineering, a direct adaptive network attack would not succeed, but by the same
token, without learning from a very large database, it would have been excruciating
to engineer a sufficiently accurate representation of the probability space.

Neural Network Recognizer for Hand-Written Zip Code Digits

Acknowledgements
It is a pleasure to acknowledge useful discussions with Patrick Gallinari and technical assistance from Roger Epworth. We thank Tim Barnum of the U.S. Postal
Service for making the Zip Code data available to us.
References
1. R. G. Casey, ""Moment Normalization of Handprinted Characters"", IBM J.
Res. Develop., 548 (1970)
2. J. S. Denker et al., ""Details of the Hand-Written Character Recognizer"", to
be published (1989)
3. R. O. Duda and P. E. Hart, Pattern Classification and Scene Analysis,
John Wiley and Sons (1973)
4. E. Gullichsen and E. Chang, ""Pattern Classification by Neural Network: An
Experimental System for Icon Recognition"", Proc. IEEE First Int. Conf. on
Neural Networks, San Diego, IV, 725 (1987)
5. H. P. Graf, W. Hubbard, L. D. Jackel, P.G.N. deVegvar, ""A CMOS Associative
Memory Chip"", Proc. IEEE First Int. Conf. on Neural Networks, San Diego,
111,461 (1987)
6. H.P Graf and P. deVegvar, ""A CMOS Implementation of a Neural Network
Model"", Proc. 1987 Stanford Conf. Advanced Res. VLSI, P. Losleben (ed.)
MIT Press, 351 (1987)
7. D. H. Hubel and T. N. Wiesel, ""Receptive fields, binocular interaction and
functional architecture in the cat's visual cortex"", J. Physiology 160, 106
(1962)
8. S. Kahan, T. Pavlidis, and H. S. Baird, ""On the Recognition of Printed Characters of Any Font and Size"", IEEE Transactions on Pattern Analysis and
Machine Intelligence, PAMI-9, 274 (1987)
9. N. J. Naccache and R. Shinghal, ''SPTA: A Proposed Algorithm for Thinning
Binary Patterns"", IEEE Trans. Systems, Man, and Cybernetics, SMC-14,
409 (1984)
10. W. C. Naylor, ""Some Studies in the Interactive Design of Character Recognition Systems"", IEEE Transactions on Computers, 1075 (1971)
11. T. Pavlidis, Algorithms for Graphics and Image Processing, Computer
Science Press (1982)
12. C. Y. Suen, M. Berthod, and S. Mori, ""Automatic Recognition of Handprinted
Characters - The State of the Art"", Proceedings of the IEEE 68 4, 469
(1980).
13. C-H. Wang and S. N. Srihari, ""A Framework for Object Recognition in a Visually Complex Environment and its Application to Locating Address Blocks
on Mail Pieces"", IntI. J. Computer Vision 2, 125 (1988)
14. S. Watanabe, Pattern Recognition, John Wiley and Sons, New York (1985)

331

"
2011,Directed Graph Embedding: an Algorithm based on Continuous Limits of Laplacian-type Operators,,4282-directed-graph-embedding-an-algorithm-based-on-continuous-limits-of-laplacian-type-operators.pdf,"This paper considers the problem of embedding directed graphs in Euclidean space while retaining directional information. We model the observed graph as a sample from a manifold endowed with a vector field, and we design an algo- rithm that separates and recovers the features of this process: the geometry of the manifold, the data density and the vector field. The algorithm is motivated by our analysis of Laplacian-type operators and their continuous limit as generators of diffusions on a manifold. We illustrate the recovery algorithm on both artificially constructed and real data.","Directed Graph Embedding: an Algorithm based on
Continuous Limits of Laplacian-type Operators

Marina Meil?a
Department of Statistics
University of Washington
Seattle, WA 98195
mmp@stat.washington.edu

Dominique C. Perrault-Joncas
Department of Statistics
University of Washington
Seattle, WA 98195
dcpj@stat.washington.edu

Abstract
This paper considers the problem of embedding directed graphs in Euclidean
space while retaining directional information. We model the observed graph as
a sample from a manifold endowed with a vector field, and we design an algorithm that separates and recovers the features of this process: the geometry of the
manifold, the data density and the vector field. The algorithm is motivated by our
analysis of Laplacian-type operators and their continuous limit as generators of
diffusions on a manifold. We illustrate the recovery algorithm on both artificially
constructed and real data.

1

Motivation

Recent advances in graph embedding and visualization have focused on undirected graphs, for which
the graph Laplacian properties make the analysis particularly elegant [1, 2]. However, there is
an important number of graph data, such as social networks, alignment scores between biological
sequences, and citation data, which are naturally asymmetric. A commonly used approach for this
type of data is to disregard the asymmetry by studying the spectral properties of W +W T or W T W ,
where W is the affinity matrix of the graph.
Some approaches have been offered to preserve the asymmetry information contained in data: [3],
[4], [5] or to define directed Laplacian operators [6]. Although quite successful, these works adopt
a purely graph-theoretical point of view. Thus, they are not concerned with the generative process
that produces the graph, nor with the interpretability and statistical properties of their algorithms.
In contrast, we view the nodes of a directed graph as a finite sample from a manifold in Euclidean
space, and the edges as macroscopic observations of a diffusion kernel between neighboring points
on the manifold. We explore how this diffusion kernel determines the overall connectivity and
asymmetry of the resulting graph and demonstrate how Laplacian-type operators of this graph can
offer insights into the underlying generative process.
Based on the analysis of the Laplacian-type operators, we derive an algorithm that, in the limit of infinite sample and vanishing bandwidth, recovers the key features of the sampling process: manifold
geometry, sampling distribution, and local directionality, up to their intrinsic indeterminacies.

2

Model

The first premise here is that we observe a directed graph G, with n nodes, having weights
W = [Wij ] for the edge from node i to node j. In following with common Laplacian-based embedding approaches, we assume that G is a geometric random graph constructed from n points sampled
according to distribution p = e?U on an unobserved compact smooth manifold M ? Rl of known
intrinsic dimension d ? l. The edge weight Wij is then determined by a directed similarity kernel
k (xi , xj ) with bandwidth . The directional component of k (xi , xj ) will be taken to be derived
1

from a vector field r on M, which assigns a preferred direction between weights Wij and Wji . The
choice of a vector field r to characterize the directional component of G might seem restrictive at
first. In the asymptotic limit of  ? 0 and n ? ? however, kernels are characterized by their
diffusion, drift, and source components [7]. As such, r is sufficient to characterize any directionality
associated with a drift component and as it turns out, the component of r normal M in Rl can also
be use to characterize any source component. As for the diffusion component, it is not possible
to uniquely identify it from G alone [8]. Some absolute knownledge of M is needed to say anything about it. Hence, without loss of generality, we will construct k (xi , xj ) so that the diffusion
component ends being isotropic and constant, i.e. equal to Laplace-Beltrami operator ? on M.
The schematic of this generative process is shown in the top left of Figure 1 below.

From left to right: the graph generative process mapping the sample on M to geometric random
graph G via the kernel k (x, y),
then the subsequent embedding
(?)
?n of G by operators Haa,n ,
(?)
Hss,n (defined in section 3.1).
As these operators converge to
(?)
their respective limits, Haa and
(?)
Hss , so will ?n ? ?, pn ? p,
and rn ? r.
We design an algorithm that,
given G, produces the top right
embedding (?n , pn , and rn ).

Figure 1: Schematic of our framework.

The question is then as follows: can the generative process? geometry M, distribution p = e?U , and
directionality r, be recovered from G? In other words, is there an embedding of G in Rm , m ? d
that approximates all three components of the process and that is also consistent as sample size
increases and the bandwidth vanishes? In the case of undirected graphs, the theory of Laplacian
eigenmaps [1] and Diffusion maps [9] answers this question in the affirmative, in that the geometry
of M and p = e?U can be inferred using spectral graph theory. The aim here is to build on
the undirected problem and recover all three components of the generative process from a directed
graph G.
The spectral approach to undirected graph embedding relies on the fact that eigenfunctions of the
Laplace-Beltrami operator are known to preserve the local geometry of M [1]. With a consistent
empirical Laplace-Beltrami operator based on G, its eigenvectors also recover the geometry of M
and converge to the corresponding eigenfunctions on M. For a directed graph G, an additional
operator is needed to recover the local directional component r, but the principle remains the same.
(?)
The schematic for this is shown in Figure 1 where two operators - Hss,n , introduced in [9] for
(?)
undirected embeddings, and Haa,n , a new operator defined in section 3.1 - are used to obtain the
(?)
(?)
(?)
embedding ?n , distribution pn , and vector field rn . As Haa,n and Hss,n converge to Haa and
(?)
Hss , ?n , pn , and rn also converge to ?, p, and r, where ? is the local geometry preserving the
embedding of M into Rm .
(?)

The algorithm we propose in Section 4 will calculate the matrices corresponding to H?,n from the
graph G, and with their eigenvectors, will find estimates for the node coordinates ?, the directional
component r, and the sampling distribution p. In the next section we briefly describe the mathematical models of the diffusion processes that our model relies on.
2

2.1

Problem Setting

The similarity kernel k (x, y) can be used to define transport operators on M. The natural transport
operator is defined by normalizing k (x, y) as
Z
Z
k (x, y)
T [f ](x) =
f (y)p(y)dy , where p (x) =
k (x, y)p(y)dy .
(1)
M p (x)
M
T [f ](x) represents
the diffusion of a distribution f (y) by the transition density
R
k (x, y)p(y)/ k (x, y 0 )p(y 0 )dy 0 . The eigenfunctions of this infinitesimal operator are the
continuous limit of the eigenvectors of the transition probability matrix P = D?1 W given by
normalizing the affinity matrix W of G by D = diag(W 1) [10]. Meanwhile, the infinitesimal
transition
?f
(T ? I)f
= lim
(2)
?0
?t

defines the backward equation for this diffusion process over M based on kernel k . Obtaining the
explicit expression for transport operators like (2) is then the main technical challenge.
2.2

Choice of Kernel

In order for T [f ] to have the correct asymptotic form, some hypotheses about the similarity kernel k (x, y) are required. The hypotheses are best presented by considering the decomposition of
k (x, y) into symmetric h (x, y) = h (y, x) and anti-symmetric a (x, y) = ?a (y, x) components:
k (x, y) = h (x, y) + a (x, y) .

(3)

The symmetric component h (x, y) is assumed to satisfy the following properties: 1. h (||y ?
2
/)
, and 2. h ? 0 and h is exponentially decreasing as ||y ? x|| ? ?. This form
x||2 ) = h(||y?x||
d/2
of symmetric kernel was used in [9] to analyze the diffusion map. For the asymmetric part of the
similarity kernel, we assume the form
a (x, y) =

r(x, y)
h(||y ? x||2 /)
? (y ? x)
,
2
d/2

(4)

with r(x, y) = r(y, x) so that a (x, y) = ?a (y, x). Here r(x, y) is a smooth vector field on the
manifold that gives an orientation to the asymmetry of the kernel k (x, y). It is worth noting that the
dependence of r(x, y) on both x and y implies that r : M ? M ? Rl with Rl the ambient space of
M; however in the asymptotic limit, the dependence in y is only important ?locally? (x = y), and
as such it is appropriate to think of r(x, x) being a vector field on M. As a side note, it is worth
pointing out that even though the form of a (x, y) might seem restrictive at first, it is sufficiently
rich to describe any vector field . This can be seen by taking r(x, y) = (w(x) + w(y))/2 so that at
x = y the resulting vector field is given by r(x, x) = w(x) for an arbitrary vector field w(x).

3

Continuous Limit of Laplacian Type Operators

We are now ready to state the main asymptotic result.
Proposition 3.1 Let M be a compact, closed, smooth manifold of dimension d and k (x, y) an
asymmetric similarity kernel satisfying the conditions of section 2.2, then for any function f ?
C 2 (M), the integral operator based on k has the asymptotic expansion
Z
k (x, y)f (y)dy = m0 f (x) + g(f (x), x) + o() ,
(5)
M

where
m2
(?(x)f (x) + ?f (x) + r ? ?f (x) + f (x)? ? r + c(x)f (x))
2
R
R
and m0 = Rd h(||u||2 )du, m2 = Rd u2i h(||u||2 )du.
g(f (x), x) =

3

(6)

The proof can be found in [8] along with the definition of ?(x) and c(x) in (6). For now, it suffices
to say that ?(x) corresponds to an interaction between the symmetric kernel h and the curvature of
M and was first derived in [9]. Meanwhile, c(x) is a new term that originates from the interaction
between h and the component of r that is normal to M in the ambient space Rl . Proposition 3.1
foreshadows a general fact about spectral embedding algorithms: in most cases, Laplacian operators
confound the effects of spatial proximity, sampling density and directional flow due to the presence
of the various terms above.
3.1

Anisotropic Limit Operators

Proposition 3.1 above can be used to derive the limits of a variety of Laplacian type operators
associated with spectral embedding algorithms like [5, 6, 3]. Although we will focus primarily on
a few operators that give the most insight into the generative process and enable us to recover the
model defined in Figure 1, we first present four distinct families of operators for completeness.
These operator families are inspired by the anisotropic family of operators that [9] introduced for
undirected graphs, which make use of anisotropic kernels of the form:
k(?) (x, y) =

k (x, y)
?
p (x)p?
 (y)

,

(7)

with ? ? [0, 1] where ? = 0 is the isotropic limit. To normalize the anisotropic kernels, we need
R
(?)
(?)
(?)
to redefine the outdegrees distribution of k as p (x) = M k (x, y)p(y)dy. From (7), four
families of diffusion processes of the form ft = H (?) [f ](x) can be derived depending on which
kernel is normalized and which outdegree distribution is used for the normalization. Specifically,
(?)
(?)
we define transport operators by normalizing
the asymmetric k or symmetric h kernels with the
R
1
asymmetric p or symmetric q = M h (x, y)p(y)dy outdegree distribution . To keep track of all
options, we introduce the following notation: the operators will be indexed by the type of kernel and
outdegree distribution they correspond to (symmetric or asymmetric), with the first index identifying
the kernel and the second index identifying the outdegree distribution. For example, the family of
anisotropic limit operators introduced by [9] is defined by normalizing the symmetric kernel by
(?)
the symmetric outdegree distribution, hence they will be denoted as Hss , with the superscript
corresponding to the anisotropic power ?.
Proposition 3.2 With the above notation,
(?)
Haa
[f ] = ?f ? 2 (1 ? ?) ?U ??f + r??f
(?)
Has
[f ]
(?)
Hsa [f ]
(?)
Hss
[f ]

(8)

= ?f ? 2 (1 ? ?) ?U ? ?f ? cf + (? ? 1)(r ? ?U )f ? (? ? r)f + r ? ?f (9)
=

?f ? 2 (1 ? ?) ?U ? ?f + (c + ? ? r + (? ? 1)r ? ?U )f

= ?f ? 2(1 ? ?)?U ? ?f.

(10)
(11)

The proof of this proposition, which can be found in [8], follows from repeated application of
Proposition 3.1 to p(y) or q(y) and then to k ? (x, y) or h? (x, y), as well as the fact that p1? =
1
p??

[1 ? ?(? +

?p
p

+ 2r ?

?p
p



+ 2? ? r + c)] + o().
(?)

Thus, if we use the asymmetric k and p , we get Haa , defined by the advected diffusion equa(?)
tion (8). In general, Haa is not hermitian, so it commonly has complex eigenvectors. This makes
(1)
embedding directed graphs with this operator problematic. Nevertheless, Haa will play an important role in extracting the directionality of the sampling process.
If we use the symmetric kernel h but the asymmetric outdegree distribution p , we get the family
(?)
of operators Hsa , of which the WCut of [3] is a special case (? = 0). If we reverse the above, i.e.
(?)
(?)
(?)
use k and q , we obtain Has . This turns out to be merely a combination of Haa and Hsa .
1
The reader may notice that there are in fact eight possible combinations of kernel and degree distribution,
since the anisotripic kernel (7) could also be defined using a symmetric or asymmetric outdegree distribution.
However, there are only four distinct asymptotic results and they are all covered by using one kernel (symmetric
or asymmetric) and one degree distribution (symmetric or asymmetric) throughout.

4

Algorithm 1 Directed Embedding
Input: Affinity matrix Wi,j and embedding dimension m, (m ? d)
T
1. S ? (W
Pn + W )/2 (Steps 1?6 estimate the coordinates as in [11])
2. qi ? j=1 Si,j , Q = diag(q)
3. V ? Q?1 SQ?1
Pn
(1)
4. qi ? j=1 Vi,j , Q(1) = diag(q (1) )
(1)

?1

5. Hss,n ? Q(1) V
6. Compute the ? the n ? (m + 1) matrix with orthonormal columns containing the m + 1 largest
(1)
right eigenvector (by eigenvalue) of Hss,n as well as the ? the (m + 1) ? (m + 1) diagonal matrix
of eigenvalues. Eigenvectors 2 to m + 1 from ? are the m coordinates of the embedding.
(1)
7. ComputeP
? the left eigenvector of Hss,n with eigenvalue 1. (Steps 7?8 estimate the density)
n
8. ? ? ?/
Pn i=1 ?i is the density distribution over the embedding.
9. pi ? j=1 Wi,j , P = diag(p) (Steps 9?13 estimate the vector field r)
10. T ? P ?1
P ?1
PW
(1)
n
11. pi ? j=1 Ti,j , P (1) = diag(p(1) )
(1)

?1

12. Haa,n ? P (1) T
(1)
(1)
13. R ? (Haa,n ? Hss,n )?/2. Columns 2 to m + 1 of R are the vector field components in the
direction of the corresponding coordinates of the embedding.

(?)

Finally, if we only consider the symmetric kernel h and degree distribution q , we recover Hss , the
anisotropic kernels of [9] for symmetric graphs. This operator for ? = 1 is shown to separate the
manifold from the probability distribution [11] and will be used as part of our recovery algorithm.

Isolating the Vector Field r

4

Our aim is to esimate the manifold M, the density distribution p = e?U , and the vector field r. The
(1)
first two components of the data can be recovered from Hss as shown in [11] and summarized in
Algorithm 1.
At this juncture, one feature of generative process is missing: the vector field r. The natural approach
(?)
(?)
for recovering r is to isolate the linear operator r ? ? from Haa by substracting Hss :
(?)
(?)
Haa
? Hss
= r ? ?.

(12)

The advantage of recovering r in operator form as in (12) is that r ? ? is coordinate free. In other
words, as long as the chosen embedding of M is diffeomorphic to M2 , (12) can be used to express
the component of r that lies in the tangent space T M, which we denote by r|| .
Specifically, let ? be a diffeomorphic embedding of M ; the component of r along coordinate ?k is
then given by r ? ??k = rk , and so, in general,
r|| = r ? ?? .

(13)

The subtle point that only r|| is recovered from (13) follows from the fact that the operator r ? ? is
only defined along M and hence any directional derivative is necessarily along T M.
Equation (13) and the previous observations are the basis for Algorithm 1, which recovers the three
important features of the generative process for an asymmetric graph with affinity matrix W .
A similar approach can be employed to recover c + ? ? r, or simply ? ? r if r has no component
perpendicular to the tangent space T M (meaning that c ? 0). Recovering c + ? ? r is achieved by
taking advantage of the fact that
(1)
(1)
(Hsa
? Hss
) = (c + ? ? r) ,
2

(14)
(1)

A diffeomorphic embedding is guaranteed by using the eigendecomposition of Hss .

5

(1)

(1)

which is a diagonal operator. Taking into account that for finite n (Hsa,n ? Hss,n ) is not perfectly
(1)
(1)
diagonal, using ?n ? 1n (vector of ones), i.e. (Hsa,n ? Hss,n )[1n ] = (cn + ? ? rn ), has been found
(1)
(1)
empirically to be more stable than simply extracting the diagonal of (Hsa,n ? Hss,n ).

5

Experiments

Artificial Data For illustrative purposes, we begin by applying our method to an artificial example.
We use the planet Earth as a manifold with a topographic density distribution, where sampling
probability is proportional to elevation. We also consider two vector fields: the first is parallel to the
line of constant latitude and purely tangential to the sphere, while the second is parallel to the line
of constant longitude with a component of the vector field perpendicular to the manifold. The true
model with constant latitude vector field is shown in Figure 2, along with the estimated density and
vector field projected on the true manifold (sphere).

Model

Recovered

Latitudinal

(a)

Longitudinal

(b)

Figure 2: (a): Sphere with latitudinal vector field, i.e East-West asymmetry, with Wew > Wwe if node w
lies to the West of node e. The graph nodes are sampled non-uniformly, with the topographic map of the world
as sampling density. We sample n = 5000 nodes, and observe only the resulting W matrix, but not the node
locations. From W , our algorithm estimates the sample locations (geometry), the vector field (black arrows)
generating the observed asymmetries, and the sampling distribution at each data point (colormap). (b) Vector
fields on a spherical region (blue), and their estimates (red): latitudinal vector field tangent to the manifold
(left) and longitudinal vector field with component perpendicular to manifold tangent plane (right).
Both the estimated density and vector field agree with the true model, demonstrating that for artificial
data, the recovery algorithm 1 performs quite well. We note that the estimated density does not
recover all the details of the original density, even for large sample size (here n = 5000 with  =
0.07). Meanwhile, the estimated vector field performs quite well even when the sampling is reduced
to n = 500 with  = 0.1. This can be seen in Figure 2, b, where the true and estimated vector fields
are superimposed. Figure 2 also demonstrates how r ? ? only recovers the tangential component of
r. The estimated geometry is not shown on any of these figures, since the success of the diffusion
map in recovering the geometry for such a simple manifold is already well established [2, 9].
Real DataThe National Longitudinal Survey of Youth (NLSY) 1979 Cohort is a representative sample of young men and women in the United States who were followed from 1979 to 2000 [12, 13].
The aim here is to use this survey to obtain a representation of the job market as a diffusion process
over a manifold.
The data set consists of a sample of 7,816 individual career sequences of length 64, listing the jobs
a particular individual held every quarter between the ages of 20 and 36. Each token in the sequence
identifies a job. Each job corresponds to an industry ? occupation pair. There are 25 unique industry
and 20 unique occupation indices. Out of the 500 possible pairings, approximately 450 occur in the
data, with only 213 occurring with sufficient frequency to be included here. Thus, our graph G has
213 nodes - the jobs - and our observations consist of 7,816 walks between the graph nodes.
We convert these walks to a directed graph with affinity matrix W . Specifically, Wij represents the
number of times a transition from job i to job j was observed (Note that this matrix is asymmetric,
6

i.e Wij 6= Wji ). Normalizing each row i of W by its outdegree di gives P = diag(di )?1 W , the
non-parametric maximum likelihood estimator for the Markov chain over G for the progression
(0)
of career sequences. This Markov chain has as limit operator Haa , as the granularity of the job
market increases along with the number of observations. Thus, in trying to recover the geometry,
distribution and vector field, we are actually interested in estimating the full advective effect of the
(0)
diffusion process generated by Haa ; that is, we want to estimate r ? ? ? 2?U ? ? where we can use
(0)
(1)
?2?U ? ? = Hss ? Hss to complement Algorithm 1.

2000

0.25
0.2

0.1

3

0.9

0.2

0.8

0.15

0.7

1800

0.15

!

0.25

0.05

1600

0.1

1400

!30.05

0.6

0.5

0

0

0.4

1200

?0.05

?0.05
?0.1

0.3

?0.1

1000

0.2

?0.15

?0.15
800
?0.2

?0.2

?0.25

?0.25

?0.1

?0.05

0

!2

0.05

0.1

0.15

0.2

(a)

0.1

?0.1

?0.05

0

!2

0.05

0.1

0.15

0.2

(b)

Figure 3: Embedding the job market along with field r ? 2?U over the first two non-constant eigenvectors.
The color map corresponds to the mean monthly wage in dollars (a) and to the female proportion (b) for each
job.
We obtain an embedding of the job market that describes the relative position of jobs, their distribution, and the natural time progression from each job. Of these, the relative position and natural
time progression are the most interesting. Together, they summarize the job market dynamics by
describing which jobs are naturally ?close? as well as where they can lead in the future. From a
public policy perspective, this can potentially improve focus on certain jobs for helping individuals
attain better upward mobility.
The job market was found to be a high dimensional manifold. We present only the first two dimen(0)
sions, that is, the second and third eigenvectors of Hss , since the first eigenvector is uninformative
(constant) by construction. The eigenvectors showed correlation with important demographic data,
such as wages and gender. Figure 3 displays this two-dimensional sub-embedding along with the
directional information r ? 2?U for each dimension. The plot shows very little net progression
toward regions of increasing mean salary3 . This is somewhat surprising, but it is easy to overstate
this observation: diffusion alone would be enough to move the individuals towards higher salary.
What Figure 3 (a) suggests is that there appear to be no ?external forces? advecting individuals towards higher salary. Nevertheless, there appear to be other external forces at play in the job market:
Figure 3 (b), which is analogous to Figure 3 (a), but with gender replacing the salary color scheme,
suggests that these forces push individuals towards greater gender differentiation. This is especially
true amongst male-dominated jobs which appear to be advected toward the left edge of the embedding. Hence, this simple analysis of the job market can be seen as an indication that males and
females tend to move away from each other over time, while neither seems to have a monopoly on
high- or low- paying jobs.

6

Discussion

This paper makes three contributions: (1) it introduces a manifold-based generative model for directed graphs with weighted edges, (2) it obtains asymptotic results for operators constructed from
the directed graphs, and (3) these asymptotic results lead to a natural algorithm for estimating the
model.
3
It is worth noting that in the NLSY data set, high paying jobs are teacher, nurse and mechanic. This is due
to the fact that the career paths observed stop at at age 36, which is relatively early in an individual?s career.

7

Generative Models that assume that data are sampled from a manifold are standard for undirected
graphs, but to our knowledge, none have yet been proposed for directed graphs. When W is symmetric, it is natural to assume that it depends on the points? proximity. For asymmetric affinities W ,
one must include an additional component to explain the asymmetry. In the asymptotic limit, this is
tantamount to defining a vector field on the manifold.
Algorithm We have used from [9] the idea of defining anisotropic kernels (indexed by ?) in order to
separate the density p and the manifold geometry M. Also, we adopted their general assumptions
about the symmetric part of the kernel. As a consequence, the recovery algorithm for p and M is
identical to theirs.
However, insofar as the asymmetric part of the kernel is concerned, everything, starting from the
definition and the introduction of the vector field r as a way to model the asymmetry, through the
derivation of the asymptotic expression for the symmetric plus asymmetric kernel, is new. We go
significantly beyond the elegant idea of [9] regarding the use of anisotropic kernels by analyzing the
four distinct renormalizations possible for a given ?, each of them combining different aspects of
M, p and r. Only the successful (and novel) combination of two different anisotropic operators is
able to recover the directional flow r.
Algorithm 1 is natural, but we do not claim it is the only possible one in the context of our model.
(?)
For instance, we can also use Hsa to recover the operator ? ? r (which empirically seems to have
worse numerical properties than r ? ?). In the National Longitudinal Survery of Youth study, we
were interested in the whole advective term, so we estimated it from a different combination of
operators. Depending on the specific question, other features of the model could be obtained
Limit Results Proposition 3.1 is a general result on the asymptotics of asymmetric kernels. Recovering the manifold and r is just one, albeit the most useful, of the many ways of exploiting these
(0)
results. For instance, Hsa is the limit operator of the operators used in [3] and [5]. The limit analysis
could be extended to other digraph embedding algorithms such as [4, 6].
How general is our model? Any kernel can be decomposed into a symmetric and an asymmetric
part, as we have done. The assumptions on the symmetric part h are standard. The paper of [7] goes
one step further from these assumptions; we will discuss it in relationship with our work shortly.
The more interesting question is how limiting are our assumptions regarding the choice of kernel,
especially the asymmetric part, which we parameterized as a (x, y) = r/2 ? (y ? x)h (x, y) in (4).
In the asymptotic limit, this choice turns out to be fully general, at least up to the identifiable aspects
of the model. For a more detailed discussion of this issue, see [8].
In [7], Ting, Huang and Jordan presented asymptotic results for a general family of kernels that
includes asymmetric and random kernels. Our k can be expressed in the notation of [7] by taking
wx (y) ? 1+r(x, y)?(y?x), rx (y) ? 1, K0 ? h, h ? . Their assumptions are more general than
the assumptions we make here, yet our model is general up to what can be identified from G alone.
The distinction arises because [7] focuses on the graph construction methods from an observed
sample of M, while we focus on explaining an observed directed graph G through a manifold
generative process. Moreover, while the [7] results can be used to analyze data from directed graphs,
they differ from our Proposition 3.1. Specifically, with respect to the limit in Theorem 3 from
[7], we obtain the additional source terms f (x)? ? r and c(x)f (x) that follow from not enforcing
(?)
(?)
conservation of mass while defining operators Hsa and Has .
We applied our theory of directed graph embedding to the analysis of the career sequences in
Section 5, but asymmetric affinity data abound in other social contexts, and in the physical and
life sciences. Indeed, any ?similarity? score that is obtained from a likelihood of the form
Wvu =likelihood(u|v) is generally asymmetric. Hence our methods can be applied to study not
only social networks, but also patterns of human movement, road traffic, and trade relations, as well
as alignment scores in molecular biology. Finally, the physical interpretation of our model also
makes it naturally applicable to physical models of flows.

Acknowledgments
This research was partially supported by NSW awards IIS-0313339 and IIS-0535100.
8

References
[1] Belkin and Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural
Computation, 15:1373?1396, 2002.
[2] Nadler, Lafon, and Coifman. Diffusion maps, spectral clustering and eigenfunctions of fokker-planck
operators. In Neural Information Processing Systems Conference, 2006.
[3] Meila and Pentney. Clustering by weighted cuts in directed graphs. In SIAM Data Mining Conference,
2007.
[4] Zhou, Huang, and Scholkopf. Learning from labeled and unlabeled data on a directed graph. In International Conference on Machine Learning, pages 1041?1048, 2005.
[5] Zhou, Schlkopf, and Hofmann. Semi-supervised learning on directed graphs. In Advances in Neural
Information Processing Systems, volume 17, pages 1633?1640, 2005.
[6] Fan R. K. Chung. The diameter and laplacian eigenvalues of directed graphs. Electr. J. Comb., 13, 2006.
[7] Ting, Huang, and Jordan. An analysis of the convergence of graph Laplacians. In International Conference on Machine Learning, 2010.
[8] Dominique Perrault-Joncas and Marina Meil?a. Directed graph embedding: an algorithm based on continuous limits of laplacian-type operators. Technical Report TR 587, University of Washington - Department
of Statistics, November 2011.
[9] Coifman and Lafon. Diffusion maps. Applied and Computational Harmonic Analysis, 21:6?30, 2006.
[10] Mikhail Belkin and Partha Niyogi. Convergence of laplacian eigenmaps. preprint, short version NIPS
2008, 2008.
[11] Coifman, Lafon, Lee, Maggioni, Warner, and Zucker. Geometric diffusions as a tool for harmonic analysis
and structure definition of data: Diffusion maps. In Proceedings of the National Academy of Sciences,
pages 7426?7431, 2005.
[12] United States Department of Labor.
National longitudinal survey of youth 1979 cohort.
http://www.bls.gov/nls/, retrived October 2011.
[13] Marc A. Scott. Affinity models for career sequences. Journal of the Royal Statistical Society: Series C
(Applied Statistics), 60(3):417?436, 2011.

9

"
2012,"The Lov?sz ? function, SVMs and finding large dense subgraphs",,4503-the-lovasz-function-svms-and-finding-large-dense-subgraphs.pdf,"The Lovasz $\theta$ function of a graph, is a fundamental tool in combinatorial optimization and approximation algorithms.  Computing $\theta$ involves solving a SDP  and is extremely expensive even for moderately sized graphs.  In this paper we establish that the Lovasz $\theta$ function is equivalent to  a kernel learning problem related to one class SVM. This interesting connection opens up many opportunities  bridging graph theoretic algorithms and machine learning.   We show that there exist graphs, which we call $SVM-\theta$ graphs, on which the Lovasz $\theta$ function can be approximated well by a one-class  SVM.    This leads to a novel use of SVM techniques to solve algorithmic problems in large graphs e.g. identifying a planted clique  of size $\Theta({\sqrt{n}})$ in a random graph $G(n,\frac{1}{2})$. A classic approach for this problem involves computing  the $\theta$ function, however it is not scalable due to SDP computation. We show that the random graph with a planted clique is an example of $SVM-\theta$ graph, and as a consequence a SVM based approach  easily identifies the clique in large graphs and is competitive with the  state-of-the-art.    Further, we introduce  the notion of a ''common orthogonal labeling'' which extends the notion  of a ''orthogonal labelling of a single  graph (used in defining the $\theta$ function)  to multiple graphs.  The problem of finding the optimal common orthogonal labelling is cast as a  Multiple Kernel Learning problem and is used to identify a large common dense region in multiple graphs.  The proposed algorithm achieves an order of magnitude scalability compared to the state of the art.","The Lov?asz ? function, SVMs and finding large dense
subgraphs

Vinay Jethava ?
Computer Science & Engineering Department,
Chalmers University of Technology
412 96, Goteborg, SWEDEN
jethava@chalmers.se
Chiranjib Bhattacharyya
Department of CSA,
Indian Institute of Science
Bangalore, 560012, INDIA
chiru@csa.iisc.ernet.in

Anders Martinsson
Department of Mathematics,
Chalmers University of Technology
412 96, Goteborg, SWEDEN
andemar@student.chalmers.se

Devdatt Dubhashi
Computer Science & Engineering Department,
Chalmers University of Technology
412 96, Goteborg, SWEDEN
dubhashi@chalmers.se

Abstract
The Lov?asz ? function of a graph, a fundamental tool in combinatorial optimization and approximation algorithms, is computed by solving a SDP. In this paper
we establish that the Lov?asz ? function is equivalent to a kernel learning problem
related to one class SVM. This interesting connection opens up many opportunities bridging graph theoretic algorithms and machine learning. We show that there
exist graphs, which we call SVM ? ? graphs, on which the Lov?asz ? function
can be approximated well by a one-class SVM. This leads to novel use of SVM
techniques for solving algorithmic
problems in large graphs e.g. identifying a
?
planted clique of size ?( n) in a random graph G(n, 21 ). A classic approach for
this problem involves computing the ? function, however it is not scalable due
to SDP computation. We show that the random graph with a planted clique is an
example of SVM ? ? graph. As a consequence a SVM based approach easily
identifies the clique in large graphs and is competitive with the state-of-the-art.
We introduce the notion of common orthogonal labelling and show that it can be
computed by solving a Multiple Kernel learning problem. It is further shown that
such a labelling is extremely useful in identifying a large common dense subgraph
in multiple graphs, which is known to be a computationally difficult problem. The
proposed algorithm achieves an order of magnitude scalability compared to state
of the art methods.

1

Introduction

The Lov?asz ? function [19] plays a fundamental role in modern combinatorial optimization and
in various approximation algorithms on graphs, indeed Goemans was led to say It seems all
roads lead to ? [10]. The function is an instance of semidefinite programming(SDP) and
hence computing it is an extremely demanding task even for moderately sized graphs. In this paper
we establish that the ? function is equivalent to solving a kernel learning problem in the one-class
SVM setting. This surprising connection opens up many opportunities which can benefit both graph
theory and machine learning. In this paper we exploit this novel connection to show an interesting
application of the SVM setup for identfying large dense subgraphs. More specifically we make the
following contributions.
?
Relevant code and datasets
jethava/svm-theta.html

can

be

found

1

on

http://www.cse.chalmers.se/ e

1.1

Contributions:

1.We give a new SDP characterization of Lov?asz ? function,
min ?(K) = ?(G)
K?K(G)

where ?(K) is computed by solving an one-class SVM. The matrix K is a kernel matrix, associated
with any orthogonal labelling of G. This is discussed in Section 2.
2. Using an easy to compute orthogonal labelling we show that there exist graphs, which we call
SVM ? ? graphs, on which Lov?asz ? function can be well approximated by solving an one-class
SVM. This is discussed in Section 3.
3. The problem of finding a large common dense subgraph in multiple graphs arises in a variety
of domains including Biology, Internet, Social Sciences [18]. Existing state-of-the-art methods
[14] are enumerative in nature and has complexity exponential in the size of the subgraph. We
introduce the notion of common orthogonal labelling which can be used to develop a formulation
which is close in spirit to a Multiple Kernel Learning based formulation. Our results on the well
known DIMACS benchmark dataset show that it can identify large common dense subgraphs in
wide variety of settings, beyond the realm of state-of-the-art methods. This is discussed in Section
4.
4. Lastly, in Section 5, we show that the famous planted clique problem, can be easily solved for
large graphs by solving an one-class SVM. Many problems of interest in the area of machine learning
can be reduced to the problem of detecting planted clique, e.g detecting correlations [1, section 4.6],
correlation clustering [21] etc. The planted clique problem consists of identifying a large clique in
a random graph. There is an elegant approach for identifying the planted clique by computing the
Lov?asz ? function [8], however it is not practical for large graphs as it requires solving an SDP.
We show that the graph associated with the planted clique problem is a SVM ? ? graph, paving
the way for identifying the clique by solving an one-class SVM. Apart from the method based on
computing the ? function, there are other methods for planted clique identification, which do not
require solving an SDP [2, 7, 24]. Our result is also competitive with the state-of-the-art non-SDP
based approaches [24].
Notation We denote the Euclidean norm by k ? k and the infinity norm by k ? k? . Let S d?1 =
{u ? Rd | kuk = 1} denote a d dimensional sphere. Let Sn denote the set of n?n square symmetric
matrices and S+
n denote n ? n square symmetric positive semidefinite matrices. For any A ? Sn
we denote the eigenvalues ?1 (A) ? . . . ? ?n (A). diag(r) will denote a diagonal matrix with
diagonal entries defined by components of r. We denote the one-class SVM objective function by
!
n
n
X
X
?i ?j Kij
?i ?
?(K) =
max
2
(1)
?i ?0,i=1,...,n

i=1

|

i=1

{z

f (?;K)

}

where K ? S+
n . Let G = (V, E) be a graph on vertices V = {1, . . . , n} and edge set E. Let
A ? Sn denote the adjacency matrix of G where Aij = 1 if edge (i, j) ? E, and 0 otherwise. An
? denote the
eigenvalue of graph G would mean the eigenvalue of the adjacency matrix of G. Let G
? = ee> ? I ? A, where e = [1, 1, . . . , 1]>
? is A
complement graph of G. The adjacency matrix of G
is a vector of length n containing all 1?s, and I denotes the identity matrix. Let GS = (S, ES ) denote

the subgraph induced by S ? V in graph G; having density ?(GS ) is given by ?(GS ) = |ES |/ |S|
2 .
Let Ni (G) = {j ? V : (i, j) ? E} denote the set of neighbours of vertex i in graph G, and degree
? is a subset of vertices
of node i to be di (G) = |Ni (G)|. An independent set in G (a clique in G
? The notation is standard e.g.
S ? V for which no (every) pair of vertices has an edge in G (in G).
see [3].

2

Lov?asz ? function and Kernel learning

Consider the problem of embedding a graph G = (V, E) on a d dimensional unit sphere S d?1 . The
study of this problem was initiated in [19] which introduced the idea of orthogonal labelling: An
2

orthogonal labelling of graph G = (V, E) with |V | = n, is a matrix U = [u1 , . . . , un ] ? Rd?n such
d?1
that u>
? i = 1, . . . , n.
i uj = 0 whenever (i, j) 6? E and ui ? S
An orthogonal labelling defines an embedding of a graph on a d dimensional unit sphere: for every
vertex i there is a vector ui on the unit sphere and for every (i, j) 6? E ui and uj are orthogonal.
Using the notion of orthogonal labellings, [19] defined a function, famously known as Lov?asz ?
function, which upper bounds the size of maximum independent set. More specifically
for any graph G : ALPHA(G) ? ?(G),
where ALPHA(G) is the size of the largest independent set. Finding large independent sets is
a fundamental problem in algorithm design and analysis and computing ALPHA(G) is a classic
NP-hard problem which is even very hard even to approximate [11]. However, the Lov?asz function
?(G) gives a tractable upper-bound and since then Lov?asz ? function has been extensively used
in solving a variety of algorithmic problems e.g. [6]. It maybe useful to recall the definition of
Lov?asz ? function. Denote the set of all possible orthogonal labellings of G by Lab(G) = {U =
[u1 , . . . , un ]|ui ? S d?1 , u>
i uj = 0 ?(i, j) 6? E}.
?(G) =

min

min max

U?Lab(G) c?S d?1

i

1
(c> ui )2

(2)

There exist several other equivalent definitions of ?, for a comprehensive discussion see [16].
However computation of Lov?asz ? function is not practical even for moderately sized graphs as it
requires solving a semidefinite program on a matrix which is of the size of the graph. In the following
theorem, we show that there exist connections between the ? function and the SVM formulation.
Theorem 2.1. For a undirected graph G = (V, E), with |V | = n, let K(G) := {K ? S+
n | Kii =
1, i ? [n], Kij = 0, (i, j) 6? E} Then, ?(G) = minK?K(G) ?(K)
Proof. We begin by noting that any K ? K(G) is positive semidefinite and hence there exists
U ? Rd?n such that K = U> U. Note that Kij = u>
i uj where ui is a column of U. Hence by
inspection it is clear that the columns of U defines an orthogonal labelling on G, i.e U ? Lab(G).
Using a similar argument we can show that for any U ? Lab(G), the matrix K = U> U, is an
element of K(G). The set of valid kernel matrices K(G) is thus equivalent to Lab(G). Note that if
U is a labelling then U = Udiag() is also an orthogonal labelling for any > = [1 , . . . , n ], i =
?1 i = 1, . . . , n. It thus suffices to consider only those labellings for which c> ui ? 0 ? i =
1, . . . , n holds. For a fixed c one can write maxi (c>1ui )2 = mint t2 subject to c>1ui ? t. This is
true because the minimum over t is attained at maxi c>1ui . Setting w = 2tc yields the following
2

relation minc?S d?1 maxi (c>1ui )2 = minw?Rd kwk
with constraints w> ui ? 2. This establishes
4
that for a labelling, U, the optimal c is obtained by solving an one-class SVM. Application of
strong duality immediately leads to the claim minc?S d?1 maxi (c>1ui )2 = ?(K) where K = U> U
and ?(K) is defined in (1). As there is a correspondence between each element of Lab(G) and K
minimization of ?(K) over K is equivalent to computing the ?(G) function.
This is a significant result which establishes connections between two well studied formulations,
namely ? function and the SVM formulation. An important consequence of Theorem 2.1 is an
easily computable upperbound on ?(G) namely that for any graph G
ALPHA(G) ? ?(G) ? ?(K) ?K ? K(G)

(3)

Since solving ?(K) is a convex quadratic program, it is indeed a computationally efficient alternative
to the ? function. In fact we will show that there exist families of graphs for which ?(G) can be
approximated to within a constant factor by ?(K) for suitable K. Theorem 2.1 is closely related to
the following result proved in [20].
Theorem 2.2. [20] For a graph G = (V, E) with |V | = n let C ? Sn matrix with Cij = 0
whenever (i, j) 6? E. Then,


 
C
>
>
+I x
?(G) = minC v(G, C) = max 2x e ? x
x?0
??n (C)
3

Proof. See [20]
See that for any feasible C the matrix I + ??nC(C) ? K(G). Theorem 2.1 is a restatement of
Theorem 2.2, but has the additional advantage that the stated optimization problem can be solved
as an SDP. The optimization problem minC v(G, C) with constraints on C is not an SDP. If we
fix C = A, the adjacency matrix, we obtain a very interesting orthogonal labelling, which we will
refer to as LS labelling, introduced in [20]. Indeed there exists family of graphs, called Q graphs
for which LS labelling yields the interesting result ALPHA(G) = v(G, A), see [20]. Indeed
on a Q graph one does not need to compute a SDP, but can solve an one-class SVM, which has
obvious computational benefits. Inspired by this result, in the remaining part of the paper, we study
this labelling more closely. As a labelling is completely defined by the associated kernel matrix, we
refer to the following kernel as the LS labelling,
K=

3

A
+ I where ? ? ??n (A).
?

(4)

SVM ? ? graphs: Graphs where ? function can be approximated by SVM

We now introduce a class of graphs on which ? function can be well approximated by ?(K) for K
defined by (4). In the spirit of approximation algorithms we define:
Definition 3.1. A graph G is a SVM ? ? graph if ?(K) ? (1 + O(1))?(G) where K is a LS labelling.
Such classes of graphs are interesting because on them, one can approximate the Lov?asz ? function
by solving an SVM, instead of an SDP, which in turn can be extremely useful in the design and analysis of approximation algorithms. We will demosntrate two examples of SVM ? ? graphs namely
(a.) the Erd?os?Renyi random graph G(n, 1/2) and (b.) a planted variation. Here the relaxation
?(K) could be used in place of ?(G), resulting in algorithms with the same quality guarantees but
with faster running time ? in particular, this will allow the algorithms to be scaled to large graphs.
The classical Erd?os-Renyi random graph G(n, 1/2) has n vertices and each edge (i, j) is present
independently with probability 1/2. We list a few facts about G(n, 1/2) that will be used repeatedly.
Fact 3.1. For G(n, 1/2),
?
? With probability 1 ? O(1/n), the degree of each vertex is in the range n/2 ? O( n log n).
c

? With probability 1 ? e?n for some c >?0, the
? maximum eigenvalue is n/2 ? o(n) and the
minimum eigenvalue is in the range [? n, n] [9].
?
Theorem 3.1. Let  > 2 ? 1. For G = G(n, 1/2)
? , with probability 1 ? O(1/n), ?(K) ?
?
(1 + )?(G) where K is defined in (4) with ? = 1+
n.
2
?
Proof. We begin by considering the case for ? = (1 + 2? ) n. By Fact 3.1 for all choices of ? > 0,
the minimum eigenvalue of ?1 A + I is, almost surely, greater than 0 which implies that f (?, K)
(see (1)) is strongly concave. For such functions KKT conditions are neccessary and sufficient for
optimality. The KKT conditions for a G(n, 21 ) are given by the following equation
?i +

1 X
Ai,j ?j = 1 + ?i , ?i ?i = 0, ?i ? 0
?

(5)

(i,j)?E

As A is random we begin by analyzing the case for expectation of A. Let E(A) = 12 (ee> ? I), be
? = E(A) + I is positive definite. More
the expectation of A. For the given choice of ?, the matrix K
?
? is again strongly concave and attains maximum at a KKT point. By direct
importantly f (?, K)
? where ?? = 2? satisfies
verification ?
? = ?e
n?1+2?

1
? + E(A)? = e.
?
4

(6)

Thus ?
? is the KKT point for the problem,
? =
f? = max f (?, K)
??0

n
X

?
???
?

i=1

>




E(A)
+I ?
? = n??
?

(7)

?
with the optimal objective function value f?. By choice of ? = (1 + 2? ) n we can write ?? =
2?/n + O(1/n). Using the fact about degrees of vertices in G(n, 1/2), we know that
p
n?1
a>
+ ?i with |?i | ? n log n
(8)
i e=
2
where a>
i is the ith row of the adjacency matrix A. As a consequence we note that




X
 ??

1
?
Aij ?
? j ? 1 = ?i
 ?i + ?
?


j

(9)

Recalling the definition of f and using the above equation along with (8) gives
|f (?
?; K) ? f?| ? n

??2 p
n log n
?

(10)

?
As noted before the function f (?; K) is strongly concave with ?2? f (?; K)  ? 2+?
I for all feasible
?. Recalling a useful result from convex optimization, see Lemma 3.1, we obtain


1
?(K) ? f (?
?; K) ? 1 +
k?f (?
?; K)k2
(11)
?

Observing that ?f (?; K) = 2(e ? ? ? A
? ?) and using the relation between k ? k? and 2 norm along
?
??
with (9) and (8) gives k?f (?
?; K)k ? nk?f (?
? ; K)k? = 2n ?? log n. Plugging this estimate in
?
(11) and using equation (10) we obtain ?(K) ? f? + O(log n) = (2 + ?) n + O(log n) The
second
? ?
equality follows by plugging in the value of ?? in (7). It is well known [6] that ?(G) = 2 n for
?
? ?(G) + o( n) and the theorem
G(n, 12 ) with high probability. One concludes that ?(K) ? 2+?
2
follows by choice of ?.
Discussion: Theorem 3.1 establishes that instead of SDP one can solve an SVM to evaluate
? function on G(n, 1/2). Although it is well known that ALPHA(G(n, 1/2)) = 2 log n whp,
there is no known polynomial time algorithm for computing the maximum independent set. [6]
gives an approximation algorithm that finds an independent set in G(n, p) which runs in expected
polynomial time, via a computation of ?(G(n, p)),which also applies to p = 1/2. The ? function
also serves as a guarantee of the approximation which other algorithms a simple Greedy algorithm
cannot give. Theorem 3.1 allows us to obtain similar guarantees but without the computational overhead of solving an SDP. Apart from finding independent sets computing ?(G(n, p)) is also used as
a subroutine in colorability [6], and here again one can use the SVM based approach to approximate
the ? function.
Similar arguments show also that other families of graphs such as the 11 families of pseudo?random
graphs described in [17] are SVM ? ? graphs.
Lemma 3.1. [4] A function g : C ? Rd ? R is said to be strongly concave over S if there
exists t > 0 such that ?2 g(x)  ?tI ? x ? C. For such functions one can show that if p? =
maxx?C g(x) < ? then
1
?x ? C p? ? g(x) ? k?g(x)k2
2t

4

Dense common subgraph detection

The problem of finding a large dense subgraph in multiple graphs has many applications [23, 22,
18]. We introduce the notion of common orthogonal labelling, and show that it is indeed possible
to recover dense regions in large graphs by solving a MKL problem. This constitutes significant
progress with respect to state of the art enumerative methods [14].
5

Problem definition Let G = {G(1) , . . . , G(M ) } be a set of simple, undirected graphs G(m) =
(V, E (m) ) defined on vertex set V = {1, . . . , n}. Find a common subgraph which is dense in all the
graphs.
Most algorithms which attempts the problem of finding a dense region are enumerative in nature and
hence do not scale well to finding large cliques. [14], first studied a related problem of finding all
possible common subgraphs for a given choice of parameters {? (1) , . . . , ? (M ) } which is atleast ?i
dense in G(i) . In the worst case, the algorithm performs depth first search over space of nnT possible

cliques of size nT . This has ?( nnT ) space and time complexity, which makes it impractical for
moderately large nT . For example, finding quasicliques of size 60 requires 8 hours (see Section 6).
In the remainder of this section, we focus on finding a large common sparse subgraph in a given
collection of graphs; with the observation that this is equivalent to finding a large common dense
subgraph in the set of complement graphs. To this end we introduce the following definition
Definition 4.1. Given simple unweighted graphs, G(m) = (V, E (m) ) m = 1, . . . , M on a common
vertex set V with |V | = n, the common orthogonal labelling on all the labellings is given by
ui ? S d?1 such that u>
/ E (m) ? m = 1, . . . , M }.1
i uj = 0 if (i, j) ?
Following the arguments of Section 2 it is immediate that size of the largest common independent
set is upper bounded by minK?L ?(K) where L = {K ? S+
: Kii = 1 ?i ? [n], Kij =
n
0 whenever (i, j) ?
/ E (m) ? m = 1, . . . , M }. We wish to exploit this fact in identifying large
common sparse regions in general graphs. Unfortunately this problem is a SDP and will not scale
well to large graphs. Taking cue from MKL literature we pose a restricted version of the problem
namely
min
?(K)
(12)
P
P
K=

(m)

M
m=1

?m K(m) , ?m ?0

M
m=1

?m =1

(m)

where K
is an orthogonal labelling of G . Direct verification shows that any feasible K is
also a common orthogonal labelling. Using the fact that ?x ? RM minpm ?0,PM
p> x =
m=1 pm =1
minm xm = max{t|xm ? t ? m = 1, . . . , M } one can recast the optimization problem in (12) as
follows
max t s.t. f (?; K(m) ) ? t ? m = 1, . . . , M
(13)
t?R,?i ?0

(m)

where K
is the LS labelling for G(m) , ?m = 1, . . . , M . The above optimization can be readily
solved by state of the art MKL solvers. This result allows us to build a parameter free common
sparse subgraph (CSS) algorithm shown in Figure 1 having following advantages: it provides a
theoretical bound on subgraph density (Claim 4.1 below); and, it requires no parameters from the
user beyond the set of graphs G(1) , . . . , G(M ) .
Let ?? be the optimal solution in (13); and SV = {i : ?i? > 0} and S1 =
{i : ?i? = 1} with
P
(m)

cardinalities nsv = |SV | and n1 = |S1 | respectively. Let ?
? min,S = mini?S

(m)
j?Ni (G
)
S
(m)
di (GS )

??
j

denote

(m)
Ni (GS )

the average of the support vector coefficients in the neighbourhood
of vertex i in induced
(m)
(m)
(m)
subgraph GS having degree di (GS ) = |Ni (GS )|. We define
(
)
(m)
(1
?
c)?
(m)
T (m) = i ? SV : di (GSV ) <
where c = min ?i?
(14)
(m)
i?SV
?
? min,SV
Claim 4.1. Let T ? V be computed as in Al(m)
gorithm 1. The subgraph GT induced by T ,
in graph G(m) , has density at most ? (m) where
(1?c)?(m)
? (m) = ?? min,SV
(nT ?1)

?? = Use MKL solvers to solve eqn. (13)
T = ?m T (m) {eqn. (14)}
Return T

Figure 1: Algorithm for finding common sparse
Pn
subgraph: T = CSS(G(1) , . . . , G(M ) )
?
Proof. (Sketch) At optimality, t =
?
.
i=1 i
P
P
P
(m) ?
?
?
This allows us to write 0 ?
i?S ?i (2 ? ?i ?
j6=i Kij ?j ) ? t as 0 ?
i?T (1 ? c ?
(m)

di (GT ) (m)
nT
?
? min,SV ) Dividing by 2 completes the proof.
?(m)
1

This is equivalent to defining an orthogonal labelling on the Union graph of G(1) , . . . , G(M )

6

5

Finding Planted Cliques in G(n, 1/2) graphs

Finding large cliques or independent sets is a computationally difficult problem even in random
graphs. While it is known that the size of the largest clique or independent set in G(n, 1/2) is 2 log n
with high probability, there is no known efficient algorithm to find a clique of size significantly larger
than log n - even a cryptographic application was suggested based on this (see the discussion and
references in the introduction of [8]).
Hidden planted clique A random graph G(n, 1/2) is chosen first and then a clique of size k is
introduced in the first 1, . . . , k vertices. The problem is to identify the clique.
?
[8] showed that if k = ?( n), then the hidden clique can be discovered in polynomial time by computing the Lov?asz ? function. There are other approaches [2, 7, 24] which do not require computing
the ? function.
We consider the (equivalent) complement model G(n, 1/2,?k) where a independent set is planted on
? 1/2, k) is a SVM ? ? graph.
the set of k vertices. We show that in the regime k = ?( n), G(n,
We will further demonstrate that as a consequence one can identify the hidden independent set with
high probability by solving an SVM. The following is the main result of the section.
?
?
Theorem 5.1.
? For G = G(n, 1/2, k) and k = 2t n for large enough constant t ? 1 with K as in
(4) and ? = n + k/2,


?
1
?(K) = 2(t + 1) n + O(log n) = 1 + + o(1) ?(G)
t
with probability at least 1 ? O(1/n).
?
Proof. The proof is analogous to that of Theorem 3.1. Note that |?n (G)| ? n + k/2. First we
consider the expected case where all vertices outside the planted part S are adjacent to k/2 vertices
in S and (n ? k)/2 vertices outside
part
? have degree (n ? k)/2.
? S. and all verties in the planted
n for i 6? S and ?i = 2(t + 1)2 / n for i ? S satisfy KKT
We check that ?i = 2(t + 1)/ ?
conditions with an error of O(1/ n). Now apply p
Chernoff bounds to conclude that with high
probability, all vertices in S have degree (n ? k)/2 ? (n ? k)
p log(n ? k) and those outside S are
?
adjacent to k/2 ? k log k vertices in S and to (n ? k)/2 ? (n ? k) log(n ? k) vertices ouside
?
S. Nowwe check
 that the same solution satisfies KKT conditions of G(n, 1/2, k) with an error of
q
log n
 = O
. Using the same arguments as in the proof of Theorem 3.1, we conclude that
n
?
?
?(K) ? 2(t + 1) n + O(log n). Since ?(G) = 2t n for this case [8], the result follows.
The above theorem suggests that the planted independent set can be recovered by taking the top k
values in the optimal solution. In the experimental section we will discuss the performance of this
recovery algorithm. The runtime of this algorithm is one call to SVM solver, which is considerably
cheaper than the SDP option. Indeed the algorithm due to [8], requires computation of ? function.
The current best known algorithm for ? computation has an O(n5 log n)[5], run time complexity. In
contrast the proposed approach needs to solve an SVM and hence scales well to large graphs. Our
approach is competitive with the state of the art [24] as it gives the same high probability guarantees
and have the same running time, O(n2 ). Here we have assumed that we are working with a SVM
solver which has a time complexity of O(n2 ) [13].

6

Experimental evaluation

Comparison with exhaustive approach [14] We generate synthetic m = 3 random graphs over
?
n vertices with average density ? = 0.2, and having single (common) quasi-clique of size k = 2 n
with density ? = 0.95 in all the three graphs. This is similar to the synthetic graphs generated
in the original paper [see 14, Section 6.1.2]. We note that both our MKL-based approach and
exhaustive search in [14] recovers the quasi-clique. However, the time requirements are drastically
different. All experiments were conducted on a computer with 16 GB RAM and Intel X3470 quadcore processor running at 2.93 GHz. Three values of k namely k = 50, 60 and k = 100 were used.
It is interesting to note that CROCHET [14] took 2 hours and 9 hours for k = 50 and k = 60 sized
cliques and failed to find a clique of size of 100. The corresponding numbers for MKL are 47.5,54.8
and 137.6 seconds respectively.
7

Common dense subgraph detection We evaluate our algorithm for finding large dense regions on
the DIMACS Challenge graphs 2 [15], which is a comprehensive benchmark for testing of clique
finding and related algorithms. For the families of dense graphs (brock, san, sanr), we focus on
finding large dense region in the complement of the original graphs.
We run Algorithm 1 using SimpleMkl3 to find large common dense subgraph. In order to evaluate the performance of our algorithm, we compute a
? = maxm a(m) and a = minm a(m) where
(m)
(m)
(m)
a
= ?(GT )/?(G ) is relative density of induced subgraph (compared to original graph density); and nT /N is relative size of induced subgraph compared to original graph size. We want
a high value of nT /N ; while a should not be lower than 1. Table 1 shows evaluation of Algorithm 1 on DIMACS dataset. We note that our algorithm finds a large subgraph (large nT /N )
with higher density compared to original graph in all of DIMACS graph classes making it suitable for finding large dense regions in multiple graphs. In all cases the size of the subgraph, nT
was more than 100. The MKL experiments reported in Table 1 took less than 1 minute (for each
graph family); while the algorithm in [14] aborts after several hours due to memory constraints.

Planted clique recovery We generate 100
random graphs based on planted clique
model G(n, 1/2, k) where
? n = 30000 and
hidden clique size k = 2t n for each choice
of t. We evaluate the recovery algorithm
discussed in Section 4.2. The SVM problem is solved using Libsvm4 . For t ? 2 we
find perfect recovery of the clique on all the
graphs, which is agreement with Theorem
5.1.
It is worth noting that the approach takes 10
minutes to recover the clique in this graph of
30000 vertices which is far beyond the scope
of SDP based procedures.

Graph family
c-fat200
c-fat500
brock200?
brock400?
brock800?
p hat300
p hat500
p hat700
p hat1000
p hat1500
san200?
san400?
sanr200?
sanr400?

N
200
500
200
400
800
300
500
700
1000
1500
200
400
200
400

M
3
4
4
4
4
3
3
3
3
3
5
3
2
2

nT
N

0.50
0.31
0.41
0.50
0.50
0.53
0.48
0.45
0.43
0.38
0.50
0.42
0.39
0.43

a
?
2.12
3.57
1.36
1.15
1.08
1.53
1.55
1.58
1.60
1.63
1.51
1.19
1.86
1.20

a
0.99
1.01
0.99
1.05
1.01
1.15
1.17
1.18
1.19
1.20
1.08
1.02
1.04
1.02

Table 1: Common dense subgraph recovery on multiple graphs in DIMACS dataset. Here a
? and a denote
the maximum and minimum relative density of the
In this paper we have established that the induced subgraph (relative to density of the original
Lov?asz ? function, well studied in graph the- graph) and nT /N is the relative size of the induced
ory can be linked to the one-class SVM for- subgraph compared to original graph size.
mulation. This link allows us to design scalable algorithms for computationally difficult
problems. In particular we have demonstrated that finding a common dense region
in multiple graphs can be solved by a MKL problem, while finding a large planted clique can be
solved by an one class SVM.

7

Conclusion

Acknowledgements
CB is grateful to Department of CSE, Chalmers University of Technology for their hospitality and
was supported by grants from ICT and Transport Areas of Advance, Chalmers University. VJ and
DD were supported by SSF grant Data Driven Secure Business Intelligence.

2

ftp://dimacs.rutgers.edu/pub/challenge/graph/benchmarks/clique/
http://asi.insa-rouen.fr/enseignants/?arakotom/code/mklindex.html
4
http://www.csie.ntu.edu.tw/?cjlin/libsvm/

3

8

References
[1] Louigi Addario-berry, Nicolas Broutin, Gbor Lugosi, and Luc Devroye. Combinatorial testing
problems. Annals of Statistics, 38:3063?3092, 2010.
[2] Noga Alon, Michael Krivelevich, and Benny Sudakov. Finding a large hidden clique in a
random graph. Random Structures and Algorithms, pages 457?466, 1998.
[3] B. Bollob?as. Modern graph theory, volume 184. Springer Verlag, 1998.
[4] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press,
New York, NY, USA, 2004.
[5] T.-H. Hubert Chan, Kevin L. Chang, and Rajiv Raman. An sdp primal-dual algorithm for
approximating the lov?asz-theta function. In ISIT, 2009.
[6] Amin Coja-Oghlan and Anusch Taraz. Exact and approximative algorithms for coloring g(n,
p). Random Struct. Algorithms, 24(3):259?278, 2004.
[7] U. Feige and D. Ron. Finding hidden cliques in linear time. In AofA10, 2010.
[8] Uriel Feige and Robert Krauthgamer. Finding and certifying a large hidden clique in a semirandom graph. Random Struct. Algorithms, 16:195?208, March 2000.
[9] Z. F?uredi and J. Koml?os. The eigenvalues of random symmetric matrices. Combinatorica,
1:233?241, 1981.
[10] Michel X. Goemans. Semidefinite programming in combinatorial optimization. Math. Program., 79:143?161, 1997.
[11] J. H?astad. Clique is hard to approximate within n1?? . Acta Mathematica, 182(1):105?142,
1999.
[12] Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, 1990.
[13] Don R. Hush, Patrick Kelly, Clint Scovel, and Ingo Steinwart. Qp algorithms with guaranteed
accuracy and run time for support vector machines. Journal of Machine Learning Research,
7:733?769, 2006.
[14] D. Jiang and J. Pei. Mining frequent cross-graph quasi-cliques. ACM Transactions on Knowledge Discovery from Data (TKDD), 2(4):16, 2009.
[15] D.S. Johnson and M.A. Trick. Cliques, coloring, and satisfiability: second DIMACS implementation challenge, October 11-13, 1993, volume 26. Amer Mathematical Society, 1996.
[16] Donald Knuth. The sandwich theorem. Electronic Journal of Combinatorics, 1(A1), 1994.
[17] Michael Krivelevich and Benny Sudakov. Pseudo-random graphs. In More Sets, Graphs and
Numbers, volume 15 of Bolyai Society Mathematical Studies, pages 199?262. Springer Berlin
Heidelberg, 2006.
[18] V.E. Lee, N. Ruan, R. Jin, and C. Aggarwal. A survey of algorithms for dense subgraph
discovery. Managing and Mining Graph Data, pages 303?336, 2010.
[19] L. Lovasz. On the Shannon capacity of a graph. Information Theory, IEEE Transactions on,
25(1):1?7, 1979.
[20] C.J. Luz and A. Schrijver. A convex quadratic characterization of the lov?asz theta number.
SIAM Journal on Discrete Mathematics, 19(2):382?387, 2006.
[21] Claire Mathieu and Warren Schudy. Correlation clustering with noisy input. In Proceedings
of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ?10, pages
712?728, Philadelphia, PA, USA, 2010. Society for Industrial and Applied Mathematics.
[22] P. Pardalos and S. Rebennack. Computational challenges with cliques, quasi-cliques and clique
partitions in graphs. Experimental Algorithms, pages 13?22, 2010.
[23] V. Spirin and L.A. Mirny. Protein complexes and functional modules in molecular networks.
Proceedings of the National Academy of Sciences, 100(21):12123, 2003.
[24] Dekel Yael, Gurel-Gurevich Ori, and Peres Yuval. Finding hidden cliques in linear time with
high probability. In ANALCO11, 2011.

9

"
2012,Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions,,4569-efficient-monte-carlo-counterfactual-regret-minimization-in-games-with-many-player-actions.pdf,"Counterfactual Regret Minimization (CFR) is a popular, iterative algorithm for computing strategies in extensive-form games. The Monte Carlo CFR (MCCFR) variants reduce the per iteration time cost of CFR by traversing a sampled portion of the tree. The previous most effective instances of MCCFR can still be very slow in games with many player actions since they sample every action for a given player. In this paper, we present a new MCCFR algorithm, Average Strategy Sampling (AS), that samples a subset of the player's actions according to the player's average strategy. Our new algorithm is inspired by a new, tighter bound on the number of iterations required by CFR to converge to a given solution quality. In addition, we prove a similar, tighter bound for AS and other popular MCCFR variants. Finally, we validate our work by demonstrating that AS converges faster than previous MCCFR algorithms in both no-limit poker and Bluff.","Efficient Monte Carlo Counterfactual Regret
Minimization in Games with Many Player Actions

Richard Gibson, Neil Burch, Marc Lanctot, and Duane Szafron
Department of Computing Science, University of Alberta
Edmonton, Alberta, T6G 2E8, Canada
{rggibson | nburch | lanctot | dszafron}@ualberta.ca

Abstract
Counterfactual Regret Minimization (CFR) is a popular, iterative algorithm for
computing strategies in extensive-form games. The Monte Carlo CFR (MCCFR)
variants reduce the per iteration time cost of CFR by traversing a smaller, sampled
portion of the tree. The previous most effective instances of MCCFR can still be
very slow in games with many player actions since they sample every action for a
given player. In this paper, we present a new MCCFR algorithm, Average Strategy Sampling (AS), that samples a subset of the player?s actions according to the
player?s average strategy. Our new algorithm is inspired by a new, tighter bound on
the number of iterations required by CFR to converge to a given solution quality.
In addition, we prove a similar, tighter bound for AS and other popular MCCFR
variants. Finally, we validate our work by demonstrating that AS converges faster
than previous MCCFR algorithms in both no-limit poker and Bluff.

1

Introduction

An extensive-form game is a common formalism used to model sequential decision making problems containing multiple agents, imperfect information, and chance events. A typical solution concept in games is a Nash equilibrium profile. Counterfactual Regret Minimization (CFR) [12] is an
iterative algorithm that, in 2-player zero-sum extensive-form games, converges to a Nash equilibrium. Other techniques for computing Nash equilibria of 2-player zero-sum games include linear
programming [8] and the Excessive Gap Technique [6]. Theoretical results indicate that for a fixed
solution quality, CFR takes a number of iterations at most quadratic in the size of the game [12, Theorem 4]. Thus, as we consider larger games, more iterations are required to obtain a fixed solution
quality. Nonetheless, CFR?s versatility and memory efficiency make it a popular choice.
Monte Carlo CFR (MCCFR) [9] can be used to reduce the traversal time per iteration by considering
only a sampled portion of the game tree. For example, Chance Sampling (CS) [12] is an instance of
MCCFR that only traverses the portion of the game tree corresponding to a single, sampled sequence
of chance?s actions. However, in games where a player has many possible actions, such as no-limit
poker, iterations of CS are still very time consuming. This is because CS considers all possible
player actions, even if many actions are poor or only factor little into the algorithm?s computation.
Our main contribution in this paper is a new MCCFR algorithm that samples player actions and is
suitable for games involving many player choices. Firstly, we provide tighter theoretical bounds on
the number of iterations required by CFR and previous MCCFR algorithms to reach a fixed solution
quality. Secondly, we use these new bounds to propel our new MCCFR sampling algorithm. By
using a player?s average strategy to sample actions, convergence time is significantly reduced in
large games with many player actions. We prove convergence and show that our new algorithm
approaches equilibrium faster than previous sampling schemes in both no-limit poker and Bluff.
1

2

Background

A finite extensive game contains a game tree with nodes corresponding to histories of actions h ? H
and edges corresponding to actions a ? A(h) available to player P (h) ? N ? {c} (where N is the
set of players and c denotes chance). When P (h) = c, ?c (h, a) is the (fixed) probability of chance
generating action a at h. Each terminal history z ? Z has associated utilities ui (z) for each player
i. We define ?i = maxz,z0 ?Z ui (z) ? ui (z 0 ) to be the range of utilities for player i. Non-terminal
histories are partitioned into information sets I ? Ii representing the different game states that
player i cannot distinguish between. For example, in poker, player i does not see the private cards
dealt to the opponents, and thus all histories differing only in the private cards of the opponents are
in the same information set for player i. The action sets A(h) must be identical for all h ? I, and we
denote this set by A(I). We define |Ai | = maxI?Ii |A(I)| to be the maximum number of actions
available to player i at any information set. We assume perfect recall that guarantees players always
remember information that was revealed to them and the order in which it was revealed.
A (behavioral) strategy for player i, ?i ? ?i , is a function that maps each information set I ? Ii to
a probability distribution over A(I). A strategy profile is a vector of strategies ? = (?1 , ..., ?|N | ) ?
?, one for each player. Let ui (?) denote the expected utility for player i, given that all players play
according to ?. We let ??i refer to the strategies in ? excluding ?i . Let ? ? (h) be the probability
?
of
Q history h ?occurring if all? players choose actions according to ?. We can decompose ? (h) =
i?N ?{c} ?i (h), where ?i (h) is the contribution to this probability from player i when playing
?
according to ?i (or from chance when i = c). Let ??i
(h) be the product of all players? contributions
?
(including chance) except that of player i. Let ? (h, h0 ) be the probability of history h0 occurring
after h, given h has occurred. Furthermore, for I ? Ii , the probability of player i playing to reach I
is ?i? (I) = ?i? (h) for any h ? I, which is well-defined due to perfect recall.
A best response to ??i is a strategy that maximizes player i?s expected payoff against ??i . The
best response value for player i is the value of that strategy, bi (??i ) = max?i0 ??i ui (?i0 , ??i ). A
strategy profile ? is an -Nash equilibrium if no player can unilaterally deviate from ? and gain
more than ; i.e., ui (?) +  ? bi (??i ) for all i ? N . A game is two-player zero-sum if N = {1, 2}
and u1 (z) = ?u2 (z) for all z ? Z. In this case, the exploitability of ?, e(?) = (b1 (?2 )+b2 (?1 ))/2,
measures how much ? loses to a worst case opponent when players alternate positions. A 0-Nash
equilibrium (or simply a Nash equilibrium) has zero exploitability.
Counterfactual Regret Minimization (CFR) [12] is an iterative algorithm that, for two-player zero
sum games, computes an -Nash equilibrium profile with  ? 0. CFR has also been shown to work
well in games with more than two players [1, 3]. On each iteration t, the base algorithm, ?vanilla?
CFR, traverses the entire game tree once per player, computing the expected utility for player i at
each information set I ? Ii under the current profile ? t , assuming
to reach I. This
P player i plays
?
expectation is the counterfactual value for player i, vi (I, ?) = z?ZI ui (z)??i
(z[I])? ? (z[I], z),
where ZI is the set of terminal histories passing through I and z[I] is that history along z contained
in I. For each action a ? A(I), these values determine the counterfactual regret at iteration t,
t
rit (I, a) = vi (I, ?(I?a)
) ? vi (I, ? t ),

where ?(I?a) is the profile ? except that at I, action a is always taken. The regret rit (I, a) measures
how much player i would rather play action a at I than play ? t . These regrets are accumulated to
PT
obtain the cumulative counterfactual regret, RiT (I, a) = t=1 rit (I, a), and are used to update
the current strategy profile via regret matching [5, 12],
? T +1 (I, a) = P

RiT,+ (I, a)

b?A(I)

RiT,+ (I, b)

,

(1)

where x+ = max{x, 0} and actions are chosen uniformly at random when the denominator is zero.
It is well-known that in a two-player zero-sum game, if both players? average (external) regret,
T

1X
RiT
t
t
= max
ui (?i0 , ??i
) ? ui (?it , ??i
) ,
0
?i ??i T
T
t=1

is at most /2, then the average profile ?
? T is an -Nash equilibrium. During computation, CFR
PT
T
?t
t
stores a cumulative profile si (I, a) =
t=1 ?i (I)?i (I, a) and outputs the average profile
2

?
?iT (I, a) = sTi (I, a)/

P

T
b?A(I) si (I, b).

The original CFR analysis shows that player i?s regret

is bounded by the sum of the positive parts of the cumulative counterfactual regrets RiT,+ (I, a):
Theorem 1 (Zinkevich et al. [12])
RiT ?

X
I?I

max RiT,+ (I, a).

a?A(I)

Regret matching minimizes the average of the cumulative counterfactual regrets, and so player i?s
average regret is also minimized by Theorem 1. For each player i, let Bi be the partition of Ii such
that two information sets I, I 0 are in the same part B ? Bi if and only if player i?s sequence of
actions leading to I is the same as the sequence of actions leading to I 0 . Bi is well-defined
p due to
P
perfect recall. Next, define the M -value of the game to player i to be Mi = B?Bi |B|. The
best known bound on player i?s average regret is:
Theorem 2 (Lanctot et al. [9]) When using vanilla CFR, average regret is bounded by
p
?i Mi |Ai |
RiT
?
?
.
T
T
We prove a tighter bound in Section 3. For large games, CFR?s full game tree traversals can be very
expensive. Alternatively, one can traverse a smaller, sampled portion of the tree on each iteration
using Monte Carlo CFR (MCCFR) [9]. Let Q = {Q1 , ..., QK } be a set of subsets, or blocks, of
the terminal histories Z such that the union of Q spans Z. For example, Chance Sampling (CS)
[12] is an instance of MCCFR that partitions Z into blocks such that two histories are in the same
block if and only if no two chance actions differ. On each iteration, a block Qj is sampled with
PK
probability qj , where k=1 qk = 1. In CS, we generate a block by sampling a single action a at
each history h ? H with P (h) = c according to its likelihood of occurring, ?c (h, a). In general, the
sampled counterfactual value for player i is
X
?
v?i (I, ?) =
ui (z)??i
(z[I])? ? (z[I], z)/q(z),
z?ZI ?Qj

P

where q(z) =
k:z?Qk qk is the probability that z was sampled. For example, in CS, q(z) =
?
t
?c (z). Define the sampled counterfactual regret for action a at I to be r?it (I, a) = v?i (I, ?(I?a)
)?
P
T
t
T
t
?
v?i (I, ? ). Strategies are then generated by applying regret matching to R (I, a) =
r? (I, a).
i

t=1 i

CS has been shown to significantly reduce computing time in poker games [11, Appendix A.5.2].
Other instances of MCCFR include External Sampling (ES) and Outcome Sampling (OS) [9].
ES takes CS one step further by considering only a single action for not only chance, but also for
t
the opponents, where opponent actions are sampled according to the current profile ??i
. OS is the
most extreme version of MCCFR that samples a single action at every history, walking just a single
trajectory through the tree on each traversal (Qj = {z}). ES and OS converge to equilibrium faster
than vanilla CFR in a number of different domains [9, Figure 1].
ES and OS yield a probabilistic bound on the average regret, and thus provide a probabilistic guarantee that ?
? T converges to a Nash equilibrium. Since
Q both algorithms generate blocks by sampling
actions independently, we can decompose q(z) = i?N ?{c} qi (z) so that qi (z) is the probability
contributed to q(z) by sampling player i?s actions.
Theorem 3 (Lanctot et al. [9]) 1 Let X be one of ES or OS (assuming OS also samples opponent
actions according to ??i ), let p ? (0, 1], and let ? = minz?Z qi (z) > 0 over all 1 ? t ? T . When
using X, with probability 1 ? p, average regret is bounded by
!  p
p
2|Ii ||Bi |
1 ?i |Ai |
RiT
?
? Mi +
.
?
T
p
?
T
1

The bound
p presented by Lanctot et al. appears slightly different, but the last step of their proof mistakenly
used Mi ? |Ii ||Bi |, which is actually incorrect. The bound we present here is correct.

3

3

New CFR Bounds

While Zinkevich et al. [12] bound a player?s regret by a sum of cumulative counterfactual regrets (Theorem 1), we can actually equate a player?s regret to a weighted sum of counterfactual
set I ? Ii , define RiT (I, ?i ) =
P regrets. For aT strategy ?i ? ?i and an information
?
In addition, let ?i ? ?i be a player i strategy such that
a?A(I) ?i (I, a)Ri (I, a).
PT
PT
t
?
t
?
0
). Note that in a two-player game,
?i = arg max?i ??i t=1 ui (?i0 , ??i
t=1 ui (?i , ??i ) =
?
T
?
T ui (?i , ?
??i ), and thus ?i is a best response to the opponent?s average strategy after T iterations.
Theorem 4

RiT =

X

?

?i? (I)RiT (I, ?i? ).

I?Ii

All proofs in this paper are provided in full as supplementary material. Theorem 4 leads to a tighter
bound on the average regret when
p using CFR. For a strategy ?i ? ?i , define the M -value of ?i to
P
be Mi (?i ) = B?Bi ?i? (B) |B|, where ?i? (B) = maxI?B ?i? (I). Clearly, Mi (?i ) ? Mi for all
?i ? ?i since ?i? (B) ? 1. For vanilla CFR, we can simply replace Mi in Theorem 2 with Mi (?i? ):
Theorem 5 When using vanilla CFR, average regret is bounded by
p
?i Mi (?i? ) |Ai |
RiT
?
.
?
T
T
For MCCFR, we can show a similar improvement to Theorem 3. Our proof includes a bound for CS
that appears to have been omitted in previous work. Details are in the supplementary material.
Theorem 6 Let X be one of CS, ES, or OS (assuming OS samples opponent actions according to
??i ), let p ? (0, 1], and let ? = minz?Z qi (z) > 0 over all 1 ? t ? T . When using X, with
probability 1 ? p, average regret is bounded by
!  p
p
2|Ii ||Bi |
1 ?i |Ai |
RiT
?
?
? Mi (?i ) +
.
?
T
p
?
T
Theorem 4 states that player i?s regret is equal to the weighted sum of player i?s counterfactual
regrets at each I ? Ii , where the weights are equal to player i?s probability of reaching I under ?i? .
Since our goal is to minimize average regret, this means that we only need to minimize the average
cumulative counterfactual regret at each I ? Ii that ?i? plays to reach. Therefore, when using
MCCFR, we may want to sample more often those information sets that ?i? plays to reach, and less
often those information sets that ?i? avoids. This inspires our new MCCFR sampling algorithm.

4

Average Strategy Sampling

Leveraging the theory developed in the previous section, we now introduce a new MCCFR sampling algorithm that can minimize average regret at a faster rate than CS, ES, and OS. As we just
described, we want our algorithm to sample more often the information sets that ?i? plays to reach.
Unfortunately, we do not have the exact strategy ?i? on hand. Recall that in a two-player game, ?i? is
T
a best response to the opponent?s average strategy, ?
??i
. However, for two-player zero-sum games,
T
we do know that the average profile ?
? converges to a Nash equilibrium. This means that player i?s
T
average strategy, ?
?iT , converges to a best response of ?
??i
. While the average strategy is not an exact
best response, it can be used as a heuristic to guide sampling within MCCFR. Our new sampling algorithm, Average Strategy Sampling (AS), selects actions for player i according to the cumulative
profile and three predefined parameters. AS can be seen as a sampling scheme between OS and ES
where a subset of player i?s actions are sampled at each information set I, as opposed to sampling
one action (OS) or sampling every action (ES). Given the cumulative profile sTi (I, ?) on iteration
T , an exploration parameter  ? (0, 1], a threshold parameter ? ? [1, ?), and a bonus parameter
? ? [0, ?), each of player i?s actions a ? A(I) are sampled independently with probability
(
)
? + ? sTi (I, a)
P
?(I, a) = max ,
,
(2)
? + b?A(I) sTi (I, b)
4

Algorithm 1 Average Strategy Sampling (Two-player version)
1: Require: Parameters , ?, ?
2: Initialize regret and cumulative profile: ?I, a : r(I, a) ? 0, s(I, a) ? 0
3:
4: WalkTree(history h, player i, sample prob q):
5:
if h ? Z then return ui (h)/q end if
6:
if h ? P (c) then Sample action a ? ?c (h, ?), return WalkTree(ha, i, q) end if
7:
I ? Information set containing h , ?(I, ?) ? RegretMatching(r(I, ?))
8:
if h ?
/ P (i) then
9:
for a ? A(I) do s(I, a) ? s(I, a) + (?(I, a)/q) end for
10:
Sample action a ? ?(I, ?), return WalkTree(ha, i, q)
11:
end if
12:
for a ? A(I) ndo
o
13:

?+? s(I,a)
? ? max , ?+P
?(a) ? 0
s(I,b) , v
b?A(I)

14:
15:
16:
17:

if Random(0, 1) < ? then v?(a) ? WalkTree(ha, i, q ? min{1, ?}) end if
end for
P
for a ? A(I) do r(I, a) ? r(I, a) + v?(a) ? a?A(I) ?(I, a)?
v (a) end for
P
return a?A(I) ?(I, a)?
v (a)

P
or with probability 1 if either ?(I, a) > 1 or ? + b?A(I) sTi (I, b) = 0. As in ES, at opponent and
T
chance nodes, a single action is sampled on-policy according to the current opponent profile ??i
and the fixed chance probabilities ?c respectively.
If ? = 1P
and ? = 0, then ?(I, a) is equal to the probability that the average strategy ?
?iT =
T
T
si (I, a)/ b?A(I) si (I, b) plays a at I, except that each action is sampled with probability at least
. For choices greater than 1, ? acts as a threshold so that any action taken with probability at least
1/? by the average strategy is always sampled by AS. Furthermore, ??s purpose is to increase the
rate of exploration during early AS iterations. When ? > 0, we effectively add ? as a bonus to the
cumulative value sTi (I, a) before normalizing. Since player i?s average strategy ?
?iT is not a good
?
approximation of ?i for small T , we include ? to avoid making ill-informed choices early-on. As
the cumulative profile sTi (I, ?) grows over time, ? eventually becomes negligible. In Section 5, we
present a set of values for , ? , and ? that work well across all of our test games.
Pseudocode for a two-player version of AS is presented in Algorithm 1. In Algorithm 1, the recursive
function WalkTree considers four different cases. Firstly, if we have reached a terminal node, we
return the utility scaled by 1/q (line 5), where q = qi (z) is the probability of sampling z contributed
from player i?s actions. Secondly, when at a chance node, we sample a single action according to ?c
and recurse down that action (line 6). Thirdly, at an opponent?s choice node (lines 8 to 11), we again
sample a single action and recurse, this time according to the opponent?s current strategy obtained
via regret matching (equation (1)). At opponent nodes, we also update the cumulative profile (line
9) for reasons that we describe in a previous paper [2, Algorithm 1]. For games with more than two
players, a second tree walk is required and we omit these details.
The final case in Algorithm 1 handles choice nodes for player i (lines 7 to 17). For each action a, we
compute the probability ? of sampling a and stochastically decide whether to sample a or not, where
Random(0,1) returns a random real number in [0, 1). If we do sample a, then we recurse to obtain
t
the sampled counterfactual value v?(a) = v?i (I, ?(I?a)
) (line 14). Finally, we update the regrets at I
P
(line 16) and return the sampled counterfactual value at I, a?A(I) ?(I, a)?
v (a) = v?i (I, ? t ).
Repeatedly running WalkTree(?, i, 1) ?i ? N provides a probabilistic guarantee that all players?
average regret will be minimized. In the supplementary material, we prove that AS exhibits the same
regret bound as CS, ES, and OS provided in Theorem 6. Note that ? in Theorem 6 is guaranteed
to be positive for AS by the inclusion of  in equation (2). However, for CS and ES, ? = 1 since
all of player i?s actions are sampled, whereas ? ? 1 for OS and AS. While this suggests that fewer
iterations of CS or ES are required to achieve the same regret bound compared to OS and AS,
iterations for OS and AS are faster as they traverse less of the game tree. Just as CS, ES, and OS
5

have been shown to benefit from this trade-off over vanilla CFR, we will show that in practice, AS
can likewise benefit over CS and ES and that AS is a better choice than OS.

5

Experiments

In this section, we compare the convergence rates of AS to those of CS, ES, and OS. While AS can
be applied to any extensive game, the aim of AS is to provide faster convergence rates in games
involving many player actions. Thus, we consider two domains, no-limit poker and Bluff, where we
can easily scale the number of actions available to the players.
No-limit poker. The two-player poker game we consider here, which we call 2-NL Hold?em(k),
is inspired by no-limit Texas Hold?em. 2-NL Hold?em(k) is played over two betting rounds. Each
player starts with a stack of k chips. To begin play, the player denoted as the dealer posts a small
blind of one chip and the other player posts a big blind of two chips. Each player is then dealt two
private cards from a standard 52-card deck and the first betting round begins. During each betting
round, players can either fold (forfeit the game), call (match the previous bet), or raise by any
number of chips in their remaining stack (increase the previous bet), as long as the raise is at least as
big as the previous bet. After the first betting round, three public community cards are revealed (the
flop) and a second and final betting round begins. If a player has no more chips left after a call or a
raise, that player is said to be all-in. At the end of the second betting round, if neither player folded,
then the player with the highest ranked five-card poker hand wins all of the chips played. Note that
the number of player actions in 2-NL Hold?em(k) at one information set is at most the starting stack
size, k. Increasing k adds more betting options and allows for more actions before being all-in.
Bluff. Bluff(D1 , D2 ) [7], also known as Liar?s Dice, Perduo, and Dudo, is a two-player dice-bidding
game played with six-sided dice over a number of rounds. Each player i starts with Di dice. In each
round, players roll their dice and look at the result without showing their opponent. Then, players
alternate by bidding a quantity q of a face value f of all dice in play until one player claims that
the other is bluffing (i.e., claims that the bid does not hold). To place a new bid, a player must
increase q or f of the current bid. A face value of six is considered ?wild? and counts as any other
face value. The player calling bluff wins the round if the opponent?s last bid is incorrect, and loses
otherwise. The losing player removes one of their dice from the game and a new round begins.
Once a player has no more dice left, that player loses the game and receives a utility of ?1, while
the winning player earns +1 utility. The maximum number of player actions at an information set
is 6(D1 + D2 ) + 1 as increasing Di allows both players to bid higher quantities q.
Preliminary tests. Before comparing AS to CS, ES, and OS, we first run some preliminary experiments to find a good set of parameter values for , ? , and ? to use with AS. All of our preliminary
experiments are in two-player 2-NL Hold?em(k). In poker, a common approach is to create an abstract game by merging similar card dealings together into a single chance action or ?bucket? [4]. To
keep the size of our games manageable, we employ a five-bucket abstraction that reduces the branching factor at each chance node down to five, where dealings are grouped according to expected hand
strength squared as described by Zinkevich et al. [12].
Firstly, we fix ? = 1000 and test different values for  and ? in 2-NL Hold?em(30). Recall that
? = 1000 implies actions taken by the average strategy with probability at least 0.001 are always
sampled by AS. Figure 1a shows the exploitability in the five-bucket abstract game, measured in
milli-big-blinds per game (mbb/g), of the profile produced by AS after 1012 nodes visited. Recall
that lower exploitability implies a closer approximation to equilibrium. Each data point is averaged
over five runs of AS. The  = 0.05 and ? = 105 or 106 profiles are the least exploitable profiles
within statistical noise (not shown).
Next, we fix  = 0.05 and ? = 106 and test different values for ? . Figure 1b shows the abstract
game exploitability over the number of nodes visited by AS in 2-NL Hold?em(30), where again each
data point is averaged over five runs. Here, the least exploitable strategies after 1012 nodes visited
are obtained with ? = 100 and ? = 1000 (again within statistical noise). Similar results to Figure
1b hold in 2-NL Hold?em(40) and are not shown. Throughout the remainder of our experiments, we
use the fixed set of parameters  = 0.05, ? = 106 , and ? = 1000 for AS.
6

Exploitability (mbb/g)
1

0.5
0.4
0.3
? 0.2
0.1
0.05
0.01

0.8
0.6
0.4
0.2
0
100 101 102 103 104 105 106 107 108 109
?

Abstract game exploitability (mbb/g)

102

1

10

0

100

?=101
?=10
?=1023
?=104
?=10
5
?=106
?=10

10-1 10
10

11

10
Nodes Visited

12

10

(b)  = 0.05, ? = 106

(a) ? = 1000

Figure 1: (a) Abstract game exploitability of AS profiles for ? = 1000 after 1012 nodes visited
in 2-NL Hold?em(30). (b) Log-log plot of abstract game exploitability over the number of nodes
visited by AS with  = 0.05 and ? = 106 in 2-NL Hold?em(30). For both figures, units are in
milli-big-blinds per hand (mbb/g) and data points are averaged over five runs with different random
seeds. Error bars in (b) indicate 95% confidence intervals.

Main results. We now compare AS to CS, ES, and OS in both 2-NL Hold?em(k) and Bluff(D1 , D2 ).
Similar to Lanctot et al. [9], our OS implementation is -greedy so that the current player i samples
a single action at random with probability  = 0.5, and otherwise samples a single action according
to the current strategy ?i .
Firstly, we consider two-player 2-NL Hold?em(k) with starting stacks of k = 20, 22, 24, ..., 38,
and 40 chips, for a total of eleven different 2-NL Hold?em(k) games. Again, we apply the same
five-bucket card abstraction as before to keep the games reasonably sized. For each game, we ran
each of CS, ES, OS, and AS five times, measured the abstract game exploitability at a number of
checkpoints, and averaged the results. Figure 2a displays the results for 2-NL Hold?em(36), a game
with approximately 68 million information sets and 5 billion histories (nodes). Here, AS achieved
an improvement of 54% over ES at the final data points. In addition, Figure 2b shows the average
exploitability in each of the eleven games after approximately 3.16 ? 1012 nodes visited by CS, ES,
and AS. OS performed much worse and is not shown. Since one can lose more as the starting stacks
are increased (i.e., ?i becomes larger), we ?normalized? exploitability across each game by dividing
the units on the y-axis by k. While there is little difference between the algorithms for the smaller
20 and 22 chip games, we see a significant benefit to using AS over CS and ES for the larger games
that contain many player actions. For the most part, the margins between AS, CS, and ES increase
with the game size.
Figure 3 displays similar results for Bluff(1, 1) and Bluff(2, 1), which contain over 24 thousand and
3.5 million information sets, and 294 thousand and 66 million histories (nodes) respectively. Again,
AS converged faster than CS, ES, and OS in both Bluff games tested. Note that the same choices
of parameters ( = 0.05, ? = 106 , ? = 1000) that worked well in 2-NL Hold?em(30) also worked
well in other 2-NL Hold?em(k) games and in Bluff(D1 , D2 ).

6

Conclusion

This work has established a number of improvements for computing strategies in extensive-form
games with CFR, both theoretically and empirically. We have provided new, tighter bounds on the
average regret when using vanilla CFR or one of several different MCCFR sampling algorithms.
These bounds were derived by showing that a player?s regret is equal to a weighted sum of the
player?s cumulative counterfactual regrets (Theorem 4), where the weights are given by a best response to the opponents? previous sequence of strategies. We then used this bound as inspiration for
our new MCCFR algorithm, AS. By sampling a subset of a player?s actions, AS can provide faster
7

3

10

102
101
100
-1

10

CS
ES
OS
AS

1010

1011
Nodes Visited

0.16

Abstract game exploitability (mbb/g) / k

Abstract game exploitability (mbb/g)

104

1012

0.14
0.12

CS
ES
AS

k=40

0.1
0.08
0.06
0.04

k=30

0.02 k=20
0

106

107
108
Game size (# information sets)

(b) 2-NL Hold?em(k), k ? {20, 22, ..., 40}

(a) 2-NL Hold?em(36)

100

100

10-1

10-1

10-2

10-2

Exploitability

Exploitability

Figure 2: (a) Log-log plot of abstract game exploitability over the number of nodes visited by CS,
ES, OS, and AS in 2-NL Hold?em(36). The initial uniform random profile is exploitable for 6793
mbb/g, as indicated by the black dashed line. (b) Abstract game exploitability after approximately
3.16 ? 1012 nodes visited over the game size for 2-NL Hold?em(k) with even-sized starting stacks
k between 20 and 40 chips. For both graphs, units are in milli-big-blinds per hand (mbb/g) and data
points are averaged over five runs with different random seeds. Error bars indicate 95% confidence
intervals. For (b), units on the y-axis are normalized by dividing by the starting chip stacks.

-3

10

10-4

CS
ES
OS
AS

10-5 7
10

108

109
1010 1011
Nodes Visited

1012

1013

(a) Bluff(1, 1)

-3

10

10-4

CS
ES
OS
AS

10-5 7
10

108

109
1010 1011
Nodes Visited

1012

1013

(b) Bluff(2, 1)

Figure 3: Log-log plots of exploitability over number of nodes visited by CS, ES, OS, and AS in
Bluff(1, 1) and Bluff(2, 1). The initial uniform random profile is exploitable for 0.780 and 0.784
in Bluff(1, 1) and Bluff(2, 1) respectively, as indicated by the black dashed lines. Data points are
averaged over five runs with different random seeds and error bars indicate 95% confidence intervals.

convergence rates in games containing many player actions. AS converged faster than previous MCCFR algorithms in all of our test games. For future work, we would like to apply AS to games with
many player actions and with more than two players. All of our theory still applies, except that
player i?s average strategy is no longer guaranteed to converge to ?i? . Nonetheless, AS may still find
strong strategies faster than CS and ES when it is too expensive to sample all of a player?s actions.

Acknowledgments
We thank the members of the Computer Poker Research Group at the University of Alberta for helpful conversations pertaining to this work. This research was supported by NSERC, Alberta Innovates
? Technology Futures, and computing resources provided by WestGrid and Compute Canada.
8

References
[1] Nick Abou Risk and Duane Szafron. Using counterfactual regret minimization to create competitive multiplayer poker agents. In Ninth International Conference on Autonomous Agents
and Multiagent Systems (AAMAS), pages 159?166, 2010.
[2] Richard Gibson, Marc Lanctot, Neil Burch, Duane Szafron, and Michael Bowling. Generalized
sampling and variance in counterfactual regret minimization. In Twenty-Sixth Conference on
Artificial Intelligence (AAAI), pages 1355?1361, 2012.
[3] Richard Gibson and Duane Szafron. On strategy stitching in large extensive form multiplayer
games. In Advances in Neural Information Processing Systems 24 (NIPS), pages 100?108,
2011.
[4] Andrew Gilpin and Tuomas Sandholm. A competitive Texas Hold?em poker player via automated abstraction and real-time equilibrium computation. In Twenty-First Conference on
Artificial Intelligence (AAAI), pages 1007?1013, 2006.
[5] Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. Econometrica, 68:1127?1150, 2000.
[6] Samid Hoda, Andrew Gilpin, Javier Pe?na, and Tuomas Sandholm. Smoothing techniques
for computing Nash equilibria of sequential games. Mathematics of Operations Research,
35(2):494?512, 2010.
[7] Reiner Knizia. Dice Games Properly Explained. Blue Terrier Press, 2010.
[8] Daphne Koller, Nimrod Megiddo, and Bernhard von Stengel. Fast algorithms for finding
randomized strategies in game trees. In Annual ACM Symposium on Theory of Computing
(STOC?94), pages 750?759, 1994.
[9] Marc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael Bowling. Monte Carlo sampling
for regret minimization in extensive games. In Advances in Neural Information Processing
Systems 22 (NIPS), pages 1078?1086, 2009.
[10] Marc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael Bowling. Monte Carlo sampling
for regret minimization in extensive games. Technical Report TR09-15, University of Alberta,
2009.
[11] Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization in games with incomplete information. Technical Report TR07-14, University of
Alberta, 2007.
[12] Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization in games with incomplete information. In Advances in Neural Information Processing
Systems 20 (NIPS), pages 905?912, 2008.

9

"
2009,Strategy Grafting in Extensive Games,,3634-strategy-grafting-in-extensive-games.pdf,"Extensive games are often used to model the interactions of multiple agents within an environment.  Much recent work has focused on increasing the size of an extensive game that can be feasibly solved.  Despite these improvements, many interesting games are still too large for such techniques.  A common approach for computing strategies in these large games is to first employ an abstraction technique to reduce the original game to an abstract game that is of a manageable size.  This abstract game is then solved and the resulting strategy is used in the original game.  Most top programs in recent AAAI Computer Poker Competitions use this approach.  The trend in this competition has been that strategies found in larger abstract games tend to beat strategies found in smaller abstract games.  These larger abstract games have more expressive strategy spaces and therefore contain better strategies.  In this paper we present a new method for computing strategies in large games.  This method allows us to compute more expressive strategies without increasing the size of abstract games that we are required to solve.  We demonstrate the power of the approach experimentally in both small and large games, while also providing a theoretical justification for the resulting improvement.","Strategy Grafting in Extensive Games

Kevin Waugh
waugh@cs.cmu.edu
Department of Computer Science
Carnegie Mellon University

Nolan Bard, Michael Bowling
{nolan,bowling}@cs.ualberta.ca
Department of Computing Science
University of Alberta

Abstract
Extensive games are often used to model the interactions of multiple agents within
an environment. Much recent work has focused on increasing the size of an extensive game that can be feasibly solved. Despite these improvements, many interesting games are still too large for such techniques. A common approach for
computing strategies in these large games is to first employ an abstraction technique to reduce the original game to an abstract game that is of a manageable size.
This abstract game is then solved and the resulting strategy is played in the original
game. Most top programs in recent AAAI Computer Poker Competitions use this
approach. The trend in this competition has been that strategies found in larger abstract games tend to beat strategies found in smaller abstract games. These larger
abstract games have more expressive strategy spaces and therefore contain better
strategies. In this paper we present a new method for computing strategies in large
games. This method allows us to compute more expressive strategies without increasing the size of abstract games that we are required to solve. We demonstrate
the power of the approach experimentally in both small and large games, while
also providing a theoretical justification for the resulting improvement.

1

Introduction

Extensive games provide a general model for describing the interactions of multiple agents within an
environment. They subsume other sequential decision making models such as finite horizon MDPs,
finite horizon POMDPs, and multiagent scenarios such as stochastic games. This makes extensive
games a powerful tool for representing a variety of complex situations. Moreover, it means that techniques for computing strategies in extensive games are a valuable commodity that can be applied
in many different domains. The usefulness of the extensive game model is dependent on the availability of solution techniques that scale well with respect to the size of the model. Recent research,
particularly motivated by the domain of poker, has made significant developments in scalable solution techniques. The classic linear programming techniques [5] can solve games with approximately
107 states [1], while more recent techniques [2, 9] can solve games with over 1012 states.
Despite the improvements in solution techniques for extensive games, even the motivating domain of
two-player limit Texas Hold?em is far too large to solve, as the game has approximately 1018 states.
The typical solution to this challenge is abstraction [1]. Abstraction involves constructing a new
game that is tractably sized for current solution techniques, but restricts the information or actions
available to the players. The hope is that the abstract game preserves the important strategic structure
of the game, and so playing a near equilibrium solution of the abstract game will still perform well in
the original game. In poker, employed abstractions include limiting the possible betting sequences,
replacing all betting in the first round with a fixed policy [1], and, most commonly, by grouping the
cards dealt to each player into buckets based on a strength metric [4, 9].
With these improvements in solution techniques, larger abstract games have become tractable, and
therefore increasingly fine abstractions have been employed. Because a finer abstraction can rep1

resent players? information more accurately and provide a more expressive space of strategies, it is
generally assumed that a solution to a finer abstraction will produce stronger strategies for the original game than those computed using a coarser abstraction. Although this assumption is in general
not true [7], results from the AAAI Computer Poker Competition [10] have shown that it does often
hold: near equilibrium strategies with the largest expressive power tend to win the competition.
In this paper, we increase the expressive power of computable strategies without increasing the
size of game that can be feasibly solved. We do this by partitioning the game into tractably sized
sub-games called grafts, solving each independently, and then combining the solutions into a single
strategy. Unlike previous, subsequently abandoned, attempts to solve independent sub-games [1, 3],
the grafting approach uses a base strategy to ensure that the grafts will mesh well as a unit. In fact,
we prove that grafted strategies improve on near equilibrium base strategies. We also empirically
demonstrate this improvement both in a small poker game as well as limit Texas Hold?em.

2

Background

Informally, an extensive game is a game tree where a player cannot distinguish between two histories
that share the same information set. This means a past action, from either chance or another player,
is not completely observed, allowing one to model situations of imperfect information.
Definition 1 (Extensive Game) [6, p. 200] A finite extensive game with imperfect information is
denoted ? and has the following components:
? A finite set N of players.
? A finite set H of sequences, the possible histories of actions, such that the empty sequence
is in H and every prefix of a sequence in H is also in H. Z ? H are the terminal histories.
No sequence in Z is a strict prefix of any sequence in H. A(h) = {a : (h, a) ? H} are the
actions available after a non-terminal history h ? H \ Z.
? A player function P that assigns to each non-terminal history a member of N ? {c}, where
c represents chance. P (h) is the player who takes an action after the history h. Let Hi be
the set of histories where player i chooses the next action.
? A function fc that associates with every history h ? Hc a probability distribution fc (?|h)
on A(h). fc (a|h) is the probability that a occurs given h.
? For each player i ? N , a utility function ui that assigns each terminal history a real value.
ui (z) is rewarded to player i for reaching terminal history z. If N = {1, 2} and for all
z ? Z, u1 (z) = ?u2 (z), an extensive game is said to be zero-sum.
? For each player i ? N , a partition Ii of Hi with the property that A(h) = A(h0 ) whenever
h and h0 are in the same member of the partition. Ii is the information partition of player
i; a set Ii ? Ii is an information set of player i.
In this paper, we exclusively focus on two-player zero-sum games with perfect recall, which is a
restriction on the information partitions that excludes unrealistic situations where a player is forced
to forget her own past information or decisions.
To play an extensive game each player specifies a strategy. A strategy determines how a player
makes her decisions when confronted with a choice.
Definition 2 (Strategy) A strategy for player i, ?i , that assigns a probability distribution over A(h)
to each h ? Hi . This function is constrained so that ?i (h) = ?i (h0 ) whenever h and h0 are in the
same information set. A strategy is pure if no randomization is required. We denote ?i as the set of
all strategies for player i.
Definition 3 (Strategy Profile) A strategy profile in extensive game ? is a set of strategies, ? =
{?1 , . . . , ?n }, that contains one strategy for each player. We let ??i denote the set strategies for all
players except player i. We call the set of all strategy profiles ?.
When all players play according to a strategy profile, ?, we can define the expected utility of each
player as ui (?). Similarly, ui (?i , ??i ) is the expected utility of player i when all other players play
according to ??i and player i plays according to ?i .
The traditional solution concept for extensive games is the Nash equilibrium concept.
2

Definition 4 (Nash Equilibrium) A Nash equilibrium is a strategy profile ? where
?i ? N ??i0 ? ?i

ui (?i ) ? ui (?i0 , ??i )

(1)

An approximation of a Nash equilibrium or ?-Nash equilibrium is a strategy profile ? where
?i ? N ??i0 ? ?i

ui (?i ) + ? ? ui (?i0 , ??i )

(2)

A Nash (?-Nash) equilibrium is a strategy profile where no player can gain (more than ?) through
unilateral deviation. A Nash equilibrium exists in all extensive games. For zero-sum extensive
games with perfect recall we can efficiently compute an ?-Nash equilibrium using techniques such as
linear programming [5], counterfactual regret minimization [9] and the excessive gap technique [2].
In a zero-sum game we say it is optimal to play any strategy belonging to an equilibrium because
this guarantees the equilibrium player the highest expected utility in the worst case. Any deviation
from equilibrium by either player can be exploited by a knowledgeable opponent. In this sense we
can call computing an equilibrium in a zero-sum game solving the game.
Many games of interest are far too large to solve directly and abstraction is often employed to reduce
the game to one of a more manageable size. The abstract game is solved and the resulting strategy
is presumed to be strong in the original game. Abstraction can be achieved by merging information
sets together, restricting the actions a player can take from a given history, or a combination of both.



Definition 5 (Abstraction) [7] An abstraction for player i is a pair ?i = ?iI , ?iA , where,
? ?iI is a partition of Hi , defining a set of abstract information sets coarser1 than Ii , and
? ?iA is a function on histories where ?iA (h) ? A(h) and ?iA (h) = ?iA (h0 ) for all histories
h and h0 in the same abstract information set. We will call this the abstract action set.
The null abstraction for player i, is ?i = hIi , Ai. An abstraction ? is a set of abstractions ?i ,
one for each player. Finally, for any abstraction ?, the abstract game, ?? , is the extensive game
obtained from ? by replacing Ii with ?iI and A(h) with ?iA (h) when P (h) = i, for all i.
Strategies for abstract games are defined in the same manner as for unabstracted games. However,
the strategy must assign the same distribution to all histories in the same block of the abstraction?s
information partition, as well as assigning zero probability to actions not in the abstract action set.

3

Strategy Grafting

Though there is no guarantee that optimal strategies in abstract games are strong in the original
game [7], these strategies have empirically been shown to perform well against both other computers [9] and humans [1]. Currently, strong strategies are solved for in one single equilibrium
computation for a single abstract game. Advancement typically involves developing algorithmic improvements to equilibrium finding techniques in order to find solutions to yet larger abstract games.
It is simple to show that a strategy space must include at least as good, if not better, strategies than
a smaller space that it refines [7]. At first glance, this would seem to imply that a larger abstraction
would always be better, but upon closer inspection we see this depends on our method of selecting
a strategy from the space. In poker, when using arbitrary equilibrium strategies that are evaluated in
a tournament setting, this intuition empirically holds true.
One potentially important factor for the empirical evidence is the presence of dominated strategies
in the support of the abstract equilibrium strategies.
Definition 6 (Dominated Strategy) A dominated strategy for player i is a pure strategy, ?i , such
that there exists another strategy, ?i0 , where for all opponent strategies ??i ,
ui (?i0 , ??i ) ? ui (?i , ??i )

(3)

and the inequality must hold strictly for at least one opponent strategy.
1
Partition A is coarser than partition B, if and only if every set in B is a subset of some set in A, or
equivalently x and y are in the same set in A if x and y are in the same set in B.

3

This implies that a player can never benefit by playing a dominated strategy. When abstracting one
can, in effect, merge a dominated strategy in with a non-dominated strategy. In the abstract game,
this combined strategy might become part of an equilibrium and hence the abstract strategy would
make occasional mistakes. That is, abstraction does not necessarily preserve strategy domination.
As a result of their expressive power, finer abstractions may better preserve domination and thus can
result in less play of dominated strategies.
Decomposition is a natural approach for using larger strategy spaces without incurring additional
computational costs and indeed it has been employed toward this end. In extensive games with
imperfect information, though, straightforward decomposition can be problematic. One way that
equilibrium strategies guard against exploitation is information hiding, i.e., the equilibrium plays in
a fashion that hinders an opponent?s ability to effectively reconstruct the player?s private information. Independent solutions to a set of sub-games, though, may not ?mesh?, or hide information,
effectively as a whole. For example, an observant opponent might be able to determine which subgame is being played, which itself could be valuable information that could be exploited.
Armed with some intuition for why increasing the size of the strategy space may improve the quality
of the solution and why decomposition can be problematic, we will now begin describing the strategy
grafting algorithm and provide some theoretical results regarding the quality of grafted strategies.
First, we will explain how a game of imperfect information is formally divided into sub-games.
Definition 7 (Grafting Partition) G = {G0 , G1 , . . . , Gp } is a grafting partition for player i if
1. G is a partition of Hi ,
2. ?I ? Ii ?j ? {0, . . . , p} such that I ? Gj , and
3. ?j ? {1, . . . , p} if h is a prefix of h0 ? Hi and h ? Gj then h0 ? Gj ? G0 .
Using the elements of a grafting partition, we construct a set of sub-games. The solutions to these
sub-games are called grafts, and we can combine them naturally, since they are disjoint sets, into
one single grafted strategy.
Definition 8 (Grafted Strategy) Given a strategy ?i ? ?i and a grafting partition G for player i.
For j ? {1, . . . , p}, define ??i ,j to be an extensive game derived from the original game ? where
for all h ? Hi \ Gj , P (h) = c and fc (a|h) = ?i (h, a). That is, player i only controls her actions
for histories in Gj and is forced to play according to ?i elsewhere. Let the graft of Gj , ? ?,j , be an
-Nash equilibrium of the game ??i ,j . Finally, define the grafted strategy for player i ?i? as,

?i (h, a)
if h ? G0
?i? (h, a) =
?,j
?i (h, a) if h ? Gj
We will call ?i the base strategy and G the grafting partition for the grafted strategy ?i? .
There are a few key ideas to observe about grafted strategies that distinguish them from previous
sub-game decomposition methods. First, we start out with a base strategy for the player. This base
strategy can be constructed using current techniques for a tractably sized abstraction. It is important
that we use the same base strategy for all grafts, as it is the only information that is shared between
the grafts. Second, when we construct a graft, only the portion of the game that the graft plays is
allowed to vary for our player of interest. The actions over the remainder of the game are played
according to the base strategy. This allows us to refine the abstraction for that block of the grafting
partition, so that it itself is as large as the largest tractably solvable game. Third, note that when we
construct a graft, we continue to use an equilibrium finding technique, but we are not interested in
the pair of strategies ? we are only interested in the strategy for the player of interest. This means
in games like poker, where we are interested in a strategy for both players, we must construct a
grafted strategy separately for each player. Finally, when we construct a graft, our opponent must
learn a strategy for the entire, potentially abstract, game. By letting our opponent?s strategy vary
completely, our graft will be a strategy that is less prone to exploitation, forcing each individual
graft to mesh well with the base strategy and in turn with each other graft when combined.
Strategy grafting allows us to construct a strategy with more expressive power that what can be
computed by solving a single game. We now show that strategy grafting uses this expressive power
to its advantage, causing an (approximate) improvement over its base strategy. Note that we cannot
guarantee a strict improvement as the base strategy may already be an optimal strategy.
4

Theorem 1 For strategies ?1 , ?2 where ?2 is an -best response to ?1 , if ?1? is the grafted strategy
for player 1 where ?1 is used as the base strategy and G is the grafting partition then,
u1 (?1? , ?2 ) ? u1 (?1 , ?2 ) =

p 
X


u1 (?1?,j , ?2 ) ? u1 (?1 , ?2 ) ? ?3p.

j=1

In other words, the grafted strategy?s improvement against ?2 is equal to the sum of the gains of the
individual grafts against ?2 and this gain is no less than ?3p.
P ROOF. Define Zj as follows,
?j ? {1, . . . , p}

Zj = {z ? Z | ?h ? Gj with h a prefix of z}
p
[
Z0 = Z \
Zj

(4)
(5)

j=1

By condition (3) of Definition 7, Zj=0,...,p are disjoint and therefore form a partition of Z.
p 
X


u1 (?1?,j , ?2 ) ? u1 (?1 , ?2 )

(6)

j=1

=

=

p
X

!
X

u1 (z) Pr(z|?1?,j , ?2 )

j=1 z?Z
p X
p X
X

?

X

u1 (z) Pr(z|?1 , ?2 )

(7)

z?Z



u1 (z) Pr(z|?1?,j , ?2 ) ? Pr(z|?1 , ?2 )

(8)

j=1 k=0 z?Zk

Notice that for all z ? Zk6=j , Pr(z|?1?,j , ?2 ) = Pr(z|?1 , ?2 ), so only when k = j is the summand
non-zero.
p X


X
=
(9)
u1 (z) Pr(z|?1?,j , ?2 ) ? Pr(z|?1 , ?2 )
j=1 z?Zj

=

p X
X

u1 (z) (Pr(z|?1? , ?2 ) ? Pr(z|?1 , ?2 ))

(10)

j=1 z?Zj

=

X

u1 (z) (Pr(z|?1? , ?2 ) ? Pr(z|?1 , ?2 ))

(11)

z?Z

!
=
=

X

u1 (z) Pr(z|?1? , ?2 )

z?Z
u1 (?1? , ?2 )

?

X

u1 (z) Pr(z|?1 , ?2 )

(12)

z?Z

? u1 (?1 , ?2 )

(13)

Furthermore, since ?1?,j and ?2?,j are strategies of the -Nash equilibrium ? ?,j ,
u1 (?1?,j , ?2 ) +  ? u1 (?1?,j , ?2?,j ) ? u1 (?1 , ?2?,j ) ? 

(14)

Moreover, because ?2 is an -best response to ?1 ,
u1 (?1 , ?2?,j ) ? u1 (?1 , ?2 ) ? 

Pp 
So, j=1 u1 (?1?,j , ?2 ) ? u1 (?1 , ?2 ) ? ?3p.

(15)

The main application of this theorem is in the following corollary, which follows immediately from
the definition of an -Nash equilibrium.
Corollary 1 Let ? be an abstraction where ?2 = ?2 and ? be an -Nash equilibrium strategy for
the game ?? , then any grafted strategy ?1? in ? with ?1 used as the base strategy will be at most 3p
worse than ?1 against ?2 .
5

Although these results suggest that a grafted strategy will (approximately) improve on its base strategy against an optimal opponent, there is one caveat: it assumes we know the opponent?s abstraction
or can solve a game with the opponent unabstracted. Without this knowledge or ability, this guarantee does not hold. However, all previous work that employs the use of abstract equilibrium strategies also implicitly makes this assumption. Though we know that refining an abstraction also has
no guarantee on improving worst-case performance in the original game [7], the AAAI Computer
Poker Competition [10] has shown that in practice larger abstractions and more expressive strategies
consistently perform well in the original game, even though competition opponents are not using the
same abstractions. We might expect a similar result even when the theorem?s assumptions are not
satisfied. In the next section we examine empirically both situations where we know our opponent?s
abstraction and situations where we do not.

4

Experimental Results

The AAAI Computer Poker Competitions use various types of large Texas Hold?em poker games.
These games are quite large and the resulting abstract games can take weeks of computation to solve.
We begin our experiments in a smaller poker game called Leduc Hold?em where we can examine
several grafted strategies. This is followed by analysis of a grafted strategy for two-player limit
Texas Hold?em that was submitted to the 2009 AAAI Poker Competition.
4.1

Leduc Hold?em

Leduc Hold?em is a two player poker game. The deck used in Leduc Hold?em contains six cards,
two jacks, two queens and two kings, and is shuffled prior to playing a hand. At the beginning of a
hand, each player pays a one chip ante to the pot and receives one private card. A round of betting
then takes place starting with player one. After the round of betting, a single public card is revealed
from the deck, which both players use to construct their hand. This card is called the flop. Another
round of betting occurs after the flop, again starting with player one, and then a showdown takes
place. At a showdown, if either player has paired their private card with the public card they win all
the chips in the pot. In the event neither player pairs, the player with the higher card is declared the
winner. The players split the money in the pot if they have the same private card.
Each betting round follows the same format. The first player to act has the option to check or bet.
When betting the player adds chips into the pot and action moves to the other player. When a player
faces a bet, they have the option to fold, call or raise. When folding, a player forfeits the hand and
all the money in the pot is awarded to the opposing player. When calling, a player places enough
chips into the pot to match the bet faced and the betting round is concluded. When raising, the player
must put more chips into the pot than the current bet faced and action moves to the opposing player.
If the first player checks initially, the second player may check to conclude the betting round or bet.
In Leduc Hold?em there is a limit of one bet and one raise per round. The bets and raises are of a
fixed size. This size is two chips in the first betting round and four chips in the second.
Tournament Setup. Despite using a smaller poker game, we aim to create a tournament setting
similar to the AAAI Poker Competition. To accomplish this we will create a variety of equilibriumlike players using abstractions of varying size. Each of these strategies will then be used as a base
strategy to create two grafted strategies. All strategies are then played against each other in a roundrobin tournament. A strategy is said to beat another strategy if its expected winnings against the other
is positive. Unlike the AAAI Poker Competition, in our smaller game we can feasibly compute the
expected value of one strategy against another and thus we are not required to sample.
The abstractions used are J.Q.K, JQ.K, and J.QK. Prior to the flop, the first abstraction can distinguish all three cards, the second abstraction cannot distinguish a jack from a queen and the third
cannot distinguish a queen from a king. Postflop, all three abstractions are only aware of if they
have paired their private card. These three abstractions were hand chosen as they are representative
of how current abstraction techniques will group hands together. The first abstraction is the biggest,
and hence we would expect it to do the best. The second and third abstractions are the same size.
We chose to train two types of grafted strategies: preflop grafts and flop grafts. Both types consist
of three individual grafts for each player: one to play each card with complete information. That is,
6

(1)
(1) J.Q.K preflop grafts
(2) J.Q.K flop grafts
(3) JQ.K flop grafts
(4) JQ.K preflop grafts
(5) J.QK preflop grafts
(6) J.Q.K
(7) JQ.K
(8) J.QK flop grafts
(9) J.QK

-2.3
-28.0
-17.5
-12.2
-26.6
-36.7
-22.3
-54.7

(2)
2.3
-28.6
-18.6
-16.9
-23.9
-39.7
-24.7
-49.6

(3)
28.0
28.6

(4)
17.5
18.6
-47.2

47.2
-67.0
0.9
-28.5
-79.9
-89.2

11.2
-9.0
-67.3
-3.7
-62.8

(5)
12.2
16.9
67.0
-11.2
-8.1
20.0
-30.9
-110.0

(6)
26.6
23.9
-0.9
9.0
8.1
-13.6
-7.5
-32.5

(7)
36.7
39.7
28.5
67.3
-20.0
13.6
-42.2
-70.6

(8)
22.3
24.7
79.9
3.7
30.9
7.5
42.2
-83.3

(9)
54.7
49.6
89.2
62.8
110.0
32.5
70.6
83.3

Avg.
25.0
25.0
20.0
17.9
5.5
-1.6
-6.6
-16.0
-69.1

Table 1: Expected winnings of the row player against the column player in millibets per hand (mb/h)
Strategy
J.Q.K preflop grafts
J.Q.K flop grafts
JQ.K preflop grafts
JQ.K flop grafts
J.QK preflop grafts
J.Q.K
JQ.K
J.QK flop grafts
J.QK

Wins
8
7
5
4
4
4
3
1
0

Losses
0
1
3
4
4
4
5
7
8

Exploitability
298.3
321.1
465.9
509.0
507.3
315.1
246.8
503.5
371.1

Table 2: Each strategy?s number of wins, losses, and exploitability in unabstracted Leduc Hold?em
in millibets per hand (mb/h)

each graft does not abstract the sub-game for the observed card. These two types differ in that the
preflop grafts play for the entire game whereas the flop grafts only play the game after the flop. For
preflop grafts, this means G0 is empty, i.e., the final grafted strategy is always using the probabilities
from some graft and never the base strategy. For flop grafts, the grafted strategy follows the base
strategy in all preflop information sets. We use ?-Nash equilibria in the three abstract games as our
base strategies. Each base strategy and graft is trained using counterfactual regret minimization for
one billion iterations. The equilibria found are ?-Nash equilibria where no player can benefit more
than ? = 10?5 chips by deviating within the abstract game. We measure the expected winnings in
millibets per hand or mb/h. A millibet is one thousandth of a small bet, or 0.002 chips.
Results. We can see in Table 1 that the grafted strategies perform well in a field of equilibriumlike strategies. The base strategy seems to be of great importance when training a grafted strategy.
Though JQ.K and J.QK are the same size, the JQ.K strategy performs better in this tournament
setting. Similarly, the grafted strategies appear to maintain the ordering of their base strategies
either when considering the expected winnings in Table 1 or the number of wins in Table 2 (though
JQ.K flop grafts switches places with JQ.K preflop grafts in the ordering). Although the choice of
base strategy is important, the grafted strategies do well under both evaluation criteria and even the
worst base strategy sees great relative improvement when used to train grafted strategies.
There are also a few other interesting trends in these results. First, our intuition that larger strategies
perform better seems to hold in all cases except for J.QK flop grafts. Larger abstractions also perform
better for the non-grafted strategies as J.Q.K is the biggest equilibrium strategy and it performs the
best out of this group. Second, it appears that the preflop grafts are usually better than the flop grafts.
This can be explained by the fact that the preflop grafts have more information about the original
game. Finally, observe that the grafted strategies can have worse exploitability in the original game
than their corresponding base strategy. Although this can make grafted strategies more vulnerable
to exploitive strategies, they appear to perform well against a field of equilibrium-like opponents.
In fact, in our experiment, grafted strategies appear to only improve upon the base strategy despite
not always knowing the opponent?s abstraction. This suggests that exploitability is not the only
important measure of strategy quality. Contrast the grafted strategies with the strategy that always
folds, which is exploitable at 500 mb/h. Although always folding is less exploitable than some of
the grafted strategies, it cannot win against any opponent and would place last in this tournament.
7

(1) 20x8 Grafted
(2) 20x32
(3) 20x8 (Base)
(4) 20x7
(5) 14
(6) 12

Relative Size
1.0
2.53
1.0
0.43
0.82
0.45

(1)

(2)
2.1

-2.1
-14.5
-18.1
-13.7
-18.7

-4.9
-9.4
-11.8
-15.5

(3)
14.5
4.9
-6.2
-7.2
-10.7

(4)
18.1
9.4
6.2
-1.7
-5.0

(5)
13.7
11.8
7.2
1.7
-5.3

(6)
18.7
15.5
10.7
5.0
5.3

Avg.
13.4
7.9
0.9
-5.4
-5.8
-11.0

Table 3: Sampled expected winnings in Texas Hold?em of the row player against the column player
in millibets per hand (mb/h). 95% confidence intervals are between 0.8 and 1.6. Relative size is the
ratio of the size of the abstract game(s) solved for the row strategy and the base strategy.
4.2

Texas Hold?em

Two-player limit Texas Hold?em bears many similarities to Leduc Hold?em but is much larger in
scale with respect to the parameters: cards in the deck, private cards, public cards, betting rounds and
bets per round. Due to the computational cost2 needed to solve a strong equilibrium, our experiments
consist of a single grafted strategy. Table 3 shows the results of running this large grafted strategy
against equilibrium-like strategies using a variety of abstractions.
The 20x32 strategy is the largest single imperfect recall abstract game solved to date. It is approximately 2.53 times larger than the base strategy used with grafting, 20x8. The 20x7 (imperfect recall)
and 12 (perfect recall) strategies were the entrants put forward by the Computer Poker Research
Group for the 2008 and 2007 AAAI Computer Poker Competitions, respectively. The 14 strategy
was considered for the 2008 competition, but it was ultimately superseded by the smaller 20x7. For
a detailed description of these abstractions and the rules of Texas Hold?em see A Practical Use of
Imperfect Recall [8].
As evident in the results, the grafted strategy beats all of the players with statistical significance, even
the largest single strategy. In addition to these results against other Computer Poker Research Group
strategies, the grafted strategy also performed well at the 2009 AAAI Computer Poker Competition.
There, against a field of thirteen strong strategies, it placed second and fourth (narrowly behind the
third place entrant) in the limit run-off and limit bankroll competitions, respectively.
These results demonstrate that strategy grafting is competitive and allows one to augment their
existing strategies. Any improvement to the quality of a base strategy should in turn improve the
quality of the grafted strategy in similar tournament settings. This means that strategy grafting can
be used transparently on top of more sophisticated strategy-computing methods.

5

Conclusion

We have introduced a new method, called strategy grafting, for independently solving and combining sub-games in large extensive games. This method allows us to create larger strategies than
previously possible by solving many sub-games. These new strategies seem to maintain the features
of good equilibrium-like strategies. By creating larger strategies we hope to play fewer dominated
strategies and, in turn, make fewer mistakes. Against a static equilibrium-like opponent, making
fewer mistakes should lead to an improvement in the quality of play. Our empirical results confirm
this intuition and demonstrate that this new method can improve the performance of the state-of-theart in both a simulated competition and the actual AAAI Computer Poker Competition. It is likely
that much of the strength of these new strategies will be bounded by the quality of the base strategy
used. In this regard, we are still limited by the capabilities of current methods.

Acknowledgments
The authors would like to thank the members of the Computer Poker Research Group at the University of Alberta for helpful conversations pertaining to this research. This research was supported by
NSERC, iCORE, and Alberta Ingenuity.
2

This particular grafted strategy was computed on a large cluster using 640 processors over almost 6 days.

8

References
[1] Darse Billings, Neil Burch, Aaron Davidson, Robert Holte, Jonathan Schaeffer, Terance
Schauenberg, and Duane Szafron. Approximating Game-Theoretic Optimal Strategies for
Full-scale Poker. In International Joint Conference on Artificial Intelligence, pages 661?668,
2003.
[2] Andrew Gilpin, Samid Hoda, Javier Pe?na, and Tuomas Sandholm. Gradient-based Algorithms
for Finding Nash Equilibria in Extensive Form Games. In Proceedings of the Eighteenth International Conference on Game Theory, 2007.
[3] Andrew Gilpin and Tuomas Sandholm. A Competitive Texas Hold?em Poker Player via Automated Abstraction and Real-time Equilibrium Computation. In Proceedings of the Twenty-First
Conference on Artificial Intelligence, 2006.
[4] Andrew Gilpin and Tuomas Sandholm. Expectation-Based Versus Potential-Aware Automated
Abstraction in Imperfect Information Games: An Experimental Comparison Using Poker. In
Proceedings of the Twenty-Third Conference on Artificial Intelligence, 2008.
[5] Daphne Koller and Avi Pfeffer. Representations and Solutions for Game-Theoretic Problems.
Artificial Intelligence, 94:167?215, 1997.
[6] Martin Osborne and Ariel Rubinstein. A Course in Game Theory. The MIT Press, Cambridge,
Massachusetts, 1994.
[7] Kevin Waugh, David Schnizlein, Michael Bowling, and Duane Szafron. Abstraction Pathologies in Extensive Games. In Proceedings of the Eighth International Joint Conference on
Autonomous Agents and Multi-Agent Systems, pages 781?788, 2009.
[8] Kevin Waugh, Martin Zinkevich, Michael Johanson, Morgan Kan, David Schnizlein, and
Michael Bowling. A Practical Use of Imperfect Recall. In Proceedings of the Eighth Symposium on Abstraction, Reformulation and Approximation, 2009.
[9] Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret Minimization in Games with Incomplete Information. In Advances in Neural Information Processing Systems Twenty, pages 1729?1736, 2008. A longer version is available as a University of
Alberta Technical Report, TR07-14.
[10] Martin Zinkevich and Michael Littman. The AAAI Computer Poker Competition. Journal of
the International Computer Games Association, 29, 2006. News item.

9

"
2008,Bayesian Model of Behaviour in Economic Games,,3589-bayesian-model-of-behaviour-in-economic-games.pdf,Classical Game Theoretic approaches that make strong rationality assumptions have difficulty modeling observed behaviour in Economic games of human subjects. We investigate the role of finite levels of iterated reasoning and non-selfish utility functions in a Partially Observable Markov Decision Process model that incorporates Game Theoretic notions of interactivity. Our generative model captures a broad class of characteristic behaviours in a multi-round Investment game. We invert the generative process for a recognition model that is used to classify 200 subjects playing an Investor-Trustee game against randomly matched opponents.,"Bayesian Model of Behaviour in Economic Games

Brooks King-Casas
Computational Psychiatry Unit
Baylor College of Medicine.
Houston, TX 77030. USA
bkcasas@cpu.bcm.tmc.edu

Debajyoti Ray
Computation and Neural Systems
California Institute of Technology
Pasadena, CA 91125. USA
dray@caltech.edu

Peter Dayan
Gatsby Computational Neuroscience Unit
University College London
London. WC1N 3AR. UK
dayan@gatsby.ucl.ac.uk

P. Read Montague
Human NeuroImaging Lab
Baylor College of Medicine.
Houston, TX 77030. USA
montague@hnl.bcm.tmc.edu

Abstract
Classical game theoretic approaches that make strong rationality assumptions have
difficulty modeling human behaviour in economic games. We investigate the role
of finite levels of iterated reasoning and non-selfish utility functions in a Partially
Observable Markov Decision Process model that incorporates game theoretic notions of interactivity. Our generative model captures a broad class of characteristic
behaviours in a multi-round Investor-Trustee game. We invert the generative process for a recognition model that is used to classify 200 subjects playing this game
against randomly matched opponents.

1

Introduction

Trust tasks such as the Dictator, Ultimatum and Investor-Trustee games provide an empirical basis
for investigating social cooperation and reciprocity [11]. Even in completely anonymous settings,
human subjects show rich patterns of behavior that can be seen in terms of such personality concepts
as charity, envy and guilt. Subjects also behave as if they model these aspects of their partners
in games, for instance acting to avoid being taken advantage of. Different subjects express quite
different personalities, or types, and also have varying abilities at modelling their opponents.
The burgeoning interaction between economic psychology and neuroscience requires formal treatments of these issues. From the perspective of neuroscience, such treatments can provide a precise
quantitative window into neural structures involved in assessing utilties of outcomes, capturing risk
and probabilities associated with interpersonal interactions, and imputing intentions and beliefs to
others. In turn, evidence from brain responses associated with these factors should elucidate the neural algorithms of complex interpersonal choices, and thereby illuminate economic decision-making.
Here, we consider a sequence of paradigmatic trust tasks that have been used to motivate a variety
of behaviorally-based economic models. In brief, we provide a formalization in terms of partially
observable Markov decision processes, approximating type-theoretic Bayes-Nash equilibria [8] using finite hierarchies of belief, where subjects? private types are construed as parameters of their
inequity averse utility functions [2]. Our inference methods are drawn from machine learning.
Figure 1a shows a simple one-round trust game. In this, an Investor is paired against a randomly
assigned Trustee. The Investor can either choose a safe option with a low payoff for both, or take
a risk and pass the decision to the Trustee who can either choose to defect (and thus keep more for
herself) or choose the fair option that leads to more gains for both players (though less profitable
1

Figure 1: (a) In a simple Trust game, the Investor can take a safe option with a payoff of $[Investor=20,Trustee=20] (i.e. the Investor gets $20 and the Trustee gets $20). The game ends if the
Investor chooses the safe option; alternatively, he can pass the decision to the Trustee. The Trustee
can now choose a fair option $[25,25] or choose to defect $[15,30]. (b) In the multi-round version
of the Trust game, the Investor gets $20 dollars at every round. He can invest any (integer) part; this
quantity is trebled on the way to the Trustee. In turn, she has the option of repaying any (integer)
amount of her resulting allocation to the Investor. The game continues for 10 rounds.
for herself alone than if she defected). Figure 1b shows the more sophisticated game we consider,
namely a multi-round, sequential, version of the Trust game [15].
The fact that even in a purely anonymized setting, Investors invest at all, and Trustees reciprocate at
all in games such as that of figure 1a, is a challenge to standard, money-maximizing doctrines (which
expect to find the Nash equilibrium where neither happens), and pose a problem for modeling. One
popular strategy is to retain the notion that subjects attempt to optimize their utilities, but to include
in these utilities social factors that penalize cases in which opponents win either more (crudely envy,
parameterized by ?) or less (guilt, parameterized by ?) than themselves [2]. One popular InequityAversion utility function [2] characterizes player i by the type Ti = (?i , ?i ) of her utility function:
U (?i , ?i ) = xi ? ?i max{(xj ? xi ), 0} ? ?i max{(xi ? xj ), 0}

(1)

where xi , xj are the amounts received by players i and j respectively.
In the multi-round version of figure 1b, reputation formation comes into play [15]. Investors have
the possibility of gaining higher rewards from giving money to the Trustee; and, at least until the
final round, the Trustee has an incentive to maintain a reputation of trustworthiness in order to coax
the Investor to offer more (against any Nash tendencies associated with solipsistic utility functions).
Social utility functions such as that of equation 1 mandate probing, belief manipulation and the like.
We cast such tasks as Bayesian Games. As in the standard formulation [8], players know their own
types but not those of their opponents; dyads are thus playing games of incomplete information.
A player also has prior beliefs about their opponent that are updated in a Bayesian manner after
observing the opponent?s actions. Their own actions also influence their opponent?s beliefs. This
leads to an infinite hierarchy of beliefs: what the Trustee thinks of the Investor; what the Trustee
thinks the Investor thinks of him; what the Trustee thinks the Investor thinks the Trustee thinks of
her; and so on. If players have common prior beliefs over the possible types in the game, and this
prior is common knowledge, then (at least one) subjective equilibrium known as the Bayes-Nash
Equilibrium (BNE), exists [8]. Algorithms to compute BNE solutions have been developed but, in
the general case, are NP-hard [6] and thus infeasible for complex multi-round games [9].
One obvious approach to this complexity is to consider finite rather than infinite belief hierarchies.
This has both theoretical and empirical support. First, a finite hierarchy of beliefs can provably
approximate the equilibrium solution that arises in an infinite belief hierarchy arbitrarily closely [10],
an idea that has indeed been employed in practice to compute equilibria in a multi-agent setting [5].
Second, based on a whole wealth of games such as the p-Beauty game [11], it has been suggested
that human subjects only employ a very restricted number of steps of strategic thinking. According
to cognitive hierarchy theory, a celebrated account of this, this number is on average a mere 1.5 [13].
In order to capture the range of behavior exhibited by subjects in these games, we built a finite
belief hierarchy model, using inequity averse utility functions in the context of a partially observable
hidden Markov model of the ignorance each subject has about its opponent?s type and in the light of
sequential choice. We used inference strategies from machine learning to find approximate solutions
to this model. In this paper, we use this generative model to investigate the qualitative classes of
behaviour that can emerge in these games.
2

Figure 2: Each player?s decision-making requires solving a POMDP, which involves solving the opponent?s POMDP. Higher order beliefs are required as each player?s action influences the opponent?s
beliefs which in turn influence their policy.

2

Partially Observable Markov Games

As in the framework of Bayesian games, player i?s inequity aversion type Ti = (?i , ?i ) is known to
it, but not to the opponent. Player i does have a prior distribution over the type of the other player j,
(0)
bi (Tj ); and, if suitably sophisticated, can also have higher-order priors over the whole hierarchy
(0)
(0) (0)0 (0)00
of recursive beliefs about types. We denote the collection of priors as ~bi = {bi , bi , bi , ...}.
(t)
Play proceeds sequentially, with player i choosing action ai at time t according to the expected future value of this choice. In this (hidden) Markovian setting, this value, called a Q-value depends on
(t)
the stage (given the finite horizon), the current beliefs of the player ~bi (which are sufficient statis(t)
tics for the past observations), and the policies P (ai = a|D(t) ) (which depend on the observations
(t)
D ) of both players up to time t:
(t) (t) (t)
(t) (t) (t)
Qi (~bi , ai ) = Ui (~bi , ai )+
X
X
(t)
(t)
(t+1) ~ (t+1) (t+1)
(t+1)
(t) (t)
P (aj |{D(t) , ai })
Qi
(bi
, ai
)P (ai+1 |{D(t) , ai , aj })
(t)

(t)

(t+1)

aj ?Aj

ai

(2)

(t+1)

?Ai

where we arbitrarily define the softmax policy,

 X


(t)
(t) (t)
(t) (t)
P (ai = a|D(t) ) = exp ?Qi (~bi , a) /
exp ?Qi (~bi , b)

(3)

b

akin to Quantal Response Equilibrium [12], which depends on player i?s beliefs about player j,
(t)
(t)
which are, in turn, updated using Bayes? rule based on the likelihood function P (aj |{D(t) , ai })
(t+1)

bi

(t)

(t)

(t)

(t)

(t)

(Tj ) = P (Tj |aj , ai , bi ) = P (Tj , ai , aj )

(t)
(t) (t) (t)
0
Tj0 bi (Tj )/P (aj |ai , bi )

P

(4)

(t)

switching between history-based (Dt ) and belief-based (bi (Tj )) representations. Given the interdependence of beliefs and actions, we expect to see probing (to find out the type and beliefs of one?s
opponent) and belief manipulation (being nice now to take advantage of one?s opponent later).
If the other player?s decisions are assumed to emerge from equivalent softmax choices, then for the
subject to calculate this likelihood, they must also solve their opponent?s POMDP. This leads to an
infinite recursion (illustrated in fig. 2). In order to break this, we assume that each player has k
levels of strategic thinking as in the Cognitive Hierarchy framework [13]. Thus each k-level player
assumes that his opponent is a k ? 1-level player. At the lowest level of the recursion, the 0-level
player uses a simple likelihood to update their opponent?s beliefs.
(t)

(t)

(t)

The utility Ui (ai ) is calculated at every round for each player i for action ai by marginalizing
(t)
over the current beliefs bi . It is extremely challenging to compute with belief states, since they
are probability distributions, and are therefore continuous-valued rather than discrete. To make this
computationally reasonable, we discretize the values of the types. As an example, if there are only
two types for a player the belief state, which is a continuous probability distribution over the interval
3

[0, 1] is discretized to take K values bi1 = 0, . . . , biK = 1. The utility of an action is obtained by
marginalizing over the beliefs as:
X
(t) (t)
(t) (t) (t)
Ui (ai ) =
bik Qi (bik , ai )
(5)
k=1:K

Furthermore, we solve the resulting POMDP using a mixture of explicit expansion of the tree from
the current start point to three stages ahead, and a stochastic, particle-filter-based scheme (as in [7]),
from four stages ahead to the end of the game.
One characteristic of this explicit process model, or algorithmic approach, is that it is possible to
consider what happens when the priors of the players differ. In this case, as indeed also for the
case of only a finite belief hierarchy, there is typically no formal Bayes-Nash equilibrium. We also
verified our algorithm against the QRE and BNE solutions provided by GAMBIT ([14]) on a 1 and
2 round Trust game for k = 1, 2 respectively. However unlike the BNE solution in the extensive
form game, our algorithm gives rise to belief manipulation and effects at the end of the game.

3

Generative Model for Investor-Trustee Game

Reputation-formation plays a particularly critical role in the Investor-Trustee game, with even the
most selfish players trying to benefit from cooperation, at least in the initial rounds. In order to
reduce complexity in analyzing this, we set ?I = ?I = 0 (i.e., a purely selfish Investor) and
consider 2 values of ?T (0.3 and 0.7) such that in the last round the Trustee with type ?T = 0.3 will
not return any amount to the Investor and will choose fair outcome if ?T = 0.7. We generate a rich
tapestry of behavior by varying the prior expectations as to ?T and the values of strategic (k) level
(0,1,2) for the players.
3.1

Factors Affecting Behaviour

As an example, fig. 3 shows the evolution of the Players? Q-values and 1st-order beliefs of the
Investor and 2nd-order beliefs of the Trustee (i.e., her beliefs as to the Investor?s beliefs about her
value of ?T ) over the course of a single game. Here, both players have kI = kT = 1 (i.e. they are
strategic players), but the Trustee is actually less guilty ?T = 0.3.
In the first round, the Investor gives $15, and receives back $30 from the Trustee. This makes
the Investor?s beliefs about ?T go from being uniform to being about 0.75 for ?T = 0.7 and 0.25
for ?T = 0.3 (showing the success in the Trustee?s exercise in belief manipulation). This causes
the Q-value for the action corresponding to giving $20 dollars to be highest, inspiring the Investor?s
generosity in round 2. Equally, the Trustee?s (2nd-order) beliefs after receiving $15 in the first round
peak for the value ?T = 0.7, corresponding to thinking that the Investor believes the Trustee is Nice.
In subsequent rounds, the Trustee?s nastiness limits what she returns, and so the Investor ceases
giving high amounts. In response, in rounds 5 and 7, the Trustee tries to coax the Investor. We find
this ?reciprocal give and take? to be a characteristic behaviour of strategic Investors and Trustees
(with k = 1). For naive Players with k = 0, a return of a very low amount for a high amount
invested would lead to a complete breakdown of Trust formation.
Fig. 4 shows the statistics of dyadic interactions between Investors and Trustees with Uniform priors.
The amount given by the Investor varies significantly depending on whether or not he is strategic,
and also on his priors. In round 1, Investors with kI = 0 and 1 offer $20 first (the optimal probing
action based on uniform prior beliefs) and for kI = 2 offers $15 dollars. The corresponding amount
returned by the Trustee depends significantly on kT . A Trustee with kT = 0 and low ?T will return
nothing whereas an unconditionally cooperative Trustee (high ?T ) returns roughly the same amount
as received. Irrespective of the Trustee?s ?T type, the amount returned by strategic Trustees with
kT = 1, 2 is higher (between 1.5 and 2 times the amount received).
In round 2 we find that the low amount received causes trust to break down for Investors with
kI = 0. In fact, naive Investors and Trustees do not form Trust in this game. Strategic Trustees return
more initially and are able to coax naive Investors to give higher amounts in the game. Generally
unconditionally cooperative Trustees return more, and form Trust throughout the game if they are
strategic or if they are playing against strategic Investors. Trustees with low ?T defect towards the
end of the game but coax more investment in the beginning of the game.
4

Figure 3: The generated game shows the amount given by an Investor with kI = 1 and a Trustee
with ?T = 0.3 and kT = 1. The red bar indicates amount given by the Investor and the blue bar
is the amount returned by the Trustee (after receiving 3 times amount given by the Investor). The
figures on the right reveal the inner workings of the algorithm: Q-values through the rounds of the
game for 5 different actions of the Investor (0, 5, 10, 15, 20) and 5 actions of the Trustee between
values 0 and 3 times amount given by Investor. Also shown are the Investor?s 1st-order beliefs (left
bar for ?T = 0.3 and right bar for ?T = 0.7) and Trustee?s 2nd-order beliefs over the rounds.

Figure 4: The dyadic interactions between the Investor and Trustee across the 10 rounds of the
game. The top half shows Investor playing against Trustee with low ?T (= 0.3) and the bottom half
is the Trustee with high ?T (= 0.7): unconditionally cooperative. The top dyad shows the amount
given the Investor and the bottom dyad shows the amount returned by Trustee. Within each dyad
the rows represent the strategic (kI ) levels of Investor (0, 1 or 2) and the columns represent kT
level of the Trustee (0, 1 or 2). The dyads are shown here for the first 2 and final 2 rounds. Two
particular examples are highlighted within the dyads: Investor with kI = 0 and Trustee with kT = 2,
uncooperative (?Tlow ) and Investor kI = 1 and Trustee kT = 2, cooperative (?Thigh ). Lighter colours
reveal higher amounts (with amount given by Investor in first round being 15 dollars).

The effect of strategic level is more dramatic for the Investor, since his ability to defect at any
point places him in effective charge of the interaction. Strategic Investors give more money in the
game than naive Investors. Consequently they also get more return on their investment because of
the beneficial effects of this on their reputations. A further observation is that strategic Investors
are more immune to the Trustee?s actions. While this means that break-downs in the game due to
5

mistakes of the Trustee (or unfortunate choices from her softmax) are more easily corrected by the
strategic Investor, he is also more likely to continue investing even if the Trustee doesn?t reciprocate.
It is also worth noting the differences between k = 1 and k = 2 players. The latter typically offer
less in the game and are also less susceptible to the actions of their opponent. Overall in this game,
the Investors with kI = 1 make the most amount of money playing against a cooperative Trustee
while kI = 0 Investors make the least. The best dyad consists of a kI = 1 Investor playing with a
cooperative Trustee with kT = 0 or 1.
A very wide range of patterns of dyadic interaction, including the main observations of [15], can
thus be captured by varying just the limited collection of parameters of our model

4

Recognition and Classification

One of the main reasons to build this generative model for play is to have a refined method for
classifying individual players on the basis of the dyadic behaviour. We do this by considering the
statistical inverse of the generative model as a recognition model. Denote the sequence of plays in the
(1) (1)
(10)
10-round Investor-Trustee game as D = {[a1 , a2 ], .., [a1 , a10
2 ]}. Since the game is Markovian
(t)
we can calculate the probability of player i taking the action sequence {ai , t = 1, ..., 10} given his
(0)
Type Ti and prior beliefs ~bi as:
(0)
P ({ati }|Ti , ~bi )

=

(1)
(0)
P (a1 |Ti , ~bi )

10
Y

(t)

P (ai |D(t) , Ti )

(6)

t=2
(1)
(0)
(1)
where P (a1 |Ti , ~bi ) is the probability of initial action ai given by the softmax distribution and
(0)
(t)
(t)
(t)
prior beliefs ~bi , and P (ai |D(t) , Ti ) is the probability of action ai after updating beliefs ~bi
(t?1)
(t)
from previous beliefs ~bi
upon the observation of the past sequence of moves D . This is a
(0)
~
likelihood function for Ti , bi , and so can be used for posterior inference about type given D. We
classify the players for their utility function (?T value for the Trustee), strategic (ToM) levels and
(0)
(0)
prior beliefs using the MAP value (Ti? , ~bi ? ) = maxTi ,~b(0) P (D|Ti , ~bi ).
i

We used our recognition model to classify subject pairs playing the 10-round Investor-Trustee game
[15]. The data included 48 student pairs playing an Impersonal task for which the opponents? identities were hidden and 54 student pairs playing a Personal task for which partners met.
Each Investor-Trustee pair was classified for their level of strategic thinking k and the Trustee?s ?T
type (cooperative/uncooperative; see the table in Figure 5). We are able to capture some characteristic behaviours with our model. The highlighted interactions reveal that many of the pairs in the
Impersonal task consisted of strategic Investors and cooperative Trustees, who formed trust in the
game with the levels of investment decreasing towards the end of the game. We also highlight the
difference between strategic and non-strategic Investors. An Investor with kI = 0 will not form
trust if the Trustee does not return a significant amount initially whilst an Investor with kI = 2 will
continue offering money in the game even if the Trustee gives back less than fair amounts in return.
There is also a strong correlation between the proportion of Trustees classified as being cooperative:
estimated as 48%, 30%, on the Impersonal and Personal tasks respectively and the corresponding
Return on Investment (how much the Investor receives for the amount Invested): 120%, 109%.
Although the recognition model captures key characteristics, we do not expect the Trustees to have
the specified values of ?Tlow = 0.3 and ?Thigh = 0.7. To test the robustness of the recognition model
we generated behaviours (450 dyads) with different values of ?T (?Tlow = [0, 0.1, 0.2, 0.3, 0.4] and
?Thigh = [0.6, 0.7, 0.8, 0.9, 1.0]), that were classified using the recognition model. Figure 5 shows
how confidently players of the given type were classified to have that type.
We find that the recognition model tends to misclassify Trustees with low ?T as having kT = 2.
This is because the Trustees with those characteristics will offer high amounts to coax the Investor.
Investor are shown to be correctly classified in most cases. Overall the recognition model has a
tendency to assign higher kT to the Trustees than their true type, though the model correctly assigns
the right cooperative/uncooperative type to the Trustee.
6

Figure 5: Subject pairs are classified into levels of Theory of Mind for the Investor (rows) and
Trustee (columns). The number of subject-pairs with the classification are shown in each entry
along with whether the Trustee was classified as uncooperative / cooperative (?Tlow , ?Thigh ). The
subjects play an Impersonal game where they do not know the identities of the opponent and a
Personal game where identities are revealed.
We reveal the dominant or unique behavioural classification within tables (highlighted): Impersonal
(kI = 1, kT = 2, cooperative) group averaged over 10 subjects, Personal group (kI = 0, kT = 0,
uncooperative) averaged over 3 subjects, and Personal group with (kI = 2, kT = 0, uncooperative)
averaged over 11 subjects.
We also show the classification confidence for the types given the behaviour was generated from our
model with other values of ?T for the Trustee, as well as the type that the player is most likely to be
classified as in brackets. (A Trustee with low ?T and kT = 1 is very likely to be misclassified as a
player with kT = 2, while a player with kT = 2 will mostly be classified with kT = 2)

5

Discussion

We built a generative model that captures classes of observed behavior in multi-round trust tasks.
The critical features of the model are a social utility function, with parameters covering different
types of subjects; partial observability, accounting for subjects? ignorance about their opponents;
an explicit and finite cognitive hierarchy to make approximate equilibrium calculations marginally
tractable; and partly deterministic and partly sample-based evaluation methods.
Despite its descriptive adequacy, we do not claim that it is uniquely competent. We also do not
suggest a normative rationale for pieces of the model such as the social utility function. Nevertheless,
the separation between the vagaries of utility and the exactness of inference is attractive, not the least
by providing clearly distinct signals as to the inner workings of the algorithm that can be extremely
useful to capture neural findings. Indeed, the model is relevant to a number of experimental findings,
including those due to [15], [18], [19]. The underlying foundation in reinforcement learning is
congenial, given the substantial studies of the neural bases of this [20].
The model does directly license some conclusions. For instance, we postulate that higher activation
will be observed in regions of the brain associated with theory of mind for Investors that give more
in the game, and for Trustees that can coax more. However, unlike [13] our Naive players still build
models, albeit unsophisticated ones, of the other player (in contrast to level 0 players who assume
the opponent to play a random strategy). So this might lead to an investigation of how sophisticated
and naive theory of mind models are built by subjects in the game.
We also constructed the recognition model, which is the statistical inverse to this generative model.
While we showed this to capture a broad class of behaviours, it only explains the coarse features
of the behaviour. We need to incorporate some of the other parameters of our model, such as
the Investor?s envy and the temperature parameter of the softmax distribution in order to capture
the nuances in the interactions. Further it would be interesting to use the recognition model in
pathological populations, looking at such conditions as autism and borderline personality disorder.
7

Finally, this computational model provides a guide for designing experiments to probe aspects of
social utility, strategic thinking levels and prior beliefs, as well as inviting ready extensions to related
tasks such as Public Goods games. The inference method may also have wider application, for
instance to identifying which of a collection of Bayes-Nash equilibria is most likely to arise, given
psychological factors about human utilities.
Acknowledgments
We thank Wako Yoshida, Karl Friston and Terry Lohrenz for useful discussions.

References
[1] K.A. McCabe, M.L. Rigdon and V.L. Smith. Positive Reciprocity and Intentions in Trust Games (2003).
Journal of Economic Behaviour and Organization.
[2] E. Fehr and K.M. Schmidt. A Theory of Fairness, Competition and Cooperation (1999). The Quarterly
Journal of Economics.
[3] E. Fehr and S. Gachter. Fairness and Retaliation: The Economics of Reciprocity (2000). Journal of Economic Perspectives.
[4] E. Fehr and U. Fischbacher. Social norms and human cooperation (2004). TRENDS in Cog. Sci. 8:4.
[5] P.J. Gmytrasiewicz and P. Doshi. A Framework for Sequential Planning in Multi-Agent Settings (2005).
Journal of Artificial Intelligence Research.
[6] V. Conitzer and T. Sandholm (2002). Complexity Results about Nash Equilibria. Technical Report CMUCS-02-135, School of Computer Science, Carnegie-Mellon University.
[7] S. Thrun. Monte Carlo POMDPs (2000). Advances in Neural Information Processing Systems 12.
[8] JC Harsanyi (1967). Games with Incomplete Information Played by ?Bayesian? Players, I-III. Management
Science.
[9] J.F. Mertens and S. Zamir. Formulation of Bayesian analysis for games with incomplete information (1985).
International Journal of Game Theory.
[10] Y. Nyarko. Convergence in Economic Models with Bayesian Hierarchies of Beliefs (1997). Journal of
Economic Theory.
[11] C. Camerer. Behavioural Game Theory: Experiments in Strategic Interaction (2003). Princeton Univ.
[12] R. McKelvey and T. Palfrey. Quantal Response Equilibria for Extensive Form Games (1998). Experimental Economics 1:9-41.
[13] C. Camerer, T-H. Ho and J-K. Chong. A Cognitive Hierarchy Model of Games (2004). The Quarterly
Journal of Economics.
[14] R.D. McKelvey, A.M. McLennan and T.L. Turocy (2007). Gambit: Software Tools for Game Theory.
[15] B. King-Casas, D. Tomlin, C. Anen, C.F. Camerer, S.R. Quartz and P.R. Montague (2005). Getting to
know you: Reputation and Trust in a two-person economic exchange. Science 308:78-83.
[16] D. Tomlin, M.A. Kayali, B. King-Casas, C. Anen, C.F. Camerer, S.R. Quartz and P.R. Montague (2006).
Agent-specific responses in cingulate cortex during economic exchanges. Science 312:1047-1050.
[17] L.P. Kaelbling, M.L. Littman and A.R. Cassandra. Planning and acting in partially observable stochastic
domains (1998). Artificial Intelligence.
[18] K. McCabe, D. Houser, L. Ryan, V. Smith, T. Trouard. A functional imaging study of cooperation in
two-person reciprocal exchange. Proc. Natl. Acad. Sci. USA 98:11832-35.
[19] K. Fliessbach, B. Weber, P. Trautner, T. Dohmen, U. Sunde, C.E. Elger and A. Falk. Social Comparison
Affects Reward-Related Brain Activity in the Human Ventral Striatum (2007). Science 318:1302-1305.
[20] B. Lau and P. W. Glimcher (2008). Representations in the Primate Striatum during Matching Behaviour.
Neuron 58.

8

"
2007,Measuring Neural Synchrony by Message Passing,,3322-measuring-neural-synchrony-by-message-passing.pdf,Abstract Missing,"Measuring Neural Synchrony by Message Passing

Justin Dauwels
Amari Research Unit
RIKEN Brain Science Institute
Wako-shi, Saitama, Japan
justin@dauwels.com
Franc?ois Vialatte, Tomasz Rutkowski, and Andrzej Cichocki
Advanced Brain Signal Processing Laboratory
RIKEN Brain Science Institute
Wako-shi, Saitama, Japan
{fvialatte,tomek,cia}@brain.riken.jp

Abstract
A novel approach to measure the interdependence of two time series is proposed,
referred to as ?stochastic event synchrony? (SES); it quantifies the alignment of
two point processes by means of the following parameters: time delay, variance
of the timing jitter, fraction of ?spurious? events, and average similarity of events.
SES may be applied to generic one-dimensional and multi-dimensional point processes, however, the paper mainly focusses on point processes in time-frequency
domain. The average event similarity is in that case described by two parameters:
the average frequency offset between events in the time-frequency plane, and the
variance of the frequency offset (?frequency jitter?); SES then consists of five parameters in total. Those parameters quantify the synchrony of oscillatory events,
and hence, they provide an alternative to existing synchrony measures that quantify amplitude or phase synchrony. The pairwise alignment of point processes
is cast as a statistical inference problem, which is solved by applying the maxproduct algorithm on a graphical model. The SES parameters are determined from
the resulting pairwise alignment by maximum a posteriori (MAP) estimation. The
proposed interdependence measure is applied to the problem of detecting anomalies in EEG synchrony of Mild Cognitive Impairment (MCI) patients; the results
indicate that SES significantly improves the sensitivity of EEG in detecting MCI.

1 Introduction
Synchrony is an important topic in neuroscience. For instance, it is hotly debated whether the
synchronous firing of neurons plays a role in cognition [1] and even in consciousness [2]. The synchronous firing paradigm has also attracted substantial attention in both the experimental (e.g., [3])
and the theoretical neuroscience literature (e.g., [4]). Moreover, medical studies have reported that
many neurophysiological diseases (such as Alzheimer?s disease) are often associated with abnormalities in neural synchrony [5, 6].
In this paper, we propose a novel measure to quantify the interdependence between point processes,
referred to as ?stochastic event synchrony? (SES); it consists of the following parameters: time delay,
variance of the timing jitter, fraction of ?spurious? events, and average similarity of the events. The
pairwise alignment of point processes is cast as a statistical inference problem, which is solved
by applying the max-product algorithm on a graphical model [7]. In the case of one-dimensional
point processes, the graphical model is cycle-free and statistical inference is exact, whereas for
1

multi-dimensional point processes, exact inference becomes intractable; the max-product algorithm
is then applied on a cyclic graphical model, which not necessarily yields the optimal alignment [7].
Our experiments, however, indicate that the it finds reasonable alignments in practice. The SES
parameters are determined from the resulting pairwise alignments by maximum a posteriori (MAP)
estimation.
The proposed method may be helpful to detect mental disorders such as Alzheimer?s disease, since
mental disorders are often associated with abnormal blood and neural activity flows, and changes in
the synchrony of brain activity (see, e.g., [5, 6]). In this paper, we will present promising results on
the early prediction of Alzheimer?s disease from EEG signals based on SES.
This paper is organized as follows. In the next section, we introduce SES for the case of onedimensional point processes. In Section 3, we consider the extension to multi-dimensional point
processes. In Section 4, we use our measure to detect abnormalities in the EEG synchrony of
Alzheimer?s disease patients.

2 One-Dimensional Point Processes
Let us consider the one-dimensional point processes (?event strings?) X and X 0 in Fig. 1(a) (ignore
Y and Z for now). We wish to quantify to which extent X and X 0 are synchronized. Intuitively
speaking, two event strings can be considered as synchronous (or ?locked?) if they are identical apart
from: (i) a time shift ?t ; (ii) small deviations in the event occurrence times (?event timing jitter?); (iii)
a few event insertions and/or deletions. More precisely, for two event strings to be synchronous, the
event timing jitter should be significantly smaller than the average inter-event time, and the number
of deletions and insertions should comprise only a small fraction of the total number of events.
This intuitive concept of synchrony is illustrated in Fig. 1(a). The event string X 0 is obtained from
event string X by successively shifting X over ?t (resulting in Y ), slightly perturbing the event
occurrence times (resulting in Z), and eventually, by adding (plus sign) and deleting (minus sign)
events, resulting in X 0 . Adding and deleting events in Z leads to ?spurious? events in X and X 0
(see Fig. 1(a); spurious events are marked in red): a spurious event in X is an event that cannot be
paired with an event in X 0 and vice versa.
The above intuitive reasoning leads to our novel measure for synchrony between two event strings,
i.e., ?stochastic event synchrony? (SES); for the one-dimensional case, it is defined as the triplet (?t ,
st , ?spur ), where st is the variance of the (event) timing jitter, and ?spur is the percentage of spurious
events
0
4 nspur + nspur
,
(1)
?spur =
n + n0
with n and n0 the total number of events in X and X 0 respectively, and nspur and n0spur the total
number of spurious events in X and X 0 respectively. SES is related to the metrics (?distances?)
proposed in [9]; those metrics are single numbers that quantify the synchrony between event strings.
In contrast, we characterize synchrony by means of three parameters, which allows us to distinguish
different types of synchrony (see [10]). We compute those three parameters by performing inference
in a probabilistic model. In order to describe that model, we consider Fig. 1(b), which shows a
symmetric procedure to generate X and X 0 . First, one generates an event string V of length `,
where the events Vk are mutually independent and uniformly distributed in [0, T0 ]. The strings Z
and Z 0 are generated by delaying V over ??t /2 and ?t /2 respectively and by (slightly) perturbing
the resulting event occurrence times (variance of timing jitter equals st /2). The sequences X and
X 0 are obtained from Z and Z 0 by removing some of the events; more precisely, from each pair
(Zk , Zk0 ), either Zk or Zk0 is removed with probability ps .
This procedure amounts to the statistical model:
p(x, x0 , b, b0 , v, ?t , st , `) = p(x|b, v, ?t , st )p(x0 |b0 , v, ?t , st )p(b, b0 |`)p(v|`)p(`)p(?t )p(st ),

(2)

where b and b0 are binary strings that indicate whether the events in X and X 0 are spurious (Bk = 1
if Xk is spurious, Bk = 0 otherwise; likewise for Bk0 ); the length ` has a geometric prior p(`) =
(1 ? ?)?` with ? ? (0, 1), and p(v|`) = T0?` . The prior on the binary strings b and b0 is given by
0

0

0

ntot

p(b, b0 |`) = (1 ? ps )n+n p2`?n?n
= (1 ? ps )n+n ps spur ,
s
2

(3)

with ntot
= nspur + n0spur = 2` ? n ? n0 the total number of spurious events in X and X 0 , nspur =
Pn spur
0
0
k=1 bk = ` ? n the number of spurious events in X, and likewise nspur , the number of spurious
0
0
events in X . The conditional distributions in X and X are equal to:
1?bk
n 
Y
? t st 
p(x|b, v, ?t , st ) =
N xk ? vik ; ? ,
(4)
2 2
k=1

p(x0 |b0 , v, ?t , st ) =

1?b0k
n0 
Y
? t st 
,
N x0k ? vi0k ; ,
2 2

(5)

k=1

where Vik is the event in V that corresponds to Xk (likewise Vi0k ), and N (x; m, s) is a univariate
Gaussian distribution with mean m and variance s. Since we do not wish/need to encode prior
information about ?t and st , we adopt improper priors p(?t ) = 1 = p(st ).
Eventually, marginalizing (2) w.r.t. v results in the model:
nnon-spur
Z
Y
0
0
0
0
ntot
spur
p(x, x , b, b , ?t , st , `) = p(x, x , b, b , v, ?t , st , `)dv ? ?
N (x0j 0 ? xjk ; ?t , st ),
k

(6)

k=1

with (xjk , x0j 0 ) the pairs of non-spurious events, nnon-spur = n + n0 ? ` the total number of nonk
q
spurious event pairs, and ? = ps T?0 ; in the example of Fig. 1(b), J = (1, 2, 3, 5, 6, 7, 8),
J 0 = (2, 3, 4, 5, 6, 7, 8), and nnon-spur = 7. In the following, we will denote model (6) by
p(x, x0 , j, j 0 , ?t , st ) instead of p(x, x0 , b, b0 , ?t , st , `), since for given x, x0 , b, and b0 (and hence given
n, n0 , and nnon-spur ), the length ` is fully determined, i.e., ` = n + n0 ? nnon-spur ; moreover, it is more
natural to describe the model in terms of J and J 0 instead of B and B 0 (cf. RHS of (6)). Note that
B and B 0 can directly be obtained from J and J 0 .
It also noteworthy that T0 , ? and ps do not need to be specified individually, since they appear in (6)
only through ?. The latter serves in practice as a knob to control the number of spurious events.
I
B

2
0

34
00

78 9
00 0

5 6
1 0

X
Z

X

V

T0

0
?t
2

Y
Z0

?t

?t
2

Z

X0

X0

B0

1

0 0 0

0

0 00

I0

1

2 3 4

6

7 89

(a) Asymmetric procedure

(b) Symmetric procedure

Figure 1: One-dimensional stochastic event synchrony.
Given event strings X and X 0 , we wish to determine the parameters ?t and st , and the hidden
variables B and B 0 ; the parameter ?spur (cf. (1)) can obtained from the latter :
Pn
P 0
bk + nk=1 b0k
4
?spur = k=1
.
(7)
n + n0
There are various ways to solve this inference problem, but perhaps the most natural one is cyclic
(0)
(0)
maximization: first one chooses initial values ??t and s?t , then one alternates the following two
update rules until convergence (or until the available time has elapsed):
(i) (i)
(?j (i+1) , ?j 0(i+1) ) = argmax p(x, x0 , j, j 0 , ??t , s?t )

(8)

b,b0

(i+1)

(??t

(i+1)

, s?t

) = argmax p(x, x0 , ?j (i+1) , ?j 0(i+1) , ?t , st ).
?t ,st

3

(9)

The update (9) is straightforward, it amounts to the empirical mean and variance, computed over
the non-spurious events. The update (8) can readily be carried out by applying the Viterbi algorithm
(?dynamic programming?) on an appropriate trellis (with the pairs of non-spurious events (xjk , x0j 0 )
k
as states), or equivalently, by applying the max-product algorithm on a suitable factor graph [7]; the
procedure is similar to dynamic time warping [8].

3 Multi-Dimensional Point Processes
In this section, we will focus on the interdependence of multi-dimensional point processes. As a
concrete example, we will consider multi-dimensional point processes in time-frequency domain;
the proposed algorithm, however, is not restricted to that particular situation, it is applicable to
generic multi-dimensional point processes.
Suppose that we are given a pair of (continuous-time) signals, e.g., EEG signals recorded from two
different channels. As a first step, the time-frequency (?wavelet?) transform of each signal is approximated as a sum of (half-ellipsoid) basis functions, referred to as ?bumps? (see Fig. 2 and [17]); each
bump is described by five parameters: time X, frequency F , width ?X, height ?F , and amplitude
W . The resulting bump models Y = ((X1 , F1 , ?X1 , ?F1 , W1 ), . . . , (Xn , Fn , ?Xn , ?Fn , Wn ))
and Y 0 = ((X10 , F10 , ?X10 , ?F10 , W10 ), . . . , (Xn0 0 , Fn0 0 , ?Xn0 0 , ?Fn0 0 , Wn0 0 )), representing the most
prominent oscillatory activity, are thus 5-dimensional point processes. Our extension of stochastic
event synchrony to multi-dimensional point processes (and bump models in particular) is derived
from the following observation (see Fig. 3): bumps in one time-frequency map may not be present
in the other map (?spurious? bumps); other bumps are present in both maps (?non-spurious bumps?),
but appear at slightly different positions on the maps. The black lines in Fig. 3 connect the centers
of non-spurious bumps, and hence, visualize the offset between pairs of non-spurious bumps. We
quantify the interdependence between two bump models by five parameters, i.e., the parameters
?spur , ?t , and st introduced in Section 2, in addition to:
? ?f : the average frequency offset between non-spurious bumps,
? sf : the variance of the frequency offset between non-spurious bumps.
We determine the alignment of two bump models in addition to the 5 above parameters by an inference algorithm similar to the one of Section 2, as we will explain in the following; we will use the
notation ? = (?t , st , ?f , sf ). Model (6) may naturally be extended in time-frequency domain as:
nnon-spur

  f0 ? f

 x0 ? x
k
k
k0
k0
;
?
,
s
N
;
?
,
s
t t
f f
?xk + ?x0k0
?fk + ?fk0 0
k=1


? p(?t )p st p(?f )p sf ,
tot

p(y, y 0 , j, j 0 , ?) ? ? nspur

x0k0

Y

N

(10)

fk0 0

where the offset
? xk in time and offset
? fk in frequency are normalized by the width and
height respectively of the bumps; we will elaborate on the priors on the parameters ? later on. In
principle, one may determine the sequences J and J 0 and the parameters ? by cyclic maximization
along the lines of (8) and (9). In the multi-dimensional case, however, the update (8) is no longer
tractable: one needs to allow permutations of events, the indices jk and jk0 0 are no longer necessarily
monotonically increasing, and as a consequence, the state space becomes drastically larger. As a
result, the Viterbi algorithm (or equivalently, the max-product algorithm applied on cycle-free factor
graph of model (10)) becomes impractical.
We solve this problem by applying the max-product algorithm on a cyclic factor graph of the system
at hand, which will amount to a suboptimal but practical procedure to obtain pairwise alignments
of multi-dimensional point processes (and bump models in particular). To this end, we introduce a
representation of model (10) that is naturally represented by a cyclic graph: for each pair of events
Yk and Yk00 , we introduce a binary variable Ckk0 that equals one if Yk and Yk00 form pair of nonspurious events and is zero otherwise. Since each event in Y associated to at most one event in Y 0 ,
we have the constraints:
0

n
X

k0 =1

0

4

C1k0 = S1 ? {0, 1},

n
X

0

4

C2k0 = S2 ? {0, 1}, . . . ,

k0 =1

n
X

k0 =1

4

4

Cnk0 = Sn ? {0, 1},

(11)

and similarly, each event in Y 0 is associated to at most one event in Y , which is expressed by a similar
set of constraints. The sequences S and S 0 are related to the sequences B and B 0 (cf. Section 2):
Bk = 1 ? Sk and Bk0 = 1 ? Sk0 . In this representation, the global statistical model (10) can be cast
as:
n
n0
Y
Y
0
0
p(y, y , b, b , c, ?) ?
(??[bk ? 1] + ?[bk ])
(??[b0k ? 1] + ?[b0k ])
k0 =1

k=1

!
  f0 ? f
 ckk0
 x0 ? x


k
k
k0
k0
; ? t , st N
; ? f , sf
p(?t )p st p(?f )p sf
N
0
0
?xk + ?xk0
?fk + ?fk0

0

?

n Y
n
Y

k=1 k0 =1

?

n
Y

0

?[bk +

n
X

0

ckk0

k0 =1

k=1

n
n
X
Y

? 1]
?[b0k0 +
ckk0 ? 1] .
k0 =1

(12)

k=1

Since we do not need to encode prior information about ?t and ?f , we choose improper priors
p(?t ) = 1 = p(?f ). On the other hand, we have prior knowledge about st and sf . Indeed, we expect
a bump in one time-frequency map to appear in the other map at about the same frequency, but there
may be some timing offset between both bumps. For example, bump nr. 1 in Fig. 3(a) (t = 10.7s)
should be paired with bump nr. 3 (t = 10.9s) and not with nr. 2 (t = 10.8s), since the former is much
closer in frequency than the latter. As a consequence, we a priori expect smaller values for sf than
for st . We encode this prior information by means of conjugate priors for st and sf , i.e., scaled
inverse chi-square distributions.
A factor graph of model (14) is shown in Fig. 4 (each edge represents a variable, each node corresponds to a factor of (14), as indicated by the arrows at the right hand side; we refer to [7] for an
introduction to factor graphs). We omitted the edges for the (observed) variables Xk , Xk0 0 , Fk , Fk0 0 ,
?Xk , ?Xk0 0 , ?Fk , and ?Fk0 0 in order not to clutter the figure.
Time-frequency map
Time-frequency map

?

?

Bump model

Bump model

?
Figure 2: Two-dimensional stochastic event synchrony.
We determine the alignment C = (C11 , C12 , . . . , Cnn0 ) and the parameters ? = (?t , st , ?f , sf ) by
maximum a posteriori (MAP) estimation:
? = argmax p(y, y 0 , c, ?),
(?
c, ?)
(13)
c,?

where p(y, y 0 , c, ?) is obtained from (14) by marginalizing over b and b0 :
0

p(y, y , c, ?) ?

n 
Y

k=1

?

n
Y

n0
Y

k=1 k0 =1

0

??

n
X

ckk0 + ?

k0 =1

n
X

ckk0 ? 1

k0 =1

??

k0 =1

n
X

n

X

ckk0 + ?
ckk0 ? 1

k=1
!ckk0

k=1



p(?t )p st p(?f )p sf .

(14)

30
1

25

2
3

25

f [Hz]

20
15
10
5
00

n0 
 Y

 x0 ? x
  f0 ? f

k
k
k0
k0
N
;
?
,
s
N
;
?
,
s
t
t
f
f
?xk + ?x0k0
?fk + ?fk0 0

30

f [Hz]

0



20
15
10
5

5

10

t [s]

15

00

20

(a) Bump models of two EEG channels.

5

10

t [s]

15

20

(b) Non-spurious bumps (?spur = 27%); the
black lines connect the centers of non-spurious
bumps.

Figure 3: Spurious and non-spurious activity.
5

?

??

?

B20

B1
?
?

0

?

?

B10

?
?

?
...

B2
?
?

?
?

= ...

=

?

Bn0 0

?[bn ] + ??[bn ? 1]

Bn
?
?

?
?

= ...

=

?[bn +

??0
??

Pn0

k0 =1 cnk0

? 1]

??

=
C11
N

= ...
C12

=

C1n0

N ...

N

=
C21
N

C2n0

C22

N ...

??(k)

...

Cn1

N

Cn2

N

=

? = (?t , st , ?f , sf )

=

N
??(k)



Cnn0

N ...
x0n0 ?xn
?xn +?x0n0

ttt

N

  0

f ?f
; ?t , st N ?fnn0+?fn0 ; ?f , sf
n0

!cnn0

p(?t , st , ?f , sf ) = p(?t )p(st )p(?f )p(sf )

Figure 4: Factor graph of model (14).
From c?, we obtain the estimate ??spur as:
Pn ?
Pn0 ?0
Pn Pn0
n + n0 ? 2 k=1 k0 =1 c?kk0
k=1 bk +
k=1 bk0
??spur =
=
.
(15)
n + n0
n + n0
The MAP estimate (13) is intractable, and we try to obtain (13) by cyclic maximization: first, the
(0)
(0)
(0)
(0)
parameters ? are initialized: ??t = 0 = ?f , s?t = s?0,t , and s?f = s0,f , then one alternates the
following two update rules until convergence (or until the available time has elapsed):
c?(i+1) = argmax p(y, y 0 , c, ??(i) )
(16)
c

??(i+1) = argmax p(y, y 0 , c?(i+1) , ?).

(17)

?

The estimate ??(i+1) (17) is available in closed-form; indeed, it is easily verified that the point es(i+1)
(i+1)
timates ??t
and ??f
are the (sample) mean of the timing and frequency offset respectively,
(i+1)

computed over all pairs of non-spurious events. The estimates s?t
larly.

(i+1)

and s?f

are obtained simi-

Update (16), i.e., finding the optimal pairwise alignment C for given values ??(i) of the parameters ?,
is less straightforward: it involves an intractable combinatorial optimization problem. We attempt
to solve that problem by applying the max-product algorithm to the (cyclic) factor graph depicted
in Fig. 4 [7]. Let us first point out that, since the alignment C is computed for given ? = ??(i) ,
the (upward) messages along the edges ? are the point estimate ??(i) (cf. (16)); equivalently, for the
purpose of computing (16), one may remove the ? edges and the two bottom nodes in Fig. 4; the
N -nodes then become leaf nodes. The other messages in the graph are iteratively updated according
to the generic max-product update rule [7].
The resulting inference algorithm for computing (16) is summarized in Table 1. The messages
?
??(ckk0 ) and ?? 0 (ckk0 ) propagate upward along the edges ckk0 towards the ?-nodes
connected to
the edges Bk and Bk0 0 respectively (see Fig. 4, left hand side); the messages ??(ckk0 ) and ??0 (ckk0 )
?
propagate downward along the edges ckk0 from the ?-nodes
connected to the edges Bk and Bk0 0
respectively. After initialization (18) of the messages ??(ckk0 ) and ?? 0 (ckk0 ) (k = 1, 2, . . . , n; k 0
= 1, 2, . . . , n0 ), one alternatively updates (i) the messages ??(ckk0 ) (19) and ??0 (ckk0 ) (20), (ii) the
messages ??(ckk0 ) (21) and ?? 0 (ckk0 ) (22), until convergence; it is noteworthy that, although the
max-product algorithm is not guaranteed to converge on cyclic graphs, we observed in our experiments (see Section 4) that alternating the updates (19)?(22) always converged to a fixed point. At
last, one computes the marginals p(ckk0 ) (23), and from the latter, one may determine the decisions
c?kk0 by greedy decimation.

4 Diagnosis of MCI from EEG
We analyzed rest eyes-closed EEG data recorded from 21 sites on the scalp based on the 10?20
system. The sampling frequency was 200 Hz, and the signals were bandpass filtered between 4
6

Initialization
??(ckk0 ) = ??0 (ckk0 ) ?

N

 x0 ? x
  f0 ? f

k
k
k0
k0
; ? t , st N
; ? f , sf
?xk + ?x0k0
?fk + ?fk0 0

!ckk0

Iteratively compute messages until convergence
A. Downward messages:

 

??(ckk0 = 0)
max (?, max`0 6=k0 ??(ck`0 = 1)/??(ck`0 = 0))
?
??(ckk0 = 1)
1
 0
 

?? (ckk0 = 0)
max (?, max`6=k ??0 (c`k0 = 1)/??0 (c`k0 = 0))
?
0
?? (ckk0 = 1)
1

(18)

(19)
(20)

B. Upward messages:
!
  f0 ? f
 ckk0
 x0 ? x
k
k
k0
k0
; ? t , st N
; ? f , sf
0
0
?xk + ?xk0
?fk + ?fk0
!
 x0 ? x
  f0 ? f
 ckk0
0
k
k
k
k0
??0 (ckk0 ) ? ??(ckk0 ) N
;
?
,
s
N
;
?
,
s
t t
f f
?xk + ?x0k0
?fk + ?fk0 0
??(ckk0 ) ? ??0 (ckk0 ) N

Marginals
p(ckk0 ) ? ??(ckk0 )??0 (ckk0 ) N

 x0 ? x
  f0 ? f

k
k
k0
k0
; ? t , st N
; ? f , sf
?xk + ?x0k0
?fk + ?fk0 0

!ckk0

(21)
(22)

(23)

Table 1: Inference algorithm.
and 30Hz. The subjects comprised two study groups: the first consisted of a group of 22 patients
diagnosed as suffering from MCI, who subsequently developed mild AD. The other group was a
control set of 38 age-matched, healthy subjects who had no memory or other cognitive impairments.
Pre-selection was conducted to ensure that the data were of a high quality, as determined by the
presence of at least 20s of artifact free data. We computed a large variety of synchrony measures
for both data sets; the results are summarized in Table 2. We report results for global synchrony,
obtained by averaging the synchrony measures over 5 brain regions (frontal, temporal left and right,
central, occipital). For SES, the bump models were clustered by means of the aggregation algorithm
described in [17].
The strongest observed effect is a significantly higher degree of background noise (?spur ) in MCI
patients, more specifically, a high number of spurious, non-synchronous oscillatory events (p =
0.00021). We verified that the SES measures are not correlated (Pearson r) with other synchrony
measures (p > 0.10); in contrast to the other measures, SES quantifies the synchrony of oscillatory
events (instead of more conventional amplitude or phase synchrony). Combining ?spur with ffDTF
yields good classification of MCI vs. Control patients (see Fig.5(a)). Interestingly, we did not observe a significant effect on the timing jitter st of the non-spurious events (p = 0.91). In other words,
AD seems to be associated with a significant increase of spurious background activity, while the
non-spurious activity remains well synchronized. Moreover, only the non-spurious activity slows
down (p = 0.0012; see Fig.5(c)), the average frequency of the spurious activity is not affected in MCI
patients (see Fig.5(c)). In future work, we will verify those observations by means of additional data
sets.
Measure

Cross-correlation

Coherence

Phase Coherence

Corr-entropy

Wave-entropy

p-value

0.028?

0.060

0.72

0.27

0.012?

References

[16]

[18]

[20]

Measure

Granger coherence

Partial Coherence

PDC

DTF

ffDTF

dDTF

p-value

0.15

0.16

0.60

0.34

0.0012??

0.030?

Measure

Kullback-Leibler

R?enyi

Jensen-Shannon

Jensen-R?enyi

IW

I

p-value

0.072

0.076

0.084

0.12

0.060

0.080

Measure

Nk

Sk

Hk

S-estimator

p-value

0.032?

0.29

0.090

0.33

Wavelet Phase

Evolution Map

Instantaneous Period

0.082

0.072

References

[13]

References

[23]

References

[22]

[15]

Measure

Hilbert Phase

p-value

0.15

References

[21]

[24]

0.020?
[19]

Measure

st

?spur

p-value

0.91

0.00021??

Table 2: Sensitivity of synchrony measures for early prediction of AD (p-values for Mann-Whitney
test; * and ** indicate p < 0.05 and p < 0.005 respectively). N k , S k , and H k are three measures
of nonlinear interdependence [15].
7

MCI
CTR

0.4

0.3

fspur

?spur

0.35

0.25

19

19

18

18

17

17

fnon-spur

0.45

16
15

0.2

14

16
15
14

0.15

13
0.1
0.045

0.05

Fij2

0.055

(a) ?spur vs. ffDTF

0.06

12

13

MCI

CTR

12

CTR

MCI

(b) Av. frequency of the spuri- (c) Av. frequency of the nonous activity (p = 0.87)
spurious activity (p = 0.0019)

Figure 5: Results.

References
[1] F. Varela, J. P. Lachaux, E. Rodriguez, and J. Martinerie, ?The Brainweb: Phase Synchronization and
Large-Scale Integration?, Nature Reviews Neuroscience, 2(4):229?39, 2001.
[2] W. Singer, ?Consciousness and the Binding Problem,? Annals of the New York Academy of Sciences,
929:123?146, April 2001.
[3] M. Abeles, H. Bergman, E. Margalit, and E. Vaadia, ?Spatiotemporal Firing Patterns in the Frontal Cortex
of Behaving Monkeys,? J. Neurophysiol, 70(4):1629?1638. 1993.
[4] S. Amari, H. Nakahara, S. Wu, and Y. Sakai, ?Synchronous Firing and Higher-Order Interactions in Neuron
Pool,? Neural Computation, 15:127?142, 2003.
[5] H. Matsuda, ?Cerebral Blood Flow and Metabolic Abnormalities in Alzheimer?s Disease,? Ann. Nucl. Med.,
vol. 15, pp. 85?92, 2001.
[6] J. Jong, ?EEG Dynamics in Patients with Alzheimer?s Disease,? Clinical Neurophysiology, 115:1490?1505
(2004).
[7] H.-A. Loeliger, ?An Introduction to Factor Graphs,? IEEE Signal Processing Magazine, Jan. 2004, pp. 28?
41.
[8] C. S. Myers and L. R. Rabiner, ?A Comparative Study of Several Dynamic Time-Warping Algorithms for
Connected Word Recognition,? The Bell System Technical Journal, 60(7):1389?1409, September 1981.
[9] J. D. Victor and K. P. Purpura, ?Metric-space Analysis of Spike Trains: Theory, Algorithms, and Application,? Network: Comput. Neural Systems, 8:17, 164, 1997.
[10] H. P. C. Robinson, ?The Biophysical Basis of Firing Variability in Cortical Neurons,? Chapter 6 in Computational Neuroscience: A Comprehensive Approach, Mathematical Biology & Medicine Series, Edited
By Jianfeng Feng, Chapman & Hall/CRC, 2003.
[11] E. Pereda, R. Q. Quiroga, and J. Bhattacharya, ?Nonlinear Multivariate Analysis of Neurophsyiological
Signals,? Progress in Neurobiology, 77 (2005) 1?37.
[12] M. Breakspear, ?Dynamic Connectivity in Neural Systems: Theoretical and Empirical Considerations,?
Neuroinformatics, vol. 2, no. 2, 2004.
[13] M. Kami?nski and Hualou Liang, ?Causal Influence: Advances in Neurosignal Analysis,? Critical Review
in Biomedical Engineering, 33(4):347?430 (2005).
[14] C. J. Stam, ?Nonlinear Dynamical Analysis of EEG and MEG: Review of an Emerging Field,? Clinical
Neurophysiology 116:2266?2301 (2005).
[15] R. Q. Quiroga, A. Kraskov, T. Kreuz, and P. Grassberger, ?Performance of Different Synchronization
Measures in Real Data: A Case Study on EEG Signals,? Physical Review E, vol. 65, 2002.
[16] P. Nunez and R. Srinivasan, Electric Fields of the Brain: The Neurophysics of EEG, Oxford University
Press, 2006.
[17] F. Vialatte, C. Martin, R. Dubois, J. Haddad, B. Quenet, R. Gervais, and G. Dreyfus, ?A Machine Learning
Approach to the Analysis of Time-Frequency Maps, and Its Application to Neural Dynamics,? Neural
Networks, 2007, 20:194?209.
[18] Jian-Wu Xu, H. Bakardjian, A. Cichocki, and J. C. Principe, ?EEG Synchronization Measure: a Reproducing Kernel Hilbert Space Approach,? submitted to IEEE Transactions on Biomedical Engineering
Letters, Sept. 2006.
[19] M. G. Rosenblum, L. Cimponeriu, A. Bezerianos, A. Patzak, and R. Mrowka, ?Identification of Coupling
Direction: Application to Cardiorespiratory Interaction,? Physical Review E, 65 041909, 2002.
[20] C. S. Herrmann, M. Grigutsch, and N. A. Busch, ?EEG Oscillations and Wavelet Analysis,? in Todd
Handy (ed.) Event-Related Potentials: a Methods Handbook, pp. 229-259, Cambridge, MIT Press, 2005.
[21] C. Carmeli, M. G. Knyazeva, G. M. Innocenti, and O. De Feo, ?Assessment of EEG Synchronization
Based on State-Space Analysis,? Neuroimage, 25:339?354 (2005).
[22] A. Kraskov, H. St?ogbauer, and P. Grassberger, ?Estimating Mutual Information,? Phys. Rev. E 69 (6)
066138, 2004.
[23] S. Aviyente, ?A Measure of Mutual Information on the Time-Frequency Plane,? Proc. of ICASSP 2005,
vol. 4, pp. 481?484, March 18?23, 2005, Philadelphia, PA, USA.
[24] J.-P. Lachaux, E. Rodriguez, J. Martinerie, and F. J. Varela, ?Measuring Phase Synchrony in Brain Signals,? Human Brain Mapping 8:194208 (1999).

8

"
1988,Applications of Error Back-Propagation to Phonetic Classification,,109-applications-of-error-back-propagation-to-phonetic-classification.pdf,Abstract Missing,"206

APPLICATIONS OF
~RROR BACK-PROPAGATION
TO PHONETIC CLASSIFICATION
Hong C. Leung & Victor W. Zue
Spoken Language Systems Group
Laboratory for Computer Science
Massachusetts Institute of Technology
Cambridge, MA 02139

ABSTRACT
This paper is concerced with the use of error back-propagation
in phonetic classification. Our objective is to investigate the basic characteristics of back-propagation, and study how the framework of multi-layer perceptrons can be exploited in phonetic recognition. We explore issues such as integration of heterogeneous
sources of information, conditioll~ that can affect performance of
phonetic classification, internal representations, comparisons with
traditional pattern classification techniques, comparisons of different error metrics, and initialization of the network. Our investigation is performed within a set of experiments that attempts to recognize the 16 vowels in American English independent of speaker.
Our results are comparable to human performance.
Early approaches in phonetic recognition fall into two major extremes: heuristic
and algorithmic. Both approaches have their own merits and shortcomings. The
heuristic approach has the intuitive appeal that it focuses on the linguistic information in the speech signal and exploits acoustic-phonetic knowledge. HO'fever, the
weak control strategy used for utilizing our knowledge has been grossly inadequate.
At the other extreme, the algorithmic approach relies primarily on the powerful control strategy offered by well-formulated pattern recognition techniques. However,
relatively little is known about how our speech knowledge accumulated over the
past few decades can be incorporated into the well-formulated algorithms. We feel
that artificial neural networks (ANN) have some characteristics that can potentially
enable them to bridge the gap between these two extremes. On the one hand, our
speech knowledge can provide guidance to the structure and design of the network.
On the other hand, the self-organizing mechanism of ANN can provide a control
strategy for utilizing our knowledge.
In this paper, we extend our earlier work on the use of artificial neural networks
for phonetic recognition [2]. Specifically, we focus our investigation on the following
sets of issues. First, we describe the use of the network to integrate heterogeneous
sources of information. We will see how classification performance improves as more

Error Back-Propagation to Phonetic Classification

information is available. Second, we discuss several important factors that can substantially affect the performance of phonetic classification. Third, we examine the
internal representation of the network. Fourth, we compare the network with two
traditional classification techniques: K-nearest neighbor and Gaussian classification. Finally, we discuss our specific implementations of back-propagation that
yield improved performance and more efficient learning time.

EXPERIMENTS
Our investigation is performed within the context of a set of experiments that
attempts to recognize the 16 vowels in American English independent of speaker.
The vowels are excised from continuous speech and they can be preceded and followed by any phonemes, thus providing a rich environment to study contextual
influence. We assume that the locations of the vowels have been detected. Given a
time region, the network determines which one of the 16 vowels was spoken.

CORPUS
As Table 1 shows, our training set consists of 20,000 vowel tokens, excised from
2,500 continuous sentences spoken by 500 male and female speakers. The test set
consists of about 2,000 vowel tokens, excised from 250 sentences spoken by 50 different speakers. All the data are extracted from the TIMIT database, which has a
wide range of American dialectical variations [1]. The speech signal is represented
by spectral vectors obtained from an auditory model [4]. Speaker and energy normalization are also performed [5].

Training
Testing

Tokens
20,000
2,000

Sentences
2500
250

Speakers (M/F)
500 (350/150)
50 (33/17)

Table 1: Corpus extracted from the TIMIT database.

NETWORK STRUCTURE
The structure of the network we have examined most extensively has 1 hidden
layer as shown in Figure 1. It has 16 output units, with one unit for each of the 16
vowels. In order to capture dynamic information, the vowel region is divided into
three equal subregions. An average spectrum is then computed in each subregion.
These 3 average spectra are then applied to the first 3 sets of input units. Additional
sources of information, such as duration and local phonetic contexts, can also be
made available to the network. While spectral and durational inputs are continuous
and numerical, the contextual inputs are discrete and symbolic.

207

208

Leung and Zue

output from auditory model
(synchrony spectrogram)

-.. . . . .-..-..~.~j:.I._._. . ---.. . . . .-.. -.~
Figure 1: Basic structure of the network.

HETEROGENEOUS INFORMATION INTEGRATION
In our earlier study, we have examined the integration of the Synchrony Envelopes and the phonetic contexts [2]. The Synchrony Envelopes, an output of
the auditory model, have been shown to enhance the formant information. In this
study, we add additional sources of information. Figure 2 shows the performance
as heterogeneous sources of information are made available to the network. The
performance is about 60% when only the Synchrony Envelopes are available. The
performance improves to 64% when the Mean Rate Response, a different output of
the auditory model which has been shown to enhance the temporal aspects of the
speech signal, is also available. We can also see that the performance improves consistently to 77% as durational and contextual inputs are provided to the network.
This experiment suggests that the network is able to make use of heterogeneous
sources of information, which can be numerical and/or symbolic.

Error Back-Propagation to Phonetic Classification

One may ask how well human listeners can recognize the vowels. Experiments
have been performed to study how well human listeners agree with each other when
they can only listen to sequences of 3 phonemes, i.e. the phoneme before the vowel,
the vowel itself, and the phoneme after the vowel [3]. Results indicate that the
average agreement among the listeners on the identities of the vowels is between
65% and 70%.
80?
:

70?

605lr
Synchrony
Add
Envelopes Mean Rate
Response

Add
Duration

Add
Phonetic
Context

Sources of Information
Figure 2: Integration of heterogeneous sources of information.

PERFORMANCE RESULTS
We have seen that one of the important factors for the network performance
is the amount of information available to the network. To gain additional insights
about how the network performs under different conditions, several experiments
were conducted using different databases. In these and the subsequent experiments
we describe in this paper, only the Synchrony Envelopes are available to the network.
Table 2 shows the performance results for several recognition tasks. In each
of these tasks, the network is trained and tested with independent sets of speech
data. The first task recognizes vowels spoken by one speaker and excised from the
fbf-vowel-ftf environment, spoken in isolation. This recognition task is relatively
straightforward, resulting in perfect performance. In the second experiment, vowel
tokens are extracted from the same phonetic context, but spoken by 17 male and
female speakers. Due to inter-speaker variability, the accuracy degrades to 86%.
The third task recognizes vowels spoken by one speaker and excised from an unrestricted context, spoken continuously. We can see that the accuracy decreases
further to 70%. Finally, data from the TIM IT database are used, spoken by multiple speakers. The accuracy drops to 60%. These results indicate that a substantial
difference in performance can be expected under different conditions, depending on
whether the task is speaker-independent, what is the restriction on the phonetic

209

210

Leung and Zue

Speakers(M/F)
1(1/0)
17(8/9)
1(1/0)
500(350/150)

Context
b
b

-

-

t
t

* *
*- *

Training
Tokens
64
256
3,000
20,000

Percent
Correct
100
86
70
60

Remark
isolated
isolated
continuous
continuous

Table 2: Performance for different tasks, using only the synchrony spectral information. ""*,, stands for any phonetic contexts.

contexts, whether the speech material is spoken continuously, and how much data
are used to train the network.

INTERNAL REPRESENTATION
To understand how the network makes use of the input information, we examined the connection weights of the network. A vector is formed by extracting the
connections from all the hidden units to one output unit as shown in Figure 3a. The
same process is repeated for all output units to obtain a total of 16 vectors. The
correlations among these vectors are then examined by measuring the inner products or the angles between them. Figure 3b shows the distribution of the angles
after the network is trained, as a function of the number of hidden units. The circles
represent the mean of the distribution and the vertical bars stand for one standard
deviation away from the mean. As the number of hidden units increases, the distribution becomes more and more concentrated and the vectors become increasingly
orthogonal to each other.
The correlations of the connection weights before training were also examined,
as shown in Figure 3c. Comparing parts (b) and (c) of Figure 3, we can see that
the distributions before and after training overlap more and more as the number of

hidden units increases. With 128 hidden units, the two distributions are actually
quite similar. This leads us to suspect that perhaps the connection weights between
the hidden and the output layer need not be trained if we have a sufficient number
of hidden units.
Figure 4a shows the performance of recognizing the 16 vowels using three different techniques: (i) train all the connections in the network, (ii) fix the connections
between the hidden and output layers after random initialization and train only
the connections between the input and hidden layers, and (iii) fix the connections
between the input and hidden layers and train only the connections between the
hidden and output layers. We can see that with enough hidden units, training only
the connections between the input and the hidden layers achieves almost the same
performance as training all the connections in the network. We can also see that

Error Back-Propagation to Phonetic Classification

for the same number of hidden units, training only the connections between the
input and the hidden layer can achieve higher performance than training only the
connections between the hidden and the output layer.
Figure 4b compares the three training techniques for 8 vowels, resulting in 8
output units only. We can see similar characteristics in both parts (a) and (b) of
Figure 4.
Output
Layer

Hidden
Layer

Cl"" Y

""j

?r""Y""J'

??

'~',~,"">-<.""-----';,..
.........: . ,

Input

Layer

(a)

150

150

- 130
l110
!w 90

-;;- 130

Ja

70
~
< 50
30

1

I
f
I

l

Iff

!

w

u

I

Cib
c

110
90
70

< 50

10

100

Number of Hidden Units
(b)

toOO

30

1

10

100

1000

Number of Hidden Units

(c)

Figure 3: (a) Correlations of the vectors from the hidden to output layers are
examined. (b) Distribution of the angles between these vectors after training. (c)
Distribution of the angles between these vectors before training.

COMPARISONS WITH TRADITIONAL TECHNIQUES
One of the appealing characteristics of back-propagation is that it does not assume any probability distributions or distance metrics. To gain further insights, we
compare with two traditional pattern classification techniques: K-nearest neighbor
(KNN) and multi-dimensional Gaussian classifiers.

211

212

Leung and Zue

70

y.

(i)

60

~... 50
...

8
1:

~

CD

/
o

30

Q.

cI/

20
1

o ..?. o-..?

/--C)
III

/

~...

8
1:

70
50

~.c//\

?

(iii)

~.

...
CD

\("")

~

I

,....-0/

u

Q.

I

,

CD

""""

.?..
"".?..0.???.0

p....

?~D

..p/

iI

40

10

90 (i) __ ..0.???? 0-._.

30

o

'Qi)

II

10

100

Number of Hidden Units

(a)

1000

10

1

10
100
Number of Hidden Units

1000

(b)

Figure 4: Performance of recognizing (a) 16 vowels, (b) 8 vowels when (i) all the
connections in the network are trained, (ii) only the connections between the input
and hidden layers are trained, and (iii) only the connections between the hidden
and output layers are trained.
Figure 5a compares the performance results of the network with those of KNN,
for different amounts of training tokens. Again, only the Synchrony Envelopes
are made available to the network, resulting in input vectors of 100 dimensions.
Each cluster of crosses corresponds to performance results of ten networks, each
one randomly initialized differently. Due to different initialization, a fluctuation of
2% to 3% is observed even for the same training size. For comparison, we perform
KNN using the Euclidean distance metric. For each training size, we run KNN 6
times, each one with a different K, which is chosen to be proportional to the square
root of the number of training tokens, N. For simplicity, Figure 5a shows results for
only 3 different values of K: (i) K = Vii, (ii) K = 10Vii, and (iii) K = 1. In this
experiment, we have found that the performance is the best when K = ..fFi and is
the worst when K
1. We have also found that up to 20,000 training tokens, the
network consistently compares favorably to KNN. It is possible that the network is
able to find its own distance metric to achieve better performance.

=

Since the true underlying probability distribution is unknown, we assume multidimensional Gaussian distribution in the second experiment. (i) We use the full
covariance matrix, which has 100zl00 elements. To avoid problems with singularity,
we obtain results only for large number of training tokens. (ii) We use the diagonal
covariance matrix which has non-zero elements only along the diagonal. We can
see from Figure 5b that the network compares favorably to the Gaussian classifiers.
Our results also suggest that the Gaussian assumption is invalid.

Error Back-Propagation to Phonetic Classification

60

~

50

~

40

iii

.t.

! ...!

~--....-,
.......

... ...........
j.. .......-:.?._.....
/

J"""""".'
........
/""Jr/

(ii)
(iii)

] 50
'e!

~

(.-

Q.

~

??

60

(i)

40

I

.....
)..-/

D????

~~:~)

I

30~----------------------------------~
~~------------------------------------~
100000
1000
10000
100
1000
10000
100000 100
Number of Training Tokens
Number of Training Tokens

(a)

(b)

Figure 5: (a) Comparison with KNN for different values of K (See text). (b)
Comparison with Gaussian classification when using the (i) full covariance matrix,
and (ii) diagonal covariance matrix. Each cluster of 10 crosses corresponds to the
results of 10 different networks, each one randomly initialized.

ERROR METRIC AND INITIALIZATION
In order to take into account the classification performance of the network more
explicitly, we have introduced a weighted mean square error metric [2]. By modulating the mean square error with weighting factors that depend on the classification performance, we have shown that the rank order statistics can be improved.
Like simulated annealing, gradient descent takes relatively big steps when the performance is poor, and takes smaller and smaller steps as the performance of the
network improves.
Results also indicate that it is more likely for a unit output to be initially in the
saturation regions of the sigmoid function if the network is randomly initialized.
This is not desirable since learning is slow when a unit output is in a saturation
region. Let the sigmoid function goes from -1 to 1. If the connection weights
between the input and the hidden layers are initialized with zero weights, then all
the hidden unit outputs in the network will initially be zero, which in turn results in
zero output values for all the output units. In other words, all the units will initially
operate at the center of the transition region of the sigmoid function, where learning
is the fastest. We call this method center initialization (CI).
Parts (a) and (b) of Figure 6 compare the learning speed and performance,
respectively, of the 3 different techniques: (i) mean square error (MSE), (ii) weighted
mean square error (WMSE), and (iii) center initialization (CI) with WMSE. We can
see that both WMSE and CI seem to be effective in improving the learning time
and the performance of the network.

213

214

Leung and Zue

K'""

(iii)

(i)
(ii)

30~--~----------------~

o

10

20

30

40

Number of Training Iterations

50

30~4-------------~----~

100

1000

10000

100000

Number of Training Tokens

(a)

(b)

Figure 6: Comparisons of the (a) learning characteristics and, (b) performance
results, for the 3 different techniques: (i) MSE, (ii) WMSE, and (iii) CI with WMSE.
Each point corresponds to the average of 10 different networks, each one initialized
randomly.

SUMMARY
In summary, we have described a set of experiments that were designed to help
us get a better understanding of the use of back-propagation in phonetic classification. Our results are encouraging and we are hopeful that artificial neural networks
may provide an effective framework for utilizing our acoustic-phonetic knowledge
in speech recognition.

References
[1] Fisher, W.E., Doddington, G.R., and Goudie-Marshall, K.M., ""The DARPA
Speech Recognition Research Database: Specifications and Status,"" Proceedings of the DARPA Speech Recognition Workshop Report No. SAIC-86/1546,
February, 1986.
[2] Leung, H.C., ""Some phonetic recognition experiments using artificial neural
nets/' ICASSP-88, 1988.
[3] Phillips, M.S., ""Speaker independent classification of vowels and diphthongs
in continuous speech,"" Proc. of the 11th International Congress of Phonetic
Sciences, Estonia, USSR, 1987.
[4] Seneff S., ""A computational model for the peripheral auditory system: application to speech recognition research,"" Proc. ICASSP, Tokyo, 1986.
[5] Seneff S., ""Vowel recognition based on 'line-formants' derived from an auditorybased spect(al representation,"" Proc. of the 11th International Congress of
Phonetic Sciences, Estonia, USSR, 1987.

"
2013,Inferring neural population dynamics from multiple partial recordings of the same neural circuit,Spotlight,4874-inferring-neural-population-dynamics-from-multiple-partial-recordings-of-the-same-neural-circuit.pdf,"Simultaneous recordings of the activity of large neural populations are extremely valuable as they can be used to infer the dynamics and interactions of neurons in a local circuit, shedding light on the computations performed. It is now possible to measure the activity of hundreds of neurons  using 2-photon calcium imaging. However, many computations are thought to involve circuits consisting of thousands of neurons, such as cortical barrels in rodent somatosensory cortex. Here we contribute a statistical method for stitching"" together sequentially imaged sets of neurons into one model  by phrasing the problem as fitting a latent dynamical system with missing observations. This method allows us to substantially expand the population-sizes for which population dynamics can be characterized---beyond the number of simultaneously imaged neurons. In particular, we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible to predict noise correlations between non-simultaneously recorded neuron pairs.""","Inferring neural population dynamics from multiple
partial recordings of the same neural circuit
Srinivas C. Turaga?1,2 , Lars Buesing1 , Adam M. Packer2 , Henry Dalgleish2 , Noah Pettit2 , Michael
H?ausser2 and Jakob H. Macke3,4
1
2

Gatsby Computational Neuroscience Unit, University College London
Wolfson Institute for Biomedical Research, University College London
3
Max-Planck Institute for Biological Cybernetics, T?ubingen
4
Bernstein Center for Computational Neuroscience, T?ubingen

Abstract
Simultaneous recordings of the activity of large neural populations are extremely
valuable as they can be used to infer the dynamics and interactions of neurons in
a local circuit, shedding light on the computations performed. It is now possible
to measure the activity of hundreds of neurons using 2-photon calcium imaging.
However, many computations are thought to involve circuits consisting of thousands of neurons, such as cortical barrels in rodent somatosensory cortex. Here we
contribute a statistical method for ?stitching? together sequentially imaged sets of
neurons into one model by phrasing the problem as fitting a latent dynamical system with missing observations. This method allows us to substantially expand the
population-sizes for which population dynamics can be characterized?beyond
the number of simultaneously imaged neurons. In particular, we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible
to predict noise correlations between non-simultaneously recorded neuron pairs.

1

Introduction

The computation performed by a neural circuit is a product of the properties of single neurons in the
circuit and their connectivity. Simultaneous measurements of the collective dynamics of all neurons
in a neural circuit will help us understand their function and test theories of neural computation.
However, experimental limitations make it difficult to measure the joint activity of large populations
of neurons. Recent progress in 2-photon calcium imaging now allows for recording of the activity of hundreds of neurons nearly simultaneously [1, 2]. However, in neocortex where circuits or
subnetworks can span thousands of neurons, current imaging techniques are still inadequate.
We present a computational method to more effectively leverage currently available experimental
technology. To illustrate our method consider the following example: A whisker barrel in the mouse
somatosensory cortex consists of a few thousand neurons responding to stimuli from one whisker.
Modern microscopes can only image a small fraction?a few hundred neurons?of this circuit. But
since nearby neurons couple strongly to one another [3], by moving the microscope to nearby locations, one can expect to image neurons which are directly coupled to the first population of neurons.
In this paper we address the following question: Could we characterize the joint dynamics of the
first and second populations of neurons, even though they were not imaged simultaneously? Can we
estimate correlations in variability across the two populations? Surprisingly, the answer is yes.
We propose a statistical tool for ?stitching? together measurements from multiple partial observations of the same neural circuit. We show that we can predict the correlated dynamics of large
?

sturaga@gatsby.ucl.ac.uk

1

a

imaging
session 1

b

imaging
session 2

couplings (A)

simultaneously
measured pairs
non-simultaneously
measured pairs

Figure 1: Inferring neuronal interactions from non-simultaneous measurements. a) If two
subsets of a neural population can only be recorded from in two separate imaging sessions, can
we infer the connectivity across the sub-populations (red connections)? b) We want to infer the
functional connectivity matrix, and in particular those entries which correspond to pairs of neurons
that were not simultaneously measured (red off-diagonal block). While the two sets of neurons
are pictured as non-overlapping here, we will also be interested in the case of partially overlapping
measurements.

populations of neurons even if many of the neurons have not been imaged simultaneously. In sensory cortical neurons, where large variability in the evoked response is observed [4, 5], our model
can successfully predict the magnitude of (so-called) noise correlations between non-simultaneously
recorded neurons. Our method can help us build data-driven models of large cortical circuits and
help test theories of circuit function.
Related recent research. Numerous studies have addressed the question of inferring functional
connectivity from 2-photon imaging data [6, 7] or electrophysiological measurements [8, 9, 10, 11].
These approaches include detailed models of the relationship between fluorescence measurements, calcium transients and spiking activity [6] as well as model-free information-theoretic approaches [7]. However, these studies do not attempt to infer functional connections between
non-simultaneously observed neurons. On the other hand, a few studies have presented statistical methods for dealing with sub-sampled observations of neural activity or connectivity, but these
approaches are not applicable to our problem: A recent study [12] presented a method for predicting noise correlations between non-simultaneously recorded neurons, but this method requires the
strong assumption that noise correlations are monotonically related to stimulus correlations. [13]
presented an algorithm for latent GLMs, but this algorithm does not scale to the population sizes
of interest here. [14] presented a method for inferring synaptic connections on dendritic trees from
sub-sampled voltage observations. In this setting, one typically obtains a measurement from each
location every few imaging frames, and it is therefore possible to interpolate these observations.
In contrast, in our application, imaging sessions are of much longer duration than the time-scale
of neural dynamics. Finally, [15] presented a statistical framework for reconstructing anatomical
connectivity by superimposing partial connectivity matrices derived from fluorescent markers.

2

Methods

Our goal is to estimate a joint model of the activity of a neural population which captures the correlation structure and stimulus selectivity of the population from partial observations of the population
activity. We model the problem as fitting a latent dynamical system with missing observations. In
principle, any latent dynamical system model [13] can be used?here we demonstrate our main point
using the simple linear gaussian dynamical system for its computational tractability.
2.1

A latent dynamical system model for combining multiple measurements of population
activity

Linear dynamics. We denote by xk the activity of N neurons in the population on recording session
k, and model its dynamics as linear with Gaussian innovations in discrete time,
2

xkt = Axkt?1 + Bukt + ?t ,

where ?t ? N (0, Q).

(1)

Here, the N ? N coupling matrix A models correlations across neurons and time. An entry Aij
being non-zero implies that activity of neuron j at time t has a statistical influence on the activity of
neuron i on the next time-step t + 1, but does not necessarily imply a direct synaptic connection. For
this reason, entries of A are usually referred to as the ?functional? (rather than anatomical) couplings
or connectivity of the population. The entries of A also shape trial-to-trial variability which is
correlated across neurons, i.e. noise-correlations. Further, we include an external, observed stimulus
ukt (of dimension Nu ) as well as receptive fields B (of size N ? Nu ) which model the stimulus
dependence of the population activity. We model neural noise (which could include the effect of
other influences not modeled explicitly) using zero-mean innovations ?t , which are Gaussian i.i.d.
with covariance matrix Q, assuming the latter to be diagonal (see below for how our framework also
can allow for correlated noise). The mean x0 and covariance Q0 of the initial state xk0 were chosen
such that the system is stationary (apart from the stimulus contribution Bukt ), i.e. x0 = 0 and Q0
satisfies the Lyapunov equation Q0 = AQ0 A> + Q.
For the sake of simplicity, we work directly in the space of continuous valued imaging measurements
(rather than on the underlying spiking activity), i.e. xkt models the relative calcium fluorescence signal. While this model does not capture the nonlinear and non-Gaussian cascade of neural couplings,
calcium dynamics, fluorescence measurements and imaging noise [16, 6], we will show that this
model nevertheless is able to predict correlations across non-simultaneously observed pairs of neurons.
Incomplete observations. In each imaging session k we measure the activity of Nk neurons simultaneously, where Nk is smaller than the total number of neurons N . Since these measurements are
noisy and incomplete observations of the full state vector, the true underlying activity of all neurons
xkt is treated as a latent variable. The vector of the Nk measurements at time t in session k is denoted
as ytk and is related to the underlying population activity by
ytk = C k (xkt + d + t )

t ? N (0, R),

(2)

where the ?measurement matrix? C k is of size Nk ? N . Further assuming that the recording sites
correspond to identified cells (which typically is the case for 2-photon calcium imaging), we can
k
assume C k to be known and of the following form: The element Cij
is 1 if neuron j of the population
is being recorded from on session k (as the i-th recording site); the remaining elements of C k are
0. The measurement noise is modeled as a Gaussian random variable t with covariance R, and
the parameter d captures a constant offset. One can also envisage using our model with dimensions
of xkt which are never observed? such latent dimensions would then model correlated noise or the
input from unobserved neurons into the population [17, 18].
Fitting the model. Our goal is to estimate the parameters (A, B, Q, R) of the latent linear dynamical
system (LDS) model described by equations (1) and (2) from experimental data. One can learn these
parameters using the standard expectation maximization (EM) algorithm that finds a local maximum
of the log-likelihood of the observed data [19]. The E-step can be performed via Kalman Smoothing
(with a different C k for each session). In the M-step, the updates for A, B and Q are as in standard
linear dynamical systems, and the updates for R and d are element-wise given by



1 X k k
?j yt,?k ? xkt,j
j
T nj
k,t
E
1 X kD k
=
?j (yt,?k ? xkt,j ? dj )2 ,
j
T nj

dj =
Rjj

k,t

where h?i denotes the expectation over the posterior distribution calculated in the E-step, and T is
the number of time steps in each recording P
session (assumed to be the same for each session for
k
the sake of simplicity). Furthermore, ?kj := i Cij
is 1 if neuron j was imaged in session k and 0
P k
otherwise, nj = k ?j is the total number of sessions in which neuron j was imaged and ?jk is the
index of the recording site of neuron j during session k. To improve the computational efficiency of
the fitting procedure as well as to avoid shallow local maxima, we used a variant of online-EM with
randomly selected mini-batches [20] followed by full batch EM for fine-tuning.
3

2.2

Details of simulated and experimental data

Simulated data. We simulated a population of 60 neurons which were split into 3 pools (?cell
types?) of 20 neurons each, with both connection probability and strength being cell-type specific.
Within each pool, pairs were coupled with probability 50% and random weights, cell-types one and
two had excitatory connections onto the other cells, and type three had weak but dense inhibitory
couplings (see Figure 2a, top left). Coupling weights were truncated at ?0.2. The 4-dimensional external stimulus was delivered into the first pool. On average, 24% of the variance of each neuron was
noise, 2% driven by the stimulus, 25% by self-couplings and a further 49% by network-interactions.
After shuffling the ordering of neurons (resulting in the connectivity matrix displayed in Fig. 2a,
top middle), we simulated K = 10 trials of length T = 1000 samples from the population. We
then pretended that the population was imaged in two sessions with non-overlapping subsets of 30
neurons each (Figure 2a, green outlined blocks) of K = 5 trials each, and that observation noise 
was uncorrelated and very small, std(ii ) = 0.006.
Experimental data. We also applied the stitching method to two calcium imaging datasets recorded
in the somatosensory cortex of awake or anesthetized mice. We imaged calcium signals in the
superficial layers of mouse barrel cortex (S1) in-vivo using 2-photon laser scanning microscopy [1].
A genetically encoded calcium indicator (GCaMP6s) was virally expressed, driven pan-neuronally
by the human-synapsin promoter, in the C2 whisker barrel and the activity of about 100-200 neurons
was imaged simultaneously in-vivo at about 3Hz, compatible with the slow timescales of the calcium
dynamics revealed by GCaMP6s. The anesthetized dataset was collected during an experiment in
which the C2 whisker of an anesthetized mouse was repeatedly flicked randomly in one of three
different directions (rostrally, caudally or ventrally). About 200 neurons were imaged for about
27min at a depth of 240?m in the C2 whisker barrel. The awake dataset was collected while an
awake animal was performing a whisker flick detection task. In this session, about 80 neurons were
imaged for about 55min at a depth of 190?m, also in the C2 whisker barrel. Regions of interest
(ROI) corresponding to putative GCaMP expressing soma (and in some instances isolated neuropil)
were manually defined and the time-series corresponding to the calcium signal for each such ROI
was extracted. The calcium time-series were high-pass filtered with a time-constant of 1s.
2.3

Quantifying and comparing model performance

Fictional imaging scenario in experimental data. To evaluate how well stitching works on real
data, we created a fictional imaging scenario. We pretended that the neurons, which were in reality
simultaneously imaged, were not imaged in one session but instead were ?imaged? in two subsets in
two different sessions. The subsets corresponding to different ?sessions? c = 60% of the neurons,
meaning that the subsets overlapped and a few neurons in common. We also experimented with
c = 50% as in our simulation above, but failed to get good performance without any overlapping
neurons. We imagined that we spent the first 40% of the time ?imaging? subset 1 and the second 40%
of the time ?imaging? subset 2. The final 20% of the data was withheld for use as the test set. We
then used our stitching method to predict pairwise correlations from the fictional imaging session.
Upper and lower bounds on performance. We wanted to benchmark how well our method is doing
both compared to the theoretical optimum and to a conventional approach. On synthetic data, we
can use the ground-truth parameters as the optimal model. In lieu of ground-truth on the real data,
we fit a ?fully observed? model to the simulatenous imaging data of all neurons (which would be
impossible of course in practice, but is possible in our fictional imaging scenario). We also analyzed
the data using a conventional, ?naive? approach in which we separately fit dynamical system models
to each of the two imaging sessions and then combined their parameters. We set coefficients of nonsimultaneously recorded pairs to 0 and averaged coefficients for neurons which were part of both
imaging sessions (in the c = 60% scenario). The ?fully observed? and the ?naive? models constitute
an upper and lower bound respectively on our performance. Certainly we can not expect to do better
at predicting correlations, than if we had observed all neurons simultaneously.

3

Results

We tested our ability to stitch multiple observations into one coherent model which is capable of
predicting statistics of the joint dynamics, such as correlations across non-simultaneously imaged
4

a

true couplings

naive couplings

b

noise correlations

stitched

0.5

naive
estimate

shuffle

0

?0.5
?0.5

stitched
estimate

stitched couplings

true

stitching
estimate

c

0

true

0.5

off-diagonal
coupling

unshuffle
blocks

stitched

0.2

unshuffle

0

?0.2
?0.2

0

true

0.2

Figure 2: Noise correlations and coupling parameters can be well recovered in a simulated
dataset. a) A coupling matrix for 60 neurons arranged in 3 blocks was generated (true coupling
matrix) and shuffled. We simulated the imaging of non-overlapping subsets of 30 neurons each in
two sessions. Couplings were recovered using a ?naive? strategy and using our proposed ?stitching?
method. b) Noise correlations estimated by our stitching method match true noise correlations
well. c) Couplings between non-simultaneously imaged neuron pairs (red off-diagonal block) are
estimated well by our method.
neuron pairs. We first apply our method to a synthetic dataset to explain its properties, and then
demonstrate that it works for real calcium imaging measurements from the mouse somatosensory
cortex.
3.1

Inferring correlations and model parameters in a simulated population

It might seem counterintuitive that one can infer the cross-couplings, and hence noise-correlations,
between neurons observed in separate sessions. An intuition for why this might work nevertheless
can gained by considering the artificial scenario of a network of linearly interacting neurons driven
by Gaussian noise: Suppose that during the first recording session we image half of these neurons.
We can fit a linear state-space model to the data in which the other, unobserved half of the population
constitutes the latent space. Given enough data, the maximum likelihood estimate of the model
parameters (which is consistent) lets us identify the true joint dynamics of the whole population up
to an invertible linear transformation of the unobserved dimensions [21]. After the second imaging
session, where we image the second (and previously unobserved) half of the population, we can
identify this linear transformation, and thus identify all model parameters uniquely, in particular the
cross-couplings. To demonstrate this intuition, we simulated such an artificial dataset (described in
2.2) and describe here the results of the stitching procedure.
Recovering the coupling matrix. Our stitching method was able to recover the true coupling
matrix, including the off-diagonal blocks which correspond to pairs of neurons that were not imaged
simultaneously (see red-outlined blocks in 2a, bottom middle). As expected, recovery was better for
couplings across observed pairs (correlation between true and estimated parameters 0.95, excluding
self-couplings) than for non-simultaneously recorded pairs (Figure 2c; correlation 0.73). With the
?naive? approach couplings between non-simultaneously observed pairs cannot be recovered, and
even for simultaneously observed pairs, the estimate of couplings is biased (correlation 0.75).
Recovering noise correlations. We also quantified the degree to which we are able to predict
statistics of the joint dynamics of the whole network, in particular noise correlations across pairs
of neurons that were never observed simultaneously. We calculated noise correlations by computing correlations in variability of neural activity after subtracting contributions due to the stimulus.
We found that the stitching method was able to accurately recover the noise-correlations of nonsimultaneously recorded pairs (correlation between predicted and true correlations was 0.92; Figure
2b). In fact, we generally found the prediction of correlations to be more accurate than prediction
5

fully observed

stitched

naive

b

fully observed

0.5

0

d

correlations

c

?0.5
?0.5
0.3
0.2

0

0.5

Naive
Stitched

partially observed

couplings

a

0.1
0

0

0.1

0.2

0.3

Figure 3: Examples of correlation and coupling recovery in the anesthetized calcium imaging
experiments. a) Coupling matrices fit to calcium signal using all neurons (fully observed) or fit after
?imaging? two overlapping subsets of 60% neurons each (stitched and naive). The naive approach
is unable to estimate coupling terms for ?non-simultaneously imaged? neurons, so these are set to
zero. b) Scatter plot of coupling terms for ?non-simultaneously imaged? neuron pairs estimated
using the stitching method vs the fully observed estimates. c) Correlations predicted using the
coupling matrices. d) Scatter plot of correlations in c for ?non-simultaneously imaged? neuron pairs
estimated using the stitching and the naive approaches.

of the underlying coupling parameters. In contrast, a naive approach would not be able to estimate
noise correlations between non-simultaneously observed pairs. (We note that, as the stimulus drive
in this simulation was very weak, inferring noise correlations from stimulus correlations [12] would
be impossible).
Predicting unobserved neural activity. Given activity measurements from a subset of neurons,
our method can predict the activity of neurons in the unobserved subset. This prediction can be
calculated by doing inference in the resulting LDS, i.e. by calculating the posterior mean ?k1:T =
k
, hk1:T ) and looking at those entries of ?k1:T which correspond to unobserved neurons.
E(xk1:T |y1:T
On our simulated data, we found that this prediction was strongly correlated with the underlying
ground-truth activity (average correlation 0.70 ? 0.01 s.e.m across neurons, using a separate testset which was not used for parameter fitting.). The upper bound for this prediction metric can be
obtained by using the ground-truth parameters to calculate the posterior mean. Use of this groundtruth model resulted in a performance of 0.82 ? 0.01. In contrast, the ?naive? approach can only
utilize the stimulus, but not the activity of the observed population for prediction and therefore only
achieved a correlation of 0.23 ? 0.01.
3.2

Inferring correlations in mouse somatosensory cortex

Next, we applied our stitching method to two real datasets: anesthetized and awake (described in
Section 2.2). We demonstrate that it can predict correlations between non-simultaneously accessed
neuron pairs with accuracy approaching that of the upper bound (?fully observed? model trained on
all neurons), and substantially better than the lower bound ?naive? model.
Example results. Figure 3a displays coupling matrices of a population consisting of the 50 most
correlated neurons in the anesthetized dataset (see Section 2.2 for details) estimated using all three
methods. Our stitching method yielded a coupling matrix with structure similar to the fully observed model (Figure 3a, central panel), even in the off-diagonal blocks which correspond to nonsimultaneously recorded pairs. In contrast, the naive method, by definition, is unable to infer couplings for non-simultaneously recorded pairs, and therefore over-estimates the magnitude of observed couplings (Figure 3a, right panel). Even for non-simultaneously recorded pairs, the stitched
model predicted couplings which were correlated with the fully observed predictions (Figure 3b,
correlation 0.38).
6

a

predicting correlations

b

predicting neural activity

0.2

0.4

correlation

anesthetized

0.6

0.6

20

40

60

80

100

0.8

0
0.8

0.4

full obs (UB)
stitched
naive (LB)

0.2

1

predicting couplings

0.8

0.8

0

c
1

0.4

1

20

40

60

80

0.2
0
100

60

80

80

100

awake

40

60

0.4

0.2

20

40

0.6

0.4

0.4

20

0.8

0.6

0.6

1

20

40

60

80

0.2

20

40

60

80

population size

Figure 4: Recovering correlations and coupling parameters in a real calcium imaging experiments. 100 neurons were simultaneously imaged in an anesthetized mouse (top row) and an awake
mouse (bottom row). Random populations of these neurons, ranging in size from 10 to 100 were
chosen and split into two slightly overlapping sub-sets each containing 60% of the neurons. The
activity of these sub-sets were imagined to be ?imaged? in two separate ?imaging? sessions (see
Section 2.2). a) Pairwise correlations for ?non-simultaneously imaged? neuron pairs estimated by
the ?naive? and our ?stitched? strategies compared to correlations predicted by a model fit to all neurons (?full obs?). b) Accuracy of predicting the activity of one sub-set of neurons, given the activities
of the other sub-set of neurons. c) Comparison of estimated couplings for ?non-simultaneously imaged? neuron pairs to those estimated using the ?fully observed? model. Note that true coupling
terms are unavailable here.
However, of greater interest is how well our model can recover pairwise correlations between nonsimultaneously measured neuron pairs. We found that our stitching method, but not the naive
method, was able to accurately reconstruct these correlations (Figure 3c). As expected, the naive
method strongly under-estimated correlations in the non-simultaneously recorded blocks, as it can
only model stimulus-correlations but not noise-correlations across neurons. 1 In contrast, our stitching method predicted correlations well, matching those of the fully observed model (correlation 0.84
for stitchLDS, 0.15 for naiveLDS, figure 3d).
Summary results across multiple populations. Here, we investigate the robustness of our findings. We drew random neuronal populations of sizes ranging from 10 to 80 (for awake) or 100
(for anesthetized) from the full datasets. For each population, we fit three models (fully observed,
stitch, naive) and compared their correlations, parameters and activity cross-prediction accuracy.
We repeated this process 20 times for each population size and dataset (anesthetized/awake) to characterize the variability. We found that for both datasets, the correlations predicted by the stitching
method for non-simultaneously recorded pairs were similar to the fully observed ones, and that this
similarity is almost independent of population size (Figure 4a). In fact, for the awake data (in which
the overall level of correlation was higher), the correlation matrices were extremely similar (lower
panel). The stitching method also substantially outperformed the naive approach, for which the
similarity was lower by a factor of about 2.
We compared the accuracy of the models at predicting the neural activity of one subset of neurons given the stimulus and the activity of the other subset (Figure 4b). We find that our model
makes significantly better predictions than the lower bound naive model, whose performance comes
from modeling the stimulus and neurons in the overlap between both subsets. Indeed for the more
active and correlated awake dataset, predictions are nearly as good as those of the fully observed
1
The naive approach also over-estimated correlations within each view. This is a consequence of biases
resulting from averaging couplings across views for neurons in the overlap between the two fictional sessions.

7

model. We also found that prediction accuracy increased slightly with population size, perhaps
since a larger population provides more neurons from which the activity of the other subset can be
predicted. Apparently, this gain in accuracy from additional neurons outweighed any potential drop
in performance resulting from increased potential for over-fitting on larger populations.
While we have no access to the true cross-couplings for the real data, we can nonetheless compare
the couplings from our stitched model to those estimated by the fully observed model. We find
that the stitching model is indeed able to estimate couplings that correlate positively with the fully
observed couplings, even for non-simultaneously imaged neuron pairs. Interestingly, this correlation
drops with increasing population size, perhaps due to possible near degeneracy of parameters for
large systems.

4

Discussion

It has long been appreciated that a dynamical system can be reconstructed from observations of only
a subset of its variables [22, 23, 21]. These theoretical results suggest that while only measuring
the activity of one population of neurons, we can infer the activity of a second neural population
that strongly interacts with the first, up to re-parametrization. Here, we go one step further. By later
measuring the activity of the second population, we recover the true parametrization allowing us to
predict aspects of the joint dynamics of the two populations, such as noise correlations.
Our essential finding is that we can put these theoretical insights to work using a simple linear
dynamical system model that ?stitches? together data from non-simultaneously recorded but strongly
interacting populations of neurons. We applied our method to analyze 2-photon population calcium
imaging measurements from the superficial layers of the somatosensory cortex of both anesthetized
and awake mice, and found that our method was able to successfully combine data not accessed
simultaneously. In particular, this approach allowed us to accurately predict correlations even for
pairs of non-simultaneously recorded neurons.
In this paper, we focused our demonstration to stitching together two populations of neurons. Our
framework can be generalized to more than two populations, however it remains to be empirically
seen how well larger numbers of populations can be combined. An experimental variable of interest
is the degree of overlap (shared neurons) between different populations of neurons. We found that
some overlap was critical for stitching to work, and increasing overlap improves stitching performance. Given a fixed imaging time budget, determining a good trade-off between overlap and total
coverage is an intriguing open problem in experimental design.
We emphasise that our linear gaussian dynamical system provides only a statistical description of the
observed data. However, even this simple model makes accurate predictions of correlations between
non-simultaneously observed neurons. Nevertheless, more realistic models [16, 6] can help improve
the accuracy of these predictions and disentangle the contributions of spiking activity, calcium dynamics, fluorescence measurements and imaging noise to the observed statistics. Similarly, better
priors on neural connectivity [24] might improve reconstruction performance. Indeed, we found
in unreported simulations that using a sparsifying penalty on the connectivity matrix [6] improves
parameter estimates slightly. We note that our model can easily be extended to model potential
common input from neurons which are never observed [13] as a low dimensional LDS [17, 18].
The simultaneous measurement of the activity of all neurons in a neural circuit will shed much light
on the nature of neural computation. While there is much progress in developing faster imaging
modalities, there are fundamental physical limits to the number of neurons which can be simultaneously imaged. Our paper suggests a means for expanding our limited capabilities. With more
powerful algorithmic tools, we can imagine mapping population dynamics of all the neurons in an
entire neural circuit such as the zebrafish larval olfactory bulb, or layers 2 & 3 of a whisker barrel?
an ambitious goal which has until now been out of reach.
Acknowledgements
We thank Peter Dayan for valuable comments on our manuscript and members of the Gatsby Unit for discussions. We are grateful for support from the Gatsby Charitable Trust, Wellcome Trust, ERC, EMBO, People
Programme (Marie Curie Actions) and German Federal Ministry of Education and Research (BMBF; FKZ:
01GQ1002, Bernstein Center T?ubingen).

8

References
[1] J. N. D. Kerr and W. Denk, ?Imaging in vivo: watching the brain in action,? Nat Rev Neurosci, vol. 9,
no. 3, pp. 195?205, 2008.
[2] C. Grienberger and A. Konnerth, ?Imaging calcium in neurons.,? Neuron, vol. 73, no. 5, pp. 862?885,
2012.
[3] S. Lefort, C. Tomm, J.-C. Floyd Sarria, and C. C. H. Petersen, ?The excitatory neuronal network of the
C2 barrel column in mouse primary somatosensory cortex.,? Neuron, vol. 61, no. 2, pp. 301?316, 2009.
[4] D. J. Tolhurst, J. A. Movshon, and A. F. Dean, ?The statistical reliability of signals in single neurons in
cat and monkey visual cortex,? Vision research, vol. 23, no. 8, pp. 775?785, 1983.
[5] W. R. Softky and C. Koch, ?The highly irregular firing of cortical cells is inconsistent with temporal
integration of random epsps,? The Journal of Neuroscience, vol. 13, no. 1, pp. 334?350, 1993.
[6] Y. Mishchenko, J. T. Vogelstein, and L. Paninski, ?A bayesian approach for inferring neuronal connectivity from calcium fluorescent imaging data,? The Annals of Applied Statistics, vol. 5, no. 2B, pp. 1229?
1261, 2011.
[7] O. Stetter, D. Battaglia, J. Soriano, and T. Geisel, ?Model-free reconstruction of excitatory neuronal
connectivity from calcium imaging signals,? PLoS Comp Bio, vol. 8, no. 8, p. e1002653, 2012.
[8] J. W. Pillow, J. Shlens, L. Paninski, A. Sher, A. M. Litke, E. J. Chichilnisky, and E. P. Simoncelli,
?Spatio-temporal correlations and visual signalling in a complete neuronal population.,? Nature, vol. 454,
no. 7207, pp. 995?999, 2008.
[9] I. H. Stevenson, J. M. Rebesco, L. E. Miller, and K. P. K?ording, ?Inferring functional connections between
neurons,? Current opinion in neurobiology, vol. 18, no. 6, pp. 582?588, 2008.
[10] A. Singh and N. A. Lesica, ?Incremental mutual information: A new method for characterizing the
strength and dynamics of connections in neuronal circuits,? PLoS Comp Bio, vol. 6, no. 12, p. e1001035,
2010.
[11] D. Song, H. Wang, C. Y. Tu, V. Z. Marmarelis, R. E. Hampson, S. A. Deadwyler, and T. W. Berger,
?Identification of sparse neural functional connectivity using penalized likelihood estimation and basis
functions,? J Comp Neursci, pp. 1?23, 2013.
[12] A. Wohrer, R. Romo, and C. Machens, ?Linear readout from a neural population with partial correlation
data,? in Advances in Neural Information Processing Systems, vol. 22, Curran Associates, Inc., 2010.
[13] J. W. Pillow and P. Latham, ?Neural characterization in partially observed populations of spiking neurons,? Adv Neural Information Processing Systems, vol. 20, no. 3.5, 2008.
[14] A. Pakman, J. H. Huggins, and P. L., ?Fast penalized state-space methods for inferring dendritic synaptic
connectivity,? Journal of Computational Neuroscience, 2013.
[15] Y. Mishchenko and L. Paninski, ?A bayesian compressed-sensing approach for reconstructing neural
connectivity from subsampled anatomical data,? J Comp Neurosci, vol. 33, no. 2, pp. 371?388, 2012.
[16] J. T. Vogelstein, B. O. Watson, A. M. Packer, R. Yuste, B. Jedynak, and L. Paninski, ?Spike inference from
calcium imaging using sequential monte carlo methods,? Biophysical Journal, vol. 97, no. 2, pp. 636?
655, 2009.
[17] M. Vidne, Y. Ahmadian, J. Shlens, J. Pillow, J. Kulkarni, A. Litke, E. Chichilnisky, E. Simoncelli, and
L. Paninski, ?Modeling the impact of common noise inputs on the network activity of retinal ganglion
cells.,? J Comput Neurosci, 2011.
[18] J. H. Macke, L. B?using, J. P. Cunningham, B. M. Yu, K. V. Shenoy, and M. Sahani., ?Empirical models of
spiking in neural populations.,? in Advances in Neural Information Processing Systems, vol. 24, Curran
Associates, Inc., 2012.
[19] A. P. Dempster, N. M. Laird, and D. B. Rubin, ?Maximum likelihood from incomplete data via the EM
algorithm,? J R Stat Soc Ser B, vol. 39, no. 1, pp. 1?38, 1977.
[20] P. Liang and D. Klein, ?Online EM for unsupervised models,? in NAACL ?09: Proceedings of Human
Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association
for Computational Linguistics, Association for Computational Linguistics, 2009.
[21] T. Katayama, Subspace methods for system identification. Springer Verlag, 2005.
[22] L. E. Baum and T. Petrie, ?Statistical Inference for Probabilistic Functions of Finite State Markov
Chains,? The Annals of Mathematical Statistics, vol. 37, no. 6, pp. 1554?1563, 1966.
[23] F. Takens, ?Detecting Strange Attractors In Turbulence,? in Dynamical Systems and Turbulence (D. A.
Rand and L. S. Young, eds.), vol. 898 of Lecture Notes in Mathematics, (Warwick), pp. 366?381,
Springer-Verlag, Berlin, 1981.
[24] S. W. Linderman and R. P. Adams, ?Inferring functional connectivity with priors on network topology,?
in Cosyne Abstracts, 2013.

9

"
2015,Bayesian dark knowledge,Poster,5965-bayesian-dark-knowledge.pdf,"We consider the problem of Bayesian parameter estimation for deep neural networks, which is important in problem settings where we may have little data, and/ or where we need accurate posterior predictive densities p(y|x, D), e.g., for applications involving bandits or active learning. One simple approach to this is to use online Monte Carlo methods, such as SGLD (stochastic gradient Langevin dynamics). Unfortunately, such a method needs to store many copies of the parameters (which wastes memory), and needs to make predictions using many versions of the model (which wastes time).We describe a method for ?distilling? a Monte Carlo approximation to the posterior predictive density into a more compact form, namely a single deep neural network. We compare to two very recent approaches to Bayesian neural networks, namely an approach based on expectation propagation [HLA15] and an approach based on variational Bayes [BCKW15]. Our method performs better than both of these, is much simpler to implement, and uses less computation at test time.","Bayesian Dark Knowledge

Anoop Korattikara, Vivek Rathod, Kevin Murphy
Google Research
{kbanoop, rathodv, kpmurphy}@google.com

Max Welling
University of Amsterdam
m.welling@uva.nl

Abstract
We consider the problem of Bayesian parameter estimation for deep neural networks, which is important in problem settings where we may have little data, and/
or where we need accurate posterior predictive densities p(y|x, D), e.g., for applications involving bandits or active learning. One simple approach to this is to use
online Monte Carlo methods, such as SGLD (stochastic gradient Langevin dynamics). Unfortunately, such a method needs to store many copies of the parameters
(which wastes memory), and needs to make predictions using many versions of
the model (which wastes time).
We describe a method for ?distilling? a Monte Carlo approximation to the posterior predictive density into a more compact form, namely a single deep neural
network. We compare to two very recent approaches to Bayesian neural networks,
namely an approach based on expectation propagation [HLA15] and an approach
based on variational Bayes [BCKW15]. Our method performs better than both of
these, is much simpler to implement, and uses less computation at test time.

1

Introduction

Deep neural networks (DNNs) have recently been achieving state of the art results in many fields.
However, their predictions are often over confident, which is a problem in applications such as
active learning, reinforcement learning (including bandits), and classifier fusion, which all rely on
good estimates of uncertainty.
A principled way to tackle this problem is to use Bayesian inference. Specifically, we first comQN
pute the posterior distribution over the model parameters, p(?|DN ) ? p(?) i=1 p(yi |xi , ?), where
N
DN = {(xi , yi )}i=1 , xi ? X D is the i?th input (where D is the number of features), and
Ryi ? Y is the i?th output. Then we compute the posterior predictive distribution, p(y|x, DN ) =
p(y|x, ?)p(?|DN )d?, for each test point x.
For reasons of computational speed, it is common to approximate the posterior distribution by a
point estimate such as the MAP estimate, ?? = argmax p(?|DN ). When N is large, we often use
? Finally, we make a plug-in approximation to the
stochastic gradient descent (SGD) to compute ?.
? Unfortunately, this loses most of the benefits
predictive distribution: p(y|x, DN ) ? p(y|x, ?).
of the Bayesian approach, since uncertainty in the parameters (which induces uncertainty in the
predictions) is ignored.
Various ways of more accurately approximating p(?|DN ) (and hence p(y|x, DN )) have been developed. Recently, [HLA15] proposed a method called ?probabilistic backpropagation? (PBP) based
on an online version of expectation propagation (EP), (i.e., using repeated assumed density filtering
(ADF)), where the posterior
Q is approximated as a product of univariate Gaussians, one per parameter: p(?|DN ) ? q(?) , i N (?i |mi , vi ).
An alternative to EP is variational Bayes (VB) where we optimize a lower bound on the marginal
likelihood. [Gra11] presented a (biased) Monte Carlo estimate of this lower bound and applies
1

his method, called ?variational inference? (VI), to infer the neural network weights. More recently,
[BCKW15] proposed an approach called ?Bayes by Backprop? (BBB), which extends the VI method
with an unbiased MC estimate of the lower bound based on the ?reparameterization trick? of [KW14,
RMW14]. In both [Gra11] and [BCKW15], the posterior is approximated by a product of univariate
Gaussians.
Although EP and VB scale well with data size (since they use online learning), there are several
problems with these methods: (1) they can give poor approximations when the posterior p(?|DN )
does not factorize, or if it has multi-modality or skew; (2) at test time, computing the predictive
density p(y|x, DN ) can be much slower than using the plug-in approximation, because of the need
to integrate out the parameters; (3) they need to use double the memory of a standard plug-in method
(to store the mean and variance of each parameter), which can be problematic in memory-limited
settings such as mobile phones; (4) they can be quite complicated to derive and implement.
A common alternative to EP and VB is to use MCMC methods to approximate p(?|DN ). Traditional MCMC methods are batch algorithms, that scale poorly with dataset size. However, recently a method called stochastic gradient Langevin dynamics (SGLD) [WT11] has been devised
that can draw samples approximately from the posterior in an online fashion, just as SGD updates a
point estimate of the parameters online. Furthermore, various extensions of SGLD have been proposed, including stochastic gradient hybrid Monte Carlo (SGHMC) [CFG14], stochastic gradient
Nos?e-Hoover Thermostat (SG-NHT) [DFB+ 14] (which improves upon SGHMC), stochastic gradient Fisher scoring (SGFS) [AKW12] (which uses second order information), stochastic gradient
Riemannian Langevin Dynamics [PT13], distributed SGLD [ASW14], etc. However, in this paper,
we will just use ?vanilla? SGLD [WT11].1
All these MCMC methods (whether batch or online) produce a Monte Carlo approximation to the
PS
posterior, q(?) = S1 s=1 ?(? ? ?s ), where S is the number of samples. Such an approximation can be more accurate than that produced by EP or VB, and the method is much easier to
implement (for SGLD, you essentially just add Gaussian noise to your SGD updates). However,
at test time, things are S times slower than using a plug-in estimate, since we need to compute
PS
q(y|x) = S1 s=1 p(y|x, ?s ), and the memory requirements are S times bigger, since we need to
store the ?s . (For our largest experiment, our DNN has 500k parameters, so we can only afford to
store a single sample.)
In this paper, we propose to train a parametric model S(y|x, w) to approximate the Monte Carlo
posterior predictive distribution q(y|x) in order to gain the benefits of the Bayesian approach while
only using the same run time cost as the plugin method. Following [HVD14], we call q(y|x) the
?teacher? and S(y|x, w) the ?student?. We use SGLD2 to estimate q(?) and hence q(y|x) online;
we simultaneously train the student online to minimize KL(q(y|x)||S(y|x, w)). We give the details
in Section 2.
Similar ideas have been proposed in the past. In particular, [SG05] also trained a parametric student
model to approximate a Monte Carlo teacher. However, they used batch training and they used
mixture models for the student. By contrast, we use online training (and can thus handle larger
datasets), and use deep neural networks for the student.
[HVD14] also trained a student neural network to emulate the predictions of a (larger) teacher network (a process they call ?distillation?), extending earlier work of [BCNM06] which approximated
an ensemble of classifiers by a single one. The key difference from our work is that our teacher
is generated using MCMC, and our goal is not just to improve classification accuracy, but also to
get reliable probabilistic predictions, especially away from the training data. [HVD14] coined the
term ?dark knowledge? to represent the information which is ?hidden? inside the teacher network,
and which can then be distilled into the student. We therefore call our approach ?Bayesian dark
knowledge?.

1
We did some preliminary experiments with SG-NHT for fitting an MLP to MNIST data, but the results
were not much better than SGLD.
2
Note that SGLD is an approximate sampling algorithm and introduces a slight bias in the predictions of
the teacher and student network. If required, we can replace SGLD with an exact MCMC method (e.g. HMC)
to get more accurate results at the expense of more training time.

2

In summary, our contributions are as follows. First, we show how to combine online MCMC methods with model distillation in order to get a simple, scalable approach to Bayesian inference of the
parameters of neural networks (and other kinds of models). Second, we show that our probabilistic
predictions lead to improved log likelihood scores on the test set compared to SGD and the recently
proposed EP and VB approaches.

2

Methods

Our goal is to train a student neural network (SNN) to approximate the Bayesian predictive distribution of the teacher, which is a Monte Carlo ensemble of teacher neural networks (TNN).
If we denote the predictions of the teacher by p(y|x, DN ) and the parameters of the student network
by w, our objective becomes
L(w|x) = KL(p(y|x, DN )||S(y|x, w)) = ?Ep(y|x,DN ) log S(y|x, w) + const

Z Z
=?
p(y|x, ?)p(?|DN )d? log S(y|x, w)dy
Z
Z
= ? p(?|DN ) p(y|x, ?) log S(y|x, w)dy d?
Z


= ? p(?|DN ) Ep(y|x,?) log S(y|x, w) d?
(1)
Unfortunately, computing this integral is not analytically tractable. However, we can approximate
this by Monte Carlo:
1 X
?
L(w|x)
=?
Ep(y|x,?s ) log S(y|x, w)
(2)
|?| s
? ??

where ? is a set of samples from p(?|DN ).
To make this a function just of w, we need to integrate out x. For this, we need a dataset to train
the student network on, which we will denote by D0 . Note that points in this dataset do not need
ground truth labels; instead the labels (which will be probability distributions) will be provided
by the teacher. The choice of student data controls the domain over which the student will make
accurate predictions. For low dimensional problems (such as in Section 3.1), we can uniformly
sample the input domain. For higher dimensional problems, we can sample ?near? the training
data, for example by perturbing the inputs slightly. In any case, we will compute a Monte Carlo
approximation to the loss as follows:
Z
1 X
?
L(w|x0 )
L(w)
=
p(x)L(w|x)dx ? 0
|D | 0 0
x ?D
1 1 X X
? ?
Ep(y|x0 ,?s ) log S(y|x0 , w)
(3)
|?| |D0 | s
0
0
? ?? x ?D

It can take a lot of memory to pre-compute and store the set of parameter samples ? and the set of
data samples D0 , so in practice we use the stochastic algorithm shown in Algorithm 1, which uses a
single posterior sample ?s and a minibatch of x0 at each step.
The hyper-parameters ? and ? from Algorithm 1 control the strength of the priors for the teacher
and student networks. We use simple spherical Gaussian priors (equivalent to L2 regularization);
we set the precision (strength) of these Gaussian priors by cross-validation. Typically ?  ?, since
the student gets to ?see? more data than the teacher. This is true for two reasons: first, the teacher
is trained to predict a single label per input, whereas the student is trained to predict a distribution,
which contains more information (as argued in [HVD14]); second, the teacher makes multiple passes
over the same training data, whereas the student sees ?fresh? randomly generated data D0 at each
step.
2.1

Classification

For classification problems, each teacher network ?s models the observations using a standard softmax model, p(y = k|x, ?s ). We want to approximate this using a student network, which also has a
3

Algorithm 1: Distilled SGLD
Input: DN = {(xi , yi )}N
i=1 , minibatch size M , number of iterations T , teacher learning schedule
?t , student learning schedule ?t , teacher prior ?, student prior ?
for t = 1 : T do
// Train teacher (SGLD step)
Sample minibatch indices S ? [1, N ] of size M
Sample zt ? N (0, ?t I)

P
N
Update ?t+1 := ?t + ?2t ?? log p(?|?) + M
i?S ?? log p(yi |xi , ?) + zt
// Train student (SGD step)
Sample D0 of sizeM from student data generator

P
0
?
wt+1 := wt ? ?t 1
0
0 ?w L(w, ?t+1 |x ) + ?wt
x ?D

M

softmax output, S(y = k|x, w). Hence from Eqn. 2, our loss function estimate is the standard cross
entropy loss:
s
?
L(w|?
, x) = ?

K
X

p(y = k|x, ?s ) log S(y = k|x, w)

(4)

k=1

The student network outputs ?k (x, w) = log S(y = k|x, w). To estimate the gradient w.r.t. w, we
just have to compute the gradients w.r.t. ? and back-propagate through the network. These gradients
s
?
L(w,?
|x)
are given by ???
= ?p(y = k|x, ?s ).
k (x,w)
2.2

Regression

In regression, the observations are modeled as p(yi |xi , ?) = N (yi |f (xi |?), ??1
n ) where f (x|?) is
the prediction of the TNN and ?n is the noise precision. We want to approximate the predictive
distribution as p(y|x, DN ) ? S(y|x, w) = N (y|?(x, w), e?(x,w) ). We will train a student network
to output the parameters of the approximating distribution ?(x, w) and ?(x, w); note that this is
twice the number of outputs of the teacher network, since we want to capture the (data dependent)
variance.3 We use e?(x,w) instead of directly predicting the variance ? 2 (x|w) to avoid dealing with
positivity constraints during training.
To train the SNN, we will minimize the objective defined in Eqn. 2:
s
?
L(w|?
, x)

= ?Ep(y|x,?s ) log N (y|?(x, w), e?(x,w) )
h
i
1
=
Ep(y|x,?s ) ?(x, w) + e??(x,w) (y ? ?(x, w)2 )
2


1
1
2
??(x,w)
s
=
?(x, w) + e
(f (x|? ) ? ?(x, w)) +
2
?n

?
Now, to estimate ?w L(w,
?s |x), we just have to compute
through the network. These gradients are:
?
? L(w,
?s |x)
??(x, w)
?
? L(w,
?s |x
??(x, w)

3

?
?L
??(x,w)

and

?
?L
??(x,w) ,

and back propagate

=

e??(x,w) {?(x, w) ? f (x|?s )}

(5)

=




1
1
1 ? e??(x,w) (f (x|?s ) ? ?(x, w))2 +
2
?n

(6)

Experimental results

In this section, we compare SGLD and distilled SGLD with other approximate inference methods,
including the plugin approximation using SGD, the PBP approach of [HLA15], the BBB approach of
3

This is not necessary in the classification case, since the softmax distribution already captures uncertainty.

4

Dataset
ToyClass
MNIST
ToyReg
Boston Housing

N
20
60k
10
506

D
2
784
1
13

Y
{0, 1}
{0, . . . , 9}
R
R

PBP
N
N
Y
Y

BBB
N
Y
N
N

HMC
Y
N
Y
N

Table 1: Summary of our experimental configurations.

(a)

(b)

(c)

(d)

(e)

(f)

Figure 1: Posterior predictive density for various methods on the toy 2d dataset. (a) SGD (plugin)
using the 2-10-2 network. (b) HMC using 20k samples. (c) SGLD using 1k samples. (d-f) Distilled
SGLD using a student network with the following architectures: 2-10-2, 2-100-2 and 2-10-10-2.
[BCKW15], and Hamiltonian Monte Carlo (HMC) [Nea11], which is considered the ?gold standard?
for MCMC for neural nets. We implemented SGD and SGLD using the Torch library (torch.ch).
For HMC, we used Stan (mc-stan.org). We perform this comparison for various classification
and regression problems, as summarized in Table 1.4
3.1

Toy 2d classification problem

We start with a toy 2d binary classification problem, in order to visually illustrate the performance
of different methods. We generate a synthetic dataset in 2 dimensions with 2 classes, 10 points per
class. We then fit a multi layer perceptron (MLP) with one hidden layer of 10 ReLu units and 2
softmax outputs (denoted 2-10-2) using SGD. The resulting predictions are shown in Figure 1(a).
We see the expected sigmoidal probability ramp orthogonal to the linear decision boundary. Unfortunately, this method predicts a label of 0 or 1 with very high confidence, even for points that are far
from the training data (e.g., in the top left and bottom right corners).
In Figure 1(b), we show the result of HMC using 20k samples. This is the ?true? posterior predictive
density which we wish to approximate. In Figure 1(c), we show the result of SGLD using about 1000
samples. Specifically, we generate 100k samples, discard the first 2k for burnin, and then keep every
100?th sample. We see that this is a good approximation to the HMC distribution.
In Figures 1(d-f), we show the results of approximating the SGLD Monte Carlo predictive distribution with a single student MLP of various sizes. To train this student network, we sampled points at
random from the domain of the input, [?10, 10] ? [?10, 10]; this encourages the student to predict
accurately at all locations, including those far from the training data. In (d), the student has the same
4
Ideally, we would apply all methods to all datasets, to enable a proper comparison. Unfortunately, this was
not possible, for various reasons. First, the open source code for the EP approach only supports regression, so
we could not evaluate this on classification problems. Second, we were not able to run the BBB code, so we just
quote performance numbers from their paper [BCKW15]. Third, HMC is too slow to run on large problems, so
we just applied it to the small ?toy? problems. Nevertheless, our experiments show that our methods compare
favorably to these other methods.

5

Model
SGD
SGLD
Distilled 2-10-2
Distilled 2-100-2
Distilled 2-10-10-2

Num. params.
40
40k
40
400
140

KL
0.246
0.007
0.031
0.014
0.009

Table 2: KL divergence on the 2d classification dataset.
SGD [BCKW15]
1.83

Dropout
1.51

BBB
1.82

SGD (our impl.)
1.536 ? 0.0120

SGLD
1.271 ? 0.0126

Dist. SGLD
1.307 ? 0.0169

Table 3: Test set misclassification rate on MNIST for different methods using a 784-400-400-10
MLP. SGD (first column), Dropout and BBB numbers are quoted from [BCKW15]. For our implmentation of SGD (fourth column), SGLD and distilled SGLD, we report the mean misclassification
rate over 10 runs and its standard error.
size as the teacher (2-10-2), but this is too simple a model to capture the complexity of the predictive
distribution (which is an average over models). In (e), the student has a larger hidden layer (2-1002); this works better. However, we get best results using a two hidden layer model (2-10-10-2), as
shown in (f).
In Table 2, we show the KL divergence between the HMC distribution (which we consider as ground
truth) and the various approximations mentioned above. We computed this by comparing the probability distributions pointwise on a 2d grid. The numbers match the qualitative results shown in
Figure 1.
3.2

MNIST classification

Now we consider the MNIST digit classification problem, which has N = 60k examples, 10
classes, and D = 784 features. The only preprocessing we do is divide the pixel values by 126
(as in [BCKW15]). We train only on 50K datapoints and use the remaining 10K for tuning hyperparameters. This means our results are not strictly comparable to a lot of published work, which
uses the whole dataset for training; however, the difference is likely to be small.
Following [BCKW15], we use an MLP with 2 hidden layers with 400 hidden units per layer, ReLU
activations, and softmax outputs; we denote this by 784-400-400-10. This model has 500k parameters.
We first fit this model by SGD, using these hyper parameters: fixed learning rate of ?t = 5 ? 10?6 ,
prior precision ? = 1, minibatch size M = 100, number of iterations T = 1M . As shown in
Table 3, our final error rate on the test set is 1.536%, which is a bit lower than the SGD number
reported in [BCKW15], perhaps due to the slightly different training/ validation configuration.
Next we fit this model by SGLD, using these hyper parameters: fixed learning rate of ?t = 4?10?6 ,
thinning interval ? = 100, burn in iterations B = 1000, prior precision ? = 1, minibatch size
M = 100. As shown in Table 3, our final error rate on the test set is about 1.271%, which is better
than the SGD, dropout and BBB results from [BCKW15].5
Finally, we consider using distillation, where the teacher is an SGLD MC approximation of the
posterior predictive. We use the same 784-400-400-10 architecture for the student as well as the
teacher. We generate data for the student by adding Gaussian noise (with standard deviation of
0.001) to randomly sampled training points6 We use a constant learning rate of ? = 0.005, a batch
size of M = 100, a prior precision of 0.001 (for the student) and train for T = 1M iterations. We
obtain a test error of 1.307% which is very close to that obtained with SGLD (see Table 4).
5

We only show the BBB results with the same Gaussian prior that we use. Performance of BBB can be
improved using other priors, such as a scale mixture of Gaussians, as shown in [BCKW15]. Our approach
could probably also benefit from such a prior, but we did not try this.
6
In the future, we would like to consider more sophisticated data perturbations, such as elastic distortions.

6

SGD
-0.0613 ? 0.0002

SGLD
-0.0419 ? 0.0002

Distilled SGLD
-0.0502 ? 0.0007

Table 4: Log likelihood per test example on MNIST. We report the mean over 10 trials ? one
standard error.
Method
PBP (as reported in [HLA15])
VI (as reported in [HLA15])
SGD
SGLD
SGLD distilled

Avg. test log likelihood
-2.574 ? 0.089
-2.903 ? 0.071
-2.7639 ? 0.1527
-2.306 ? 0.1205
-2.350 ? 0.0762

Table 5: Log likelihood per test example on the Boston housing dataset. We report the mean over
20 trials ? one standard error.
We also report the average test log-likelihood of SGD, SGLD and distilled SGLD in Table 4. The
log-likelihood is equivalent to the logarithmic scoring rule [Bic07] used in assessing the calibration
of probabilistic models. The logarithmic rule is a strictly proper scoring rule, meaning that the
score is uniquely maximized by predicting the true probabilities. From Table 4, we see that both
SGLD and distilled SGLD acheive higher scores than SGD, and therefore produce better calibrated
predictions.
Note that the SGLD results were obtained by averaging predictions from ? 10,000 models sampled
from the posterior, whereas distillation produces a single neural network that approximates the average prediction of these models, i.e. distillation reduces both storage and test time costs of SGLD
by a factor of 10,000, without sacrificing much accuracy. In terms of training time, SGD took 1.3
ms, SGLD took 1.6 ms and distilled SGLD took 3.2 ms per iteration. In terms of memory, distilled
SGLD requires only twice as much as SGD or SGLD during training, and the same as SGD during
testing.
3.3

Toy 1d regression

We start with a toy 1d regression problem, in order to visually illustrate the performance of different
methods. We use the same data and model as [HLA15]. In particular, we use N = 20 points in
D = 1 dimensions, sampled from the function y = x3 + n , where n ? N (0, 9). We fit this data
with an MLP with 10 hidden units and ReLU activations. For SGLD, we use S = 2000 samples.
For distillation, the teacher uses the same architecture as the student.
The results are shown in Figure 2. We see that SGLD is a better approximation to the ?true? (HMC)
posterior predictive density than the plugin SGD approximation (which has no predictive uncertainty), and the VI approximation of [Gra11]. Finally, we see that distilling SGLD incurs little loss
in accuracy, but saves a lot computationally.
3.4

Boston housing

Finally, we consider a larger regression problem, namely the Boston housing dataset, which was
also used in [HLA15]. This has N = 506 data points (456 training, 50 testing), with D = 13
dimensions. Since this data set is so small, we repeated all experiments 20 times, using different
train/ test splits.
Following [HLA15], we use an MLP with 1 layer of 50 hidden units and ReLU activations. First
we use SGD, with these hyper parameters7 : Minibatch size M = 1, noise precision ?n = 1.25,
prior precision ? = 1, number of trials 20, constant learning rate ?t = 1e ? 6, number of iterations
T = 170K. As shown in Table 5, we get an average log likelihood of ?2.7639.
Next we fit the model using SGLD. We use an initial learning rate of ?0 = 1e ? 5, which we reduce
by a factor of 0.5 every 80K iterations; we use 500K iterations, a burnin of 10K, and a thinning
7
We choose all hyper-parameters using cross-validation whereas [HLA15] performs posterior inference on
the noise and prior precisions, and uses Bayesian optimization to choose the remaining hyper-parameters.

7

Figure 2: Predictive distribution for different methods on a toy 1d regression problem. (a) PBP of
[HLA15]. (b) HMC. (c) VI method of [Gra11]. (d) SGD. (e) SGLD. (f) Distilled SGLD. Error bars
denote 3 standard deviations. (Figures a-d kindly provided by the authors of [HLA15]. We replace
their term ?BP? (backprop) with ?SGD? to avoid confusion.)

interval of 10. As shown in Table 5, we get an average log likelihood of ?2.306, which is better
than SGD.
Finally, we distill our SGLD model. The student architecture is the same as the teacher. We use the
following teacher hyper parameters: prior precision ? = 2.5; initial learning rate of ?0 = 1e ? 5,
which we reduce by a factor of 0.5 every 80K iterations. For the student, we use generated training
data with Gaussian noise with standard deviation 0.05, we use a prior precision of ? = 0.001, an
initial learning rate of ?0 = 1e ? 2, which we reduce by 0.8 after every 5e3 iterations. As shown
in Table 5, we get an average log likelihood of ?2.350, which is only slightly worse than SGLD,
and much better than SGD. Furthermore, both SGLD and distilled SGLD are better than the PBP
method of [HLA15] and the VI method of [Gra11].

4

Conclusions and future work

We have shown a very simple method for ?being Bayesian? about neural networks (and other kinds
of models), that seems to work better than recently proposed alternatives based on EP [HLA15] and
VB [Gra11, BCKW15].
There are various things we would like to do in the future: (1) Show the utility of our model in
an end-to-end task, where predictive uncertainty is useful (such as with contextual bandits or active
learning). (2) Consider ways to reduce the variance of the algorithm, perhaps by keeping a running
minibatch of parameters uniformly sampled from the posterior, which can be done online using
reservoir sampling. (3) Exploring more intelligent data generation methods for training the student.
(4) Investigating if our method is able to reduce the prevalence of confident false predictions on
adversarially generated examples, such as those discussed in [SZS+ 14].
Acknowledgements
We thank Jos?e Miguel Hern?andez-Lobato, Julien Cornebise, Jonathan Huang, George Papandreou,
Sergio Guadarrama and Nick Johnston.
8

References
[AKW12]

S. Ahn, A. Korattikara, and M. Welling. Bayesian Posterior Sampling via Stochastic Gradient
Fisher Scoring. In ICML, 2012.

[ASW14]

Sungjin Ahn, Babak Shahbaba, and Max Welling. Distributed stochastic gradient MCMC. In
ICML, 2014.

[BCKW15] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural networks. In ICML, 2015.
[BCNM06] Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In KDD,
2006.
[Bic07]

J Eric Bickel. Some comparisons among quadratic, spherical, and logarithmic scoring rules. Decision Analysis, 4(2):49?65, 2007.

[CFG14]

Tianqi Chen, Emily B Fox, and Carlos Guestrin. Stochastic Gradient Hamiltonian Monte Carlo.
In ICML, 2014.

[DFB+ 14] N Ding, Y Fang, R Babbush, C Chen, R Skeel, and H Neven. Bayesian sampling using stochastic
gradient thermostats. In NIPS, 2014.
[GG15]

Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. 6 June 2015.

[Gra11]

Alex Graves. Practical variational inference for neural networks. In NIPS, 2011.

[HLA15]

J. Hern?andez-Lobato and R. Adams. Probabilistic backpropagation for scalable learning of
bayesian neural networks. In ICML, 2015.

[HVD14]

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In
NIPS Deep Learning Workshop, 2014.

[KW14]

Diederik P Kingma and Max Welling. Stochastic gradient VB and the variational auto-encoder.
In ICLR, 2014.

[Nea11]

Radford Neal. MCMC using hamiltonian dynamics. In Handbook of Markov chain Monte Carlo.
Chapman and Hall, 2011.

[PT13]

Sam Patterson and Yee Whye Teh. Stochastic gradient riemannian langevin dynamics on the
probability simplex. In NIPS, 2013.

[RBK+ 14] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and
Yoshua Bengio. FitNets: Hints for thin deep nets. Arxiv, 19 2014.
[RMW14]

D. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference
in deep generative models. In ICML, 2014.

[SG05]

Edward Snelson and Zoubin Ghahramani. Compact approximations to bayesian predictive distributions. In ICML, 2005.

[SZS+ 14]

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014.

[WT11]

Max Welling and Yee W Teh. Bayesian learning via stochastic gradient Langevin dynamics. In
ICML, 2011.

9

"
2002,Mean Field Approach to a Probabilistic Model in Information Retrieval,,2250-mean-field-approach-to-a-probabilistic-model-in-information-retrieval.pdf,Abstract Missing,"Mean-Field Approach to a Probabilistic Model
in Information Retrieval
Bin Wu, K. Y. Michael Wong
Department of Physics
Hong Kong University of Science and Technology
Clear Water Bay, Hong Kong
phwbd@ust.hk phkywong@ust.hk
David Bodoff
Department of ISMT
Hong Kong University of Science and Technology
Clear Water Bay, Hong Kong
dbodoff@ust.hk

Abstract
We study an explicit parametric model of documents, queries, and relevancy assessment for Information Retrieval (IR). Mean-field methods
are applied to analyze the model and derive efficient practical algorithms
to estimate the parameters in the problem. The hyperparameters are estimated by a fast approximate leave-one-out cross-validation procedure
based on the cavity method. The algorithm is further evaluated on several
benchmark databases by comparing with standard algorithms in IR.

1 Introduction
The area of information retrieval (IR) studies the representation, organization and access of
information in an information repository. With the advent and boom of the Internet, especially the World Wide Web (WWW), more and more information is available to be shared
online. Search on the Internet becomes increasingly popular. In this respect, probabilistic
models have become very useful in empowering information searches [1, 2].
In fact, information searches themselves contain rich information, which can be recorded
and fruitfully used to improve the performance of subsequent retrievals. This is an extension of the process of relevance feedback [3], which incorporates the relevance assessments
supplied by the user to construct new representations for queries, during the procedure of
the users interactive document retrieval. In the process, the feedback information helps
to refine the queries continuously, but the effects pertain only to the particular retrieval
session. On the other hand, our objective is to refine the representations of documents
and queries with the help of relevancy data, so that subsequent retrieval sessions can be
benefited.
Based on Fuhr and Buckley?s meta-structure [4] relating documents, queries and relevancy
assessments, one of us recently proposed a probabilistic model [5] in which these objects

are described by explicit parametric distribution functions, facilitating the construction of
a likelihood function, whose maximum can be used to characterize the documents and
queries. Rather than relying on heuristics as in many previous work, the proposed model
provides a unified formal framework for the following two tasks: (a) ad hoc information
retrieval, in which a query is given and the goal is to return a list of ranked documents
according to their similarities with the query; (b) document routing, in which a document
is given and the goal is to categorize it using a list of ranked queries according to their similarities with the document. (Here we assume a model in which categories are represented
by queries.)
In this paper, we report our recent progress in putting this new theoretical approach to
empirical tests. Since documents and queries are represented by high dimensional vectors in a vector space model, a mean-field approach will be adopted. mean-field methods
were commonly used to study magnetic systems in statistical physics, but thanks to their
ability to deal with high dimensional systems, they are increasingly applied to many areas of information processing recently [6]. In the present context, a mean-field treatment
implies that when a particular component of a document or query vector is analyzed, all
other components of the same and other vectors can be considered as background fields
satisfying appropriate average properties, and correlations of statistical fluctuations with
the background vectors can be neglected.
After introducing the parametric model in Section 2, the mean-field approach will be used
in two steps. First, in Section 3, the true representations of documents and queries will be
estimated by maximizing the total probability of observation. It results in a set of meanfield equations, which can be solved by a fast iterative algorithm. Respectively, the estimated true documents and queries will then be used for ad hoc information retrieval and
document routing.
Secondly, the model depends on a few hyperparameters which are conventionally determined by the cross-validation method. Here, as described in Section 4, the mean-field approach can be used again to accelerate the otherwise tedious leave-one-out cross-validation
procedure. For a given set of hyperparameter values, it enables us to carry out the systemwide iteration only once (rather than repeating once for each left-out document or
query), and the leave-one-out estimations of the document and query representations can
be obtained by a version of mean-field theory called the cavity method [7].
In Section 6, we compare the model with the standard tf-idf [8] and latent semantic indexing
(LSI) [9] on benchmark test collections. As we shall see, the validity of our model is well
supported by its superior performance. The paper is concluded in Section 7.

2 A Unified Probabilistic Model





Our work is motivated by Fuhr and Buckley?s conceptual model. Assume that a set of
documents and
queries is available to us. In the vector space model, each document
and query is represented by an
dimensional vector. The vectors are denoted by ( ),
which are referred to as the true meaning of the document (query). Our model consists of
the following 3 components:





 

(a) The document
we really observe is distributed around the true document vector
according to the probability distribution
, the difference resulting from the
documents containing terms that do not ideally represent the meaning of the document. In
other words, the document
is generated from its true meaning .



 

 

	

  



that the user actually submits is also distributed around the true
(b) Similarly, the query
query vector according to the probability distribution distribution
.



	  

(c) There is some relation between the document and query, called relevancy assessment.
We denote this relation with a binary variable for each pair of document and query. If
, we say the document is relevant to the query, that is, the document is what the user
wants. Otherwise,
and the document is irrelevant to the query. Suppose we have
some relevancy relations between documents and queries (through historical records, from
experts, etc.). Then we hypothesize that the true documents and queries are distributed
, that is, the true representation of documents and
according to the distribution
queries should satisfy their relevancy relations.





		 
   

We summarize the idea through a probabilistic meta-structure shown in Figure 1.

fQ (Q 0 | Q)

Q
B

fB (D,Q | B )
fD (D 0 | D)

D
data

Q0

D0
data

unknown
parameters
Figure 1: Probabilistic meta-structure

In order to complete the model, we need to hypothesize the form of the distribution functions. In this paper, we restrict the documents and queries to a hypersphere, since usually
only the cosines of the angles between documents and queries are used to determine the
similarity between documents and queries. Hence, we assume the following distribution
functions:













   !        
	 
    
  
(1)


""
(b) The distribution of each observed query   given its true location  :
 
  $   %    &  
!
(2)
	    #
  
""
(c) The prior distribution of the documents and queries, given the relevance relation between them:
1 )2 *  /3 )4*  )   *   )      * &5  
!
	  (' #)%
 +*-,  ./ 0

 (3)
!
!
!
where  76  is the Dirac  -function, and 
 ,  and  are normalization constants of 	
 ,
	  and 	  respectively, and are hence independent of  and  .

(a) The distribution of each observed document

given its true location

:

If we further assume that the observation of documents and queries are independent of each
other, we can obtain the total probability of observing all documents and queries, given the
relevancy relation between them:

8 ('  ) 
  * ,  9: 0 ! ! !<; !    )        *     A

  
%=?>    ?= @

(4)

!;   #) +*   #)    %  +*       / 

(5)
)2 * 
//  )2 * ) *  )  +* 
   )  )  #)  
   *  *   *-

(6)
 
and : denotes all hyperparameters ' 
 

 
%/<, . There is now an appealing correspondence
! ; is
between the present model and spin models in statistical physics. It is observed that
where

just the familiar partition function and

	



is the energy function.

By maximizing the probability in Eq. (4), we can obtain an estimation of the true documents , which can be used in ad hoc retrieval: we define the similarity function between
two vectors as the cosine of the angle between them, and rank the similarities between
(instead of
) with a new query to determine whether the documents should be retrieved
or not. As a byproduct, we can also obtain the estimation of the true queries , which in
turn can be used in document routing: new documents should be compared with to determine whether it belongs to this category or not. So our model gives a unifying procedure
for both ad hoc retrieval and routing.

 

	

	

	

3 Parameter Estimation



!;

In this section, we derive a fast iterative algorithm for parameter estimation. First, we
replace the -function by its Fourier transform. Then
can be written as

! ;  
 i

 i
where   ) 
  * 
 ) 
 * 
   





)  i) *   * i   )   *  %A


)  )   ) <   *  *   * <   /

(7)
. In writing this

formula, we have changed the integration to the imaginary axis.









,
and , when the integration can be
Mean-field theory works in the limit of large
well approximated by taking the saddle point of . This is obtained by equating the partial
derivatives of with respect to , , and to zero, yielding



#	 )

	 *

	 )
	 *

 

  * )  *  	 *  
   )
/



 	)


 /  ) +)4*  #	 	 * )  
  * 

  A/  *  ) *  	 *  
   ) -

  A/  ) ) * #	 ) 
   *  ""

(8)
(9)
(10)
(11)

This set of equations is referred to as the mean-field equations, since fluctuations around
the mean values of the parameters have been neglected. Due to its simple form, it can be
solved by an iterative scheme. Though we have not studied the theoretical convergence
of the iterative scheme, its effectiveness can be seen from the following arguments. If we
replace in Eq. (8) and in Eq. (9) by the respective values of
and
at the saddle
point, then the iteration process becomes a linear one. Now, Eqs. (8) and (9) differ from
and
respectively. Hence after
this linear iteration problem by scale factors of
using Eqs. (10) and (11), the problem is equivalent to rescaling the lengths of the iterated

	 )

	 *

)!

""
"" )$! #  	 )  *! #  	 *

 * !

 .) 

 +*   

vectors back to the hypersphere defined by
and
. This alternate
operation of linear iteration and rescaling back to the hypersphere makes it a very stable
algorithm. The complexity of the algorithm is linear in the number of documents and
queries. Empirically, it converges in just a few tens of steps. Alternatively, one may use
the Augmented Lagrangian method to find the saddle point of , whose convergence is
guaranteed, but is computationally more complex [10].



4 Hyperparameter Estimation

/ 

	






	
 	

In our model, the parameters ,
and
determine the shape of the distributions ,
and , and influence the parameter estimation described in Section 3. We refer to them as
hyperparameters. They have to be chosen so that the model performs optimally when new
queries are raised to retrieve documents, or when new documents are routed.
A standard method for hyperparameter estimation in machine learning is leave-one-out
cross-validation [11]. Suppose we have
examples for training the model. Then each
time we pick one data as the validation set and train the model with the rest of the
examples. The hyperparameters are chosen as the ones that give the optimal performance
averaged over the test examples.

 

The exact leave-one-out cross-validation is very tedious, especially for multiple hyperparameters, because of the need to train the model times for each combination of hyperparameters. For this model, we propose an approximate leave-one-out procedure based on
the cavity method [7]. Suppose we have trained the model with all data, and obtain the
estimation
, which satisfies the steady state equation

' #	 )
  	 * ,
  )

/

)

*

	
*


*


#	 ) 
 	)


  *
/

	



4
)
*
)
)
	 * 
 	*
""




(12)

If the query
were left out from the training set of queries, the cavity estimation should
satisfy the equation


  )

/

)
4
*

	



*

*



	 ) 
 	 )



  *
/

	



4
)
*
)
)
	 * 

	 
 "" (13)
 	 *
  
By subtracting (7) by (8), and assuming that ' ) 
 * , is approximately the same as
 
'  )%
  *	, , we can get the difference,

 +*
 /  )  )4*   ) 
	
 #)
 / *  ) *   *  /3 )   

 
 "" (14)
	 ) 
	 *

For ad hoc retrieval, we eliminate  * to obtain a set of linear equations for  ) . The solution can be further simplified by using the mean-field argument that the changes induced








by removing the query
on documents can be decoupled. Hence we can neglect the
off-diagonal terms, yielding

 

' 	 )%
 	 *	,
 	 )   )     )


)   )  /3/ )      ""
 	  *     
	

(15)



	

)



Note that
have been known in the systemwide training. Then
can be estimated
by
. The similarities between
and
are then used to predict the
leave-one-out ad hoc retrieval performance of the model. Equations for document routing
can be derived analogously.



)

Note that we need to train the model only once, and the leave-one-out estimation of documents and queries can be obtained in one step. So the algorithm is extremely fast. Amazingly, it also gives reasonable estimations of hyperparameters, as shown in the following
experiments.
We remark that the mean-field technique can be applied to distributions of documents,
queries and relevance feedbacks other than those described by Eqs. (1-3). In the present
case spectified by Eqs. (1-3), our model is similar to the Gaussian model, if the spherical
constraint on ?s and ?s are replaced by a spherical Gaussian prior. Though leave-oneout cross-validation can be done exactly in the Gaussian model, it involves the inversion of
a large matrix. On the other hand, the mean-field estimation greatly simplifies the process
by neglecting the off-diagonal elements.





5 Experimental Results
We have applied the proposed method to ad hoc retrieval and routing for the test collections
of Cranfield and CISI. Because we treat both tasks identically, we use the same evaluation
criterion: the recall precision curve and the average retrieval precision. We have run two
versions of our algorithm: (a) in the original dimension, the observed documents
and
queries
are represented by the original tf-idf weights; (b) in the reduced dimension of
100, in which the original vectors are reduced by singular value decomposition (SVD) in
LSI.

 

 

In Figs. 2 (a-b), we show the recall precision curves at the optimal hyperparameters. The
mean-field estimates are compared with the baseline results of LSI. It is clear that our
method gives significant gains in retrieval precision. Comparisons using the original dimension or the Cranfield collection, not shown here due to space limitations, yield equally
satisfactory results.

0.4

0.4
Precision

0.6

Precision

0.6

MF

MF
0.2

0.2
LSI
LSI

0

0

0.2

0.4

0.6
Recall

0.8

1

0

0

0.2

0.4

0.6

0.8

1

Recall

Figure 2: The recall precision curves of the mean-field estimation (MF) and the baseline
(LSI) for (a) ad hoc retrieval (b) document routing for CISI in reduced dimension
For hyperparameter estimation, we can compare the mean-field results and those for exact
leave-one-out cross-validation in reduced dimension, since the computation of the exact
ones is still feasible. In Fig. 3, we have plotted the average precision versus the two hyperparameters, as computed by the two methods. They have very similar contours, although
there is a uniform displacement between their values. This demonstrates the usefulness of
the mean-field approximation in hyperparameter estimation.
In Table 1, we obtain the values of the optimal hyperparameters from the mean-field leave-

one-out method, and the average precisions of the exact leave-one-out are then computed
using these optimal hyperparameters. These are compared with the results of the exact
leave-one-out and listed in Table 1. For the hyperparameter estimation in the original
dimension, the exact leave-one-out is not available since it is too tedious. Instead, we
compare the hyperparameters with the ones from the -fold cross-validation. Whether we
compare the mean-field with the exact leave-one-out or -fold cross-validation, the optimal
hperparameters are comparable in most cases, and when there are discrepancies, one can
observe that the average precisions are essentially the same.

 
  # /0

  # / +   
"" 


""   ""  

Figure 3: Average retrieval precision versus hyperparameters for ad hoc retrieval in reduced
;
dimension for CISI: (a) mean-field leave-one-out, peaked at
(b) exact leave-one-out. peaked at
.

 
  # /

  # /    

"" 





Table 1: The average retrieval precision for leave-one-out cross-validation in reduced dimension: mean-field versus exact.
CISI
Cranfield
Average precision
Average precision
ad hoc retrieval
LSI
?
?
0.079
?
?
0.178
Mean-Field
0.3
12.0
0.142
0.4
1.1
0.248
Exact
0.3
10.1
0.142
0.6
1.5
0.250
Document Routing
LSI
?
?
0.104
?
?
0.240
Mean-Field 28.9
1.6
0.192
2.5
1.1
0.351
Exact
23.0
2.5
0.193
0.9
0.7
0.356


 #/ 
 #/


 #/ 
 #/

6 Conclusion
We have considered a probabilistic model of documents, queries and relevancy assessments. Fast algorithms are derived for parameter and hyperparameter estimations. Significant improvement is achieved for both ad hoc retrieval and routing compared with tf-idf
and LSI. In another paper [12], we have compared the model with other heuristic methods such as Rocchio heuristics [3] and Bartell?s Multidimensional Scaling [13], and the
mean-field method still outperforms them. These successes illustrate the potentials of the
mean-field approach, which is especially suitable for systems with high dimensions and

numerous mutually interacting components, such as those in IR. Hence we anticipate that
mean-field methods will have increasing applications in many other probabilistic models
in IR.
Acknowledgments
We thank R. Jin for interesting discussions. This work was supported by the grant
HKUST6157/99P of the Research Grant Council of Hong Kong.

References
[1] Cohn, D. and T. Hofmann (2001). The Missing Link ? A Probabilistic Model of Document Content and Hypertext Connectivity. Advances in Neural Information Processing Systems 13, T. K. Leen, T. G. Dietterich and V. Tresp, eds., MIT Press, Cambridge,
MA, 430-436.
[2] Jaakola, T. and H. Siegelmann (2002). Active Information Retrieval. Advances in
Neural Information Processing Systems 14, T. G. Dietterich, S. Becker and Z. Ghahramani, eds., MIT Press, Cambridge, MA, 777-784.
[3] Rocchio, J. J. (1971). Relevance Feedback in Information Retrieval. SMART Retrieval
System?Experiments in Automatic Document Processing, G. Salton ed., PrenticeHall, Englewood Cliffs, NJ, Chapter 14.
[4] Fuhr, N. and C. Buckley (1991). A Probabilistic Learning Approach for Document
Indexing. ACM Transactions on Information Systems 9(3): 223-248.
[5] Bodoff, D., D. Enabe, A. Kanbil, G. Simon and A. Yukhimets (2001). A Unified
Maximumn Likelihood Approach to Document Retrieval. Journal of the American
Society for Information Science and Technology 52(10): 785-796.
[6] Opper, M. and D. Saad, eds. (2001). Advanced Mean Field Methods, MIT Press,
Cambridge, MA.
[7] Wong, K. Y. M. and F. Li (2002). Fast Parameter Estimation Using Green?s Functions.
Advances in Neural Information Processing System 14: 535-542, T.G. Dietterich, S.
Becker and Z. Ghahramani, eds., MIT Press, Cambridge, MA.
[8] Salton, G. and M. J. McGill (1983). Introduction to Modern Information Retrieval,
McGraw-Hill, New York, 63-66.
[9] Deerwester, S., S. T. Dumais, G. W. Furnas, T. K. Landauer and R. Harshman (1990).
Indexing by Latent Semantic Analysis. Journal of the American Society for Information Science 41(16): 391-407.
[10] Nocedal, J. and S. J. Wright (1999). Numerical Optimization, Springer, Berlin, Ch.
17.
[11] Bishop, C. M. (1995). Neural Networks for Pattern Recognition, Clarendon Press,
Oxford, 372-375.
[12] Bodoff, D., B. Wu and K. Y. M. Wong (2002). Relevance Feedback meets Maximum
Likelihood, preprint.
[13] Bartell, B. T., G. W. Cottrell and R. K. Belew (1992). Latent Semantic Indexing Is
an Optimal Special Case of Multidimensional Scaling. Proceedings of the 15th International ACM SIGIR Conference on Research and Development in Information
Retrieval, 161-167.

"
2015,Subspace Clustering with Irrelevant Features via Robust Dantzig Selector,Poster,5707-subspace-clustering-with-irrelevant-features-via-robust-dantzig-selector.pdf,"This paper considers the subspace clustering problem where the data contains irrelevant or corrupted features. We propose a method termed ``robust Dantzig selector'' which can successfully identify the clustering structure even with the presence of irrelevant features. The idea is simple yet powerful: we replace the inner product by its robust counterpart, which is insensitive to the irrelevant features given an upper bound of the number of irrelevant features. We establish theoretical guarantees for the algorithm to identify the correct subspace, and demonstrate the effectiveness of the algorithm via numerical simulations. To the best of our knowledge, this is the first method developed to tackle subspace clustering with irrelevant features.","Subspace Clustering with Irrelevant Features via
Robust Dantzig Selector

Chao Qu
Department of Mechanical Engineering
National University of Singapore

Huan Xu
Department of Mechanical Engineering
National University of Singapore

A0117143@u.nus.edu

mpexuh@nus.edu.sg

Abstract
This paper considers the subspace clustering problem where the data contains
irrelevant or corrupted features. We propose a method termed ?robust Dantzig selector? which can successfully identify the clustering structure even with the presence of irrelevant features. The idea is simple yet powerful: we replace the inner
product by its robust counterpart, which is insensitive to the irrelevant features
given an upper bound of the number of irrelevant features. We establish theoretical guarantees for the algorithm to identify the correct subspace, and demonstrate
the effectiveness of the algorithm via numerical simulations. To the best of our
knowledge, this is the first method developed to tackle subspace clustering with
irrelevant features.

1

Introduction

The last decade has witnessed fast growing attention in research of high-dimensional data: images,
videos, DNA microarray data and data from many other applications all have the property that the
dimensionality can be comparable or even much larger than the number of samples. While this
setup appears ill-posed in the first sight, the inference and recovery is possible by exploiting the fact
that high-dimensional data often possess low dimensional structures [3, 14, 19]. On the other hand,
in this era of big data, huge amounts of data are collected everywhere, and such data is generally
heterogeneous. Clean data and irrelevant or even corrupted information are often mixed together,
which motivates us to consider the high-dimensional, big but dirty data problem. In particular, we
study the subspace clustering problem in this setting.
Subspace clustering is an important subject in analyzing high-dimensional data, inspired by many
real applications[15]. Given data points lying in the union of multiple linear spaces, subspace clustering aims to identify all these linear spaces, and cluster the sample points according to the linear
spaces they belong to. Here, different subspaces may correspond to motion of different objects in
video sequence [11, 17, 20], different rotations, translations and thickness in handwritten digit or
the latent communities for the social graph [15, 5].
A variety of algorithms of subspace clustering have been proposed in the last several years including algebraic algorithms [16], iterative methods [9, 1], statistical methods [11, 10], and spectral
clustering-based methods [6, 7]. Among them, sparse subspace clustering (SSC) not only achieves
state-of-art empirical performance, but also possesses elegant theoretical guarantees. In [12], the
authors provide a geometric analysis of SSC which explains rigorously why SSC is successful even
when the subspaces are overlapping [12]. [18] and [13] extend SSC to the noisy case, where data are
contaminated by additive Gaussian noise. Different from these work, we focus on the case where
some irrelevant features are involved.
1

Mathematically, SSC indeed solves for each sample a sparse linear regression problem with the
dictionary being all other samples. Many properties of sparse linear regression problem are well
understood in the clean data case. However, the performance of most standard algorithms deteriorates (e.g. LASSO and OMP) even only a few entries are corrupted. As such, it is well expected
that standard SSC breaks for subspace clustering with irrelevant or corrupted features (see Section 5
for numerical evidences). Sparse regression under corruption is a hard problem, and few work has
addressed this problem [8][21] [4].
Our contribution: Inspired by [4], we use a simple yet powerful tool called robust inner product
and propose the robust Dantzig selector to solve the subspace clustering problem with irrelevant
features. While our work is based upon the robust inner product developed to solve robust sparse
regression, the analysis is quite different from the regression case since both the data structures and
the tasks are completely different: for example, the RIP condition ? essential for sparse regression ?
is hardly satisfied for subspace clustering [18]. We provide sufficient conditions to ensure that
the Robust Dantzig selector can detect the true subspace clustering. We further demonstrate via
numerical simulation the effectiveness of the proposed method. To the best of our knowledge, this
is the first attempt to perform subspace clustering with irrelevant features.

2
2.1

Problem setup and method
Notations and model

The clean data matrix is denoted by XA ? RD?N , where each column corresponds to a data point,
normalized to a unit vector. The data points are lying on a union of L subspace S = ?L
l=1 Sl .
Each subspace Sl is of dimension dl which is smaller than D and contains Nl data samples with
N1 +N2 +? ? ?+NL = N . We denote the observed dirty data matrix by X ? R(D+D1 )?N . Out of the
T
T T
D + D1 features, up to D1 of them are irrelevant. Without loss of generality, let X = [XO
, XA
] ,
where XO ? RD1 ?N denotes the irrelevant data. The subscript A and O denote the set of row
indices corresponding to true and irrelevant features and the superscript T denotes the transpose.
Notice that we do not know O a priori except its cardinality is D1 . The model is illustrated in Figure
(l)
1. Let XA ? RD?Nl denote the selection of columns in XA that belongs to Sl . Similarly, denote
the corresponding columns in X by X (l) . Without loss of generality, let X = [X (1) , X (2) , ..., X (L) ]
be ordered. Further more, we use the subscript ??i?to describe a matrix that excludes the column
(l)
(l)
(l)
(l)
(l)
i, e.g., (XA )?i = [(xA )1 , ..., (xA )i?1 , (xA )i+1 , ..., (xA )Nl ]. We use the superscript lc to describe
c

(1)

(l?1)

(l+1)

(L)

a matrix that excludes column in subspace l, e.g., (XA )l = [XA , ..., XA , XA , ..., XA ].
For a matrix ?, we use ?s,? to denote the submatrix with row indices in set s and column indices
in set ?. For any matrix Z, P (Z) denotes the symmetrized convex hull of its column, i.e., P (Z) =
(l)
l
conv(?z1 , ?z2 , ...., ?zN ) . We define P?i
:= P ((XA )?i ) for simplification, i.e., the symmetrized
convex hull of clean data in subspace l except data i. Finally we use k ? k2 to denote the l2 norm of
a vector and k ? k? to denote infinity norm of a vector or a matrix. Caligraphic letters such as X , Xl
represent the set containing all columns of the corresponding clean data matrix.

Figure 1: Illustration of the model of irrelevant features in the subspace clustering problem. The
left one is the model addressed in this paper: Among total D + D1 features, up tp D1 of them are
irrelevant. The right one illustrates a more general case, where the value of any D1 element of each
column can be arbitrary (e.g., due to corruptions). It is a harder case and left for future work.

2

Figure 2: Illustration of the Subspace Detection Property. Here, each figure corresponds to a matrix
where each column is ci , and non-zero entries are in white. The left figure satisfies this property.
The right one does not.
2.2

Method

In this secion we present our method as well as the intuition that derives it. When all observed data
are clean, to solve the subspace clustering problem, the celebrated SSC [6] proposes to solve the
following convex programming
min kci k1
ci

s.t.

xi = X?i ci ,

(1)

for each data point xi . When data are corrupted by noise of small magnitude such as Gaussian noise,
a straightforward extension of SSC is the Lasso type method called ?Lasso-SSC? [18, 13]
min kci k1 +
ci

?
kxi ? X?i ci k22 .
2

(2)

Note that while Formulation (2) has the same form as Lasso, it is used to solve the subspace clustering task. In particular, the support recovery analysis of Lasso does not extend to this case, as X?i
typically does not satisfy the RIP condition [18].
This paper considers the case where X contains irrelevant/gross corrupted features. As we discussed above, Lasso is not robust to such corruption. An intuitive idea is to consider the following
formulation first proposed for sparse linear regression [21].
min kci k1 +
ci ,E

?
kxi ? (X?i ? E)ci k22 + ?kEk? ,
2

(3)

where k ? k? is some norm corresponding to the sparse type of E. One major challenge of this
formulation is that it is not convex. As such, it is not clear how to efficiently find the optimal
solution, and how to analyze the property of the solution (typically done via convex analysis) in the
subspace clustering task.
Our method is based on the idea of robust inner product. The robust inner product ha, bik is defined
as follows: For vector a ? RD , b ? RD , we compute qi = ai bi , i = 1, ..., N . Then {|qi |} are
P sorted
and the smallest (D ? k) are selected. Let ? be the set of selected indices, then ha, bik = i?? qi ,
i.e., the largest k terms are truncated. Our main idea is to replace all inner products involved by
robust counterparts ha, biD1 , where D1 is the upper bound of the number of irrelevant features.
The intuition is that the irrelevant features with large magnitude may affect the correct subspace
clustering. This simple truncation process will avoid this. We remark that we do not need to know
the exact number of irrelevant feature, but instead only an upper bound of it.
Extending (2) using the robust inner product leads the following formulation:
min kci k1 +
ci

? T?
c ?ci ? ??
? T ci ,
2 i

(4)

? and ?? are robust counterparts of X T X?i and X T xi . Unfortunately, ?
? may not be a
where ?
?i
?i
positive semidefinite matrix, thus (4) is not a convex program. Unlike the work [4][8] which studies
3

non-convexity in linear regression, the difficulty of non-convexity in the subspace clustering task
appears to be hard to overcome.
Instead we turn to the Dantzig Selector, which is essentially a linear program (and hence no positive
semidefiniteness is required):
T
min kci k1 + ?kX?i
(X?i ci ? xi )k? .

(5)

ci

Replace all inner product by its robust counterpart, we propose the following Robust Dantzig Selector, which can be easily recast as a linear program:
? i ? ?? k? ,
min kci k1 + ?k?c

Robust Dantzig Selector:

(6)

ci

Subspace Detection Property: To measure whether the algorithm is successful, we define the
criterion Subspace Detection Property following [18]. We say that the Subspace Detection Property holds, if and only if for all i, the optimal solution to the robust Dantzig Selector satisfies (1)
Non-triviality: ci is not a zero vector; (2) Self-Expressiveness Property: nonzeros entries of ci
correspond to only columns of X sampled from the same subspace as xi . See Figure 2 for illustrations.

3

Main Results

To avoid repetition and cluttered notations, we denote the following primal convex problem by
P (?, ?)
min kck1 + ?k?c ? ?k? .
c

Its dual problem, denoted by D(?, ?), is
maxh?, ?i subject to k?k1 = ?
?

k??k? ? 1.

(7)

Before we presents our results, we define some quantities.
The dual direction is an important geometric term introdcued in analyzing SSC [12]. Here we define
similarly the dual direction of the robust Dantzig selector: Notice that the dual of robust Dantzig
? ?? ), where ?? and ?
? are robust counterparts of X T xi and X T X?i respectively
problem is D(?,
?i
?i
? into two parts ?
? = (XA )T (XA )?i + ?,
?
(recall that X?i and xi are the dirty data). We decompose ?
?i
where the first term corresponds to the clean data, and the second term is due to the irrelevant features
and truncation from the robust inner product. Thus, the second constraint of the dual problem
? ? ? 1. Let ? be the optimal solution to the above optimization
becomes k((XA )T?i (XA )?i + ?)?k
(l)

problem, we define v(xi , X?i , ?) := (XA )?i ? and the dual direction as v l =

v(xli ,X?i ,?)
(l)
kv(xli ,X?i ,?)k2

.

l
Similarly as SSC [12], we define the subspace incoherence. Let V l = [v1l , v2l , ..., vN
]. The incoherl
(k)

ence of a point set Xl to other clean data points is defined as ?(Xl ) = maxk:k6=l k(XA )T V l k? .
? and ?? as ?
? = (XA )T (XA )?i + ?
? and ?? = (XA )T (xA )i + ?? .
Recall that we decompose ?
?i
?i
?
Intuitively, for robust Dantzig selecter to succeed, we want ? and ?? not too large. Particularly, we
assume k(xA )i k? ? 1 and k(XA )?i k? ? 2 .
l
Theorem 1 (Deterministic Model). Denote ?l := ?(Xl ), rl := mini:xi ?Xl r(P?i
), r :=
minl=1,...,L rl and suppose ?l < rl for all l. If

rl ? ul
1
< min
,
l 2D1 2
r2 ? 4D1 1 2 r ? 2D1 22
2 (ul + rl )

(8)

then the subspace detection property holds for all ? in the range
1
rl ? ul
< ? < min
.
l 2D1 2
r2 ? 4D1 1 2 r ? 2D1 22
2 (ul + rl )
4

(9)

In an ideal case when D1 = 0, the condition of the upper bound of ? reduces to rl > ul , similar to
the condition for SSC in the noiseless case [12].
Based on Condition (8), under a randomized generative model, we can derive how many irrelevant
features can be tolerated.
Theorem 2 (Random model). Suppose there are L subspaces and for simplicity, all subspaces have
same dimension d and are chosen uniformly at random. For each subspace there are ?d + 1 points
chosen independently and uniformly at random. Up to D1 features of data are irrelevant. Each
data point (including true and irrelevant features) is independent from other data points. Then for
some universal constants
C1 , C2 , the subspace detection property holds with probability at least
?
1 ? N4 ? N exp(? ?d) if
d?

Dc2 (?) log(?)
,
12 log N

and
log ?
1 2
2 c (?) d

1
1??
D
q
<?<
,
?
C1 D1 (log D+C2 log N )
1
+
?
C
D
(log
D
+ C2 log N )
log ?
1
1
? ( 2c(?)
+
1)
d
D

q
12d log N
where ? =
Dc2 (?) log ? ; c(?) is a constant only depending on the density of data points on
subspace and satisfies (1) c(?) >
? 0 for all ? > 1, (2) there is a numerical value ?0 , such that for all
? > ?0 , one can take c(?) = 1/ 8.
Simplifying the above conditions, we can determine the number of irrelevant features that can be
tolerated. In particular, if d ? 2c2 (?) log ? and we choose the ? as
?=

4d
,
c2 (?) log ?

then the maximal number of irrelevant feature D1 that can be torelated is
1??
C0 Dc2 (?) log ?
c(?)D log ?
,
},
8C1 d(log(D) + C2 log N ) 1 + ? C1 d(log(D) + C2 log N )
?
with probability at least 1 ? N4 ? N exp(? ?d).
D1 = min{

If d ? 2c2 (?) log ?, and we choose the same ?, then the number of irrelevant feature we can tolerate
is
q
Dc(?) logd ?
1??
C0 Dc2 (?) log ?
D1 = min{ ?
,
},
4 2C1 (log(D) + C2 log N ) 1 + ? C1 d(log(D) + C2 log N )
?
with probability at least 1 ? N4 ? N exp(? ?d).
Remark 1. If D is much larger than D1 , the lower bound of ? is proportional to the subspace
dimension d. When d increases, the upper bound of ? decreases, since 1??
1+? decreases. Thus the
valid range of ? shrinks when d increases.
Remark 2. Ignoring the logarithm terms, when d is large, the tolerable D1 is proportional
to
?
D
D
1?? D
min(C1 1??
,
C
).
When
d
is
small,
D
is
proportional
to
min(C
,
C
D/
d)
.
2
1
1
2
1+? d
d
1+? d

4

Roadmap of the Proof

In this section, we lay out the roadmap of proof. In specific we want to establish the condition with
the number of irrelevant features, and the structure of data (i.e., the incoherence ? and inradius r)
for the algorithm to succeed. Indeed, we provide a lower bound of ? such that the optimal solution
ci is not trivial; and an upper bound of ? so that the Self-Expressiveness Property holds. Combining
them together established the theorems.
5

4.1

Self-Expressiveness Property

The Self-Expressiveness Property is related to the upper bound of ?. The proof technique is inspired
by [18] and [12], we first establish the following lemma, which provides a sufficient condition such
that Self-Expressiveness Property holds of the problem 6.
Lemma 1. Consider a matrix ? ? RN ?N and ? ? RN ?1 , If there exist a pair (?
c, ?) such that c?
has a support S ? T and
sgn(?
cs ) + ?s,? ?? = 0,
k?sc ?T,? ?? k? ? 1,
k?k1 = ?,
k?T c ,? ?? k? < 1,

(10)

where ? is the set of indices of entry i such that |(??
c ? ?)i | = k??
c ? ?k? , then for all optimal
solution c? to the problem P (?, ?), we have c?T c = 0.
The variable ? in Lemma 1 is often termed the ?dual certificate?. We next consider an oracle problem
? to construct such a dual certificate. This
? l,l , ??l ), and use its dual optimal variable denoted by ?,
P (?
candidate satisfies all conditions in the Lemma 1 automatically except to show
? lc ,?? ????k? < 1,
k?

(11)

c

where l denotes the set of indices expect the ones corresponding to subspace l. We can compare
c
this condition with the corresponding one in analyzing SSC, in which one need k(X)(l )T vk? < 1,
c
? lc ,?? = (XA )(l )T (XA )?? + ?
? lc ,?? .
where v is the dual certificate. Recall that we can decompose ?
Thus Condition 11 becomes
k(XA )(l

c

)T

? lc ,?? ????k? < 1.
((XA )??????) + ?

(12)

? lc ,?? ????k? .
To show this holds, we need to bound two terms k(XA )??????k2 and k?
? ? , k?
Bounding k?k
? k?
? ? and k?
The following lemma relates D1 with k?k
? k? .
? and ?? are robust counterparts of X T X?i and X T xi respectively and among
Lemma 2. Suppose ?
?i
?i
? and ?? into following form ?
? =
D + D1 features, up to D1 are irrelevant. We can decompose ?
? and ?? = (XA )T (xA )i + ?? . We define ?1 := k?
?
(XA )T?i (XA )?i + ?
?
k
and
?
:=
k
?k
?
2
? .If
?i
k(xA )i k? ? 1 and k(XA )?i k? ? 2 , then ?2 ? 2D1 22 , ?1 ? 2D1 1 2 .
We then bound 1 and 2 in the random model
cap [2].
? using the upper bound of the spherical
?
Indeed we have 1 ? C1 (log D + C2 log N )/ D and 2 ? C1 (log D + C2 log N )/ D with high
probability.
Bounding kX??????k2
By exploiting the feasible condition in the dual of the oracle problem, we obtain the following
bound:
1 + 2D1 ?22
kX??????k2 ?
.
l )
r(P?i
q
log ?
l
?
Furthermore, r(P?i
) can be lower bound by c(?)
d and 2 can be upper bounded by C1 (log D+
2
?
C2 log N )/ D in the random model with high probability. Thus the RHS can be upper bounded.
Plugging this upper bound into (12), we obtain the upper bound of ?.
4.2

Non-triviality with sufficiently large ?

To ensure that the solution is not trivial (i.e., not all-zero), we need a lower bound on ?.
6

If ? satisfies the following condition, the optimal solution to problem 6 can not be zero
?>

l )
r2 (P?i

?

2D1 22

1
.
l )D  
? 4r(P?i
1 1 2

(13)

The proof idea is to show when ? is large enough, the trivial solution c = 0 can not be optimal. In
particular, if c = 0, the corresponding value in the primal problem is ?k?
?l k? . We then establish a
?
lower bound of k?
?l k? and a upper bound of kck1 + ?k?l,l c ? ??l k? so that the following inequality
always holds by some carefully choosen c.
? l,l c ? ??l k? < ?k?
kck1 + ?k?
?l k? .

(14)

l
). Notice
We then further lower bound the RHS of Equation (13) using the bound of 1 , 2 and r(P?i
that condition (14) requires that ? > A and condition (11) requires ? < B, where A and B are some
terms depending on the number of irrelevant features. Thus we require A < B to get the maximal
number of irrelevant features that can be tolerated.

5

Numerical simulations

In this section, we use three numerical experiments to demonstrate the effectiveness of our method
to handle irrelevant/corrupted features. In particular, we test the performance of our method and
effect of number of irrelevant features and dimension subspaces d with respect to different ?. In
all experiments, the ambient dimension D = 200, sample density ? = 5, the subspace are drawn
uniformly at random. Each subspace has ?d+1 points chosen independently and uniformly random.
We measure the success of the algorithms using the relative violation of the subspace detection
property defined as follows,
P
|C|i,j
(i,j)?M
/
RelV iolation(C, M) = P
,
(i,j)?M |C|i,j
where C = [c1 , c2 , ..., cN ], M is the ground truth mask containing all (i, j) such that xi , xj belong
to a same subspace. If RelV iolation(C, M) = 0, then the subspace detection property is satisfied.
We also check whether we obtain a trivial solution, i.e., if any column in C is all-zero.
We first compare the robust Dantzig selector(? = 2) with SSC and LASSO-SSC ( ? = 10). The
results are shown in Figure 3. The X-axis is the number of irrelevant features and the Y-axis is the
Relviolation defined above. The ambient dimension D = 200, L = 3, d = 5, the relative sample
density ? = 5. The values of irrelevant features are independently sampled from a uniform distribution in the region [?2.5, 2.5] in (a) and [?10, 10] in (b). We observe from Figure 3 that both SSC
and Lasso SSC are very sensitive to irrelevant information. (Notice that RelViolation=0.1 is pretty
large and can be considered as clustering failure.) Compared with that, the proposed Robust Dantzig
Selector performs very well. Even when D1 = 20, it still detects the true subspaces perfectly. In
the same setting, we do some further experiments, our method breaks when D1 is about 40. We
also do further experiment for Lasso-SSC with different ? in the supplementary material to show
Lasso-SSC is not robust to irrelevant features.
We also examine the relation of ? to the performance of the algorithm. In Figure 4a, we test the
subspace detection property with different ? and D1 . When ? is too small, the algorithm gives a
trivial solution (the black region in the figure). As we increase the value of ?, the corresponding
solutions satisfy the subspace detection property (represented as the white region in the figure).
When ? is larger than certain upper bound, RelV iolation becomes non-zero, indicating errors in
subspace clustering. In Figure 4b, we test the subspace detection property with different ? and d.
Notice we rescale ? with d, since by Theorem 3, ? should be proportional to d. We observe that the
valid region of ? shrinks with increasing d which matches our theorem.

6

Conclusion and future work

We studied subspace clustering with irrelevant features, and proposed the ?robust Dantzig selector?
based on the idea of robust inner product, essentially a truncated version of inner product to avoid
7

Original SSC
Lasso SSC
Robust Dantzig Selector

1

0.8
RelViolation

RelViolation

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0

Original SSC
Lasso SSC
Robust Dantzig Selector

1

0

5

10
Number of irrelevant features

15

0

20

0

5

10
Number of irrelevant features

(a)

15

20

(b)

2.5

10.5
10
9.5
9
8.5
8
7.5
7
6.5
6
5.5
5
4.5
4
3.5
3
2.5
2
1.5
1

2.3
2.1
1.9
1.7
1.5

?/d

?

Figure 3: Relviolation with different D1 . Simulated with D = 200, d = 5, L = 3, ? = 5, ? = 2,
and D1 from 1 to 20.

1.3
1.1
0.9
0.7
0.5
0.3
0.1

1

2

3

4

5

6

7

8

9

10

4

Number of irrelevant features D1

(a) Exact recovery with different number of
irrelevant features. Simulated with D =
200, d = 5, L = 3, ? = 5 with an increasing D1 from 1 to 10. Black region:
trivial solution. White region: Non-trivial
solution with RelViolation=0. Gray region:
RelViolation> 0.02.

6

8

10

12

14

16

Subspace dimension d

(b) Exact recovery with different subspace
dimension d. Simulated with D = 200,
L = 3, ? = 5, D1 = 5 and an increasing d from 4 to 16. Black region: trivial solution. White region: Non-trivial solution with RelViolation=0. Gray region:
RelViolation> 0.02.

Figure 4: Subspace detection property with different ?, D1 , d.
any single entry having too large influnce on the result. We established the sufficient conditions
for the algorithm to exactly detect the true subspace under the deterministic model and the random
model. Simulation results demonstrate that the proposed method is robust to irrelevant information
whereas the performance of original SSC and LASSO-SSC significantly deteriorates.
We now outline some directions of future research. An immediate future work is to study theoretical
guarantees of the proposed method under the semi-random model, where each subspace is chosen
deterministically, while samples are randomly distributed on the respective subspace. The challenge
here is to bound the subspace incoherence, previous methods uses the rotation invariance of the data,
which is not possible in our case as the robust inner product is invariant to rotations.
Acknowledgments
This work is partially supported by the Ministry of Education of Singapore AcRF Tier Two grant
R-265-000-443-112, and A*STAR SERC PSF grant R-265-000-540-305.

References
[1] Pankaj K Agarwal and Nabil H Mustafa. k-means projective clustering. In Proceedings of the
23rd ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages
8

155?165, 2004.
[2] Keith Ball. An elementary introduction to modern convex geometry. Flavors of geometry,
31:1?58, 1997.
[3] Emmanuel J Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal component
analysis? Journal of the ACM, 58(3):11, 2011.
[4] Yudong Chen, Constantine Caramanis, and Shie Mannor. Robust sparse regression under adversarial corruption. In Proceedings of the 30th International Conference on Machine Learning, pages 774?782, 2013.
[5] Yudong Chen, Ali Jalali, Sujay Sanghavi, and Huan Xu. Clustering partially observed graphs
via convex optimization. The Journal of Machine Learning Research, 15(1):2213?2238, 2014.
[6] Ehsan Elhamifar and Ren?e Vidal. Sparse subspace clustering. In CVPR 2009, pages 2790?
2797.
[7] Guangcan Liu, Zhouchen Lin, and Yong Yu. Robust subspace segmentation by low-rank representation. In Proceedings of the 27th International Conference on Machine Learning, pages
663?670, 2010.
[8] Po-Ling Loh and Martin J Wainwright. High-dimensional regression with noisy and missing
data: Provable guarantees with non-convexity. In Advances in Neural Information Processing
Systems, pages 2726?2734, 2011.
[9] Le Lu and Ren?e Vidal. Combined central and subspace clustering for computer vision applications. In Proceedings of the 23rd international conference on Machine learning, pages
593?600, 2006.
[10] Yi Ma, Harm Derksen, Wei Hong, and John Wright. Segmentation of multivariate mixed data
via lossy data coding and compression. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 29(9):1546?1562, 2007.
[11] Shankar R Rao, Roberto Tron, Ren?e Vidal, and Yi Ma. Motion segmentation via robust subspace separation in the presence of outlying, incomplete, or corrupted trajectories. In CVPR
2008.
[12] Mahdi Soltanolkotabi, Emmanuel J Candes, et al. A geometric analysis of subspace clustering
with outliers. The Annals of Statistics, 40(4):2195?2238, 2012.
[13] Mahdi Soltanolkotabi, Ehsan Elhamifar, Emmanuel J Candes, et al. Robust subspace clustering. The Annals of Statistics, 42(2):669?699, 2014.
[14] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical
Society. Series B, pages 267?288, 1996.
[15] Ren?e Vidal. A tutorial on subspace clustering. IEEE Signal Processing Magazine, 28(2):52?
68, 2010.
[16] Rene Vidal, Yi Ma, and Shankar Sastry. Generalized principal component analysis (gpca).
IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(12):1945?1959, 2005.
[17] Ren?e Vidal, Roberto Tron, and Richard Hartley. Multiframe motion segmentation with missing
data using powerfactorization and gpca. International Journal of Computer Vision, 79(1):85?
105, 2008.
[18] Yu-Xiang Wang and Huan Xu. Noisy sparse subspace clustering. In Proceedings of The 30th
International Conference on Machine Learning, pages 89?97, 2013.
[19] H. Xu, C. Caramanis, and S. Sanghavi. Robust PCA via outlier pursuit. IEEE Transactions on
Information Theory, 58(5):3047?3064, 2012.
[20] Jingyu Yan and Marc Pollefeys. A general framework for motion segmentation: Independent,
articulated, rigid, non-rigid, degenerate and non-degenerate. In ECCV 2006, pages 94?106.
2006.
[21] Hao Zhu, Geert Leus, and Georgios B Giannakis. Sparsity-cognizant total least-squares for
perturbed compressive sampling. IEEE Transactions on Signal Processing, 59(5):2002?2016,
2011.

9

"
1996,Dynamically Adaptable CMOS Winner-Take-All Neural Network,,1281-dynamically-adaptable-cmos-winner-take-all-neural-network.pdf,Abstract Missing,"Dynamically Adaptable CMOS
Winner-Take-AII Neural Network

Kunihiko Iizuka, Masayuki Miyamoto and Hirofumi Matsui
Information Technology Research Laboratories
Sharp
Tenri, Nara, lAP AN

Abstract
The major problem that has prevented practical application of analog
neuro-LSIs has been poor accuracy due to fluctuating analog device
characteristics inherent in each device as a result of manufacturing.
This paper proposes a dynamic control architecture that allows analog
silicon neural networks to compensate for the fluctuating device
characteristics and adapt to a change in input DC level. We have
applied this architecture to compensate for input offset voltages of an
analog CMOS WTA (Winner-Take-AlI) chip that we have fabricated.
Experimental data show the effectiveness of the architecture.

1

INTRODUCTION

Analog VLSI implementation of neural networks, such as silicon retinas and adaptive
filters, has been the focus of much active research. Since it utilizes physical laws that
electric devices obey for neural operation, circuit scale can be much smaller than that of a
digital counterpart and massively parallel implementation is possible. The major problem
that has prevented practical applications of these LSIs has been fluctuating analog device
characteristics inherent in each device as a result of manufacturing. Historically, this has
been the main reason most analog devices have been superseded by digital devices.
Analog neuro VLSI is expected to conquer this problem by making use of its adaptability.
This optimistic view comes from the fact that in spite of the unevenness of their
components, biological neural networks show excellent competence.

This paper proposes a CMOS circuit architecture that dynamically compensates for
fluctuating component characteristics and at the same time adapts device state to
incoming signal levels. There are some engineering techniques available to compensate

714

K. lizuka, M. Miyamoto and H. Matsui

for MOS threshold fluctuation, e.g., the chopper comparator, but they need a periodical
change of mode to achieve the desired effect. This is because there are two modes one for
the adaptation and one for the signal processing. This is quite inconvenient because extra
clock signals are needed and a break of signal processing takes place.
Incoming signals usually consist of a rapidly changing foreground component and a
slowly varying background component. To process these signals incessantly, biological
neural networks make use of multiple channels having different temporaVspatial scales.
While a relatively slow/large channel is used to suppress background floating, a
faster/smaller channel is devoted to process the foreground signal. The proposed method
inspired by this biological consideration utilizes different frequency bands for adaptation
and signal processing (Figure 1), where negative feedback is applied through a low pass
filter so that the feedback will not affect the foreground signal processing.

Gain
LOW PASS

COMI'ARATOR

FILTER

FOREGROUND
BAND

BACKGROUND
nAND

Output SignAl

Input Signal

Frequency

(a)

(b)

Figure 1: Dynamic adaptation by frequency divided control. (a) model diagram, (b)
frequency division.
In the first part of this paper, a working analog CMOS WTA chip that we have test
fabricated is introduced. Then, dynamical adaptation for this WTA chip is described and
experimental results are presented.

2

ANALOG CMOS WTA CHIP

2.1

ARCHITECTURE AND SPECIFICATION

CM

v,"" I

?

2nd LAYER

Figure 2: Analog CMOS WTA chip architecture

FEEDBACK
CONTROLLER

Dynamically Adaptable CMOS Wmner-Take-All Neural Network

715

Vdd

M5

M4

Vhl--f

I--Vb2

eM

M

In l)U~

Olltput

VII\~

I
I
I

MI

114 2
V!!IS

(b)

(ll)

Figure 3: Circuit diagrams for (a) the competitive cell and (b) the feedback controller.
As a basic building block to construct neuro-chips, analog WfA circuits have been
investigated by researchers such as [Lazzaro, 1989] and [Pedroni, 1994]. All CMOS
analog WfA circuits are based on voltage follower circuits [Pedroni, 1995] to realize
competition through inhibitory interaction, and they use feedback mechanisms to enhance
resolution gain. The architecture of the chip that we have fabricated is shown in Figure 2
and the circuit diagram is in Figure 3. This WfA chip indicates the lowest input voltage
by making the output voltage corresponds to the lowest input voltage near Vss (winner),
and others nearly the power supply voltage Vdd (loser). The circuit is similar to [Sheu,
1993], but represents two advances.

1. The steering current that the feedback controller absorbs from the line CM is
enlarged, allowing the winner cell can compete with others in the region where
resolution gain is the largest.
2 The feedback controller originally placed after the second competitive layer is
removed in order to guarantee the existence of at least one output node whose voltage
is nearly zero.
Table 1 shows the specifications of the fabricated chip.

Table 1: Specifications of the fabricated WfA chip

2.2

INPUT OFFSET VOLTAGE

Input offset voltages of a WfA chip may greatly deteriorate chip performance. Examples
of input offset voltage distribution of the fabricated chips are shown in Figure 4. Each
input offset voltage is measured relative to the first input node. The input offset voltage

716

K. lizuJea, M. Miyamoto and H. Matsui

=

~Vj

of the j-th input node is defined as ~Vj Vinj - Vin1 when the voltages of output
nodes Outj and Out1 are equal; Vin1 is fixed to a certain voltage and the voltage of other
input nodes are fixed at a relatively high voltage.

Figure 4: Examples of measured input offset voltage distribution.
The primary factor of the input offset voltage is considered to be fluctuation of MOS
transistor threshold voltages in the first layer competitive cell. Then, the input offset
voltage ~Vj of this cell yielded by the small fluctuation ~Vth i of Vth i is calculated as
follows:

_

-~Vtht + gd 1 + gd 2 + gm2 (~Vth2 _ ~Vth3) + gm4(gd 1 + gd 2 + gm2) ~Vth4
gmt

gm1 gm3

where gmi and gdl are the transconductance and the drain conductance of MOS Mi,
respectively. Using design and process parameters, we can estimate the input offset
voltage to be
AVj. -AVthl + (AVth 2 -AVth 3 )+O.l5AVth4

,

Based on our experiences, the maximum fluctuation of Vth in a chip is usually smaller
than 20 mY, and it is reasonable to consider that the difference I~Vth2 - ~VtJrI is even
smaller; perhaps less than 5 mV, because M2 and M3 compose a current mirror and are
closely placed. This implies that the maximum of ~Vj is about 28 mY, which is in rough
agreement with the measured data.
i

3

DYNAMICAL ADAPTATION ARCmTECTURE

In Figure 5, we show circuit implementation of the dynamically adaptable wrA function.
In each feedback channel, the difference between each output and the reference Vref is
fed back to the input node through a low pass filter consisting of Rand C. The charge
stored in capacitor C is controlled by this feedback signal.
Let the linear approximation of the wrA chip DC characteristic be
Vouti =A ( Vin , - VOJ,
where Vin i and Voutt are the voltages at the nodes In/ and Out, respectively, andA and VOL
are functions of Vinj (j '"" i ). The input offset voltage relative to the node In] is considered
to be the difference between VOL and V0 1. On the other hand, the DC characteristic of the
i-th feedback path can be approximated as

717

Dynamically Adaptable CMOS Winner- Take-All Neural Network
Inl'

In2'

Cl..

Cl..

In32

,

cl..
?

?

?

?

?

?

R

R'

Inl

In2

? ? ?

WTA Chip

Out l

Out2

?

?

?
Vref

?
Outl

?

Vref

?
Outn

Out2

Figure 5: wrA chip equipped with adaptation circuit where R=10MQ and C=0.33JAF.

Yin; = B (Yout; - Vref).
It follows from the above two equations that
AB

B

Vin ? - - - - Vo? - - - Vre r ? Vo ?
I
1- AB
I
1- AB
'.J
I

The last term is derived using the assumptions A ? 1 and B ? -1. This means that the
voltage difference between the DC level of the input and YO; is clamped on the capacitor
C. This in turn implies that the input offset voltage will be successfully compensated for.
The role of the low pass filters is twofold.

1. They guarantee stable dynamics of the feedback loop; we can make the cutoff
frequency of the low pass filters small enough so that the gain of the feedback path is
attenuated before the phase of the feedback signal is delayed by more than 1800 ?
2. They prevent the feed-forward wrA operation from being affected, as shown in
Figure 1, the adaptive control is carried out on a different, non-overlapped frequency
band than wrA operation.

4

EXPERIMENTAL RESULTS

Experiments concerning the adaptable wr A function were carried out by applying pulses
of 90% duty to the input nodes In', and In'l, while other input nodes were fixed to a
certain voltage. In Figures 6 (a) and 6 (b), the output waveforms of Outl , Out2 , Out] and
the waveform of the pulse applied to the node In', are shown. Figure 6(a) shows the result
when the same pulse was applied to both In', and In'z. Figure 6(b) shows the result when
the amplitude of the pulse to In', was greater than that of the pulse to In'z by 10 mY. The
schematic explanation of this behavior is in Figure 7. The outputs remained at the same
levels for a while after the inputs were shut off, since there was no strong inducement. As
a result of adaptation, the winning frequencies of every output nodes become equal in a
long time scale. This explains the unstable output during the period of quiescent inputs.

718

K. Iizuka. M. Miyamoto and H. Matsui

The chip used in this measurement had a relative input offset voltage of 15 mV between
nodes In1 and In 2 ? We can see in Figure 6 (a) that this offset voltage was completely
compensated for because the output waveforms of corresponding nodes were the same.

(a)

(b)

Figure 6: The output waveforms ot the dynamically adaptable CMOS WTA neural
network. Pulse waves were applied to nodes In'] and In'2; other nodes voltages were fixed.
When the amplitude of each pulse was the same (a), the corresponding output waveforms
were the same. When the amplitude of the pulse fed to In'I was greater than that to In '2 by
10 mV (b), the output voltage at Out] was low (winner) and that at Out2 was high (loser)
during the period the pulse was low (on).

Outputs

Inputs
quiescent

Out 1

i;i

L.."" mii~iii~~iii~!iim

Q,

Out2

~
?
?
?

Out3

~mmmmmm~mm

iiii ..........Lo-s-er-

?
?

?

roser

.

~mmmmm~mmm

~'---v---'

Hysteresis

Unstable

Figure 7: The schematic explanation of the dynamically adaptable WT A behavior.

5

CONCLUSION

We have proposed a dynamic adaptation architecture that uses frequency divided control
and applied this to a CMOS WTA chip that we have fabricated. Experimental results
show that the architecture successfully compensated for input offset voltages of the WTA

719

Dynamically Adaptable CMOS Winner- Take-All Neural Network

chip due to inherent device characteristic fluctuations. Moreover, this architecture gives
analog neuro-chips the ability to adapt to incoming signal background levels. This
adaptability has a lot of applications. For example, in vision chips, the adaptation may be
used to compensate for the fluctuation of photo sensor characteristics, to adapt the gain of
photo sensors to background illumination level and to automatically control color balance.
As another application, Figure 8 describes an analog neuron with weighted synapses,
where the time constant RC is much larger than the time constant of input signals.

Inputs

11

???

11

c

Output

Figure 8: Analog neuron with weighted synapses where the time constant RC is much
larger than that of input signals.
The key to this architecture is use of non-overlapping frequency bands for adaptation to
background and foreground signal processing. For neuro-VLSIs, this requires
implementing circuits with completely different time scale constants. In modern VLSI
technology, however, this is not a difficult problem because processes for very high
resistances, i.e., teraohms, are available.

Acknowledgment
The authors would like to thank Morio Osaka for his help in chip fabrication and Kazuo
Hashiguchi for his support in experimental work.

References
Choi, J. & Sheu, B.J. (1993) A high-precision VLSI winner-take-all circuit for selforganizing neural networks. IEEE J. Solid-State Circuits, vo1.28, no.5, pp.576-584.
Lazzaro, J., Ryckebush, S., Mahowald, M.A., & Mead, C. (1989) Winner-take-all
networks of O(N) complexity. In D.S. Touretzky (eds.), Advances in Neural Information
Processing Systems 1, pp. 703-711. Cambridge, MA: MIT Press.
Pedroni, V.A. (1994) Neural n-port voltage comparator network, Electron. Lett., vo1.30,
no.21, pp1774-1775.
Pedroni, V.A. (1995) Inhibitory Mechanism Analysis of Complexity O(N) MOS WinnerTake-All Networks. IEEE Trans. Circuits Syst. I, vo1.42, no.3, pp.172-175.

"
2016,Maximal Sparsity with Deep Networks?,Poster,6346-maximal-sparsity-with-deep-networks.pdf,"The iterations of many sparse estimation algorithms are comprised of a fixed linear filter cascaded with a thresholding nonlinearity, which collectively resemble a typical neural network layer.  Consequently, a lengthy sequence of algorithm iterations can be viewed as a deep network with shared, hand-crafted layer weights.  It is therefore quite natural to examine the degree to which a learned network model might act as a viable surrogate for traditional sparse estimation in domains where ample training data is available.  While the possibility of a reduced computational budget is readily apparent when a ceiling is imposed on the number of layers, our work primarily focuses on estimation accuracy.  In particular, it is well-known that when a signal dictionary has coherent columns, as quantified by a large RIP constant, then most tractable iterative algorithms are unable to find maximally sparse representations.  In contrast, we demonstrate both theoretically and empirically the potential for a trained deep network to recover minimal $\ell_0$-norm representations in regimes where existing methods fail.  The resulting system, which can effectively learn novel iterative sparse estimation algorithms, is deployed on a practical photometric stereo estimation problem, where the goal is to remove sparse outliers that can disrupt the estimation of surface normals from a 3D scene.","Maximal Sparsity with Deep Networks?

Bo Xin1,2
Yizhou Wang1
Wen Gao1
Baoyuan Wang3
David Wipf2
2
3
Peking University
Microsoft Research, Beijing
Microsoft Research, Redmond
{boxin, baoyuanw, davidwip}@microsoft.com {yizhou.wang, wgao}@pku.edu.cn
1

Abstract
The iterations of many sparse estimation algorithms are comprised of a fixed linear filter cascaded with a thresholding nonlinearity, which collectively resemble a
typical neural network layer. Consequently, a lengthy sequence of algorithm iterations can be viewed as a deep network with shared, hand-crafted layer weights. It
is therefore quite natural to examine the degree to which a learned network model
might act as a viable surrogate for traditional sparse estimation in domains where
ample training data is available. While the possibility of a reduced computational
budget is readily apparent when a ceiling is imposed on the number of layers, our
work primarily focuses on estimation accuracy. In particular, it is well-known that
when a signal dictionary has coherent columns, as quantified by a large RIP constant, then most tractable iterative algorithms are unable to find maximally sparse
representations. In contrast, we demonstrate both theoretically and empirically the
potential for a trained deep network to recover minimal `0 -norm representations in
regimes where existing methods fail. The resulting system, which can effectively
learn novel iterative sparse estimation algorithms, is deployed on a practical photometric stereo estimation problem, where the goal is to remove sparse outliers
that can disrupt the estimation of surface normals from a 3D scene.

1

Introduction

Our launching point is the optimization problem
min kxk0 s.t. y = ?x,
x

(1)

where y ? Rn is an observed vector, ? ? Rn?m is some known, overcomplete dictionary of
feature/basis vectors with m > n, and k?k0 denotes the `0 norm of a vector, or a count of the number
of nonzero elements. Consequently, (1) can be viewed as the search for a maximally sparse feasible
vector x? (or approximately feasible if the constraint is relaxed). Unfortunately however, direct
assault on (1) involves an intractable, combinatorial optimization process, and therefore efficient
alternatives that return a maximally sparse x? with high probability in restricted regimes are sought.
Popular examples with varying degrees of computational overhead include convex relaxations such
as `1 -norm minimization [2, 5, 21], greedy approaches like orthogonal matching pursuit (OMP)
[18, 22], and many flavors of iterative hard-thresholding (IHT) [3, 4].
Variants of these algorithms find practical relevance in numerous disparate domains, including feature selection [7, 8], outlier removal [6, 13], compressive sensing [5], and source localization [1, 16].
However, a fundamental weakness underlies them all: If the Gram matrix ?> ? has significant offdiagonal energy, indicative of strong coherence between columns of ?, then estimation of x? may
be extremely poor. Loosely speaking this occurs because, as higher correlation levels are present,
the null-space of ? is more likely to include large numbers of approximately sparse vectors that
tend to distract existing algorithms in the feasible region, an unavoidable nuisance in many practical
applications.
In this paper we consider recent developments in the field of deep learning as an entry point for
improving the performance of sparse recovery algorithms. Although seemingly unrelated at first
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

glance, the layers of a deep neural network (DNN) can be viewed as iterations of some algorithm
that have been unfolded into a network structure [9, 11]. In particular, iterative thresholding approaches such as IHT mentioned above typically involve an update rule comprised of a fixed, linear
filter followed by a non-linear activation function that promotes sparsity. Consequently, algorithm
execution can be interpreted as passing an input through an extremely deep network with constant
weights (dependent on ?) at every layer. This ?unfolding? viewpoint immediately suggests that
we consider substituting discriminatively learned weights in place of those inspired by the original
sparse recovery algorithm. For example, it has been argued that, given access to a sufficient number
of {x? , y} pairs, a trained network may be capable of producing quality sparse estimates with a
few number of layers. This in turn can lead to a dramatically reduced computational burden relative to purely optimization-based approaches [9, 19, 23] or to enhanced non-linearities for use with
traditional iterative algorithms [15].
While existing empirical results are promising, especially in terms of the reduction in computational
footprint, there is as of yet no empirical demonstration of a learned deep network that can unequivocally recover maximally sparse vectors x? with greater accuracy than conventional, state-of-the-art
optimization-based algorithms, especially with a highly coherent ?. Nor is there supporting theoretical evidence elucidating the exact mechanism whereby learning may be expected to improve the
estimation accuracy, especially in the presence of coherent dictionaries. This paper attempts to fill
in some of these gaps, and our contributions can be distilled to the following points:
Quantifiable Benefits of Unfolding: We rigorously dissect the benefits of unfolding conventional
sparse estimation algorithms to produce trainable deep networks. This includes a precise characterization of exactly how different architecture choices can affect the ability to improve so-called
restrictive isometry property (RIP) constants, which measure the degree of disruptive correlation
in ?. This helps to quantify the limits of shared layer weights, which are the standard template
of existing methods [9, 19, 23], and motivates more flexible network constructions reminiscent of
LSTM cells [12] that account for multi-resolution structure in ? in a previously unexplored fashion.
Note that we defer all proofs, as well as many additional analyses and problem details, to a longer
companion paper [26].
Isolation of Important Factors: Based on these theoretical insights, and a better understanding
of the essential factors governing performance, we establish the degree to which it is favorable to
diverge from strict conformity to any particular unfolded algorithmic script. In particular, we argue
that layer-wise independent weights and/or activations are essential, while retainment of original
thresholding non-linearities and squared-error loss implicit to many sparse algorithms is not. We
also recast the the core problem as deep multi-label classification given that optimal support pattern
recovery is the primary concern. This allows us to adopt a novel training paradigm that is less
sensitive to the specific distribution encountered during testing. Ultimately, we development the
first, ultra-fast sparse estimation algorithm (or more precisely a learning procedure that produces
such an algorithm) that can effectively deal with coherent dictionaries and adversarial RIP constants.
State-of-the-Art Empirical Performance: We apply the proposed system to a practical photometric stereo computer vision problem, where the goal is to estimate the 3D geometry of an object
using only 2D photos taken from a single camera under different lighting conditions. In this context, shadows and specularities represent sparse outliers that must be simultaneously removed from
? 104 ? 106 surface points. We achieve state-of-the-art performance using only weak supervision
despite a minuscule computational budget appropriate for real-time mobile environments.

2

From Iterative Hard Thesholding (IHT) to Deep Neural Networks

Although any number of iterative algorithms could be adopted as our starting point, here we examine
IHT because it is representative of many other sparse estimation paradigms and is amenable to
theoretical analysis. With knowledge of an upper bound on the true cardinality, solving (1) can be
replaced by the equivalent problem
min 12 ky ? ?xk22 s.t. kxk0 ? k.
(2)
x
IHT attempts to minimize (2) using what can be viewed as computationally-efficient projected gradient iterations [3]. Let x(t) denote the estimate of some maximally sparse x? after t iterations. The
aggregate IHT update computes
h

i
x(t+1) = Hk x(t) ? ??> ?x(t) ? y ,
(3)
2

where ? is a step-size parameter and Hk [?] is a hard-thresholding operator that sets all but the k
largest values (in magnitude) of a vector to zero. For the vanilla version of IHT, the step-size ? = 1
leads to a number of recovery guarantees whereby iterating (3), starting from x(0) = 0 is guaranteed
to reduce (2) at each step before eventually converging to the globally optimal solution. These
results hinge on properties of ? which relate to the coherence structure of dictionary columns as
encapsulated by the following definition.
Definition 1 (Restricted Isometry Property) A dictionary ? satisfies the Restricted Isometry
Property (RIP) with constant ?k [?] < 1 if
(1 ? ?k [?])kxk22 ? k?xk22 ? (1 + ?k [?])kxk22

(4)

holds for all {x : kxk0 ? k}.
In brief, the smaller the value of the RIP constant ?k [?], the closer any sub-matrix of ? with k
columns is to being orthogonal (i.e., it has less correlation structure). It is now well-established that
dictionaries with smaller values of ?k [?] lead to sparse recovery problems that are inherently easier
to solve. For example,
that if y = ?x? , with kx? k0 ? k
? in the context of IHT, it has been shown [3]
(t)
and ?3k [?] < 1/ 32, then at iteration t of (3) we will have kx ? x? k2 ? 2?t kx? k2 . It follows
that as t ? ?, x(t) ? x? , meaning that we recover the true, generating x? . Moreover, it can be
shown that this x? is also the unique, optimal solution to (1) [5].
The success of IHT in recovering
maximally sparse solutions crucially depends on the RIP-based
?
condition that ?3k [?] < 1/ 32, which heavily constrains the degree of correlation structure in ?
that can be tolerated. While dictionaries with columns drawn independently and uniformly from the
surface of a unit hypersphere (or with elements drawn iid from N (0, 1/n) ) will satisfy this condition
with high probability provided k is small enough [6], for many/most practical problems of interest
we cannot rely on this type of IHT recovery guarantee. In fact, except for randomized dictionaries
in high dimensions where tight bounds exist,
 we cannot even compute the value of ?3k [?], which
m
requires calculating the spectral norm of 3k
subsets of dictionary columns.
There are many ways nature might structure a dictionary such that IHT (or most any other existing
sparse estimation algorithm) will fail. Here we examine one of the most straightforward forms
of
 dictionary
 coherence that can easily disrupt performance. Consider the situation where ? =
A + uv > N , where columns of A ? Rn?m and u ? Rn are drawn iid from the surface of a
unit hypersphere, while v ? Rm is arbitrary. Additionally,  > 0 is a scalar and N is a diagonal
normalization matrix that scales each column of ? to have unit `2 norm. It then follows that if  is
sufficiently small,
? the rank-one component begins to dominate, and there is no value of 3k such that
?3k [?] < 1/ 32. In this type of problem we hypothesize that DNNs provide a potential avenue for
improvement to the extent that they might be able to compensate for disruptive correlations in ?.
For example, at the most basic level we might consider general networks with the layer t defined by
h
i
x(t+1) = f ?x(t) + ?y ,
(5)
where f : Rm ? Rm is a non-linear activation function, and ? ? Rm?m and ? ? Rm?n are
arbitrary. Moreover, given access to training pairs {x? , y}, where x? is a sparse vector such that
y = ?x? , we can optimize ? and ? using traditional stochastic gradient descent just like any other
DNN structure. We will first precisely characterize the extent to which this adaptation affords any
benefit over IHT where f (?) = Hk [?]. Later we will consider flexible, layer-specific non-linearities
f (t) and parameters {?(t) , ?(t) }.

3

Analysis of Adaptable Weights and Activations

For simplicity in this section we restrict ourselves to the fixed hard-threshold operator Hk [?] across
all layers; however, many of the conclusions borne out of our analysis nonetheless carry over to a
much wider range of activation functions f . In general it is difficult to analyze how arbitrary ?
and ? may improve upon the fixed parameterization from (3) where ? = I ? ?> ? and ? =
?> (assuming ? = 1). Fortunately though, we can significantly collapse the space of potential
weight matrices by including the natural requirement that if x? represents the true, maximally sparse
solution, then it must be a fixed-point of (5). Indeed, without this stipulation the iterations could
3

diverge away from the globally optimal value of x, something IHT itself will never do. These
considerations lead to the following:
Proposition 1 Consider a generalized IHT-based network layer given by (5) with f (?) = Hk [?] and
let x? denote any unique, maximally sparse feasible solution to y = ?x with kxk0 ? k. Then to
ensure that any such x? is a fixed point it must be that ? = I ? ??.
Although ? remains unconstrained, this result has restricted ? to be a rank-n factor, parameterized
by ?, subtracted from an identity matrix. Certainly this represents a significant contraction of the
space of ?reasonable? parameterizations for a general IHT layer. In light of Proposition 1, we may
then further consider whether the added generality of ? (as opposed to the original fixed assignment
? = ?> ) affords any further benefit to the revised IHT update
h
i
x(t+1) = Hk (I ? ??) x(t) + ?y .
(6)
For this purpose we note that (6) can be interpreted as a projected gradient descent step for solving
min 21 x> ??x ? x> ?y s.t. kxk0 ? k.
(7)
x
However, if ?? is not positive semi-definite, then this objective is no longer even convex, and
combined with the non-convex constraint is likely to produce an even wider constellation of troublesome local minima with no clear affiliation with the global optimum of our original problem from
(2). Consequently it does not immediately appear that ? 6= ?> is likely to provide any tangible
benefit. However, there do exist important exceptions. The first indication of how learning a general
? might help comes from the following result:
Proposition 2 Suppose that ? = D?> W W > , where W is an arbitrary matrix of appropriate
dimension and D is a full-rank diagonal that jointly solve
?
?3k
[?] , inf ?3k [W ?D] .
(8)
W ,D
Moreover, assume that ? is substituted with ?D in (6), meaning we have simply replaced ? with
a new dictionary ?
that has scaled columns. Given these qualifications, if y = ?x? , with kx? k0 ? k
?
and ?3k [?] < 1/ 32, then at iteration t of (6)

kD ?1 x(t) ? D ?1 x? k2 ? 2?t kD ?1 x? k2 .

(9)

It follows that as t ? ?, x(t) ? x? , meaning that we recover the true, generating x? . Additionally, it can be guaranteed that after a finite number of iterations, the correct support pattern will be
discovered. And it should be emphasized that rescaling ? by some known diagonal D is a common prescription for sparse estimation (e.g., column normalization) that does not alter the optimal
`0 -norm support pattern.1
?
But the real advantage over regular IHT comes from the fact that ?3k
[?] ? ?k [?], and in many prac?
tical cases, ?3k [?]  ?3k [?], which implies success can be guaranteed
 across a much wider range
of RIP conditions. For example, if we revisit the dictionary ? = A + uv > N , an immediate
?
benefit can be observed. More concretely, for  sufficiently small we argued that ?3k [?] > 1/ 32
for all k, and consequently convergence to the optimal solution may fail. In contrast, it can be shown
?
?
that ?3k
[?] will remain quite small, satisfying ?3k
[?] ? ?3k [A], implying that performance will
nearly match that of an equivalent recovery problem using A (and as we discussed above, ?3k [A] is
likely to be relatively small per its unique, randomized design). The following result generalizes a
sufficient regime whereby this is possible:

Corollary 1 Suppose ? = [A + ?r ] N , where elements of A are drawn iid from N (0, 1/n), ?r
is any arbitrary matrix with rank[?r ] = r < n, and N is a diagonal matrix (e.g, one that enforces
unit `2 column norms). Then

h i
?
e ,
E (?3k
[?]) ? E ?3k A
(10)
e denotes the matrix A with any r rows removed.
where A
1

Inclusion of this diagonal factor D can be equivalently viewed as relaxing Proposition 1 to hold under
some fixed rescaling of ?, i.e., an operation that preserves the optimal support pattern.

4

Additionally, as the size of h? grows
proportionally larger, it can be shown that with overwhelming
i
?
e
probability ?3k [?] ? ?3k A . Overall, these results suggest that we can essentially annihilate
any potentially disruptive rank-r component ?r at the cost of implicitly losing r measurements
(linearly independent rows of A, and implicitly the corresponding
elements of y). Therefore, at
h i
e
least provided that r is sufficiently small such that ?3k A ? ?3k [A], we can indeed be confident
that a modified form of IHT can perform much like a system with an ideal RIP constant. And of
course in practice we may not know how ? decomposes as some ? ? [A + ?r ] N ; however, to
the extent that this approximation can possibly hold, the RIP constant can be improved nonetheless.
It should be noted that globally solving (8) is non-differentiable and intractable, but this is the whole
point of incorporating a DNN network to begin with. If we have access to a large number of training
pairs {x? , y} generated using the true ?, then during the course of the learning process a useful
W and D can be implicitly estimated such that a maximal number of sparse vectors can be successfully recovered. Of course we will experience diminishing marginal returns as more non-ideal
components enter the picture. In fact, it is not difficult to describe a slightly more sophisticated scenario such that use of layer-wise constant weights and activations are no longer capable of lowering
?3k [?] significantly at all, portending failure when it comes to accurate sparse recovery.
One such example is a clustered dictionary model (which we describe in detail in [26]), whereby
columns of ? are grouped into a number of tight clusters with minimal angular dispersion. While the
clusters themselves may be well-separated, the correlation within clusters can be arbitrarily large. In
some sense this model represents the simplest partitioning of dictionary column correlation structure
into two scales: the inter- and intra-cluster structures. Assuming the number of such clusters is larger
than n, then layer-wise constant weights and activations are unlikely to provide adequate relief, since
the implicit ?r factor described above will be full rank.
Fortunately, simple adaptations of IHT, which are reflective of many generic DNN structures, can
remedy the problem. The core principle is to design a network such that earlier layers/iterations
are tasked with exposing the correct support at the cluster level, without concern for accuracy within
each cluster. Once the correct cluster support has been obtained, later layers can then be charged with
estimating the fine-grain details of within-cluster support. We believe this type of multi-resolution
sparse estimation is essential when dealing with highly coherent dictionaries. This can be accomplished with the following adaptations to IHT:
1. The hard-thresholding operator is generalized to ?remember? previously learned clusterlevel sparsity patterns, in much the same way that LSTM gates allow long term dependencies to propagate [12] or highway networks [20] facilitate information flow unfettered to
deeper layers. Practically speaking this adaptation can be computed by passing the prior
layer?s activations x(t) through linear filters followed by indicator functions, again reminiscent of how DNN gating functions are typically implemented.
2. We allow the layer weights {?(t) , ?(t) } to vary from iteration to iteration t sequencing
through a fixed set akin to layers of a DNN.
In [26] we show that hand-crafted versions of these changes allow IHT to provably recovery maximally sparse vectors x? in situations where existing algorithms fail.

4

Discriminative Multi-Resolution Sparse Estimation

As implied previously, guaranteed success for most existing sparse estimation strategies hinges on
the dictionary ? having columns drawn (approximately) from a uniform distribution on the surface
of a unit hypersphere, or some similar condition to ensure that subsets of columns behave approximately like an orthogonal basis. Essentially this confines the structure of the dictionary to operate on
a single universal scale. The clustered dictionary model described in the previous section considers
a dictionary built on two different scales, with a cluster-level distribution (coarse) and tightly-packed
within-cluster details (fine). But practical dictionaries may display structure operating across a variety of scales that interleave with one another, forming a continuum among multiple levels.
When the scales are clearly demarcated, we have argued that it is possible to manually define a
multi-resolution IHT-inspired algorithm that guarantees success in recovering the optimal support
pattern; and indeed, IHT could be extended to handle a clustered dictionary model with nested
5

structures across more than two scales. However, without clearly partitioned scales it is much less
obvious how one would devise an optimal IHT modification. It is in this context that learning flexible
algorithm iterations is likely to be most advantageous. In fact, the situation is not at all unlike
many computer vision scenarios whereby handcrafted features such as SIFT may work optimally in
confined, idealized domains, while learned CNN-based features are often more effective otherwise.
Given a sufficient corpus of {x? , y} pairs linked via some fixed ?, we can replace manual filter
construction with a learning-based approach. On this point, although we view our results from
Section 3 as a convincing proof of concept, it is unlikely that there is anything intrinsically special
about the specific hard-threshold operator and layer-wise construction we employed per se, as long
as we allow for deep, adaptable layers that can account for structure at multiple scales. For example,
we expect that it is more important to establish a robust training pipeline that avoids stalling at the
hand of vanishing gradients in a deep network, than to preserve the original IHT template analogous
to existing learning-based methods. It is here that we propose several deviations:
Multi-Label Classification Loss: We exploit the fact that in producing a maximally sparse vector x? , the main challenge is estimating supp[x? ]. Once the support is obtained, computing the
actual nonzero coefficients just boils down to solving a least squares problem. But any learning
system will be unaware of this and could easily expend undue effort in attempting to match coefficient magnitudes at the expense of support recovery. Certainly the use of a data fit penalty of the
form ky ? ?xk22 , as is adopted by nearly all sparse recovery algorithms, will expose us to this issue. Therefore we instead formulate sparse recovery as a multi-label classification problem. More
specifically, instead of directly estimating x? , we attempt to learn s? = [s?1 , . . . , s?m ]> , where s?i
equals the indicator function I[x?i 6= 0]. For this purpose we may then incorporate a traditional
multi-label classification loss function via a final softmax output layer, which forces the network to
only concern itself with learning support patterns. This substitution is further justified by the fact
that even with traditional IHT, the support pattern will be accurately recovered before the iterations
converge exactly to x? . Therefore we may expect that fewer layers (as well as training data) are
required if all we seek is a support estimate, opening the door for weaker forms of supervision.
Instruments for Avoiding Bad Local Solutions: Given that IHT can take many iterations to converge on challenging problems, we may expect that a relatively deep network structure will be
needed to obtain exact support recovery. We must therefore take care to avoid premature convergence to local minima or areas with vanishing gradient by incorporating several recent countermeasures proposed in the DNN community. For example, the adaptive variant of IHT described
previously is reminiscent of highway networks or LSTM cells, which have been proposed to allow longer range flow of gradient information to improve convergence through the use of gating
functions. An even simpler version of this concept involves direct, un-gated connections that allow
much deeper ?residual? networks to be trained [10] (which is even suggestive of the residual factor
embedded in the original IHT iterations). We deploy this tool, along with batch-normalization [14]
to aid convergence, for our basic feedforward pipeline, along with an alternative structure based
on recurrent LSTM cells. Note that unfolded LSTM networks frequently receive a novel input for
every time step, whereas here y is applied unaltered at every layer (more on this in [26]). We also
replace the non-integrable hard-threshold operator with simple rectilinear (ReLu) units [17], which
are functionally equivalent to one-sided soft-thresholding; this convex selection likely reduces the
constellation of sub-optimal local minima during the training process.

5

Experiments and Applications

Synthetic
We generate a dictionary matrix ? ? Rn?m using
Pn Tests with Correlated Dictionaries:
n
m
? = i=1 i12 ui v >
,
where
u
?
R
and
v
?
R
have iid elements drawn from N (0, 1). We also
i
i
i
rescale each column of ? to have unit `2 norm. ? generated in this way has super-linear decaying
singular values (indicating correlation between the columns) but is not constrained to any specific
structure. Many dictionaries in real applications have such a property. As a basic experiment, we
generate N = 700000 ground truth samples x? ? Rm by randomly selecting d nonzero entries, with
nonzero amplitudes drawn iid from the uniform distribution U[?0.5, 0.5], excluding the interval
[?0.1, 0.1] to avoid small, relatively inconsequential contributions to the support pattern. We then
create y ? Rn via y = ?x? . As d increases, the estimation problem becomes more difficult. In
fact, to guarantee
 success with such correlated data (and high RIP constant) requires evaluating on
the order of m
n linear systems of size n ? n, which is infeasible even for small values, indicative of
how challenging it can be to solve sparse inverse problems of any size. We set n=20 and m=100.
6

0.5

0

2

4

6

8

1

1
Res
NoRes
HardAct
LSLoss

acc

?1
IHT
ISTA-Net
IHT-Net
Ours-Res
Ours-LSTM

acc

acc

1

0.5

0

2

4

d

6
d

8

0.5

0

U2U-test
U2U-train
U2N-test
U2N-train
N2U-test
N2U-train
N2N-test
N2N-train

2

4

6

8

d

Figure 1: Average support recovery accuracy. Left: Uniformly distributed nonzero elements. Mid:
Different network variants. Right: Different training and testing distr. (LSTM-Net results).
We used N1 = 600000 samples for training and the remaining N2 = 100000 for testing. Echoing
our arguments in Section 4, we explored both a feedforward network with residual connections
[10] and a recurrent network with vanilla LSTM cells [12]. To evaluate the performance, we check
whether the d ground truth nonzeros are aligned with the predicted top-d values produced by our
network, a common all-or-nothing metric in the compressive sensing literature. Detailed network
design, optimization setup, and alternative metrics can be found in [26].
Figure 1(left) shows comparisons against a battery of existing algorithms, both learning- and
optimization-based. These include standard `1 minimization via ISTA iterations [2], IHT [3] (supplied with the ground truth number of nonzeros), an ISTA-based network [9], and an IHT-inspired
network [23]. For both the ISTA- and IHT-based networks, we used the exact same training data
described above. Note that given the correlated ? matrix, the recovery performance of IHT, and to
a lesser degree `l minimization using ISTA, is rather modest as expected given that the associated
RIP constant will be quite large by construction. In contrast our two methods achieve uniformly
higher accuracy, including over other learning-based methods trained with the same data. This improvement is likely the result of three significant factors: (i) Existing learning methods initialize
using weights derived from the original sparse estimation algorithms, but such an initialization will
be associated with locally optimal solutions in most cases with correlated dictionaries. (ii) As described in Section 3, constant weights across layers have limited capacity to unravel multi-resolution
dictionary structure, especially one that is not confined to only possess some low rank correlating
component. (iii) The quadratic loss function used by existing methods does not adequately focus
resources on the crux of the problem, which is accurate support recovery. In contrast we adopt
an initialization motivated by DNN-based training considerations, unique layer weights to handle a
multi-resolution dictionary, and a multi-label classification output layer to focus on support recovery.
To further isolate essential factors affecting performance, we next consider the following changes:
(1) We remove the residual connections from Res-Net. (2) We replace ReLU with hard-threshold
activations. In particular, we utilize the so-called HELU? function introduced in [23], which is a
continuous and piecewise linear approximation of the scalar hard-threshold operator. (3) We use
a quadratic penalty layer instead of a multi-label classification loss layer, i.e., the loss function is
PN1
changed to i=1
ka(i) ? y (i) k22 (where a is the output of the last fully-connected layer) during
training. Figure 1(middle) displays the associated recovery percentages, where we observe that
in each case performance degrades. Without the residual design, and also with the inclusion of a
rigid, non-convex hard-threshold operator, local minima during training appear to be a likely culprit,
consistent with observations from [10]. Likewise, use of a least-squares loss function is likely to
over-emphasize the estimation of coefficient amplitudes rather than focusing on support recovery.
Finally, from a practical standpoint we may expect that the true amplitude distribution may deviate
at times from the original training set. To explore robustness to such mismatch, as well as different
amplitude distributions, we consider two sets of candidate data: the original data, and similarlygenerated data but with the uniform distribution of nonzero elements replaced with the Gaussians
N (?0.3, 0.1), where the mean is selected with equal probability as either ?0.3 or 0.3, thus avoiding
tiny magnitudes with high probability. Figure 1(right) reports accuracies under different distributions for both training and testing, including mismatched cases. (The results are obtained using
LSTM-Net, but the Res-net showed similar pattern.) The label ?U2U? refers to training and testing
with the uniformly distributed amplitudes, while ?U2N? uses uniform training set and a Gaussian test
set. Analogous definitions apply for ?N2N? and ?N2U?. In all cases we note that the performance is
7

(a) GT

1

1

1

0.9

0.9

0.9

0.8

0.8

0.8

0.7

0.7

0.7

0.6

0.6

0.6

0.5

0.5

0.5

0.4

0.4

0.4

0.3

0.3

0.3

0.2

0.2

0.2

0.1

0.1

0.1

0

0

0

(b) LS (E=12.1 T=4.1)

(c) `1 (E=7.1 T=33.7)

(d) Ours (E=1.5 T=1.2)

Figure 2: Reconstruction error maps. Angular error in degrees (E) and runtime in sec. (T) are provided.
quite stable across training and testing conditions. We would argue that our recasting of the problem
as multi-label classification contributes, at least in part, to this robustness. The application example
described next demonstrates further tolerance of training-testing set mismatches.
Practical Application - Photometric Stereo: Suppose we have q observations of a given surface
point from a Lambertian scene under different lighting directions. Then the resulting measurements
from a standard calibrated photometric stereo design (linear camera response function, an orthographic camera projection, and known directional light sources), denoted o ? Rq , can be expressed
as o = ?Ln, where n ? R3 denotes the true 3D surface normal, each row of L ? Rq?3 defines
a lighting direction, and ? is the diffuse albedo, acting here as a scalar multiplier [24]. If specular
highlights, shadows, or other gross outliers are present, then the observations are more realistically
modeled as o = ?Ln + e, where e is an an unknown sparse vector [13, 25]. It is apparent that,
since n is unconstrained, e need not compensate for any component of o in the range of L. Given
that null[L> ] is the orthogonal complement to range[L], we may consider the following problem
min kek0 s.t. Projnull[L> ] (o) = Projnull[L> ] (e)
(11)
e
which ultimately collapses to our canonical sparse estimation problem from (1), where lightinghardware-dependent correlations may be unavoidable in the implicit dictionary.
Following [13], we use 32-bit HDR gray-scale images of the object Bunny (256?256) with foreground masks under different lighting conditions whose directions, or rows of L, are randomly
selected from a hemisphere with the object placed at the center. To apply our method, we first compute ? using the appropriate projection operator derived from the lighting matrix L. As real-world
training data is expensive to acquire, we instead use weak supervision by synthetically generating a
training set as follows. First, we draw a support pattern for e randomly with cardinality d sampled
uniformly from the range [d1 , d2 ]. The values of d1 and d2 can be tuned in practice. Nonzero values
of e are assigned iid random values from a Gaussian distribution whose mean and variance are also
tunable. Beyond this, no attempt was made to match the true outlier distributions encountered in
applications of photometric stereo. Finally, for each e we can naturally compute observations via
the linear constraint in (11), which serve as candidate network inputs.
Given synthetic training data acquired in this way, we learn a network with the exact same structure
and optimization parameters as in Section 5; no application-specific tuning was introduced. We then
deploy the resulting network on the gray-scale Bunny images. For each surface point, we use our
DNN model to approximately solve (11). Since the network output will be a probability map for the
outlier support set instead of the actual values of e, we choose the 4 indices with the least probability
as inliers and use them to compute n via least squares.
We compare our method against the baseline least squares estimate from [24] and `1 norm minimization. We defer more quantitative comparisons to [26]. In Figure 2, we illustrate the recovered
surface normal error maps of the hardest case (fewest lighting directions). Here we observe that our
DNN estimates lead to far fewer regions of significant error and the runtime is orders of magnitude
faster. Overall though, this application example illustrates that weak supervision with mismatched
synthetic training data can, at least for some problem domains, be sufficient to learn a quite useful
sparse estimation DNN; here one that facilitates real-time 3D modeling in mobile environments.
Discussion: In this paper we have shown that deep networks with hand-crafted, multi-resolution
structure can provably solve certain specific classes of sparse recovery problems where existing
algorithms fail. However, much like CNN-based features can often outperform SIFT on many computer vision tasks, we argue that a discriminative approach can outperform manual structuring of
layers/iterations and compensate for dictionary coherence under more general conditions.
8

Acknowledgements: This work was done while the first author was an intern at Microsoft Research, Beijing. It is also funded by 973-2015CB351800, NSFC-61231010, NSFC-61527804,
NSFC-61421062, NSFC-61210005 and MOEMicrosoft Key Laboratory, Peking University.

References
[1] S. Baillet, J.C. Mosher, and R.M. Leahy. Electromagnetic brain mapping. IEEE Signal Processing
Magazine, pages 14?30, Nov. 2001.
[2] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.
SIAM J. Imaging Sciences, 2(1), 2009.
[3] T. Blumensath and M.E. Davies. Iterative hard thresholding for compressed sensing. Applied and Computational Harmonic Analysis, 27(3), 2009.
[4] T. Blumensath and M.E. Davies. Normalized iterative hard thresholding: Guaranteed stability and performance. IEEE J. Selected Topics Signal Processing, 4(2), 2010.
[5] E. Cand`es, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction from
highly incomplete frequency information. IEEE Trans. Information Theory, 52(2):489?509, 2006.
[6] E. Cand`es and T. Tao. Decoding by linear programming. IEEE Trans. Information Theory, 51(12), 2005.
[7] S.F. Cotter and B.D. Rao. Sparse channel estimation via matching pursuit with application to equalization.
IEEE Trans. on Communications, 50(3), 2002.
[8] M.A.T. Figueiredo. Adaptive sparseness using Jeffreys prior. NIPS, 2002.
[9] K. Gregor and Y. LeCun. Learning fast approximations of sparse coding. In ICML, 2010.
[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. CVPR, 2016.
[11] J.R. Hershey, J. Le Roux, and F. Weninger. Deep unfolding: Model-based inspiration of novel deep
architectures. arXiv preprint arXiv:1409.2574v4, 2014.
[12] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8), 1997.
[13] S. Ikehata, D.P. Wipf, Y. Matsushita, and K. Aizawa. Robust photometric stereo using sparse regression.
In CVPR, 2012.
[14] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal
covariate shift. arXiv preprint arXiv:1502.03167, 2015.
[15] U. Kamilov and H. Mansour. Learning optimal nonlinearities for iterative thresholding algorithms. arXiv
preprint arXiv:1512.04754, 2015.
[16] D.M. Malioutov, M. C
? etin, and A.S. Willsky. Sparse signal reconstruction perspective for source localization with sensor arrays. IEEE Trans. Signal Processing, 53(8), 2005.
[17] V. Nair and G. Hinton. Rectified linear units improve restricted boltzmann machines. ICML, 2010.
[18] Y.C. Pati, R. Rezaiifar, and P.S. Krishnaprasad. Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition. In 27th Asilomar Conference on Signals, Systems
and Computers, 1993.
[19] P. Sprechmann, A.M. Bronstein, and G. Sapiro. Learning efficient sparse and low rank models. IEEE
Trans. Pattern Analysis and Machine Intelligence, 37(9), 2015.
[20] R.K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks. NIPS, 2015.
[21] R. Tibshirani. Regression shrinkage and selection via the lasso. J. of the Royal Statistical Society, 1996.
[22] J.A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on Information Theory, 50(10):2231?2242, October 2004.
[23] Z. Wang, Q. Ling, and T. Huang. Learning deep `0 encoders. arXiv preprint arXiv:1509.00153v2, 2015.
[24] R.J. Woodham. Photometric method for determining surface orientation from multiple images. Optical
Engineering, 19(1), 1980.
[25] L. Wu, A. Ganesh, B. Shi, Y. Matsushita, Y. Wang, and Y. Ma. Robust photometric stereo via low-rank
matrix completion and recovery. Asian Conference on Computer Vision, 2010.
[26] Bo Xin, Yizhou Wang, Wen Gao, and David Wipf. Maximal sparsity with deep networks? arXiv preprint
arXiv:1605.01636, 2016.

9

"
2016,Edge-exchangeable graphs and sparsity,Poster,6586-edge-exchangeable-graphs-and-sparsity.pdf,"Many popular network models rely on the assumption of (vertex) exchangeability, in which the distribution of the graph is invariant to relabelings of the vertices. However, the Aldous-Hoover theorem guarantees that these graphs are dense or empty with probability one, whereas many real-world graphs are sparse. We present an alternative notion of exchangeability for random graphs, which we call edge exchangeability, in which the distribution of a graph sequence is invariant to the order of the edges. We demonstrate that edge-exchangeable models, unlike models that are traditionally vertex exchangeable, can exhibit sparsity. To do so, we outline a general framework for graph generative models; by contrast to the pioneering work of Caron and Fox (2015), models within our framework are stationary across steps of the graph sequence. In particular, our model grows the graph by instantiating more latent atoms of a single random measure as the dataset size increases, rather than adding new atoms to the measure.","Edge-exchangeable graphs and sparsity

Diana Cai
Dept. of Statistics, U. Chicago
Chicago, IL 60637
dcai@uchicago.edu

Trevor Campbell
CSAIL, MIT
Cambridge, MA 02139
tdjc@mit.edu

Tamara Broderick
CSAIL, MIT
Cambridge, MA 02139
tbroderick@csail.mit.edu

Abstract
Many popular network models rely on the assumption of (vertex) exchangeability,
in which the distribution of the graph is invariant to relabelings of the vertices.
However, the Aldous-Hoover theorem guarantees that these graphs are dense or
empty with probability one, whereas many real-world graphs are sparse. We
present an alternative notion of exchangeability for random graphs, which we call
edge exchangeability, in which the distribution of a graph sequence is invariant
to the order of the edges. We demonstrate that edge-exchangeable models, unlike
models that are traditionally vertex exchangeable, can exhibit sparsity. To do
so, we outline a general framework for graph generative models; by contrast to
the pioneering work of Caron and Fox [12], models within our framework are
stationary across steps of the graph sequence. In particular, our model grows the
graph by instantiating more latent atoms of a single random measure as the dataset
size increases, rather than adding new atoms to the measure.

1

Introduction

In recent years, network data have appeared in a growing number of applications, such as online
social networks, biological networks, and networks representing communication patterns. As a result,
there is growing interest in developing models for such data and studying their properties. Crucially,
individual network data sets also continue to increase in size; we typically assume that the number of
vertices is unbounded as time progresses. We say a graph sequence is dense if the number of edges
grows quadratically in the number of vertices, and a graph sequence is sparse if the number of edges
grows sub-quadratically as a function of the number of vertices. Sparse graph sequences are more
representative of real-world graph behavior. However, many popular network models (see, e.g., Lloyd
et al. [19] for an extensive list) share the undesirable scaling property that they yield dense sequences
of graphs with probability one. The poor scaling properties of these models can be traced back to a
seemingly innocent assumption: that the vertices in the model are exchangeable, that is, any finite
permutation of the rows and columns of the graph adjacency matrix does not change the distribution
of the graph. Under this assumption, the Aldous-Hoover theorem [1, 16] implies that such models
generate dense or empty graphs with probability one [20].
This fundamental model misspecification motivates the development of new models that can achieve
sparsity. One recent focus has been on models in which an additional parameter is employed to
uniformly decrease the probabilities of edges as the network grows (e.g., Bollob?s et al. [3], Borgs
et al. [4, 5], Wolfe and Olhede [24]). While these models allow sparse graph sequences, the sequences
are no longer projective. In projective sequences, vertices and edges are added to a graph as a
graph sequence progresses?whereas in the models above, there is not generally any strict subgraph
relationship between earlier graphs and later graphs in the sequence. Projectivity is natural in
streaming modeling. For instance, we may wish to capture new users joining a social network and
new connections being made among existing users?or new employees joining a company and new
communications between existing employees.
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

Caron and Fox [12] have pioneered initial work on sparse, projective graph sequences. Instead of
the vertex exchangeability that yields the Aldous-Hoover theorem, they consider a notion of graph
exchangeability based on the idea of independent increments of subordinators [18], explored in depth
by Veitch and Roy [22]. However, since this Kallenberg-style exchangeability introduces a new
countable infinity of latent vertices at every step in the graph sequence, its generative mechanism
seems particularly suited to the non-stationary domain. By contrast, we are here interested in exploring
stationary models that grow in complexity with the size of the data set. Consider classic Bayesian
nonparametric models as the Chinese restaurant process (CRP) and Indian buffet process (IBP); these
engender growth by using a single infinite latent collection of parameters to generate a finite but
growing set of instantiated parameters. Similarly, we propose a framework that uses a single infinite
latent collection of vertices to generate a finite but growing set of vertices that participate in edges
and thereby in the network. We believe our framework will be a useful component in more complex,
non-stationary graphical models?just as the CRP and IBP are often combined with hidden Markov
models or other explicit non-stationary mechanisms. Additionally, Kallenberg exchangeability is
intimately tied to continuous-valued labels of the vertices, and here we are interested in providing a
characterization of the graph sequence based solely on its topology.
In this work, we introduce a new form of exchangeability, distinct from both vertex exchangeability
and Kallenberg exchangeability. In particular, we say that a graph sequence is edge exchangeable if
the distribution of any graph in the sequence is invariant to the order in which edges arrive?rather
than the order of the vertices. We will demonstrate that edge exchangeability admits a large family of
sparse, projective graph sequences.
In the remainder of the paper, we start by defining dense and sparse graph sequences rigorously.
We review vertex exchangeability before introducing our new notion of edge exchangeability in
Section 2, which we also contrast with Kallenberg exchangeability in more detail in Section 4. We
define a family of models, which we call graph frequency models, based on random measures in
Section 3. We use these models to show that edge-exchangeable models can yield sparse, projective
graph sequences via theoretical analysis in Section 5 and via simulations in Section 6. Along the way,
we highlight other benefits of the edge exchangeability and graph frequency model frameworks.

2

Exchangeability in graphs: old and new

Let (Gn )n := G1 , G2 , . . . be a sequence of graphs, where each graph Gn = (Vn , En ) consists of a
(finite) set of vertices Vn and a (finite) multiset of edges En . Each edge e ? En is a set of two vertices
in Vn . We assume the sequence is projective?or growing?so that Vn ? Vn+1 and En ? En+1 .
Consider, e.g., a social network with more users joining the network and making new connections
with existing users. We say that a graph sequence is dense if |En | = ?(|Vn |2 ), i.e., the number of
edges is asymptotically lower bounded by c ? |Vn |2 for some constant c. Conversely, a sequence is
sparse if |En | = o(|Vn |2 ), i.e., the number of edges is asymptotically upper bounded by c ? |Vn |2
for all constants c. In what follows, we consider random graph sequences, and we focus on the case
where |Vn | ? ? almost surely.
2.1

Vertex-exchangeable graph sequences

If the number of vertices in the graph sequence grows to infinity, the graphs in the sequence can
be thought of as subgraphs of an ?infinite? graph with infinitely many vertices and a correspondingly infinite adjacency matrix. Traditionally, exchangeability in random graphs is defined as the
invariance of the distribution of any finite submatrix of this adjacency matrix?corresponding to any
finite collection of vertices?under finite permutation. Equivalently, we can express this form of
exchangeability, which we henceforth call vertex exchangeability, by considering a random sequence
of graphs (Gn )n with Vn = [n], where [n] := {1, . . . , n}. In this case, only the edge sequence is
random. Let ? be any permutation of the integers [n]. If e = {v, w}, let ?(e) := {?(v), ?(w)}. If
En = {e1 , . . . , em }, let ?(En ) := {?(e1 ), . . . , ?(em )}.
Definition 2.1. Consider the random graph sequence (Gn )n , where Gn has vertices Vn = [n] and
edges En . (Gn )n is (infinitely) vertex exchangeable if for every n ? N and for every permutation ?
d ?
?
of the vertices [n], Gn = G
n , where Gn has vertices [n] and edges ?(En ).
2

2
2
1
5
1

2
1

5

1

1

4
1

5

1

3

2

2

4

5

1

1

3

2
2

1
2

3

2

1

4

3

1

5

2

4

4

6

4
2

3

4

3

1

2

3

4

4
1
1

2
2

1
6

Figure 1: Upper, left four: Step-augmented graph sequence from Ex. 2.2. At each step n, the step
value is always at least the maximum vertex index. Upper, right two: Two graphs with the same
probability under vertex exchangeability. Lower, left four: Step-augmented graph sequence from
Ex. 2.3. Lower, right two: Two graphs with the same probability under edge exchangeability.
A great many popular models for graphs are vertex exchangeable; see Appendix B and Lloyd
et al. [19] for a list. However, it follows from the Aldous-Hoover theorem [1, 16] that any vertexexchangeable graph is a mixture of sampling procedures from graphons. Further, any graph sampled
from a graphon is almost surely dense or empty [20]. Thus, vertex-exchangeable random graph
models are misspecified models for sparse network datasets, as they generate dense graphs.
2.2

Edge-exchangeable graph sequences

Vertex-exchangeable sequences have distributions invariant to the order of vertex arrival. We introduce
edge-exchangeable graph sequences, which will instead be invariant to the order of edge arrival.
As before, we let Gn = (Vn , En ) be the nth graph in the sequence. Here, though, we consider
only active vertices?that is, vertices that are connected via some edge. That lets us define Vn as a
function of En ; namely, Vn is the union of the vertices in En . Note that a graph that has sub-quadratic
growth in the number of edges as a function of the number of active vertices will necessarily have
sub-quadratic growth in the number of edges as a function of the number of all vertices, so we obtain
strictly stronger results by considering active vertices. In this case, the graph Gn is completely
defined by its edge set En .
As above, we suppose that En ? En+1 . We can emphasize this projectivity property by augmenting
each edge with the step on which it is added to the sequence. Let En0 be a collection of tuples, in
which the first element is the edge and the second element is the step (i.e., index) on which the edge
is added: En0 = {(e1 , s1 ), . . . , (em , sm )}. We can then define a step-augmented graph sequence
(En0 )n = (E10 , E20 , . . .) as a sequence of step-augmented edge sets. Note that there is a bijection
between the step-augmented graph sequence and the original graph sequence.
Example 2.2. In the setup for vertex exchangeability, we assumed Vn = [n] and every edge is
introduced as soon as both of its vertices are introduced. In this case, the step of any edge in the
step-augmented graph is the maximum vertex value. For example, in Figure 1, we have
E10 = ?, E20 = E30 = {({1, 2}, 2)}, E40 = {({1, 2}, 2), ({1, 4}, 4), ({2, 4}, 4), ({3, 4}, 4)}.
In general step-augmented graphs, though, the step need not equal the max vertex, as we see next. 
Example 2.3. Suppose we have a graph given by the edge sequence (see Figure 1):
E1 = E2 = {{2, 5}, {5, 5}}, E3 = E2 ? {{2, 5}}, E4 = E3 ? {{1, 6}}.
The step-augmented graph E40 is {({2, 5}, 1), ({5, 5}, 1), ({2, 5}, 3), ({1, 6}, 4)}.



Roughly, a random graph sequence is edge exchangeable if its distribution is invariant to finite
permutations of the steps. Let ? be a permutation of the integers [n]. For a step-augmented edge set
En0 = {(e1 , s1 ), . . . , (em , sm )}, let ?(En0 ) = {(e1 , ?(s1 )), . . . , (em , ?(sm ))}.
Definition 2.4. Consider the random graph sequence (Gn )n , where Gn has step-augmented edges
En0 and Vn are the active vertices of En . (Gn )n is (infinitely) edge exchangeable if for every n ? N
3

d

? n , where G
? n has step-augmented edges ?(En0 )
and for every permutation ? of the steps [n], Gn = G
and associated active vertices.
See Figure 1 for visualizations of both vertex exchangeability and edge exchangeability. It remains
to show that there are non-trivial models that are edge exchangeable (Section 3) and that edgeexchangeable models admit sparse graphs (Section 5).

3

Graph frequency models

We next demonstrate that a wide class of models, which we call graph frequency models, exhibit edge
exchangeability. Consider a latent infinity of vertices indexed by the positive integers N = {1, 2, . . .},
along with an infinity of edge labels (?{i,j} ), each in a set ?, and positive edge rates (or frequencies)
(w{i,j} ) in R+ . We allow both the (?{i,j} ) and (w{i,j} ) to be random, though this is not mandatory.
For instance, we might choose ?{i,j} = (i, j) for i ? j, and ? = R2 . Alternatively, the ?{i,j}
could be drawn iid from a continuous distribution such as Unif[0, 1]. For any choice of (?{i,j} ) and
(w{i,j} ),
X
W :=
w{i,j} ??{i,j}
(1)
{i,j}:i,j?N

is a measure on ?. Moreover, it is a discrete measure since it is always atomic. If either (?{i,j} ) or
(w{i,j} ) (or both) are random, W is a discrete random measure on ? since it is a random, discretemeasure-valued element. Given the edge rates (or frequencies) (w{i,j} ) in W , we next show some
natural ways to construct edge-exchangeable graphs.
P
Single edge per step. If the rates (w{i,j} ) are normalized such that {i,j}:i,j?N w{i,j} = 1, then
(w{i,j} ) is a distribution over all possible vertex pairs. In other words, W is a probability measure. We
can form an edge-exchangeable graph sequence by first drawing values for (w{i,j} ) and (?{i,j} )?and
setting E0 = ?. We recursively set En+1 = En ? {e}, where e is an edge {i, j} chosen from the
distribution (w{i,j} ). This construction introduces a single edge in the graph each step, although it
may be a duplicate of an edge that already exists. Therefore, this technique generates multigraphs
one edge at a time. Since the edge every step is drawn conditionally iid given W , we have an
edge-exchangeable graph.
Multiple edges per step. Alternatively, the rates (w{i,j} ) may not be normalized. Then W may
not be a probability measure. Let f (m|w) be a distribution over non-negative integers m given some
rate w ? R+ . We again initialize our sequence by drawing (w{i,j} ) and (?{i,j} ) and setting E0 = ?.
In this case, recursively, on the nth step, start by setting F = ?. For every possible edge e = {i, j},
ind
we draw the multiplicity of the edge e in this step as me ? f (?|we ) and add me copies of edge e to
F . Finally, En+1 = En ? F . This technique potentially introduces multiple edges in each step, in
which edges themselves may have multiplicity greater than one and may be duplicates of edges that
already exist in the graph. Therefore, this technique generates multigraphs, multiple edges at a time.
If we restrict f and W such that finitely many edges are added on every step almost surely, we have
an edge-exchangeable graph, as the edges in each step are drawn conditionally iid given W .
Given a sequence of edge sets E0 , E1 , . . . constructed via either of the above methods, we can
?0 , E
?1 , . . . by setting E
?i to have the same edges as Ei except with
form a binary graph sequence E
multiplicity 1. Although this binary graph is not itself edge exchangeable, it inherits many of the
properties (such as sparsity, as shown in Section 5) of the underlying edge-exchangeable multigraph.
The choice of the distribution on the measure W has a strong influence on the properties of the
resulting edge-exchangeable graph sampled via one of the above methods. For example, one choice is
to set w{i,j} = wi wj , where the (wi )i are a countable infinity of random values generated according
to a Poisson point process (PPP). We say that (wi )i is distributed according to a Poisson point process
parameterized by rate measure ?, (wi )i ? PPP(?), if (a) #{i : wi ? A} ? Poisson(?(A)) for any
set A with finite measure ?(A) and (b) #{i : wi ? Aj } are independent random variables across any
finite collection of disjoint sets (Aj )Jj=1 . In Section 5 we examine a particular example of this graph
frequency model, and demonstrate that sparsity is possible in edge-exchangeable graphs.
4

(a) Graph frequency model (fixed y, n steps)

(b) Caron?Fox, PPP on [0, y] ? [0, y] (1 step, y grows)

Figure 2: A comparison of a graph frequency model (Section 3 and Equation (2)) and the generative
model of Caron and Fox [12]. Any interval [0, y] contains a countably infinite number of atoms with
a nonzero weight in the random measure; a draw from the random measure is plotted at the top (and
repeated on the right side). Each atom corresponds to a latent vertex. Each point (?i , ?j ) corresponds
to a latent edge. Darker point colors on the left occur for greater edge multiplicities. On the left, more
latent edges are instantiated as more steps n are taken. On the right, the edges within [0, y]2 are fixed,
but more edges are instantiated as y grows.

4

Related work and connection to nonparametric Bayes

Given a unique label ?i for each vertex i ? N, and denoting gij = gji to be the number of undirected
edges P
between vertices i and j, the graph itself can be represented as the discrete random measure
2
G =
i,j gij ?(?i ,?j ) on R+ . A different notion of exchangeability for graphs than the ones in
Section 2 can be phrased for such atomic random measures: a point process G on R2+ is (jointly)
exchangeable if, for all finite permutations ? of N and all h > 0,
d

G(Ai ? Aj ) = G(A?(i) ? A?(j) ), for (i, j) ? N2 ,

where Ai := [h ? (i ? 1), h ? i].

This form of exchangeability, which we refer to as Kallenberg exchangeability, can intuitively be
viewed as invariance of the graph distribution to relabeling of the vertices, which are now embedded in
R2+ . As such it is analogous to vertex exchangeability, but for discrete random measures [12, Sec. 4.1].
Exchangeability for random measures was introduced by Aldous [2], and a representation theorem
was given by Kallenberg [17, 18, Ch. 9]. The use of Kallenberg exchangeability for modeling graphs
was first proposed by Caron and Fox [12], and then characterized in greater generality by Veitch and
Roy [22] and Borgs et al. [6]. Edge exchangeability is distinct from Kallenberg exchangeability, as
shown by the following example.
Example 4.1 (Edge exchangeable but not Kallenberg exchangeable). Consider the graph frequency
model developed in Section 3, with w{i,j} = (ij)?2 and ?{i,j} = {i, j}. Since the edges at each
step are drawn iid given
the corresponding
PW , the graph sequence is edge exchangeable. However,
?2
graph measure G =
)) is not Kallenberg
i,j nij ?(i,j) (where nij = nji ? Binom(N, (ij)
exchangeable, since the probability of generating edge {i, j} is directly related to the positions (i, j)
and (j, i) in R2+ of the corresponding atoms in G (in particular, the probability is decreasing in ij). 
Our graph frequency model is reminiscent of the Caron and Fox [12] generative model, but has a
number
of key differences. At a high level, this earlier model generates a weight measure W =
P
w
?(?i ,?j ) (Caron and Fox [12] used, in particular, the outer product of a completely random
ij
i,j
measure), and the graph measure G is constructed by sampling gij once given wij for each pair
i, j. To create a finite graph, the graph measure G is restricted to the subset [0, y] ? [0, y] ? R2+ for
0 < y < ?; to create a projective growing graph sequence, the value of y is increased. By contrast,
in the analogous graph frequency model of the present work, y is fixed, and we grow the network
5

by repeatedly sampling the number of edges gij between vertices i and j and summing the result.
Thus, in the Caron and Fox [12] model, a latent infinity of vertices (only finitely many of which
are active) are added to the network each time y increases. In our graph frequency model, there is
a single collection of latent vertices, which are all gradually activated by increasing the number of
samples that generate edges between the vertices. See Figure 2 for an illustration.
Increasing n in the graph frequency model has the interpretation of both (a) time passing and (b) new
individuals joining a network because they have formed a connection that was not previously there. In
particular, only latent individuals that will eventually join the network are considered. This behavior
is analogous to the well-known behavior of other nonparametric Bayesian models such as, e.g., a
Chinese restaurant process (CRP). In this analogy, the Dirichlet process (DP) corresponds to our
graph frequency model, and the clusters instantiated by the CRP correspond to the vertices that are
active after n steps. In the DP, only latent clusters that will eventually appear in the data are modeled.
Since the graph frequency setting is stationary like the DP/CRP, it may be more straightforward to
develop approximate Bayesian inference algorithms, e.g., via truncation [11].
Edge exchangeability first appeared in work by Crane and Dempsey [13, 14], Williamson [23], and
Broderick and Cai [7, 8], Cai and Broderick [10]. Broderick and Cai [7, 8] established the notion of
edge exchangeability used here and provided characterizations via exchangeable partitions and feature
allocations, as in Appendix C. Broderick and Cai [7], Cai and Broderick [10] developed a frequency
model based on weights (wi )i generated from a Poisson process and studied several types of power
laws in the model. Crane and Dempsey [13] established a similar notion of edge exchangeability
in the context of a larger statistical modeling framework. Crane and Dempsey [13, 14] provided
sparsity and power law results for the case where the weights (wi )i are generated from a Pitman-Yor
process and power law degree distribution simulations. Williamson [23] described a similar notion
of edge exchangeability and developed an edge-exchangeable model where the weights (wi )i are
generated from a Dirichlet process, a mixture model extension, and an efficient Bayesian inference
procedure. In work concurrent to the present paper, Crane and Dempsey [15] re-examined edge
exchangeability, provided a representation theorem, and studied sparsity and power laws for the same
model based on Pitman-Yor weights. By contrast, we here obtain sparsity results across all Poisson
point process-based graph frequency models of the form in Equation (2) below, and use a specific
three-parameter beta process rate measure only for simulations in Section 6.

5

Sparsity in Poisson process graph frequency models

We now demonstrate that, unlike vertex exchangeability, edge exchangeability allows for sparsity in
random graph sequences. We develop a class of sparse, edge-exchangeable multigraph sequences via
the Poisson point process construction introduced in Section 3, along with their binary restrictions.
Model. Let W be a Poisson process on [0, 1] with a nonatomic, ?-finite rate measure ? satisfying
R1
?([0, 1]) = ? and 0 w?(dw) < ?. These
P two conditions on ? guarantee that W is a countably
infinite collection of rates in [0, 1] and that w?W w < ? almost surely. We can use W to construct
the set of rates: w{i,j} = wi wj if i 6= j, and w{i,i} = 0. The edge labels ?{i,j} are unimportant in
characterizing sparsity, and so can be ignored.
To use the multiple-edges-per-step graph frequency model from Section 3, we let f (?|w) be Bernoulli
with probability w. Since edge {i, j} is added in each step with probability wi wj , its multiplicity
M{i,j} after n steps has a binomial distribution with parameters n, wi wj . Note that self-loops are
avoided by setting w{i,i} = 0. Therefore, the graph after n steps is described by:
ind

(2)
W ? PPP(?)
M{i,j} ? Binom(n, wi wj ) for i < j ? N.
As mentioned earlier, this generative model yields an edge-exchangeable graph, with
P edge multiset
En containing {i, j} with multiplicity M{i,j} , and active vertices Vn = {i :
j M{i,j} > 0}.
?n ) by
Although this model generates multigraphs, it can be modified to sample a binary graph (V?n , E
?n to the set of edges {i, j} such that {i, j} has multiplicity ? 1 in En . We
setting V?n = Vn and E
can express the number of vertices and edges, in the multi- and binary graphs respectively, as
?
?
X X
X

1X
?n | = 1
|V?n | = |Vn | =
1? M{i,j} > 0? , |En | =
M{i,j} , |E
1 M{i,j} > 0 .
2
2
i
j6=i

i6=j

6

i6=j

Moments. Recall that a sequence of graphs is considered sparse if |En | = o(|Vn |2 ). Thus, sparsity
in the present setting is an asymptotic property of a random graph sequence. Rather than consider the
asymptotics of the (dependent) random sequences |En | and |Vn | in concert, Lemma 5.1 allows us to
consider the asymptotics of their first moments, which are deterministic sequences and can be analyzed
separately. We use ? to denote asymptotic equivalence, i.e., an ? bn ?? limn?? abnn = 1. For
details on our asymptotic notation and proofs for this section, see Appendix D.
Lemma 5.1. The number of vertices and edges for both the multi- and binary graphs satisfy

a.s.
a.s.
?n | a.s.
?n | ,
|V?n | = |Vn | ? E (|Vn |) ,
|En | ? E (|En |) ,
|E
? E |E
n ? ?.
Thus, we can examine the asymptotic behavior of the random numbers of edges and vertices by
examining the asymptotic behavior of their expectations, which are provided by Lemma 5.2.
Lemma 5.2. The expected numbers of vertices and edges for the multi- and binary graphs are
 Z

Z 

E |V?n | = E (|Vn |) =
1 ? exp ? (1 ? (1 ? wv)n )?(dv) ?(dw),
ZZ
ZZ

n
?n | = 1
E (|En |) =
wv ?(dw)?(dv),
E |E
(1 ? (1 ? wv)n ) ?(dw)?(dv).
2
2
Sparsity. We are now equipped to characterize the sparsity of this random graph sequence:
Theorem 5.3. Suppose ? has a regularly varying tail, i.e., there exist ? ? (0, 1) and ` : R+ ? R+
s.t.
Z 1
`(cx)
= 1.
?(dw) ? x?? `(x?1 ), x ? 0
and
?c > 0, lim
x?? `(x)
x
Then as n ? ?,
a.s.

|Vn | = ?(n? `(n)),


 1+?

3?
?n | a.s.
|E
= O `(n1/2 ) min n 2 , `(n)n 2
.

a.s.

|En | = ?(n),

Theorem 5.3 implies that the multigraph is sparse when ? ? (1/2, 1), and that the restriction to the
binary graph is sparse for any ? ? (0, 1). See Remark D.7 for a discussion. Thus, edge-exchangeable
random graph sequences allow for a wide range of sparse and dense behavior.

6

Simulations

In this section, we explore the behavior of graphs generated by the model from Section 5 via
simulation, with the primary goal of empirically demonstrating that the model produces sparse graphs.
We consider the case when the Poisson process generating the weights in Equation (2) has the rate
measure of a three-parameter beta process (3-BP) on (0, 1) [9, 21]:
?(dw) = ?

?(1 + ?)
w?1?? (1 ? w)?+??1 dw,
?(1 ? ?)?(? + ?)

(3)

with mass ? > P
0, concentration ? > 0, and discount ? ? (0, 1). In order for the 3-BP to have
finite total mass j wj < ?, we require that ? > ??. We draw realizations of the weights from a
3-BP(?, ?, ?) according to the stick-breaking representation given by Broderick, Jordan, and Pitman
[9]. That is, the wi are the atom weights of the measure W for
W =

Ci
? X
X
i=1 j=1

(`) ind
Vi,j ?

(i)

Vi,j

i?1
Y

(`)

(1 ? Vi,j )??i,j ,

iid

Ci ? Pois(?),

l=1
iid

Beta(1 ? ?, ? + `?),

?i,j ? B0

and any continuous (i.e., non-atomic) choice of distribution B0 .
Since simulating an infinite number of atoms is not possible, we truncate the outer summation in i to
P2000
2000 rounds, resulting in i=1 Ci weights. The parameters of the beta process were fixed to ? = 3
and ? = 1, as they do not influence the sparsity of the resulting graph frequency model, and we varied
7

(a) Multigraph edges vs. active vertices

(b) Binary graph edges vs. active vertices

Figure 3: Data simulated from a graph frequency model with weights generated according to a 3-BP.
Colors represent different random draws. The dashed line has a slope of 2.
the discount parameter ?. Given a single draw W (at some specific discount ?), we then simulated
the edges of the graph, where the number of Bernoulli draws N varied between 50 and 2000.
Figure 3a shows how the number of edges varies versus the total number of active vertices for
the multigraph, with different colors representing different random seeds. To check whether the
generated graph was sparse, we determined the exponent by examining the slope of the data points
(on a log-scale). In all plots, the black dashed line is a line with slope 2. In the multigraph, we found
that for the discount parameter settings ? = 0.6, 0.7, the slopes were below 2; for ? = 0, 0.3, the
slopes were greater than 2. This corresponds to our theoretical results; for ? < 0.5 the multigraph
is dense with slope greater than 2, and for ? > 0.5 the multigraph is sparse with slope less than 2.
Furthermore, the sparse graphs exhibit power law relationships between the number of edges and
a.s.
vertices, i.e., |EN | ? c |VN |b , N ? ?, where b ? (1, 2), as suggested by the linear relationship in
the plots between the quantities on a log-scale. Note that there are necessarily fewer edges in the
binary graph than in the multigraph, and thus this plot implies that the binary graph frequency model
can also capture sparsity. Figure 3b confirms this observation; it shows how the number of edges
varies with the number of active vertices for the binary graph. In this case, across ? ? (0, 1), we
observe slopes that are less than 2. This agrees with our theory from Section 5, which states that the
binary graph is sparse for any ? ? (0, 1).

7

Conclusions

We have proposed an alternative form of exchangeability for random graphs, which we call edge
exchangeability, in which the distribution of a graph sequence is invariant to the order of the edges. We
have demonstrated that edge-exchangeable graph sequences, unlike traditional vertex-exchangeable
sequences, can be sparse by developing a class of edge-exchangeable graph frequency models that
provably exhibit sparsity. Simulations using edge frequencies drawn according to a three-parameter
beta process confirm our theoretical results regarding sparsity. Our results suggest that a variety of
future directions would be fruitful?including theoretically characterizing different types of power
laws within graph frequency models, characterizing the use of truncation within graph frequency
models as a means for approximate Bayesian inference in graphs, and understanding the full range of
distributions over sparse, edge-exchangeable graph sequences.
Acknowledgments
We would like to thank Bailey Fosdick and Tyler McCormick for helpful conversations.

8

References
[1] D. J. Aldous. Representations for partially exchangeable arrays of random variables. Journal of Multivariate
Analysis, 11(4):581?598, 1981.
[2] D. J. Aldous. Exchangeability and related topics. In ?cole d??t? de probabilit?s de Saint-Flour, XIII?1983,
volume 1117 of Lecture Notes in Math., pages 1?198. Springer, Berlin, 1985.
[3] B. Bollob?s, S. Janson, and O. Riordan. The phase transition in inhomogeneous random graphs. Random
Structures Algorithms, 31(1):3?122, 2007.
[4] C. Borgs, J. T. Chayes, H. Cohn, and Y. Zhao. An Lp theory of sparse graph convergence I: limits, sparse
random graph models, and power law distributions. arXiv e-print 1401.2906, 2014.
[5] C. Borgs, J. T. Chayes, H. Cohn, and S. Ganguly. Consistent nonparametric estimation for heavy-tailed
sparse graphs. arXiv e-print 1401.1137, 2015.
[6] C. Borgs, J. T. Chayes, H. Cohn, and N. Holden. Sparse exchangeable graphs and their limits via graphon
processes. arXiv e-print 1601.07134, 2016.
[7] T. Broderick and D. Cai. Edge-exchangeable graphs, sparsity, and power laws. In NIPS 2015 Workshop on
Bayesian Nonparametrics: The Next Generation, 2015.
[8] T. Broderick and D. Cai. Edge-exchangeable graphs and sparsity. In NIPS 2015 Workshop on Networks in
the Social and Informational Sciences, 2015.
[9] T. Broderick, M. I. Jordan, and J. Pitman. Beta processes, stick-breaking and power laws. Bayesian
Analysis, 7(2):439?475, 2012.
[10] D. Cai and T. Broderick. Completely random measures for modeling power laws in sparse graphs. In NIPS
2015 Workshop on Networks in the Social and Informational Sciences, 2015.
[11] T. Campbell, J. Huggins, J. How, and T. Broderick. Truncated random measures. arXiv e-print 1603.00861,
2016.
[12] F. Caron and E. Fox. Sparse graphs using exchangeable random measures. arXiv e-print 1401.1137v3,
2015.
[13] H. Crane and W. Dempsey. A framework for statistical network modeling. arXiv e-print 1509.08185, 2015.
[14] H. Crane and W. Dempsey. Atypical scaling behavior persists in real world interaction networks. arXiv
e-print 1509.08184, 2015.
[15] H. Crane and W. Dempsey. Edge exchangeable models for network data. arXiv e-print 1603.04571, 2016.
[16] D. N. Hoover. Relations on probability spaces and arrays of random variables. Preprint, Institute for
Advanced Study, Princeton, NJ, 1979.
[17] O. Kallenberg. Exchangeable random measures in the plane. Journal of Theoretical Probability, 3(1):
81?136, 1990.
[18] O. Kallenberg. Probabilistic symmetries and invariance principles. Probability and its Applications.
Springer, New York, 2005.
[19] J. R. Lloyd, P. Orbanz, Z. Ghahramani, and D. M. Roy. Random function priors for exchangeable arrays
with applications to graphs and relational data. In NIPS 25, 2012.
[20] P. Orbanz and D. M. Roy. Bayesian models of graphs, arrays and other exchangeable random structures.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(2):437?461, 2015.
[21] Y. W. Teh and D. G?r?r. Indian buffet processes with power-law behavior. In NIPS 23, 2009.
[22] V. Veitch and D. M. Roy. The class of random graphs arising from exchangeable random measures. arXiv
e-print 1512.03099, 2015.
[23] S. Williamson. Nonparametric network models for link prediction. Journal of Machine Learning Research,
17:1?21, 2016.
[24] P. J. Wolfe and S. C. Olhede. Nonparametric graphon estimation. arXiv e-print 1309.5936, 2013.

9

"
2012,Kernel Latent SVM for Visual Recognition,,4590-kernel-latent-svm-for-visual-recognition.pdf,"Latent SVMs (LSVMs) are a class of powerful tools that have been successfully applied to many applications in computer vision. However, a limitation of LSVMs is that they rely on linear models. For many computer vision tasks, linear models are suboptimal and nonlinear models learned with kernels typically perform much better. Therefore it is desirable to develop the kernel version of LSVM. In this paper, we propose kernel latent SVM (KLSVM) -- a new learning framework that combines latent SVMs and kernel methods. We develop an iterative training algorithm to learn the model parameters. We demonstrate the effectiveness of KLSVM using three different applications in visual recognition. Our KLSVM formulation is very general and can be applied to solve a wide range of applications in computer vision and machine learning.","Kernel Latent SVM for Visual Recognition

Yang Wang
Department of Computer Science
University of Manitoba
ywang@cs.umanitoba.ca

Weilong Yang
School of Computing Science
Simon Fraser University
wya16@sfu.ca

Greg Mori
School of Computing Science
Simon Fraser University
mori@cs.sfu.ca

Arash Vahdat
School of Computing Science
Simon Fraser University
avahdat@sfu.ca

Abstract
Latent SVMs (LSVMs) are a class of powerful tools that have been successfully
applied to many applications in computer vision. However, a limitation of LSVMs
is that they rely on linear models. For many computer vision tasks, linear models are suboptimal and nonlinear models learned with kernels typically perform
much better. Therefore it is desirable to develop the kernel version of LSVM. In
this paper, we propose kernel latent SVM (KLSVM) ? a new learning framework
that combines latent SVMs and kernel methods. We develop an iterative training algorithm to learn the model parameters. We demonstrate the effectiveness of
KLSVM using three different applications in visual recognition. Our KLSVM formulation is very general and can be applied to solve a wide range of applications
in computer vision and machine learning.

1

Introduction

We consider the problem of learning discriminative classification models for visual recognition. In
particular, we are interested in models that have the following two characteristics: 1) can be used on
weakly labeled data; 2) have nonlinear decision boundaries.
Linear classifiers are a class of popular learning methods in computer vision. In the case of binary
classification, they are prediction models in the form of f (x) = w> x, where x is the feature vector,
and w is a vector of model parameters1 . The classification decision is based on the value of f (x).
Linear classifiers are amenable to efficient and scalable learning/inference ? an important factor in
many computer vision applications that involve high dimension features and large datasets. The
person detection algorithm in [2] is an example of the success of linear classifiers in computer
vision. The detector is trained by learning a linear support vector machine based on HOG descriptors
of positive and negative examples. The model parameter w in this detector can be thought as a
statistical template for HOG descriptors of persons.
The reliance on a rigid template w is a major limitation of linear classifiers. As a result, the learned
models usually cannot effectively capture all the variations (shape, appearance, pose, etc.) in natural
images. For example, the detector in [2] usually only works well when a person is in an upright
posture.
In the literature, there are two main approaches for addressing this limitation. The first one is to
introduce latent variables into the linear model. In computer vision, this is best exemplified by the
success of deformable part models (DPM) [5] for object detection. DPM captures shape and pose
variations of an object class with a root template covering the whole object and several part templates. By allowing these parts to deform from their ideal locations with respect to the root template,
DPM provides more flexibility than a rigid template. Learning a DPM involves solving a latent
1

Without loss of generality, we assume linear models without the bias term.

1

SVM (LSVM) [5, 17] ? an extension of regular linear SVM for handling latent variables. LSVM
provides a general framework for handling ?weakly labeled data? arising in many applications. For
example, in object detection, the training data are weakly labeled because we are only given the
bounding boxes of the objects without the detailed annotation for each part. In addition to modeling
part deformation, another popular application of LSVM is to use it as a mixture model where the
mixture component is represented as a latent variable [5, 6, 16].
The other main approach is to directly learn a nonlinear classifier. The kernel method [1] is a
representative example along this line of work. A limitation of kernel methods is that the learning is
more expensive than linear classifiers on large datasets, although efficient algorithms exist for certain
types of kernels (e.g. histogram intersection kernel (HIK) [10]). One possible way to address the
computational issue is to use nonlinear mapping to convert the original feature into some higher
dimensional space, then apply linear classifiers in the high dimensional space [14].
Latent SVM and kernel methods represent two different, yet complementary approaches for learning classification models that are more expressive than linear classifiers. They both have their own
advantages and limitations. The advantage of LSVM is that it provides a general and elegant formulation for dealing with many weakly supervised problems in computer vision. The latent variables
in LSVM can often have some intuitive and semantic meanings. As a result, it is usually easy to
adapt LSVM to capture various prior knowledge about the unobserved variables in various applications. Examples of latent variables in the literature include part locations in object detection [5],
subcategories in video annotation [16], object localization in image classification [8], etc. However,
LSVM is essentially a parametric model. So the capacity of these types of models is limited by the
parametric form. In contrast, kernel methods are non-parametric models. The model complexity is
implicitly determined by the number of support vectors. Since the number of support vectors can
vary depending on the training data, kernel methods can adapt their model complexity to fit the data.
In this paper, we propose kernel latent SVM (KLSVM) ? a new learning framework that combines
latent SVMs and kernel methods. As a result, KLSVM has the benefits of both approaches. On
one hand, the latent variables in KLSVM can be something intuitive and semantically meaningful.
On the other hand, KLSVM is nonparametric in nature, since the decision boundary is defined
implicitly by support vectors. We demonstrate KLSVM on three applications in visual recognition:
1) object classification with latent localization; 2) object classification with latent subcategories; 3)
recognition of object interactions.

2

Preliminaries

In this section, we introduce some background on latent SVM and on the dual form of SVMs used
for deriving kernel SVMs. Our proposed model in Sec. 3 will build upon these two ideas.
Latent SVM: We assume a data instance is in the form of (x, h, y), where x is the observed variable
and y is the class label. Each instance is also associated with a latent variable h that captures some
unobserved information about the data. For example, say we want to learn a ?car? model from a
set of positive images containing cars and a set of negative images without cars. We know there is
a car somewhere in a positive image, but we do not know its exact location. In this case, h can be
used to represent the unobserved location of the car in the image. In this paper, we consider binary
classification for simplicity, i.e. y ? {+1, ?1}. Multi-class classification can be easily converted
to binary classification, e.g. using one-vs-all or one-vs-one strategy. To simplify the notation, we
also assume the latent variable h takes its value from a discrete set of labels h ? H. However, our
formulation is general. We will show how to deal with more complex h in Sec. 3.2 and in one of the
experiments (Sec. 4.3).
In latent SVM, the scoring function of sample x is defined as fw (x) = maxh w> ?(x, h), where
?(x, h) is the feature vector defined for the pair of (x, h). For example, in the ?car model? example,
?(x, h) can be a feature vector extracted from the image patch P
at location h of the image x. The
objective function of LSVM is defined as L(w) = 12 ||w||2 + C i max(0, 1 ? yi fw (xi )). LSVM
is essentially a non-convex optimization problem. However, the learning problem becomes convex
once the latent variable h is fixed for positive examples. Therefore, we can train the LSVM by
an iterative algorithm that alternates between inferring h on positive examples and optimizing the
model parameter w.
Dual form with fixed h on positive examples : Due to its nature of non-convexity, it is not straightforward to derive the dual form for the general LSVM. Therefore, as a starting point, we first consider a simpler scenario assuming h is fixed (or observed) on the positive training examples. As
previously mentioned, the LSVM is then relaxed to a convex problem with this assumption. Note
that we will relax this assumption in Sec. 3. In the above ?car model? example, this means that
we have the ground-truth bounding boxes of the cars in each image. More formally, we are given
2

M +N
M positive samples {xi , hi }M
i=1 , and N negative samples {xj }j=M +1 . Inspired by linear SVMs,
>
our goal is to find a linear discriminant fw (x, h) = w ?(x, h) by solving the following quadratic
program:
X
X
1
P(w? ) = min ||w||2 + C1
?i + C 2
?j,h
(1a)
w,? 2
i
j,h

s.t. w> ?(xi , hi ) ? 1 ? ?i , ?i ? {1, 2, ..., M },

(1b)

>

?w ?(xj , h) ? 1 ? ?j,h ?j ? {M + 1, M + 2, ..., M + N }, ?h ? H
?i ? 0, ?j,h ? 0 ?i, ?j, ?h ? H

(1c)
(1d)

Similar to standard SVMs, {?i } and {?j,h } are the slack variables for handling soft margins.
It is interesting to note that the optimization problem in Eq. 1 is almost identical to that of standard
linear SVMs. The only difference lies in the constraint on the negative training examples (Eq. 1c).
Since we assume h?s are not observed on negative images, we need to enumerate all possible values
for h?s in Eq. 1c. Intuitively, this means every image patch from a negative image (i.e. non-car
image) is not a car.
It is easy to show that Eq. 1 is convex. Similar to the dual form of standard SVMs, we can derive
the dual form of Eq. 1 as follows:
X
XX
XX
1 X
?i +
?j,h ? ||
D(?? , ? ? ) = max
?i ?(xi , hi ) ?
?j,h ?(xj , h)||2 (2a)
?,?
2
i
j
i
j
h

s.t.

h

0 ? ?i ? C1 , ?i; 0 ? ?j,h ? C2 , ?j, ?h ? H
?

(2b)
?

?

The optimal primal parameters w for Eq. 1 and the optimal dual parameters (? , ? ) for Eq. 2 are
related as follows:
X
XX
?
w? =
?i? ?(xi , hi ) ?
?j,h
?(xj , h)
(3)
i

j

h

Let us define ? to be the concatenations of {?i : ?i} and {?j,h : ?j, ?h ? H}, so |?| = M +N ?|H|.
Let ? be a |?| ? D matrix where D is the dimension of ?(x, h). ? is obtained by stacking together
{?(xi , hi ) : ?i} and {??(xj , h) : ?j, ?h ? H}. We also define Q = ??> and 1 to be a vector of
all 1?s. Then Eq. 2a can be rewritten as (we omit the linear constraints on ? for simplicity):
1
max ?> ? 1 ? ?> Q?
?
2

(4)

The advantage of working with the dual form in Eq. 4 is that it only involves a socalled kernel matrix Q.
Each entry of Q is a dot-product of two vectors in the
form of ?(x, h)> ?(x0 , h0 ). We can replace the dot-product with any other kernel functions in the form of k(?(x, h), ?(x0 , h0 )) to get nonlinear classifiers [1].
The scornew
ing function
can be kernelized as follows: f (xnew
P for the testing images x
 ) =
P P ?
?
new
new
new
new
maxhnew
,h
)) ? j h ?j,h k(?(xj , h), ?(x
,h
)) .
i ?i k(?(xi , hi ), ?(x
Another important, yet often overlooked fact is that the optimal values of the two quadratic programs
in Eqs. 1 and 2 have some specific meanings. They correspond to the inverse of the (soft) margin of
the resultant SVM classifier [9, 15]: P(w? ) = D(?? , ? ? ) = SVM 1margin . In the next section, we will
exploit this fact to develop the kernel latent support vector machines.

3

Kernel Latent SVM

Now we assume the variables {hi }M
i=1 on the positive training examples are unobserved. If the scoring function used for classification is in the form of f (x) = maxh w> ?(x, h), we can use the LSVM
formulation [5, 17] to learn the model parameters w. As mentioned earlier, the limitation of LSVM
is the linearity assumption of w> ?(x, h). In this section, we propose kernel latent SVM (KLSVM)
? a new latent variable learning method that only requires a kernel function K(x, h, x0 , h0 ) between
a pair of (x, h) and (x0 , h0 ).
Note that when {hi }M
i=1 are observed on the positive training examples, we can plug them in Eq. 2
to learn a nonlinear kernelized decision function that separates the positive and negative examples.
3

M
When {hi }M
i=1 are latent, an intuitive thing to do is to find the labeling of {hi }i=1 so that when
we plug them in and solve for Eq. 2, the resultant nonlinear decision function separates the two
classes as widely as possible. In other words, we look for a set of {h?i } which can maximize the
SVM margin (equivalent to minimizing D(?? , ? ? , {hi })). The same intuition was previously used
to develop the max-margin clustering method in [15]. Using this intuition, we write the optimal
function value of the dual form as D(?? , ? ? , {hi }) since now it implicitly depends on the labelings
{hi }. We can jointly find the labelings {hi } and solve for (?? , ? ? ) by the following optimization
problem:

min D(?? , ? ? , {hi })

(5a)

{hi }

= min max
{hi } ?,?

X

?i +

i

XX
j

s.t. 0 ? ?i ? C1 , ?i;

h

XX
1 X
?j,h ? ||
?i ?(xi , hi ) ?
?j,h ?(xj , h)||2 (5b)
2 i
j
h

0 ? ?j,h ? C2 , ?j, ?h ? H

(5c)

The most straightforward way of solving Eq. 5 is to optimize D(?? , ? ? , {hi }) for every possible
combination of values for {hi }, and then take the minimum. When hi takes its value from a discrete set of K possible choices (i.e. |H| = K), this naive approach needs to solve M K quadratic
programs. This is obviously too expensive. Instead, we use the following iterative algorithm:
? Fix ? and ?, compute the optimal {hi }? by
XX
1 X
?i ?(xi , hi ) ?
?j,h ?(xj , h)||2
(6)
{hi }? = arg max ||
2 i
{hi }
j
h

? Fix {hi }, compute the optimal (?? , ? ? ) by
?
?
?X
?
XX
X
X
X
1
?j,h ? ||
(?? , ? ? ) = arg max
?i +
?j,h ?(xj , h)||2
(7)
?i ?(xi , hi ) ?
?
?
2 i
?,?
i
j
j
h

h

The optimization problem in Eq. 7 is a quadratic program similar to that of a standard dual SVM.
As a result, Eq. 7 can be kernelized as Eq. 4 and solved using standard dual solver in regular SVMs.
In Sec. 3.1, we describe how to kernelize and solve the optimization problem in Eq. 6.
3.1

Optimization over {hi }

The complexity of a simple enumeration approach for solving Eq. 6 is again O(M K ), which is
clearly too expensive for practical purposes. Instead, we solve it iteratively using an algorithm
similar to co-ordinate ascent. Within an iteration, we choose one positive training example t. We
update ht while fixing hi for all i 6= t. The optimal h?t can be computed as follows:
h?t = arg max ||?t ?(xt , ht ) +
ht

X

?i ?(xi , hi ) ?

XX
j

i:i6=t

?j,h ?(xj , h)||2
?>

?
? arg max ||?t ?(xt , ht )||2 + 2 ?
ht

(8a)

h

X

?i ?(xi , hi ) ?

XX
j

i:i6=t

?j,h ?(xj , h)? ?t ?(xt , ht ) (8b)

h

By replacing the dot-product ?(x, h)> ?(x0 , h0 ) with a kernel function k(?(x, h), ?(x0 , h0 )), we obtain the kernerlized version of Eq. 8(b) as follows
X
h?t = arg max ?t ?t k(?(xt , ht ), ?(xt , ht )) + 2
?i ?t k(?(xi , hi ), ?(xt , ht ))
ht

i:i6=t

?2

XX
j

?j,h ?t k(?(xj , h), ?(xt , ht ))

(9)

h

It is interesting to notice that if the t-th example is not a support vector (i.e. ?t = 0), the function
value of Eq. 9 will be zero regardless of the value of ht . This means in KLSVM we can improve the
training efficiency by only performing Eq. 9 on positive examples corresponding to support vectors.
For other positive examples (non-support vectors), we can simply set their latent variables the same
4

as the previous iteration. Note that in LSVM, the inference during training needs to be performed
on every positive example.
Connection to LSVM: When a linear kernel is used, the inference problem (Eq. 8) has a very
interesting connection to LSVM in [5]. Recall that for linear kernels, the model parameters w and
dual variables (?, ?) are related by Eq. 3. Then Eq. 8 becomes:
>
?t ?(xt , ht )
(10a)
h?t = arg max ||?t ?(xt , ht )||2 + 2 w ? ?t ?(xt , hold
t )
ht

1
>
? arg max ?t w> ?(xt , ht ) + ?t2 ||?(xt , ht )||2 ? ?t2 ?(xt , hold
t ) ?(xt , ht )
2
ht

(10b)

where hold
t is the value of latent variable of the t-th example in the previous iteration. Let us consider the situation when ?t 6= 0 and the feature vector ?(x, h) is l2 normalized, which is commonly used in computer vision. In this case, ?t2 ?(xt , ht )> ?(xt , ht ) is a constant, and we have
>
old
old >
old
?(xt , hold
t ) ?(xt , ht ) > ?(xt , ht ) ?(xt , ht ) if ht 6= ht . Then Eq. 10 is equivalent to:
>
h?t = arg max w> ?(xt , ht ) ? ?t ?(xt , hold
t ) ?(xt , ht )

(11)

ht

Eq. 11 is very similar to the inference problem in LSVM, i.e., h?t = arg maxht w> ?(xt , ht ), but
>
with an extra term ?t ?(xt , hold
t ) ?(xt , ht ) which penalizes the choice of ht for being the same
old
value as previous iteration ht . This has a very appealing intuitive interpretation. If the t-th positive
example is a support vector, the latent variable hold from previous iteration causes this example to
lie very close to (or even on the wrong side) the decision boundary, i.e. the example is not wellseparated. During the current iteration, the second term in Eq. 11 penalizes hold to be chosen again
since we already know the example will not be well-separated if we choose hold again. The amount
>
of penalty depends on the magnitudes of ?t and ?(xt , hold
t ) ?(xt , ht ). We can interpret ?t as how
?
old >
old
?bad? ht is, and ?(xt , ht ) ?(xt , ht ) as how close ht is to hold
t . Eq. 11 penalizes the new ht to
old
be ?close? to ?bad? ht .
3.2

Composite Kernels

So far we have assumed that the latent variable h takes its value from a discrete set of labels. Given
a pair of (x, h) and (x0 , h0 ), the types of kernel function k(x, h; x0 , h0 ) we can choose from are still
limited to a handful of standard kernels (e.g. Gaussian, RBF, HIK, etc). In this section, we consider
more interesting cases where h involves some complex structures. This will give us two important
benefits. First of all, it allows us to exploit structural information in the latent variables. This is in
analog to structured output learning (e.g. [12, 13]). More importantly, it gives us more flexibility to
construct new kernel functions by composing from simple kernels.
Before we proceed, let us first motivate the composite kernel with an example application. Suppose
we want to detect some complex person-object interaction (e.g. ?person riding a bike?) in an image.
One possible solution is to detect persons and bikes in an image, then combine the results by taking
into account of their relationship (i.e. ?riding?). Imagine we already have kernel functions corresponding to some components (e.g. person, bike) of the interaction. In the following, we will show
how to compose a new kernel for the ?person riding a bike? classifier from those components.
We denote the latent variable using ~h to emphasize that now it is a vector instead of a single discrete
value. We denote it as ~h = (z1 , z2 , ...), where zu is the u-th component of ~h and takes its value
from a discrete set of possible labels. For the structured latent variable, it is assumed that there are
certain dependencies between some pairs of (zu , zv ). We can use an undirected graph G = (V, E) to
capture the structure of the latent variable, where a vertex u ? V corresponds to the label zu , and an
edge (u, v) ? E corresponds to the dependency between zu and zv . As a concrete example, consider
the ?person riding a bike? recognition problem. The latent variable in this case has two components
~h = (zperson , zbike ) corresponding to the location of person and bike, respectively. On the training
data, we have access to the ground-truth bounding box of ?person riding a bike? as a whole, but not
the exact location of ?person? or ?bike? within the bounding box. So ~h is latent in this application.
The edge connecting zperson and zbike captures the relationship (e.g. ?riding on?, ?next to?, etc.)
between these two objects.
Suppose we already have kernel functions corresponding to the vertices and edges in the graph, we
can then define the composite kernel as the summation of the kernels over all the vertices and edges.
5

Figure 1: Visualization of how the latent variable (i.e. object location) changes during the learning. The red
bounding box corresponds to the initial object location. The blue bounding box corresponds to the object
location after the learning.
Method
BOF + linear SVM BOF + kernel SVM linear LSVM
KLSVM
Acc (%)
45.57 ? 4.23
50.53 ? 6.53
75.07 ? 4.18 84.49 ? 3.63
Table 1: Results on the mammal dataset. We show the mean/std of classification accuracies over five rounds of
experiments.

K(?(x, ~h), ?(x0 , ~h0 )) =

X

ku (?(x, zu ), ?(x0 , zu0 )) +

u?V

X

kuv (?(x, zu , zv ), ?(x0 , zu0 , zv0 )) (12)

(u,v)?E

When the latent variable ~h forms a tree structure, there exist efficient inference algorithms for
solving Eq. 9, such as dynamic programming. It is also possible for Eq. 12 to include kernels
defined on higher-order cliques in the graph, as long as we have some pre-defined kernel functions
for them.

4

Experiments

We evaluate KLSVM in three different applications of visual recognition. Each application has a
different type of latent variables. For these applications, we will show that KLSVM outperforms
both the linear LSVM [5] and the regular kernel SVM. Note that we implement the learning of
linear LSVM by ourselves using the same iterative algorithm as the one in [5].
4.1

Object Classification with Latent Localization

Problem and Dataset: We consider object classification with image-level supervision. Our training
data only have image-level labels indicating the presence/absence of each object category in an
image. The exact object location in the image is not provided and is considered as the latent variable
h in our formulation. We define the feature vector ?(x, h) as the HOG feature extracted from the
image at location h. During testing, the inference of h is performed by enumerating all possible
locations of the image.
We evaluate our algorithm on the mammal dataset [8] which consists of 6 mammal categories. There
are about 45 images per category. For each category, we use half of the images for training and the
remaining half for testing. We assume the object size is the same for the images of the same category,
which is a reasonable assumption for this dataset. This dataset was used to evaluate the linear LSVM
in [8].
Results: We compare our algorithm with linear LSVM. To demonstrate the benefit of using latent
variables, we also compare with two simple baselines using linear and kernel SVMs based on bag-offeatures (BOF) extracted from the whole image (i.e. without latent variables). For both baselines, we
aggregate the quantized HOG features densely sampled from the whole image. Then, the features are
fed into the standard linear SVM and kernel SVM respectively. We use the histogram intersection
kernel (HIK) [10] since it has been proved to be successful for vision applications, and efficient
learning/inference algorithms exist for this kernel.
We run the experiments for five rounds. In each round, we randomly split the images from each
category into training and testing sets. For both linear LSVM and KLSVM, we initialize the latent
variable at the center location of each image and we set C1 = C2 = 1. For both algorithms, we use
one-versus-one classification scheme. We use the HIK kernel in the KLSVM. Table 1 summarizes
the mean and standard deviations of the classification accuracies over five rounds of experiments.
Across all experiments, both linear LSVM and KLSVM achieve significantly better results than
approaches using BOF features from the whole image. This is intuitively reasonable since most of
images on this dataset share very similar scenes. So BOF feature without latent variables cannot
capture the subtle differences between each category. Table 1 also shows KLSVM significantly
outperforms linear LSVM.
Fig. 1 shows examples of how the latent variables change on some training images during the learning of the KLSVM. For each training image, the location of the object (latent variable h) is initialized
to the center of the image. After the learning algorithm terminates, the latent variables accurately
locate the objects.
6

Figure 2: Visualization of some testing examples from the ?bird? (left) and ?boat? (right) categories. Each row
corresponds to a subcategory. We can see that visually similar images are grouped into the same subcategory.
Method
non-latent linear SVM linear LSVM non-latent kernel SVM
KLSVM
Acc (%)
50.69 ? 0.38
53.13 ? 0.63
52.98 ? 0.22
55.17 ? 0.27
Table 2: Results on CIFAR10 Dataset. We show the mean/std of classification accuracies over five folds of
experiments. Each fold uses a different batch of the training data.

4.2

Object Classification with Latent Subcategory

Problem and Dataset: Our second application is also on object classification. But here we consider a different type of latent variable. Objects within a category usually have a lot of intra-class
variations. For example, consider the images for the ?bird? category shown in the left column of
Fig. 2. Even though they are examples of the same category, they still exhibit very large appearance
variations. It is usually very difficult to learn a single ?bird? model that captures all those variations.
One way to handle the intra-class variation is to split the ?bird? category into several subcategories.
Examples within a subcategory will be more visually similar than across all subcategories. Here we
use the latent variable h to indicate the subcategory an image belongs to. If a training image belongs
to the class c, its subcategory label h takes value from a set Hc of subcategory labels corresponding
to the c-th class. Note that subcategories are latent on the training data, so they may or may not have
semantic meanings.
The feature vector ?(x, h) is defined as a sparse vector whose feature dimension is |Hc | times of
the dimension of ?(x), where ?(x) is the HOG descriptor extracted from the image x. In the
experiments, we set |Hc | = 3 for all c?s. Then we can define ?(x, h = 1) = (?(x); 0; 0), ?(x, h =
2) = (0; ?(x); 0), and so on. Similar models have been proposed to address the viewpoint changing
in object detection [6] and semantic variations in YouTube video tagging [16].
We use the CIFAR10 [7] dataset in our experiment. It consists of images from ten classes including
airplane, automobile, bird, cat, etc. The training set has been divided into five batches and each
batch contains 10000 images. There are in total 10000 test images.
Results: Again we compare with three baselines: linear LSVM, non-latent linear SVM, non-latent
kernel SVM. Similarly, we use HIK kernel for the kernel-based methods. For non-latent approaches,
we simply feed feature vector ?(x) to SVMs without using any latent variable.
We run the experiments in five folds. Each fold use a different training batch but the same testing
batch. We set C1 = C2 = 0.01 for all the experiments and initialize the subcategory labels of
training images by k-means clustering. Table 2 summarizes the results. Again, KLSVM outperforms
other baseline approaches. It is interesting to note that both linear LSVM and KLSVM outperform
their non-latent counterparts, which demonstrates the effectiveness of using latent subcategories in
object classification. We visualize examples of the correctly classified testing images from the ?bird?
and ?boat? categories in Fig. 2. Images on the same row are assigned the same subcategory labels.
We can see that visually similar images are automatically grouped into the same subcategory.
4.3

Recognition of Object Interaction

Problem and Dataset: Finally, we consider an application where the latent variable is more complex and requires the composite kernel introduced in Sec. 3.2. We would like to recognize complex
interactions between two objects (also called ?visual phrases? [11]) in static images. We build a
dataset consisting of four object interaction classes, i.e. ?person riding a bicycle?, ?person next to
a bicycle?, ?person next to a car? and ?bicycle next to a car? based on the visual phrase dataset in
[11]. Each class contains 86?116 images. Each image is only associated with one of the four object
interaction label. There is no ground-truth bounding box information for each object. We use 40
images from each class for training and the rest for testing.
Our approach: We treat the locations of objects as latent variables. For example, when learning
the model for ?person riding a bicycle?, we treat the locations of ?person? and ?bicycle? as latent
variables. In this example, each image is associated with latent variables ~h = (z1 , z2 ), where
z1 denotes the location of the ?person? and z2 denotes the location of the ?bicycle?. To reduce
the search space of inference, we first apply off-the-shelf ?person? and ?bicycle? detectors [5] on
7

Method
BOF + linear SVM BOF + kernel SVM linear LSVM
KLSVM
Acc(%)
42.92
58.46
46.33 ? 1.4
66.42 ? 0.99
Table 3: Results on object interaction dataset. For the approaches using latent variables, we show the mean/std
of classification accuracies over five folds of experiments.

Figure 3: Visualization of how latent variables (i.e. object locations) change during the learning. The left image
is from the ?person riding a bicycle? category, and the right image is from the ?person next to a car? category.
Yellow bounding boxes corresponds to the initial object locations. The blue bounding boxes correspond to the
object locations after the learning.

each image. For each object, we generate five candidate bounding boxes which form a set Zi ,
i.e. |Z1 | = |Z2 | = 5 and zi ? Zi . Then, the inference of ~h is performed by enumerating 25
combinations of z1 and z2 . We also assume there are certain dependencies between the pair of
(z1 , z2 ). Then the kernel between two images can be defined as follows:
X
K(?(x, ~h), ?(x0 , ~h0 )) =
ku (?(x, zu ), ?(x0 , zu0 )) + kp (?(z1 , z2 ), ?(z10 , z20 ))
(13)
u={1,2}

We define ?(x, zu ) as the bag-of-features (BOF) extracted from the bounding box zu in the image x.
For each bounding box, we split the region uniformly into four equal quadrants. Then we compute
the bag-of-features for each quadrant by aggregating quantized HOG features. The final feature
vector is the concatenation of these four bag-of-features histograms. This feature representation
is similar to the spatial pyramid feature representation. In our experiment, we choose HIK for
ku (?). The kernel kp (?) captures the spatial relationship between z1 and z2 such as above, below,
overlapping, next-to, near, and far. Here ?(z1 , z2 ) is a sparse binary vector and its k-th element is
set to 1 if the corresponding k-th relation is satisfied between bounding boxes z1 and z2 . Note that
kp (?) does not depend on the images. Similar representation has been used in [4]. We define kp (?)
as a simple linear kernel.
Results: We compare with the simple BOF + linear SVM, and BOF + kernel SVM approaches.
These two baselines use the same BOF feature representation as our approach except that the features
are extracted from the whole image. We choose the HIK in the kernel SVM. Note that this is a
strong baseline since [3] has shown that a similar pyramid feature representation with kernel SVM
achieves top performances on the task of person-object interaction recognition. The other baseline
is the standard linear LSVM, in which we build the feature vector ?(x, h) by simply concatenating
both unary features and pairwise features, i.e. ?(x, h) = [?(x, z1 ); ?(x, z2 ); ?(z1 , z2 )]. Again, we
set C1 = C2 = 1 for all experiments. We run the experiments for five rounds for approaches using
latent variables. In each round, we randomly initialize the choices of z1 and z2 . Table 3 summarizes
the results. The kernel latent SVM that uses HIK for ku (?) achieves the best performance.
Fig. 3 shows examples of how the latent variables change on some training images during the learning of the KLSVM. For each training image, both latent variables z1 and z2 are randomly initialized
to one of five candidate bounding boxes. As we can see, the initial bounding boxes can accurately
locate the target objects but their spatial relations are different to ground-truth labels. After learning
algorithm terminates, the latent variables not only locate the target objects, but more importantly
they also capture the correct spatial relationship between objects.

5

Conclusion

We have proposed kernel latent SVM ? a new learning framework that combines the benefits of
LSVM and kernel methods. Our learning framework is very general. The latent variables can not
only be a single discrete value, but also be more complex values with interdependent structures. Our
experimental results on three different applications in visual recognition demonstrate that KLSVM
outperforms using LSVM or using kernel methods alone. We believe our work will open the
possibility of constructing more powerful and expressive prediction models for visual recognition.
Acknowledgement: This work was supported by a Google Research Award and NSERC.
Yang Wang was partially supported by a NSERC postdoc fellowship.
8

References
[1] C. J. Burges. A tutorial on support vector machines for pattern recognition. Data Mining and Knowledge
Discovery, 2(2):121?167, 1998.
[2] N. Dalal and B. Triggs. Histogram of oriented gradients for human detection. In IEEE Computer Society
Conference on Computer Vision and Pattern Recognition, 2005.
[3] V. Delaitre, I. Laptev, and J. Sivic. Recognizing human actions in still images: a study of bag-of-features
and part-based representations. In British Machine Vision Conference, 2010.
[4] C. Desai, D. Ramanan, and C. Fowlkes. Discriminative models for multi-class object layout. In IEEE
International Conference on Computer Vision, 2009.
[5] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part based models. IEEE Transactions on Pattern Analysis and Machine Intelligence,
32(9):1672?1645, 2010.
[6] C. Gu and X. Ren. Discriminative mixture-of-templates for viewpoint classification. In European Conference on Computer Vision, 2010.
[7] A. Krizhevsky. Learning multiple layers of features from tiny images. Master?s thesis, University of
Toronto, 2009.
[8] M. P. Kumar, B. Packer, and D. Koller. Self-paced learning for latent variable models. In Advances in
Neural Information Processing Systems, 2010.
[9] G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. R. Ghaoui, and M. I. Jordan. Learning the kernel matrix
with semidefinite programming. Journal of Machine Learning Research, 5:24?72, 2004.
[10] S. Maji, A. C. Berg, and J. Malik. Classification using intersection kernel support vector machines is
efficient. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2008.
[11] M. A. Sadeghi and A. Farhadi. Recognition using visual phrases. In IEEE Computer Society Conference
on Computer Vision and Pattern Recognition, 2011.
[12] B. Taskar, C. Guestrin, and D. Koller. Max-margin markov networks. In Advances in Neural Information
Processing Systems, volume 16. MIT Press, 2004.
[13] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and
interdependent output variables. Journal of Machine Learning Research, 6:1453?1484, 2005.
[14] A. Vedaldi and A. Zisserman. Efficient additive kernels via explicit feature maps. Pattern Analysis and
Machine Intellingence, 34(3), 2012.
[15] L. Xu, J. Neufeldand, B. Larson, and D. Schuurmans. Maximum margin clustering. In L. K. Saul,
Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems, volume 17, pages
1537?1544. MIT Press, Cambridge, MA, 2005.
[16] W. Yang and G. Toderici. Discriminative tag learning on youtube videos with latent sub-tags. In IEEE
Computer Society Conference on Computer Vision and Pattern Recognition, 2011.
[17] C.-N. Yu and T. Joachims. Learning structural SVMs with latent variables. In International Conference
on Machine Learning, 2009.

9

"
2011,Prismatic Algorithm for Discrete D.C. Programming Problem,,4252-prismatic-algorithm-for-discrete-dc-programming-problem.pdf,"In this paper, we propose the first exact algorithm for minimizing the difference of two submodular functions (D.S.), i.e., the discrete version of the D.C. programming problem. The developed algorithm is a branch-and-bound-based algorithm which responds to the structure of this problem through the relationship between submodularity and convexity. The D.S. programming problem covers a broad range of applications in machine learning because this generalizes the optimization of a wide class of set functions. We empirically investigate the performance of our algorithm, and illustrate the difference between exact and approximate solutions respectively obtained by the proposed and existing algorithms in feature selection and discriminative structure learning.","Prismatic Algorithm for Discrete D.C. Programming Problem

Yoshinobu Kawahara? and Takashi Washio
The Institute of Scientific and Industrial Research (ISIR)
Osaka University
8-1 Mihogaoka, Ibaraki-shi, Osaka 567-0047 JAPAN
{kawahara,washio}@ar.sanken.osaka-u.ac.jp

Abstract
In this paper, we propose the first exact algorithm for minimizing the difference of two
submodular functions (D.S.), i.e., the discrete version of the D.C. programming problem.
The developed algorithm is a branch-and-bound-based algorithm which responds to the
structure of this problem through the relationship between submodularity and convexity.
The D.S. programming problem covers a broad range of applications in machine learning. In fact, this generalizes any set-function optimization. We empirically investigate
the performance of our algorithm, and illustrate the difference between exact and approximate solutions respectively obtained by the proposed and existing algorithms in feature
selection and discriminative structure learning.

1 Introduction
Combinatorial optimization techniques have been actively applied to many machine learning applications, where submodularity often plays an important role to develop algorithms [10, 16, 27, 14,
15, 19, 1]. In fact, many fundamental problems in machine learning can be formulated as submoular
optimization. One of the important categories would be the D.S. programming problem, i.e., the
problem of minimizing the difference of two submodular functions. This is a natural formulation
of many machine learning problems, such as learning graph matching [3], discriminative structure
learning [21], feature selection [1] and energy minimization [24].
In this paper, we propose a prismatic algorithm for the D.S. programming problem, which is a
branch-and-bound-based algorithm responding to the specific structure of this problem. To the best
of our knowledge, this is the first exact algorithm to the D.S. programming problem (although there
exists an approximate algorithm for this problem [21]). As is well known, the branch-and-bound
method is one of the most successful frameworks in mathematical programming and has been incorporated into commercial softwares such as CPLEX [13, 12]. We develop the algorithm based
on the analogy with the D.C. programming problem through the continuous relaxation of solution
spaces and objective functions with the help of the Lov?asz extension [17, 11, 18]. The algorithm is
implemented as an iterative calculation of binary-integer linear programming (BILP).
Also, we discuss applications of the D.S. programming problem in machine learning and investigate empirically the performance of our method and the difference between exact and approximate
solutions through feature selection and discriminative structure-learning problems.
The remainder of this paper is organized as follows. In Section 2, we give the formulation of the
D.S. programming problem and then describe its applications in machine learning. In Section 3,
we give an outline of the proposed algorithm for this problem. Then, in Section 4, we explain the
details of its basic operations. And finally, we give several empirical examples using artificial and
real-world datasets in Section 5, and conclude the paper in Section 6.
Preliminaries and Notation: A set function f is called submodular if f (A) + f (B) ? f (A ?
B) + f (A ? B) for all A, B ? N , where N = {1, ? ? ? , n} [5, 7]. Throughout this paper, we denote
?

http://www.ar.sanken.osaka-u.ac.jp/?kawahara/

1

by f? the Lov?asz extension of f , i.e., a continuous function f? : Rn ? R defined by
?m?1
f?(p) = j=1 (?
pj ? p?j+1 )f (Uj ) + p?m f (Um ),
where Uj = {i ? N : pi ? p?j } and p?1 > ? ? ? > p?m are the m distinct elements of p?
[17, 18]. Also,
we denote by IA ? {0, 1}n the characteristic vector of a subset A ? N , i.e., IA = i?A ei where
ei is the i-th unit vector. Note, through the definition of the characteristic vector, any subset A ? N
has the one-to-one correspondence with the vertex of a n-dimensional cube D := {x ? Rn : 0 ?
xi ? 1(i = 1, . . . , n)}. And, we denote by (A, t)(T ) all combinations of a real value plus subset
whose corresponding vectors (IA , t) are inside or on the surface of a polytope T ? Rn+1 .

2

The D.S. Programming Problem and its Applications

Let f and g are submodular functions. In this paper, we address an exact algorithm to solve the D.S.
programming problem, i.e., the problem of minimizing the difference of two submodular functions:
min f (A) ? g(A).

A?N

(1)

As is well known, any real-valued function whose second partial derivatives are continuous everywhere can be represented as the difference of two convex functions [12]. As well, the problem (1)
generalizes any set-function optimization problem. Problem (1) covers a broad range of applications
in machine learning [21, 24, 3, 1]. Here, we give a few examples.
Feature selection using structured-sparsity inducing norms: Sparse methods for supervised
learning, where we aim at finding good predictors from as few variables as possible, have attracted
much interests from machine learning community. This combinatorial problem is known to be a
submodular maximization problem with cardinality constraint for commonly used measures such as
least-squared errors [4, 14]. And as is well known, if we replace the cardinality function with its
convex envelope such as l1 -norm, this can be turned into a convex optimization problem. Recently,
it is reported that submodular functions in place of the cardinality can give a wider family of polyhedral norms and may incorporate prior knowledge or structural constraints in sparse methods [1].
Then, the objective (that is supposed to be minimized) becomes the sum of a loss function (often,
supermodular) and submodular regularization terms.
Discriminative structure learning: It is reported that discriminatively structured Bayesian classifier often outperforms generatively structured one [21, 22]. One commonly used metric for discriminative structure learning would be EAR (explaining away residual) [2]. EAR is defined as the
difference of the conditional mutual information between variables by class C and non-conditional
one, i.e., I(Xi ; Xj |C) ? I(Xi ; Xj ). In structure learning, we repeatedly try to find a subset in
variables that minimize this kind of measures. Since the (symmetric) mutual information is a submodular function, obviously this problem leads the D.S. programming problem [21].
Energy minimization in computer vision: In computer vision, an image is often modeled with
a Markov random field, where each node represents a pixel. Let G = (V, E) be the undirected
graph, where a label xs ? L is assigned on each node. Then, many tasks in computer vision
can be naturally?formulated in terms
? of energy minimization where the energy function has the
form: E(x) = p?V ?p (xp ) + (p,q)?E ?(xp , xq ), where ?p and ?p,q are univariate and pairwise
potentials. In a pairwise potential for binarized energy (i.e., L = {0, 1}), submodularity is defined
as ?pq (1, 1) + ?pq (0, 0) ? ?pq (1, 0) + ?pq (0, 1) (see, for example, [26]). Based on this, any energy
function in computer vision can be written with a submodular function E1 (x) and a supermodular
function E2 (x) as E(x) = E1 (x) + E2 (x) (ex. [24]). Or, in case of binarized energy, even if such
explicit decomposition is not known, a non-unique decomposition to submodular and supermodular
functions can be always given [25].

3

Prismatic Algorithm for the D.S. Programming Problem

By introducing an additional variable t(? R), Problem (1) can be converted into the equivalent
problem with a supermodular objective function and a submodular feasible set, i.e.,
min

A?N,t?R

t ? g(A)

s.t. f (A) ? t ? 0.

2

(2)

Obviously, if (A? , t? ) is an optimal solution of Problem (2), then A? is an optimal solution of Problem (1)
and t? = f (A? ). The proposed algorithm is a realization
of the branch-and-bound scheme which responds to this
specific structure of the problem.
To this end, we first define a prism T (S) ? Rn+1 by
T = {(x, t) ? Rn ? R : x ? S},
where S is an n-simplex. S is obtained from the ndimensional cube D at the initial iteration (as described
in Section 4.1), or by the subdivision operation described
in the later part of this section (and the detail will be described in Section 4.2). The prism T has n + 1 edges that
are vertical lines (i.e., lines parallel to the t-axis) which
pass through the n + 1 vertices of S, respectively [11].

T

v

(0,1)

(1,1)

D
r

(1,0)

(0,0)

S2
S1
S

Figure 1: Illustration of the prismatic algorithm.

Our algorithm is an iterative procedure which mainly consists of two parts; branching and bounding,
as well as other branch-and-bound frameworks [13]. In branching, subproblems are constructed by
dividing the feasible region of a parent problem. And in bounding, we judge whether an optimal
solution exists in the region of a subproblem and its descendants by calculating an upper bound of
the subproblem and comparing it with an lower bound of the original problem. Some more details
for branching and bounding are described as follows.
Branching: The branching operation in our method is carried out using the property of a simplex.
That is, since, in a n-simplex, any r + 1 vertices
?p are not on a r ? 1-dimensional hyperplane for
r ? n, any n-simplex can be divided as S = i=1 Si , where p ? 2 and Si are n-simplices such
that each pair of simplices Si , Sj (i ?= j) intersects at most in common
?pboundary points (the way of
constructing such partition is explained in Section 4.2). Then, T = i=1 Ti , where Ti = {(x, t) ?
Rn ? R : x ? Si }, is a natural prismatic partition of T induced by the above simplical partition.
Bounding: For the bounding operation on Sk (resp., Tk ) at the iteration k, we consider a polyhe? where D
? = {(x, t) ? Rn ? R : x ? D, f?(x) ? t} is the
dral convex set Pk such that Pk ? D,
region corresponding to the feasible set of Problem (2). At the first iteration, such P is obtained as
P0 = {(x, t) ? Rn ? R : x ? S, t ? t?},
where t? is a real number satisfying t? ? min{f (A) : A ? N }. Here, t? can be determined by using
some existing submodular minimization solver [23, 8]. Or, at later iterations, more refined Pk , such
? is constructed as described in Section 4.4.
that P0 ? P1 ? ? ? ? ? D,
As described in Section 4.3, a lower bound ?(Tk ) of t ? g(A) on the current prism Tk can be
calculated through the binary-integer linear programming (BILP) (or the linear programming (LP))
using Pk , obtained as described above. Let ? be the lowest function value (i.e., an upper bound of
? found so far. Then, if ?(Tk ) ? ?, we can conclude that there is no feasible solution
t ? g(A) on D)
which gives a function value better than ? and can remove Tk without loss of optimality.
The pseudo-code of the proposed algorithm is described in Algorithm 1. In the following section,
we explain the details of the operations involved in this algorithm.

4

Basic Operations

Obviously, the procedure described in Section 3 involves the following basic operations:
1. Construction of the first prism: A prism needs to be constructed from a hypercube at first,
2. Subdivision process: A prism is divided into a finite number of sub-prisms at each iteration,
3. Bound estimation: For each prism generated throughout the algorithm, a lower bound for the
objective function t ? g(A) over the part of the feasible set contained in this prism is computed,
4. Construction of cutting planes: Throughout the algorithm, a sequence of polyhedral convex sets
? Each set Pj is generated by a cutting
P0 , P1 , ? ? ? is constructed such that P0 ? P1 ? ? ? ? ? D.
plane to cut off a part of Pj?1 , and
5. Deletion of non-optimal prisms: At each iteration, we try to delete prisms that contain no
feasible solution better than the one obtained so far.
3

?
Construct a simplex S0 ? D, its corresponding prism T0 and a polyhedral convex set P0 ? D.
Let ?0 be the best objective function value known in advance. Then, solve the BILP (5)
corresponding to ?0 and T0 , and let ?0 = ?(T0 , P0 , ?0 ) and (A?0 , t?0 ) be the point satisfying
?0 = t?0 ? g(A?0 ).
Set R0 ? T0 .
while Rk ?= ?
Select a prism Tk? ? Rk satisfying ?k = ?(Tk? ), (?
v k , t?k ) ? Tk? .
k ?
? then
if (?
v , tk ) ? D
Set Pk+1 = Pk .
else
Construct lk (x, t) according to (8), and set Pk+1 = {(x, t) ? Pk : lk (x, t) ? 0}.
Subdivide Tk? = T (Sk? ) into a finite number of subprisms Tk,j (j?Jk ) (cf. Section 4.2).
For each j ? Jk , solve the BILP (5) with respect to Tk,j , Pk+1 and ?k .
Delete all Tk,j (j?Jk ) satisfying (DR1) or (DR2). Let Mk denote the collection of
remaining prisms Tk,j (j ? Jk ), and for each T ? Mk set

1
2

3
4
5
6
7
8
9
10
11
12

?(T ) = max{?(Tk? ), ?(T, Pk+1 , ?k )}.
Let Fk be the set of new feasible points detected while solving BILP in Step 11, and set

13

?k+1 = min{?k , min{t ? g(A) : (A, t) ? Fk }}.
Delete all T ?Mk satisfying ?(T )??k+1 and let Rk be Rk?1 \ Tk ? Mk .
Set ?k+1 ? min{?(T ) : T ? Mk } and k ? k + 1.

14
15

Algorithm 1: Pseudo-code of the prismatic algorithm for the D.S programming problem.
4.1

Construction of the first prism

? can be constructed as follows.
The initial simplex S0 ? D (which yields the initial prism T0 ? D)
Now,
? let v and Av be a vertex of D and its corresponding subset in N , respectively, i.e., v =
i?Av ei . Then, the initial simplex S0 ? D can be constructed by
S0 = {x ? Rn : xi ? 1(i ? Av ), xi ? 0(i ? N \ Av ), aT x ? ?},
?
where a = i?N \Av ei ? i?Av ei and ? = |N \ Av |. The n + 1 vertices of S0 are v and the n
points where the hyperplane {x ? Rn : aT x = ?} intersects the edges of the cone {x ? Rn : xi ?
1(i ? Av ), xi ? 0(i ? N \ Av )}. Note this is just an option and any n-simplex S ? D is available.
?

4.2 Sub-division of a prism
Let Sk and Tk be the simplex and prism at k-th iteration in the algorithm, respectively. We denote Sk
as Sk = [v ik , . . . , v n+1
] := conv{v 1k , . . . , v n+1
} which is defined as the convex hull of its vertices
k
k
n+1
1
v k , . . . , v k . Then, any r ? Sk can be represented as
?n+1
?n+1
r = i=1 ?i v ik , i=1 ?i = 1, ?i ? 0 (i = 1, . . . , n + 1).
Suppose that r ?= v ik (i = 1, . . . , n + 1). For each i satisfying ?i > 0, let Ski be the subsimplex of
Sk defined by
i+1
n+1
Ski = [v 1k , . . . , v i?1
].
(3)
k , r, v k , . . . , v k
i
Then, the collection {Sk : ?i > 0} defines a partition of Sk , i.e., we have
?
j
i
i
?i >0 Sk = Sk , int Sk ? int Sk = ? for i ?= j [12].
In a natural way, the prisms T (Ski ) generated by the simplices Ski defined in Eq. (3) form a partition
of Tk . This subdivision process of prisms is exhaustive,
??i.e., for every nested (decreasing) sequence
of prisms {Tq } generated by this process, we have q=0 Tq = ? , where ? is a line perpendicular
to Rn (a vertical line) [11]. Although several subdivision process can be applied, we use a classical
bisection one, i.e., each simplex is divided into subsimplices by choosing in Eq. (3) as
r = (v ik1 + v ik2 )/2,
where ?v ik1 ? v ik2 ? = max{?v ik ? v jk ? : i, j ? {0, . . . , n}, i ?= j} (see Figure 1).
4

4.3

Lower bounds

Again, let Sk and Tk be the simplex and prism at k-th iteration in the algorithm, respectively. And,
let ? be an upper bound of t ? g(A), which is the smallest value of t ? g(A) attained at a feasible
?
point known so far in the algorithm. Moreover, let Pk be a polyhedral convex set which contains D
and be represented as
Pk = {(x, t) ? Rn ? R : Ak x + ak t ? bk },
(4)
m 1
where Ak is a real (m ? n)-matrix and ak , bk ? R . Now, a lower bound ?(Tk , Pk , ?) of t ? g(A)
? can be computed as follows.
over Tk ? D
First, let v ik (i = 1, . . . , n + 1) denote the vertices of Sk , and define I(Sk ) = {i ? {1, . . . , n + 1} :
v ik ? Bn } and
{
min{?, min{f?(v ik ) ? g?(v ik ) : i ? I(S)}}, if I(S) ?= ?,
?=
?,
if I(S) = ?.
For each i = 1, . . . , n + 1, consider the point (v ik , tik ) where the edge of Tk passing through v ik
intersects the level set {(x, t) : t ? g?(x) = ?}, i.e.,
tik = g?(v ik ) + ? (i = 1, . . . , n + 1).
Then, let us denote the uniquely defined hyperplane through the points (v ik , tik ) by H = {(x, t) ?
Rn ?R : pT x?t = ?}, where p ? Rn and ? ? R. Consider the upper and lower halfspace generated
by H, i.e., H+ = {(x, t) ? Rn ? R : pT x ? t ? ?} and H? = {(x, t) ? Rn ? R : pT x ? t ? ?}.
? ? H+ , then we see from the supermodularity of g(A) (the concavity of g?(x)) that
If Tk ? D
? ? min{t ? g(A) : (A, t) ? (A, t)(Tk ? H+ )}
min{t ? g(A) : (A, t) ? (A, t)(Tk ? D)}
? min{t ? g?(x) : (x, t) ? Tk ? H+ }
= tik ? g?(xik )(i = 1, . . . , n + 1) = ?.

Otherwise, we shift the hyperplane H (downward with respect to t) until it reaches a point z =
(x? , t? ) (? Tk ? Pk ? H? , x? ? Bn ) ((x? , t? ) is a point with the largest distance to H and the
? denote the resulting
corresponding pair (A, t) (since x? ? Bn ) is in (A, t)(Tk ? Pk ? H? )). Let H
? + the upper halfspace generated by H.
? Moreover, for each
supporting hyperplane, and denote by H
i = 1, . . . , n + 1, let z i = (v ik , t?ik ) be the point where the edge of T passing through v ik intersects
? Then, it follows (A, t)(Tk ? D)
? ? (A, t)(Tk ? Pk ) ? (A, t)(Tk ? H
? + ), and hence
H.
? > min{t ? g(A) : (A, t) ? (A, t)(Tk ? H
? + )}
min{t ? g(A) : (A, t) ? (A, t)(Tk ? D)}
= min{t?ik ? g?(v ik ) : i = 1, . . . , n + 1}.

Now, the above consideration leads to the following BILP in (?, x, t):
(?
)
?n+1
n+1
max
s.t. Ak x + ak t ? bk , x = i=1 ?i v ik , x ? Bn ,
i=1 ti ?i ? t
?,x,t
?n+1
i=1 ?i = 1, ?i ? 0 (i = 1, . . . , n + 1),

(5)

where A, a and b are given in Eq. (4).
? is empty.
Proposition 1. (a) If the system (5) has no solution, then intersection (A, t)(Tk ? D)
?n+1 ?
?
? ?
?
?
(b) Otherwise, let (? , x , t ) be an optimal solution of BILP (5) and c =
i=1 ti ?i ? t its
optimal value, respectively. Then, the following statements hold:
? ? (A, t)(H+ ).
(b1) If c? ? 0, then (A, t)(Tk ? D)
?n+1
?
(b2) If c > 0, then z = ( i=1 ?i v ik , t?k ), z i = (v ik , t?ik ) = (v ik , tik ? c? ) and t?ik ? g?(v ik ) =
? ? c? (i = 1, . . . , n + 1).
?n+1
Proof. First, we prove part (a). Since every point in Sk is uniquely representable as x = i=1 ?i v i ,
we see from Eq. (4) that the set (A, t)(Tk ? Pk ) coincide with the feasible set of problem (5).
? =?
Therefore, if the system (5) has no solution, then (A, t)(Tk ?Pk ) = ?, and hence (A, t)(Tk ? D)
T
?
(because D ? Pk ). Next, we move to part (b). Since the equation of H is p x ? t = ?, it follows
1

Note that Pk is updated at each iteration, which does not depend on Sk , as described in Section 4.4.

5

? and the point z amounts to solving the binary integer linear
that determining the hyperplane H
programming problem:
max pT x ? t

s.t. (x, t) ? Tk ? Pk , x ? Bn .

(6)

Here, we note that the objective of the above can be represented as
(?
)
?n+1
n+1
i
T i
pT x ? t = pT
i=1 ?i v k ? t =
i=1 ?i p v k ? t.
On the other hand, since (v ik , tik ) ? H, we have pT v ik ? tik = ? (i = 1, . . . , n + 1), and hence
?n+1
?n+1
pT x ? t = i=1 ?i (? + tik ) ? t = i=1 tik ?i ? t + ?.
Thus, the two BILPs (5) and (6) are equivalent. And, if ? ? denotes the optimal objective function
? is
value in Eq. (6), then ? ? = c? + ?. If ? ? ? ?, then it follows from the definition of H+ that H
?
obtained by a parallel shift of H in the direction H+ . Therefore, c ? 0 implies (A, t)(Tk ? Pk ) ?
? ? (A, t)(H+ ).
(A, t)(H+ ), and hence (A, t)(Tk ? D)
? = {(x, t) ? Rn ? R : pT x ? t = ? ? } and H = {(x, t) ? Rn ? R : pT x ? t = ?}
Since H
we see that for each intersection point (v ik , t?ik ) (and (v ik , tik )) of the edge of Tk passing through v ik
? (and H), we have pT v i ? t?i = ? ? and pT v i ? ti = ?, respectively. This implies that
with H
k
k
k
k
i
t?k = tik + ? ? ? ? = tik ? c? , and (using tik = g?(v ik ) + ?) that t?ik = g?(v ik ) + ? ? c? .
From the above, we see that, in the case (b1), ? constitutes a lower bound of (t?g(A)) wheres, in the
case (b2), such a lower bound is given by min{t?ik ? g?(v ik ) : i = 1, . . . , n + 1}. Thus, Proposition 1
provides the lower bound
{
+?,
if BILP (5) has no feasible point,
?,
if c? ? 0,
(7)
?k (Tk , Pk , ?) =
?
? ? c if c? > 0.
As stated in Section 4.5, Tk can be deleted from further consideration when ?k = ? or ?.
4.4

Outer approximation

? used in the preceding section is updated in each iteration, i.e.,
The polyhedral convex set Pk ? D
? The update from Pk to Pk+1
a sequence P0 , P1 , ? ? ? is constructed such that P0 ? P1 ? ? ? ? ? D.
(k = 0, 1, . . .) is done in a way which is standard for pure outer approximation methods [12]. That
is, a certain linear inequality lk (x, t) ? 0 is added to the constraint set defining Pk , i.e., we set
Pk+1 = Pk ? {(x, t) ? Rn ? R : lk (x, t) ? 0}.
The function lk (x, t) is constructed as follows. At iteration k, we have a lower bound ?k of t ?
g(A) as defined in Eq. (7), and a point (?
v k , t?k ) satisfying t?k ? g?(?
v k ) = ?k . We update the outer
? Then, we can set
?
approximation only in the case (?
v k , tk ) ?
/ D.
lk (x, t) = sTk [(x, t) ? z k ] + (f?(x?k ) ? t?k ),

(8)

where sk is a subgradient of f?(x) ? t at z k . The subgradient can be calculated as, for example,
stated in [9] (see also [7]).
? i.e.,
Proposition 2. The hyperplane {(x, t) ? Rn ? R : lk (x, t) = 0} strictly separates z k from D,
?
?
lk (z k ) > 0, and lk (x, t) ? 0 for (x, t) ? D.
? we have lk (z k ) = (f?(x? ) ? t? ). And, the latter inequality is
Proof. Since we assume that z k ?
/ D,
k
k
an immediate consequence of the definition of a subgradient.
4.5

Deletion rules

At each iteration of the algorithm, we try to delete certain subprisms that contain no optimal solution.
To this end, we adopt the following two deletion rules:
(DR1) Delete Tk if BILP (5) has no feasible solution.
6

[b

?



[b

?
b

b





Approx. (Supermodular-submodular)





















Approx. (Supermodular-sumodular)


Time [second]



b

Exact (Prismatic)

Approx. (Supermodular-sumodular)


Test Error

Training Error



Exact (Prismatic)

Exact (Prismatic)



b

?




b

?



?

?



?

?



b



?



?



?

Figure 2: Training errors, test errors and computational time versus ? for the prismatic algorithm
and the supermodular-sumodular procedure.
p
120
120
120
120

n
150
150
150
150

k
5
10
20
40

exact(PRISM)
1.8e-4 (192.6)
2.0e-4 (262.7)
7.3e-4 (339.2)
1.7e-3 (467.6)

SSP
1.9e-4 (0.93)
2.4e-4 (0.81)
7.8e-4 (1.43)
2.1e-3 (1.17)

greedy
1.8e-4 (0.45)
2.3e-4 (0.56)
8.3e-4 (0.59)
2.9e-3 (0.63)

lasso
1.9e-4 (0.78)
2.4e-4 (0.84)
7.7e-4 (0.91)
1.9e-3 (0.87)

Table 1: Normalized mean-square prediction errors of training and test data by the prismatic algorithm, the supermodular-submodular procedure, the greedy algorithm and the lasso.
(DR2) Delete Tk if the optimal value c? of BILP (5) satisfies c? ? 0.
The feasibility of these rules can be seen from Proposition 1 as well as the D.C. programing prob? = ?, i.e., the prism Tk
lem [11]. That is, (DR1) follows from Proposition 1 that in this case Tk ? D
is infeasible, and (DR2) from Proposition 1 and from the definition of ? that the current best feasible
solution cannot be improved in T .

5 Experimental Results
We first provide illustrations of the proposed algorithm and its solution on toy examples from feature
selection in Section 5.1, and then apply the algorithm to an application of discriminative structure
learning using the UCI repository data in Section 5.2. The experiments below were run on a 2.8
GHz 64-bit workstation using Matlab and IBM ILOG CPLEX ver. 12.1.
5.1 Application to feature selection
We compared the performance and solutions by the proposed prismatic algorithm (PRISM), the
supermodular-submodular procedure (SSP) [21], the greedy method and the LASSO. To this end,
we generated data as follows: Given p, n and k, the design matrix X ? Rn?p is a matrix of i.i.d.
Gaussian components. A feature set J of cardinality k is chosen at random and the weights on the
selected features are sampled from a standard multivariate Gaussian distribution. The weights on
other features are 0. We then take y = Xw + n?1/2 ?Xw?2 ?, where w is the weights on features
and ? is a standard Gaussian vector. In the experiment, we used the trace norm of the submatrix
1
corresponding to J, XJ , i.e., tr(XJT XJ )1/2 . Thus, our problem is minw?Rp 2n
?y ? Xw?22 + ? ?
T
1/2
T
tr(XJ XJ ) , where J is the support of w. Or equivalently, minA?V g(A) + ? ? tr(XA
XA )1/2 ,
2
where g(A) := minwA ?R|A| ?y ? XA wA ? . Since the first term is a supermodular function [4] and
the second is a submodular function, this problem is the D.S. programming problem.
First, the graphs in Figure 2 show the training errors, test errors and computational time versus ? for
PRISM and SSP (for p = 120, n = 150 and k = 10). The values in the graphs are averaged over 20
datasets. For the test errors, we generated another 100 data from the same model and applied the estimated model to the data. And, for all methods, we tried several possible regularization parameters.
From the graphs, we can see the following: First, exact solutions (by PRISM) always outperform
approximate ones (by SSP). This would show the significance of optimizing the submodular-norm.
That is, we could obtain the better solutions (in the sense of prediction error) by optimizing the
objective with the submodular norm more exactly. And, our algorithm took longer especially when
7

Data
Chess
German
Census-income
Hepatitis

Attr.
36
20
40
19

Class
2
2
2
2

exact (PRISM)
96.6 (?0.69)
70.0 (?0.43)
73.2 (?0.64)
86.9 (?1.89)

approx. (SSP)
94.4 (?0.71)
69.9 (?0.43)
71.2 (?0.74)
84.3 (?2.31)

generative
92.3 (?0.79)
69.1 (?0.49)
70.3 (?0.74)
84.2 (?2.11)

Table 2: Empirical accuracy of the classifiers in [%] with standard deviation by the TANs discriminatively learned with PRISM or SSP and generatively learned with a submodular minimization
solver. The numbers in parentheses are computational time in seconds.
? smaller. This would be because smaller ? basically gives a larger size subset (solution). Also,
Table 1 shows normalized-mean prediction errors by the prismatic algorithm, the supermodularsubmodular procedure, the greedy method and the lasso for several k. The values are averaged over
10 datasets. This result also seems to show that optimizing the objective with the submodular norm
exactly is significant in the meaning of prediction errors.
5.2

Application to discriminative structure learning

Our second application is discriminative structure learning using the UCI machine learning repository.2 Here, we used CHESS, GERMAN, CENSUS-INCOME (KDD) and HEPATITIS, which have
two classes. The Bayesian network topology used was the tree augmented naive Bayes (TAN) [22].
We estimated TANs from data both in generative and discriminative manners. To this end, we used
the procedure described in [20] with a submodular minimization solver (for the generative case), and
the one [21] combined with our prismatic algorithm (PRISM) or the supermodular-submodular procedure (SSP) (for the discriminative case). Once the structures have been estimated, the parameters
were learned based on the maximum likelihood method.
Table 2 shows the empirical accuracy of the classifier in [%] with standard deviation for these
datasets. We used the train/test scheme described in [6, 22]. Also, we removed instances with
missing values. The results seem to show that optimizing the EAR measure more exactly could
improve the performance of classification (which would mean that the EAR is significant as the
measure of discriminative structure learning in the sense of classification).

6

Conclusions

In this paper, we proposed a prismatic algorithm for the D.S. programming problem (1), which is the
first exact algorithm for this problem and is a branch-and-bound method responding to the structure
of this problem. We developed the algorithm based on the analogy with the D.C. programming
problem through the continuous relaxation of solution spaces and objective functions with the help
of the Lov?asz extension. We applied the proposed algorithm to several situations of feature selection
and discriminative structure learning using artificial and real-world datasets.
The D.S. programming problem addressed in this paper covers a broad range of applications in
machine learning. In future works, we will develop a series of the presented framework specialized
to the specific structure of each problem. Also, it would be interesting to investigate the extension
of our method to enumerate solutions, which could make the framework more useful in practice.
Acknowledgments
This research was supported in part by JST PRESTO PROGRAM (Synthesis of Knowledge for
Information Oriented Society), JST ERATO PROGRAM (Minato Discrete Structure Manipulation
System Project) and KAKENHI (22700147). Also, we are very grateful to the reviewers for helpful
comments.
2

http://archive.ics.uci.edu/ml/index.html

8

References
[1] F. Bach. Structured sparsity-inducing norms through submodular functions. In Advances in Neural Information Processing Systems 23, pages 118?126, 2010.
[2] J. A. Bilmes. Dynamic Bayesian multinets. In Proc. of the 16th Conf. on Uncertainty in Artificial Intelligence (UAI?00), pages 38?45, 2000.
[3] T. S. Caetano, J. J. McAuley, L. Cheng, Q. V. Le, and A. J. Smola. Learning graph matching. IEEE Trans.
on Pattern Analysis and Machine Intelligence, 31(6):1048?1058, 2009.
[4] A. Das and D. Kempe. Algorithms for subset selection in linear regression. In Proc. of the 40th annual
ACM symp. on Theory of computing (STOC?08), pages 45?54, 2008.
[5] J. Edmonds. Submodular functions, matroids, and certain polyhedra. In R. Guy, H. Hanani, N. Sauer, and
J. Sch?onheim, editors, Combinatorial structures and their applications, pages 69?87, 1970.
[6] N. Friedman, D. Geiger, and M. Goldszmidt. Bayesian network classifier. 29:131?163, 1997.
[7] S. Fujishige. Submodular Functions and Optimization. Elsevier, 2 edition, 2005.
[8] S. Fujishige, T. Hayashi, and S. Isotani. The minimum-norm-point algorithm applied submodular function
minimization and linear programming. Technical report, Research Institute for Mathematical Sciences,
Kyoto University, 2006.
[9] E. Hazan and S. Kale. Beyond convexity: online submodular minimization. In Advances in Neural
Information Processing Systems 22, pages 700?708, 2009.
[10] S. Hoi, R. Jin, J. Zhu, and M. Lyu. Batch mode active learning and its application to medical image
classification. In Proc. of the 23rd Int?l Conf. on Machine learning (ICML?06), pages 417?424, 2006.
[11] R. Horst, T. Q. Phong, Ng. V. Thoai, and J. de Vries. On solving a D.C. programming problem by a
sequence of linear programs. Journal of Global Optimization, 1:183?203, 1991.
[12] R. Horst and H. Tuy. Global Optimization (Deterministic Approaches). Springer, 3 edition, 1996.
[13] T. Ibaraki. Enumerative approaches to combinatorial optimization. In J.C. Baltzer and A.G. Basel, editors,
Annals of Operations Research, volume 10 and 11. 1987.
[14] Y. Kawahara, K. Nagano, K. Tsuda, and J. A. Bilmes. Submodularity cuts and applications. In Advances
in Neural Information Processing Systems 22, pages 916?924. MIT Press, 2009.
[15] A. Krause and V. Cevher. Submodular dictionary selection for sparse representation. In Proc. of the 27th
Int?l Conf. on Machine learning (ICML?10), pages 567?574. Omnipress, 2010.
[16] A. Krause, H. B. McMahan, C. Guestrin, and A. Gupta. Robust submodular observation selection. Journal
of Machine Learning Research, 9:2761?2801, 2008.
[17] L. Lov?asz. Submodular functions and convexity. In A. Bachem, M. Gr?otschel, and B. Korte, editors,
Mathematical Programming ? The State of the Art, pages 235?257. 1983.
[18] K. Murota. Discrete Convex Analysis. Monographs on Discrete Math and Applications. SIAM, 2003.
[19] K. Nagano, Y. Kawahara, and S. Iwata. Minimum average cost clustering. In Advances in Neural Information Processing Systems 23, pages 1759?1767, 2010.
[20] M. Narasimhan and J. A. Bilmes. PAC-learning bounded tree-width graphical models. In Proc. of the
20th Ann. Conf. on Uncertainty in Artificial Intelligence (UAI?04), pages 410?417, 2004.
[21] M. Narasimhan and J. A. Bilmes. A submodular-supermodular procedure with applications to discriminative structure learning. In Proc. of the 21st Ann. Conf. on Uncertainty in Artificial Intelligence (UAI?05),
pages 404?412, 2005.
[22] F. Pernkopf and J. A. Bilmes. Discriminative versus generative parameter and structure learning of
bayesian network classifiers. In Proc. of the 22nd Int?l Conf. on Machine Learning (ICML?05), pages
657?664, 2005.
[23] M. Queyranne. Minimizing symmetric submodular functions. Math. Prog., 82(1):3?12, 1998.
[24] C. Rother, T. Minka, A. Blake, and V. Kolmogorov. Cosegmentation of image pairs by histogram
matching-incorporating a global constraint into mrfs. In Proc. of the 2006 IEEE Comp. Soc. Conf. on
Computer Vision and Pattern Recognition (CVPR?06), pages 993?1000, 2006.
[25] A. Shekhovtsov. Supermodular decomposition of structural labeling problem. Control Systems and Computers, 20(1):39?48, 2006.
[26] A. Shekhovtsov, V. Kolmogorov, P. Kohli, V. Hlav c, C. Rother, and P. Torr. Lp-relaxation of binarized
energy minimization. Technical Report CTU-CMP-2007-27, Czech Technical University, 2007.
[27] M. Thoma, H. Cheng, A. Gretton, H. Han, H. P. Kriegel, A. J. Smola, S. Y. Le Song Philip, X. Yan, and
K. Borgwardt. Near-optimal supervised feature selection among frequent subgraphs. In Proc. of the 2009
SIAM Conf. on Data Mining (SDM?09), pages 1076?1087, 2008.

9

"
1999,Modeling High-Dimensional Discrete Data with Multi-Layer Neural Networks,,1679-modeling-high-dimensional-discrete-data-with-multi-layer-neural-networks.pdf,Abstract Missing,"Modeling High-Dimensional Discrete Data with
Multi-Layer Neural Networks
Samy Bengio *
IDIAP
CP 592, rue du Simplon 4,
1920 Martigny, Switzerland
bengio@idiap.ch

Yoshua Bengio
Dept.IRO
Universite de Montreal
Montreal, Qc, Canada, H3C 317
bengioy@iro.umontreal.ca

Abstract
The curse of dimensionality is severe when modeling high-dimensional
discrete data: the number of possible combinations of the variables explodes exponentially. In this paper we propose a new architecture for
modeling high-dimensional data that requires resources (parameters and
computations) that grow only at most as the square of the number of variables, using a multi-layer neural network to represent the joint distribution of the variables as the product of conditional distributions. The neural network can be interpreted as a graphical model without hidden random variables, but in which the conditional distributions are tied through
the hidden units. The connectivity of the neural network can be pruned by
using dependency tests between the variables. Experiments on modeling
the distribution of several discrete data sets show statistically significant
improvements over other methods such as naive Bayes and comparable
Bayesian networks, and show that significant improvements can be obtained by pruning the network.

1 Introduction
The curse of dimensionality hits particularly hard on models of high-dimensional discrete
data because there are many more possible combinations of the values of the variables than
can possibly be observed in any data set, even the large data sets now common in datamining applications. In this paper we are dealing in particular with multivariate discrete
data, where one tries to build a model of the distribution of the data. This can be used for
example to detect anomalous cases in data-mining applications, or it can be used to model
the class-conditional distribution of some observed variables in order to build a classifier.
A simple multinomial maximum likelihood model would give zero probability to all of
the combinations not encountered in the training set, i.e., it would most likely give zero
probability to most out-of-sample test cases. Smoothing the model by assigning the same
non-zero probability for all the unobserved cases would not be satisfactory either because
it would not provide much generalization from the training set. This could be obtained by
using a multivariate multinomial model whose parameters B are estimated by the maximum
a-posteriori (MAP) principle, i.e., those that have the greatest probability, given the training
data D, and using a diffuse prior PCB) (e.g. Dirichlet) on the parameters.
A graphical model or Bayesian network [6, 5) represents the joint distribution of random
variables Zl ... Zn with
n

P(ZI ... Zn) =

II P(ZiIParentsi)
i=l

?Part of this work was done while S.B. was at CIRANO, Montreal, Qc. Canada.

Modeling High-Dimensional Discrete Data with Neural Networks

401

where Parentsi is the set of random variables which are called the parents of variable i
in the graphical model because they directly condition Zi, and an arrow is drawn, in the
graphical model, to Zi, from each of its parents. A fully connected ""left-to-right"" graphical
model is illustrated in Figure 1 (left), which corresponds to the model
n

P(ZI . .. Zn) =

II P(ZiIZl ... Zi-r) .

(1)

i= l

Figure 1: Left: a fully connected ""left-to-right"" graphical model.
Right: the architecture of a neural network that simulates a ful1y connected ""left-to-right""
graphical model. The observed values Zi = Zi are encoded in the corresponding input
unit group. hi is a group of hidden units. gi is a group of output units, which depend
on Zl ... Zi - l , representing the parameters of a distribution over Zi. These conditional
probabilities P(ZiIZl . . . Zi-r) are multiplied to obtain the joint distribution.
Note that this representation depends on the ordering of the variables (in that all previous
variables in this order are taken as parents). We call each combination of the values of
Parentsi a context. In the ""exact"" model (with the full table of all possible contexts) all the
orders are equivalent, but if approximations are used, different predictions could be made
by different models assuming different orders.
In graphical models, the curse of dimensionality shows up in the representation of conditional distributions P(Zi IParentsi) where Zi has many parents. If Zj E Parentsi can take
nj values, there are TI j nj different contexts which can occur in which one would like to
estimate the distribution of Zi. This serious problem has been addressed in the past by two
types of approaches, which are sometimes combined:
1. Not modeling all the dependencies between all the variables: this is the approach mainly
taken with most graphical models or Bayes networks [6, 5] . The set of independencies
can be assumed using a-priori or human expert knowledge or can be learned from data.
See also [2] in which the set Parentsi is restricted to at most one element, which is
chosen to maximize the correlation with Zi.
2 . Approximating the mathematicalform of the joint distribution with a form that takes only
into account dependencies of lower order, or only takes into account some of the possible dependencies, e.g., with the Rademacher-Walsh expansion or multi-binomial [1,3],
which is a low-order polynomial approximation of a full joint binomial distribution (and
is used in the experiments reported in this paper).

The approach we are putting forward in this paper is mostly of the second category, although we are using simple non-parametric statistics of the dependency between pairs of
variables to further reduce the number of required parameters.
In the multi-binomial model [3], the joint distribution of a set of binary variables is approximated by a polynomial. Whereas the ""exact"" representation of P( Zl = Z l , ... Zn = zn)
as a function of Z l . . . Zn is a polynomial of degree n, it can be approximated with a lower

Y. Bengio and S. Bengio

402

degree polynomial, and this approximation can be easily computed using the RademacherWalsh expansion [1] (or other similar expansions, such as the Bahadur-Lazarsfeld expansion [1]) . Therefore, instead of having 2 n parameters, the approximated model for
P(Zl , . . . Zn) only requires O(nk) parameters. Typically, order k = 2 is used. The model
proposed here also requires O(n 2 ) parameters, but it allows to model dependencies between tuples of variables, with more than 2 variables at a time.
In previous related work by Frey [4], a fully-connected graphical model is used (see Figure 1, left) but each of the conditional distributions is represented by a logistic, which take
into account only first-order dependency between the variables:
1
P(Zi = llZl ... Zi-d =
(
L j<i Wj Z j )'
1 + exp -Wo In this paper, we basically extend Frey's idea to using a neural network with a hidden
layer, with a particular architecture, allowing multinomial or continuous variables, and we
propose to prune down the network weights . Frey has named his model a Logistic Autoregressive Bayesian Network or LARC . He argues that the prior variances on the logistic
weights (which correspond to inverse weight decays) should be chosen inversely proportional to the number of conditioning variables (i.e. the number of inputs to the particular
output neuron). The model was tested on a task of learning to classify digits from 8x8 binary pixel images. Models with different orderings of the variables were compared and did
not yield significant differences in performance. When averaging the predictive probabilities from 10 different models obtained by considering 10 different random orderings, Frey
obtained small improvements in likelihood but not in classification. The model performed
better or equivalently to other models tested: CART, naive Bayes, K-nearest neighbors, and
various Bayesian models with hidden variables (Helmholtz machines). These results are
impressive, taking into account the simplicity of the LARC model.

2

Proposed Architecture

The proposed architecture is a ""neural network"" implementation of a graphical model
where all the variables are observed in the training set, with the hidden units playing a significant role to share parameters across different conditional distributions. Figure 1 (right)
illustrates the model in the simpler case of a fully connected (Ieft-to-right) graphical model
(Figure 1, left) . The neural network represents the parametrized function

jo(zt, . .. , zn) = log(?O(Zl = Zl,? ? ., Zn = zn))
(2)
approximating the joint distribution of the variables, with parameters 0 being the weights of
the neural network. The architecture has three layers, with each layer organized in groups
associated to each of the variables. The above log-probability is computed as the sum of
conditional log-probabilities
n

jO(Zl , . .. , zn) =

L 109(P(Zi = zilgi(zl, .. . ,

Zi-l)))

i=l

where gi( Zt, . .. , zi-d is the vector-valued output of the i-th group of output units, and
it gives the value of the parameters of the distribution of Zi when Zl = Zl , Z2 =
Z2, .. . , Zi-l = Zi-l' For example, in the ordinary discrete case, gi may be the vector
of probabilities associated with each of the possible values of the multinomial random
variable Zi . In this case, we have

P(Zi = i'lgi) = gi ,i'
In this example, a softmax output for the i-th group may be used to force these parameters
to be positive and sum to 1, i.e.,
gi ,i' =

Lil e

g'
i , i'

Modeling High-Dimensional Discrete Data with Neural Networks

403

where g~ i' are linear combinations of the hidden units outputs, with i' ranging over the
number of elements of the parameter vector associated with the distribution of Zi (for a
fixed value of Zl ... Zi-l). To guarantee that the functions gi(Zl, ... , Zi-l) only depend
on Zl ... Zi-l and not on any of Zi ... Zn, the connectivity struture of the hidden units must
be constrained as follows:

g~,i'

=

bi,i'

+ 2:
j~i

mj

2: Wi,i'

,j,j' hj,j'

j'=1

where the b's are biases and the w's are weights of the output layer, and the hj,j' is the
output of the j'-th unit (out of mj such units) in the j-th group of hidden layer nodes. It
may be computed as follows:
hj ,j'

= tanh(cj,j'

+ 2:

nk

2: Vj ,j' ,k ,k' Zk ,k')

k<j k'=l

where the c's are biases and the v's are the weights of the hidden layer, and Zk,k' is k'-th
element of the vectorial input representation of the value Zk = Zk. For example, in the
binary case (Zi = 0 or 1) we have used only one input node, i.e.,
Zi binomial -t Zi,O = Zi
and in the multinomial case we use the one-hot encoding,
Zi

E {O, 1, ... ni - I} -t Zi ,i' = 8Zi ,i'

where 8i ,i' = 1 if i = i' and 0 otherwise. The input layer has n - 1 groups because
the value Zn = Zn is not used as an input. The hidden layer also has n - 1 groups
corresponding to the variables j = 2 to n (since P(Z.) is represented unconditionally in
the first output group, its corresponding group does not need any hidden units or inputs, but
just has biases).
2.1

Discussion

The number of free parameters of the model is O(n 2 H) where H = maXi mj is the maximum number of hidden units per hidden group (i.e., associated with one of the variables).
This is basically quadratic in the number of variables, like the multi-binomial approximation that uses a polynomial expansion of the joint distribution. However, as H is increased,
representation theorems for neural networks suggest that we should be able to approximate
with arbitrary precision the true joint distribution. Of course the true limiting factor is the
amount of data, and H should be tuned according to the amount of data. In our experiments
we have used cross-validation to choose a value of mj = H for all the hidden groups. In
this sense, this neural network representation of P(ZI ... Zn) is to the polynomial expansions (such as the multi-binomial) what ordinary multilayer neural networks for function
approximation are to polynomial function approximators. It allows to capture high-order
dependencies, but not all of them. It is the number of hidden units that controls ""how
many"" such dependencies will be captured, and it is the data that ""chooses"" which of the
actual dependencies are most useful in maximizing the likelihood.
Unlike Bayesian networks with hidden random variables, learning with the proposed architecture is very simple, even when there are no conditional independencies. To optimize the
parameters we have simply used gradient-based optimization methods, either using conjugate or stochastic (on-line) gradient, to maximize the total log-likelihood which is the
sum of values of f (eq. 2) for the training examples. A prior on the parameters can be
incorporated in the cost function and the MAP estimator can be obtained as easily, by maximizing the total log-likelihood plus the log-prior on the parameters. In our experiments
we have used a ""weight decay"" penalty inspired by the analysis of Frey [4], with a penalty
proportional to the number of weights incoming into a neuron.

Y. Bengio and S. Bengio

404

However, it is not so clear how the distribution could be generally marginalized, except
by summing over possibly many combinations of the values of variables to be integrated.
Another related question is whether one could deal with missing values: if the total number
of values that the missing variables can take is reasonably small, then one can sum over
these values in order to obtain a marginal probability and maximize this probability. If
some variables have more systematically missing values, they can be put at the end of the
variable ordering, and in this case it is very easy to compute the marginal distribution (by
taking only the product of the output probabilities up to the missing variables). Similarly,
one can easily compute the predictive distribution of the last variable given the first n - 1
variables.
The framework can be easily extended to hybrid models involving both continuous and
discrete variables. In the case of continuous variables, one has to choose a parametric form
for the distribution of the continuous variable when all its parents (i.e., the conditioning
context) are fixed. For example one could use a normal, log-normal, or mixture of normals.
Instead of having softmax outputs, the i-th output group would compute the parameters
of this continuous distribution (e.g., mean and log-variance). Another type of extension
allows to build a conditional distribution, e.g., to model P(ZI ... ZnlXl ... Xm). One
just adds extra input units to represent the values of the conditioning variables Xl ... X m .
Finally, an architectural extension that we have implemented is to allow direct input-tooutput connections (still following the rules of ordering which allow gi to depend only on
Zl ... Zi-l). Therefore in the case where the number of hidden units is 0 (H = 0) we obtain
the LARC model proposed by Frey [4].

2.2

Choice of topology

Another type of extension of this model which we have found very useful in our experiments is to allow the user to choose a topology that is not fully connected (Ieft-to-right). In
our experiments we have used non-parametric tests to heuristically eliminate some of the
connections in the network, but one could also use expert or prior knowledge, just as with
regular graphical models, in order to cut down on the number of free parameters.
In our experiments we have used for a pairwise test of statistical dependency the
Kolmogorov-Smirnov statistic (which works both for continuous and discrete variables) .
The statistic for variables X and Y is

Jl sup IP(X :::; Xi, Y :::; Yi) - P(X :::; Xi)P(Y :::; Yi) I
i
where l is the number of examples and P is the empirical distribution (obtained by counting
s =

over the training data). We have ranked the pairs according to their value of the statistic s,
and we have chosen those pairs for which the value of statistic is above a threshold value
s*, which was chosen by cross-validation. When the pairs {(Zi' Zj)} are chosen to be part
of the model, and assuming without loss of generality that i < j for those pairs, then the
only connections that are kept in the network (in addition to those from the k-th hidden
group to the k-th output group) are those from hidden group i to output group j, and from
input group i to hidden group j, for every such (Zi' Zj) pair.

3 Experiments
In the experiments we have compared the following models:
? Naive Bayes: the likelihood is obtained as a product of multinomials (one per variable).
Each multinomial is smoothed with a Dirichlet prior.
? Multi-Binomial (using Rademacher-Walsh expansion of order 2) [3]. Since this only
handles the case of binary data, it was only applied to the DNA data set.
? A simple graphical model with the same pairs of variables and variable ordering as selected for the neural network, but in which each of the conditional distribution is modeled

Modeling High-Dimensional Discrete Data with Neural Networks

405

by a separate multinomial for each of the conditioning context. This works only if the
number of conditioning variables is small so in the Mushroom, Audiology, and Soybean
experiments we had to reduce the number of conditioning variables (following the order
given by the above tests). The multinomials are also smoothed with a Dirichlet prior.
? Neural network: the architecture described above, with or without hidden units (i.e.,
LARC), with or without pruning.
5-fold cross-validation was used to select the number of hidden units per hidden group
and the weight decay for the neural network and LARC. Cross-validation was also used
to choose the amount of pruning in the neural network and LARC, and the amount of
smoothing in the Dirichlet priors for the muItinomials of the naive Bayes model and the
simple graphical model.

3.1 Results
All four data sets were obtained on the web from the VCI Machine Learning and STATLOG
databases. Most of these are meant to be for classification tasks but we have instead ignored
the classification and used the data to learn a probabilistic model of all the input features.
? DNA (from STATLOG): there are 180 binary features. 2000 cases were used for training
and cross-validation, and 1186 for testing.
? Mushroom (from VCI): there are 22 discrete features (taking each between 2 and 12
values). 4062 cases were used for training and cross-validation, and 4062 for testing.
? Audiology (from VCI): there are 69 discrete features (taking each between 2 and 7 values). 113 cases are used for training and 113 for testing (the original train-test partition
was 200 + 26 and we concatenated and re-split the data to obtain more significant test
figures).
? Soybean (from VCI): there are 35 discrete features (taking each between 2 and 8 values).
307 cases are used for training and 376 for testing.
Table 1 clearly shows that the proposed model yields promising results since the pruned
neural network was superior to all the other models in all 4 cases, and the pairwise differences with the other models are statistically significant in all 4 cases (except Audiology,
where the difference with the network without hidden units, LARC, is not significant).

4

Conclusion

In this paper we have proposed a new application of multi-layer neural networks to the modelization of high-dimensional distributions, in particular for discrete data (but the model
could also be applied to continuous or mixed discrete / continuous data). Like the polynomial expansions [3] that have been previously proposed for handling such high-dimensional
distributions, the model approximates the joint distribution with a reasonable (O( n 2 )) number of free parameters but unlike these it allows to capture high-order dependencies even
when the number of parameters is small. The model can also be seen as an extension of
the previously proposed auto-regressive logistic Bayesian network [4], using hidden units
to capture some high-order dependencies.
Experimental results on four data sets with many discrete variables are very encouraging.
The comparisons were made with a naive Bayes model, with a multi-binomial expansion,
with the LARC model and with a simple graphical model, showing that a neural network
did significantly better in terms of out-of-sample log-likelihood in all cases.
The approach to pruning the neural network used in the experiments, based on pairwise
statistical dependency tests, is highly heuristic and better results might be obtained using
approaches that take into account the higher order dependencies when selecting the conditioning variables. Methods based on pruning the fully connected network (e.g., with a
""weight elimination"" penalty) should also be tried. Also, we have not tried to optimize

Y Bengio and S. Bengio

406

naive Bayes
multi-Binomial order 2
ordinary graph. model
LARC
prunedLARC
full-conn. neural net.
pruned neural network

naive Bayes
multi-Binomial order 2
ordinary graph. model
LARC
prunedLARC
full-conn. neural net.
pruned neural network

DNA
mean (stdev) p-value
100.4 (.18)
<le-9
117.8(.01)
<le-9
108.1 (.06)
<le-9
83.2 (.24)
7e-5
91.2(.15)
<le-9
120.0 (.02)
<le-9
82.9 (.21)
Audiology
mean (stdev) p-value
36.40 (2 .9)
<le-9
16.56 (.48)
17.69 (.65)
16.69 (.41)
17 .39 (.58)
16.37 (.45)

6.8e-4
<le-9
0.20
<le-9

Mushroom
mean (stdev) p-value
47 .00 (.29)
<le-9
44.68 (.26)
<le-9
42.51 (.16)
<le-9
43.87 (.13)
< le-9
33.58 (.01)
<le-9
31.25 (.04)
Soybean
mean (stdev) p-value
34.74 (1.0)
<le-9
43 .65 (.07)
16.95 (.35)
19.06 (.43)
21.65 (.43)
16.55 (.27)

<le-9
5.5e-4
<le-9
<le-9

Table 1: Average out-of-sample negative log-likelihood obtained with the various models
on four data sets (standard deviations of the average in parenthesis and p-value to test
the null hypotheses that a model has same true generalization error as the pruned neural
network). The pruned neural network was better than all the other models in in all cases,
and the pair-wise difference is always statistically significant (except with respect to the
pruned LARC on Audiology).
the order of the variables, or combine different networks obtained with different orders,
like [4] .

References
[1] RR Bahadur. A representation of the joint distribution of responses to n dichotomous
items. In ed. H. Solomon, editor, Studies in Item Analysis and Predictdion, pages
158-168. Stanford University Press, California, 1961.
[2]

c.K.

Chow. A recognition method using neighbor dependence. IRE Trans. Elec.
Comp., EC-l1 :683-690, October 1962.

[3] RO. Duda and P.E. Hart. Pattern Classification and Scene Analysis. Wiley, New York,
1973.
[4] B. Frey. Graphical models for machine learning and digital communication. MIT
Press, 1998.
[5] Steffen L. Lauritzen. The EM algorithm for graphical association models with missing
data. Computational Statistics and Data Analysis, 19:191-201,1995.
[6] Judea Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. Morgan Kaufmann, 1988.

"
